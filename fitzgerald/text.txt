fendpaper.qxd

11/4/10

12:05 PM

Page 2

Systems of Units. Some Important Conversion Factors
The most important systems of units are shown in the table below. The mks system is also known as
the International System of Units (abbreviated SI), and the abbreviations sec (instead of s),
gm (instead of g), and nt (instead of N) are also used.

System of units

Length

Mass

Time

Force

cgs system

centimeter (cm)

gram (g)

second (s)

dyne

mks system

meter (m)

kilogram (kg)

second (s)

newton (nt)

Engineering system

foot (ft)

slug

second (s)

pound (lb)

1 inch (in.) ϭ 2.540000 cm

1 foot (ft) ϭ 12 in. ϭ 30.480000 cm

1 yard (yd) ϭ 3 ft ϭ 91.440000 cm

1 statute mile (mi) ϭ 5280 ft ϭ 1.609344 km

1 nautical mile ϭ 6080 ft ϭ 1.853184 km
1 acre ϭ 4840 yd2 ϭ 4046.8564 m2

1 mi2 ϭ 640 acres ϭ 2.5899881 km2

1 fluid ounce ϭ 1/128 U.S. gallon ϭ 231/128 in.3 ϭ 29.573730 cm3
1 U.S. gallon ϭ 4 quarts (liq) ϭ 8 pints (liq) ϭ 128 fl oz ϭ 3785.4118 cm3
1 British Imperial and Canadian gallon ϭ 1.200949 U.S. gallons ϭ 4546.087 cm3
1 slug ϭ 14.59390 kg
1 pound (lb) ϭ 4.448444 nt

1 newton (nt) ϭ 105 dynes

1 British thermal unit (Btu) ϭ 1054.35 joules

1 joule ϭ 107 ergs

1 calorie (cal) ϭ 4.1840 joules
1 kilowatt-hour (kWh) ϭ 3414.4 Btu ϭ 3.6 • 106 joules
1 horsepower (hp) ϭ 2542.48 Btu/h ϭ 178.298 cal/sec ϭ 0.74570 kW
1 kilowatt (kW) ϭ 1000 watts ϭ 3414.43 Btu/h ϭ 238.662 cal/s
°F ϭ °C • 1.8 ϩ 32

1° ϭ 60Ј ϭ 3600Љ ϭ 0.017453293 radian

For further details see, for example, D. Halliday, R. Resnick, and J. Walker, Fundamentals of Physics. 9th ed., Hoboken,
N. J: Wiley, 2011. See also AN American National Standard, ASTM/IEEE Standard Metric Practice, Institute of Electrical and
Electronics Engineers, Inc. (IEEE), 445 Hoes Lane, Piscataway, N. J. 08854, website at www.ieee.org.

fendpaper.qxd

11/4/10

12:05 PM

Page 3

Differentiation
(cu)Ј ϭ cuЈ

(c constant)

Integration

͵ uvЈ dx ϭ uv Ϫ ͵ uЈv dx (by parts)
͵ x dx ϭ nx ϩ 1 ϩ c (n Ϫ1)
nϩ1

(u ϩ v)Ј ϭ uЈ ϩ vЈ
(uv)Ј ϭ uЈv ϩ uvЈ
uЈv Ϫ uvЈ
u Ј
(ᎏ) ϭ ᎏᎏ
v2
v
du
du dy
ᎏϭᎏ•ᎏ
dx
dy dx

(Chain rule)

(x n)Ј ϭ nxn؊1
(e x)Ј ϭ e x
(e ax)Ј ϭ ae ax
(a x)Ј ϭ a x ln a
(sin x)Ј ϭ cos x
(cos x)Ј ϭ Ϫsin x
(tan x)Ј ϭ sec2 x
(cot x)Ј ϭ Ϫcsc2 x
(sinh x)Ј ϭ cosh x
(cosh x)Ј ϭ sinh x
1
(ln x)Ј ϭ ᎏ
x
loga e
(loga x)Ј ϭ ᎏ
x

n

͵ 1x dx ϭ ln ͉x͉ ϩ c
͵ e dx ϭ 1a e ϩ c
͵ sin x dx ϭ Ϫcos x ϩ c
͵ cos x dx ϭ sin x ϩ c
͵ tan x dx ϭ Ϫln ͉cos x͉ ϩ c
͵ cot x dx ϭ ln ͉sin x͉ ϩ c
͵ sec x dx ϭ ln ͉sec x ϩ tan x͉ ϩ c
͵ csc x dx ϭ ln ͉csc x Ϫ cot x͉ ϩ c
1
dx
x
͵ᎏ
ϭ ᎏ arctan ᎏ ϩ c
a
x ϩa
a
ax

ax

2

2

x
dx
͵ ᎏᎏ
ϭ arcsin ᎏ ϩ c
a
͙aෆෆϪ
ෆෆx ෆ
2

2

x
dx
͵ ᎏᎏ
ϭ arcsinh ᎏ ϩ c
a
͙xෆෆ
ϩෆ
aෆ
2

2

x
dx
͵ ᎏᎏ
ϭ arccosh ᎏ ϩ c
a
͙xෆෆ
Ϫෆ
aෆ
2

2

͵ sin x dx ϭ _ x Ϫ _ sin 2x ϩ c
͵ cos x dx ϭ _ x ϩ _ sin 2x ϩ c
͵ tan x dx ϭ tan x Ϫ x ϩ c
͵ cot x dx ϭ Ϫcot x Ϫ x ϩ c
͵ ln x dx ϭ x ln x Ϫ x ϩ c
͵ e sin bx dx
2

1
2

1
4

2

1
2

1
4

2

2

1
(arcsin x)Ј ϭ ᎏᎏ
͙1ෆෆ
Ϫෆx 2ෆ
1
(arccos x)Ј ϭ Ϫ ᎏᎏ
͙1ෆෆ
Ϫෆx 2ෆ
1
(arctan x)Ј ϭ ᎏ
1 ϩ x2
1
(arccot x)Ј ϭ Ϫ ᎏ
1 ϩ x2

ax

ϭ

͵e

ax

eax

a2 ϩ b 2

(a sin bx Ϫ b cos bx) ϩ c

cos bx dx
eax
ϭ 2
(a cos bx ϩ b sin bx) ϩ c
a ϩ b2

ffirs.qxd

11/4/10

10:50 AM

Page iv

ffirs.qxd

11/4/10

10:50 AM

Page i

ffirs.qxd

11/4/10

10:50 AM

Page ii

ffirs.qxd

11/8/10

3:50 PM

Page iii

ADVANCED
ENGINEERING
MATHEMATICS

ffirs.qxd

11/4/10

10:50 AM

Page iv

ffirs.qxd

11/8/10

3:50 PM

Page v

10

TH EDITION

ADVANCED
ENGINEERING
MATHEMATICS
ERWIN KREYSZIG
Professor of Mathematics
Ohio State University
Columbus, Ohio

In collaboration with

HERBERT KREYSZIG
New York, New York

EDWARD J. NORMINTON
Associate Professor of Mathematics
Carleton University
Ottawa, Ontario

JOHN WILEY & SONS, INC.

ffirs.qxd

11/4/10

10:50 AM

Page vi

PUBLISHER
PROJECT EDITOR
MARKETING MANAGER
CONTENT MANAGER
PRODUCTION EDITOR
MEDIA EDITOR
MEDIA PRODUCTION SPECIALIST
TEXT AND COVER DESIGN
PHOTO RESEARCHER
COVER PHOTO

Laurie Rosatone
Shannon Corliss
Jonathan Cottrell
Lucille Buonocore
Barbara Russiello
Melissa Edwards
Lisa Sabatini
Madelyn Lesure
Sheena Goldstein
© Denis Jr. Tangney/iStockphoto
Cover photo shows the Zakim Bunker Hill Memorial Bridge in
Boston, MA.

This book was set in Times Roman. The book was composed by PreMedia Global, and printed and bound by
RR Donnelley & Sons Company, Jefferson City, MO. The cover was printed by RR Donnelley & Sons Company,
Jefferson City, MO.
This book is printed on acid free paper. ϱ
Founded in 1807, John Wiley & Sons, Inc. has been a valued source of knowledge and understanding for more
than 200 years, helping people around the world meet their needs and fulfill their aspirations. Our company is
built on a foundation of principles that include responsibility to the communities we serve and where we live and
work. In 2008, we launched a Corporate Citizenship Initiative, a global effort to address the environmental, social,
economic, and ethical challenges we face in our business. Among the issues we are addressing are carbon impact,
paper specifications and procurement, ethical conduct within our business and among our vendors, and community
and charitable support. For more information, please visit our website: www.wiley.com/go/citizenship.
Copyright © 2011, 2006, 1999 by John Wiley & Sons, Inc. All rights reserved. No part of this publication may
be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical,
photocopying, recording, scanning or otherwise, except as permitted under Sections 107 or 108 of the 1976 United
States Copyright Act, without either the prior written permission of the Publisher, or authorization through payment
of the appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA
01923 (Web site: www.copyright.com). Requests to the Publisher for permission should be addressed to the
Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030-5774, (201) 748-6011,
fax (201) 748-6008, or online at: www.wiley.com/go/permissions.
Evaluation copies are provided to qualified academics and professionals for review purposes only, for use in
their courses during the next academic year. These copies are licensed and may not be sold or transferred to a
third party. Upon completion of the review period, please return the evaluation copy to Wiley. Return instructions
and a free of charge return shipping label are available at: www.wiley.com/go/returnlabel. Outside of the United
States, please contact your local representative.

ISBN 978-0-470-45836-5
Printed in the United States of America
10 9 8 7 6 5 4 3 2 1

fpref.qxd

11/8/10

3:16 PM

Page vii

PREFACE
See also http://www.wiley.com/college/kreyszig

Purpose and Structure of the Book
This book provides a comprehensive, thorough, and up-to-date treatment of engineering
mathematics. It is intended to introduce students of engineering, physics, mathematics,
computer science, and related fields to those areas of applied mathematics that are most
relevant for solving practical problems. A course in elementary calculus is the sole
prerequisite. (However, a concise refresher of basic calculus for the student is included
on the inside cover and in Appendix 3.)
The subject matter is arranged into seven parts as follows:
A.
B.
C.
D.
E.
F.
G.

Ordinary Differential Equations (ODEs) in Chapters 1–6
Linear Algebra. Vector Calculus. See Chapters 7–10
Fourier Analysis. Partial Differential Equations (PDEs). See Chapters 11 and 12
Complex Analysis in Chapters 13–18
Numeric Analysis in Chapters 19–21
Optimization, Graphs in Chapters 22 and 23
Probability, Statistics in Chapters 24 and 25.

These are followed by five appendices: 1. References, 2. Answers to Odd-Numbered
Problems, 3. Auxiliary Materials (see also inside covers of book), 4. Additional Proofs,
5. Table of Functions. This is shown in a block diagram on the next page.
The parts of the book are kept independent. In addition, individual chapters are kept as
independent as possible. (If so needed, any prerequisites—to the level of individual
sections of prior chapters—are clearly stated at the opening of each chapter.) We give the
instructor maximum flexibility in selecting the material and tailoring it to his or her
need. The book has helped to pave the way for the present development of engineering
mathematics. This new edition will prepare the student for the current tasks and the future
by a modern approach to the areas listed above. We provide the material and learning
tools for the students to get a good foundation of engineering mathematics that will help
them in their careers and in further studies.

General Features of the Book Include:
• Simplicity of examples to make the book teachable—why choose complicated
examples when simple ones are as instructive or even better?
• Independence of parts and blocks of chapters to provide flexibility in tailoring
courses to specific needs.
• Self-contained presentation, except for a few clearly marked places where a proof
would exceed the level of the book and a reference is given instead.
• Gradual increase in difficulty of material with no jumps or gaps to ensure an
enjoyable teaching and learning experience.
• Modern standard notation to help students with other courses, modern books, and
journals in mathematics, engineering, statistics, physics, computer science, and others.
Furthermore, we designed the book to be a single, self-contained, authoritative, and
convenient source for studying and teaching applied mathematics, eliminating the need
for time-consuming searches on the Internet or time-consuming trips to the library to get
a particular reference book.
vii

fpref.qxd

11/8/10

3:16 PM

viii

Page viii

Preface

PARTS AND CHAPTERS OF THE BOOK

PART A

PART B

Chaps. 1–6
Ordinary Differential Equations (ODEs)

Chaps. 7–10
Linear Algebra. Vector Calculus

Chaps. 1–4
Basic Material
Chap. 5
Series Solutions

Chap. 6
Laplace Transforms

Chap. 7
Matrices,
Linear Systems

Chap. 9
Vector Differential
Calculus

Chap. 8
Eigenvalue Problems

Chap. 10
Vector Integral Calculus

PART C

PART D

Chaps. 11–12
Fourier Analysis. Partial Differential
Equations (PDEs)

Chaps. 13–18
Complex Analysis,
Potential Theory

Chap. 11
Fourier Analysis

Chaps. 13–17
Basic Material

Chap. 12
Partial Differential Equations

Chap. 18
Potential Theory

PART E

PART F

Chaps. 19–21
Numeric Analysis

Chaps. 22–23
Optimization, Graphs

Chap. 19
Numerics in
General

Chap. 20
Numeric
Linear Algebra

Chap. 21
Numerics for
ODEs and PDEs

Chap. 22
Linear Programming

Chap. 23
Graphs, Optimization

PART G

GUIDES AND MANUALS

Chaps. 24–25
Probability, Statistics

Maple Computer Guide
Mathematica Computer Guide

Chap. 24
Data Analysis. Probability Theory

Student Solutions Manual
and Study Guide

Chap. 25
Mathematical Statistics

Instructor’s Manual

fpref.qxd

11/8/10

3:16 PM

Page ix

Preface

ix

Four Underlying Themes of the Book
The driving force in engineering mathematics is the rapid growth of technology and the
sciences. New areas—often drawing from several disciplines—come into existence.
Electric cars, solar energy, wind energy, green manufacturing, nanotechnology, risk
management, biotechnology, biomedical engineering, computer vision, robotics, space
travel, communication systems, green logistics, transportation systems, financial
engineering, economics, and many other areas are advancing rapidly. What does this mean
for engineering mathematics? The engineer has to take a problem from any diverse area
and be able to model it. This leads to the first of four underlying themes of the book.
1. Modeling is the process in engineering, physics, computer science, biology,
chemistry, environmental science, economics, and other fields whereby a physical situation
or some other observation is translated into a mathematical model. This mathematical
model could be a system of differential equations, such as in population control (Sec. 4.5),
a probabilistic model (Chap. 24), such as in risk management, a linear programming
problem (Secs. 22.2–22.4) in minimizing environmental damage due to pollutants, a
financial problem of valuing a bond leading to an algebraic equation that has to be solved
by Newton’s method (Sec. 19.2), and many others.
The next step is solving the mathematical problem obtained by one of the many
techniques covered in Advanced Engineering Mathematics.
The third step is interpreting the mathematical result in physical or other terms to
see what it means in practice and any implications.
Finally, we may have to make a decision that may be of an industrial nature or
recommend a public policy. For example, the population control model may imply
the policy to stop fishing for 3 years. Or the valuation of the bond may lead to a
recommendation to buy. The variety is endless, but the underlying mathematics is
surprisingly powerful and able to provide advice leading to the achievement of goals
toward the betterment of society, for example, by recommending wise policies
concerning global warming, better allocation of resources in a manufacturing process,
or making statistical decisions (such as in Sec. 25.4 whether a drug is effective in treating
a disease).
While we cannot predict what the future holds, we do know that the student has to
practice modeling by being given problems from many different applications as is done
in this book. We teach modeling from scratch, right in Sec. 1.1, and give many examples
in Sec. 1.3, and continue to reinforce the modeling process throughout the book.
2. Judicious use of powerful software for numerics (listed in the beginning of Part E)
and statistics (Part G) is of growing importance. Projects in engineering and industrial
companies may involve large problems of modeling very complex systems with hundreds
of thousands of equations or even more. They require the use of such software. However,
our policy has always been to leave it up to the instructor to determine the degree of use of
computers, from none or little use to extensive use. More on this below.
3. The beauty of engineering mathematics. Engineering mathematics relies on
relatively few basic concepts and involves powerful unifying principles. We point them
out whenever they are clearly visible, such as in Sec. 4.1 where we “grow” a mixing
problem from one tank to two tanks and a circuit problem from one circuit to two circuits,
thereby also increasing the number of ODEs from one ODE to two ODEs. This is an
example of an attractive mathematical model because the “growth” in the problem is
reflected by an “increase” in ODEs.

fpref.qxd

11/8/10

x

3:16 PM

Page x

Preface

4. To clearly identify the conceptual structure of subject matters. For example,
complex analysis (in Part D) is a field that is not monolithic in structure but was formed
by three distinct schools of mathematics. Each gave a different approach, which we clearly
mark. The first approach is solving complex integrals by Cauchy’s integral formula (Chaps.
13 and 14), the second approach is to use the Laurent series and solve complex integrals
by residue integration (Chaps. 15 and 16), and finally we use a geometric approach of
conformal mapping to solve boundary value problems (Chaps. 17 and 18). Learning the
conceptual structure and terminology of the different areas of engineering mathematics is
very important for three reasons:
a. It allows the student to identify a new problem and put it into the right group of
problems. The areas of engineering mathematics are growing but most often retain their
conceptual structure.
b. The student can absorb new information more rapidly by being able to fit it into the
conceptual structure.
c. Knowledge of the conceptual structure and terminology is also important when using
the Internet to search for mathematical information. Since the search proceeds by putting
in key words (i.e., terms) into the search engine, the student has to remember the important
concepts (or be able to look them up in the book) that identify the application and area
of engineering mathematics.

Big Changes in This Edition
1 Problem Sets Changed
The problem sets have been revised and rebalanced with some problem sets having more
problems and some less, reflecting changes in engineering mathematics. There is a greater
emphasis on modeling. Now there are also problems on the discrete Fourier transform
(in Sec. 11.9).
2 Series Solutions of ODEs, Special Functions and Fourier Analysis Reorganized
Chap. 5, on series solutions of ODEs and special functions, has been shortened. Chap. 11
on Fourier Analysis now contains Sturm–Liouville problems, orthogonal functions, and
orthogonal eigenfunction expansions (Secs. 11.5, 11.6), where they fit better conceptually
(rather than in Chap. 5), being extensions of Fourier’s idea of using orthogonal functions.
3 Openings of Parts and Chapters Rewritten As Well As Parts of Sections
In order to give the student a better idea of the structure of the material (see Underlying
Theme 4 above), we have entirely rewritten the openings of parts and chapters.
Furthermore, large parts or individual paragraphs of sections have been rewritten or new
sentences inserted into the text. This should give the students a better intuitive
understanding of the material (see Theme 3 above), let them draw conclusions on their
own, and be able to tackle more advanced material. Overall, we feel that the book has
become more detailed and leisurely written.
4 Student Solutions Manual and Study Guide Enlarged
Upon the explicit request of the users, the answers provided are more detailed and
complete. More explanations are given on how to learn the material effectively by pointing
out what is most important.
5 More Historical Footnotes, Some Enlarged
Historical footnotes are there to show the student that many people from different countries
working in different professions, such as surveyors, researchers in industry, etc., contributed

fpref.qxd

11/8/10

3:16 PM

Page xi

Preface

xi

to the field of engineering mathematics. It should encourage the students to be creative in
their own interests and careers and perhaps also to make contributions to engineering
mathematics.

Further Changes and New Features
• Parts of Chap. 1 on first-order ODEs are rewritten. More emphasis on modeling, also
new block diagram explaining this concept in Sec. 1.1. Early introduction of Euler’s
method in Sec. 1.2 to familiarize student with basic numerics. More examples of
separable ODEs in Sec. 1.3.
• For Chap. 2, on second-order ODEs, note the following changes: For ease of reading,
the first part of Sec. 2.4, which deals with setting up the mass-spring system, has
been rewritten; also some rewriting in Sec. 2.5 on the Euler–Cauchy equation.
• Substantially shortened Chap. 5, Series Solutions of ODEs. Special Functions:
combined Secs. 5.1 and 5.2 into one section called “Power Series Method,” shortened
material in Sec. 5.4 Bessel’s Equation (of the first kind), removed Sec. 5.7
(Sturm–Liouville Problems) and Sec. 5.8 (Orthogonal Eigenfunction Expansions) and
moved material into Chap. 11 (see “Major Changes” above).
• New equivalent definition of basis (Sec. 7.4).
• In Sec. 7.9, completely new part on composition of linear transformations with
two new examples. Also, more detailed explanation of the role of axioms, in
connection with the definition of vector space.
• New table of orientation (opening of Chap. 8 “Linear Algebra: Matrix Eigenvalue
Problems”) where eigenvalue problems occur in the book. More intuitive explanation
of what an eigenvalue is at the begining of Sec. 8.1.
• Better definition of cross product (in vector differential calculus) by properly
identifying the degenerate case (in Sec. 9.3).
• Chap. 11 on Fourier Analysis extensively rearranged: Secs. 11.2 and 11.3
combined into one section (Sec. 11.2), old Sec. 11.4 on complex Fourier Series
removed and new Secs. 11.5 (Sturm–Liouville Problems) and 11.6 (Orthogonal
Series) put in (see “Major Changes” above). New problems (new!) in problem set
11.9 on discrete Fourier transform.
• New section 12.5 on modeling heat flow from a body in space by setting up the heat
equation. Modeling PDEs is more difficult so we separated the modeling process
from the solving process (in Sec. 12.6).
• Introduction to Numerics rewritten for greater clarity and better presentation; new
Example 1 on how to round a number. Sec. 19.3 on interpolation shortened by
removing the less important central difference formula and giving a reference instead.
• Large new footnote with historical details in Sec. 22.3, honoring George Dantzig,
the inventor of the simplex method.
• Traveling salesman problem now described better as a “difficult” problem, typical
of combinatorial optimization (in Sec. 23.2). More careful explanation on how to
compute the capacity of a cut set in Sec. 23.6 (Flows on Networks).
• In Chap. 24, material on data representation and characterization restructured in
terms of five examples and enlarged to include empirical rule on distribution of

fpref.qxd

11/8/10

xii

3:16 PM

Page xii

Preface

data, outliers, and the z-score (Sec. 24.1). Furthermore, new example on encription
(Sec. 24.4).
• Lists of software for numerics (Part E) and statistics (Part G) updated.
• References in Appendix 1 updated to include new editions and some references to
websites.

Use of Computers
The presentation in this book is adaptable to various degrees of use of software,
Computer Algebra Systems (CAS’s), or programmable graphic calculators, ranging
from no use, very little use, medium use, to intensive use of such technology. The choice
of how much computer content the course should have is left up to the instructor, thereby
exhibiting our philosophy of maximum flexibility and adaptability. And, no matter what
the instructor decides, there will be no gaps or jumps in the text or problem set. Some
problems are clearly designed as routine and drill exercises and should be solved by
hand (paper and pencil, or typing on your computer). Other problems require more
thinking and can also be solved without computers. Then there are problems where the
computer can give the student a hand. And finally, the book has CAS projects, CAS
problems and CAS experiments, which do require a computer, and show its power in
solving problems that are difficult or impossible to access otherwise. Here our goal is
to combine intelligent computer use with high-quality mathematics. The computer
invites visualization, experimentation, and independent discovery work. In summary,
the high degree of flexibility of computer use for the book is possible since there are
plenty of problems to choose from and the CAS problems can be omitted if desired.
Note that information on software (what is available and where to order it) is at the
beginning of Part E on Numeric Analysis and Part G on Probability and Statistics. Since
Maple and Mathematica are popular Computer Algebra Systems, there are two computer
guides available that are specifically tailored to Advanced Engineering Mathematics:
E. Kreyszig and E.J. Norminton, Maple Computer Guide, 10th Edition and Mathematica
Computer Guide, 10th Edition. Their use is completely optional as the text in the book is
written without the guides in mind.

Suggestions for Courses: A Four-Semester Sequence
The material, when taken in sequence, is suitable for four consecutive semester courses,
meeting 3 to 4 hours a week:
1st Semester
2nd Semester
3rd Semester
4th Semester

ODEs (Chaps. 1–5 or 1–6)
Linear Algebra. Vector Analysis (Chaps. 7–10)
Complex Analysis (Chaps. 13–18)
Numeric Methods (Chaps. 19–21)

Suggestions for Independent One-Semester Courses
The book is also suitable for various independent one-semester courses meeting 3 hours
a week. For instance,
Introduction to ODEs (Chaps. 1–2, 21.1)
Laplace Transforms (Chap. 6)
Matrices and Linear Systems (Chaps. 7–8)

fpref.qxd

11/8/10

8:51 PM

Page xiii

Preface

xiii

Vector Algebra and Calculus (Chaps. 9–10)
Fourier Series and PDEs (Chaps. 11–12, Secs. 21.4–21.7)
Introduction to Complex Analysis (Chaps. 13–17)
Numeric Analysis (Chaps. 19, 21)
Numeric Linear Algebra (Chap. 20)
Optimization (Chaps. 22–23)
Graphs and Combinatorial Optimization (Chap. 23)
Probability and Statistics (Chaps. 24–25)

Acknowledgments
We are indebted to former teachers, colleagues, and students who helped us directly or
indirectly in preparing this book, in particular this new edition. We profited greatly from
discussions with engineers, physicists, mathematicians, computer scientists, and others,
and from their written comments. We would like to mention in particular Professors
Y. A. Antipov, R. Belinski, S. L. Campbell, R. Carr, P. L. Chambré, Isabel F. Cruz,
Z. Davis, D. Dicker, L. D. Drager, D. Ellis, W. Fox, A. Goriely, R. B. Guenther,
J. B. Handley, N. Harbertson, A. Hassen, V. W. Howe, H. Kuhn, K. Millet, J. D. Moore,
W. D. Munroe, A. Nadim, B. S. Ng, J. N. Ong, P. J. Pritchard, W. O. Ray, L. F. Shampine,
H. L. Smith, Roberto Tamassia, A. L. Villone, H. J. Weiss, A. Wilansky, Neil M. Wigley,
and L. Ying; Maria E. and Jorge A. Miranda, JD, all from the United States; Professors
Wayne H. Enright, Francis. L. Lemire, James J. Little, David G. Lowe, Gerry McPhail,
Theodore S. Norvell, and R. Vaillancourt; Jeff Seiler and David Stanley, all from Canada;
and Professor Eugen Eichhorn, Gisela Heckler, Dr. Gunnar Schroeder, and Wiltrud
Stiefenhofer from Europe. Furthermore, we would like to thank Professors John
B. Donaldson, Bruce C. N. Greenwald, Jonathan L. Gross, Morris B. Holbrook, John
R. Kender, and Bernd Schmitt; and Nicholaiv Villalobos, all from Columbia University,
New York; as well as Dr. Pearl Chang, Chris Gee, Mike Hale, Joshua Jayasingh, MD,
David Kahr, Mike Lee, R. Richard Royce, Elaine Schattner, MD, Raheel Siddiqui, Robert
Sullivan, MD, Nancy Veit, and Ana M. Kreyszig, JD, all from New York City. We would
also like to gratefully acknowledge the use of facilities at Carleton University, Ottawa,
and Columbia University, New York.
Furthermore we wish to thank John Wiley and Sons, in particular Publisher Laurie
Rosatone, Editor Shannon Corliss, Production Editor Barbara Russiello, Media Editor
Melissa Edwards, Text and Cover Designer Madelyn Lesure, and Photo Editor Sheena
Goldstein for their great care and dedication in preparing this edition. In the same vein,
we would also like to thank Beatrice Ruberto, copy editor and proofreader, WordCo, for
the Index, and Joyce Franzen of PreMedia and those of PreMedia Global who typeset this
edition.
Suggestions of many readers worldwide were evaluated in preparing this edition.
Further comments and suggestions for improving the book will be gratefully received.
KREYSZIG

fpref.qxd

11/8/10

3:16 PM

Page xiv

ftoc.qxd

11/4/10

11:48 AM

Page xv

CONTENTS
PART A

Ordinary Differential Equations (ODEs) 1
CHAPTER 1 First-Order ODEs
2
1.1
Basic Concepts. Modeling 2
1.2 Geometric Meaning of yЈ ϭ ƒ(x, y). Direction Fields, Euler’s Method 9
1.3 Separable ODEs. Modeling 12
1.4 Exact ODEs. Integrating Factors 20
1.5 Linear ODEs. Bernoulli Equation. Population Dynamics 27
1.6 Orthogonal Trajectories. Optional 36
1.7 Existence and Uniqueness of Solutions for Initial Value Problems 38
Chapter 1 Review Questions and Problems 43
Summary of Chapter 1 44
CHAPTER 2 Second-Order Linear ODEs
46
2.1 Homogeneous Linear ODEs of Second Order 46
2.2 Homogeneous Linear ODEs with Constant Coefficients 53
2.3 Differential Operators. Optional 60
2.4 Modeling of Free Oscillations of a Mass–Spring System 62
2.5 Euler–Cauchy Equations 71
2.6 Existence and Uniqueness of Solutions. Wronskian 74
2.7 Nonhomogeneous ODEs 79
2.8 Modeling: Forced Oscillations. Resonance 85
2.9 Modeling: Electric Circuits 93
2.10 Solution by Variation of Parameters 99
Chapter 2 Review Questions and Problems 102
Summary of Chapter 2 103
CHAPTER 3 Higher Order Linear ODEs
105
3.1 Homogeneous Linear ODEs 105
3.2 Homogeneous Linear ODEs with Constant Coefficients 111
3.3 Nonhomogeneous Linear ODEs 116
Chapter 3 Review Questions and Problems 122
Summary of Chapter 3 123
CHAPTER 4 Systems of ODEs. Phase Plane. Qualitative Methods
4.0 For Reference: Basics of Matrices and Vectors 124
4.1 Systems of ODEs as Models in Engineering Applications 130
4.2 Basic Theory of Systems of ODEs. Wronskian 137
4.3 Constant-Coefficient Systems. Phase Plane Method 140
4.4 Criteria for Critical Points. Stability 148
4.5 Qualitative Methods for Nonlinear Systems 152
4.6 Nonhomogeneous Linear Systems of ODEs 160
Chapter 4 Review Questions and Problems 164
Summary of Chapter 4 165
CHAPTER 5 Series Solutions of ODEs. Special Functions
5.1 Power Series Method 167
5.2 Legendre’s Equation. Legendre Polynomials Pn(x) 175

124

167

xv

ftoc.qxd

11/4/10

11:48 AM

xvi

Page xvi

Contents

5.3 Extended Power Series Method: Frobenius Method 180
5.4 Bessel’s Equation. Bessel Functions J␯ (x) 187
5.5 Bessel Functions of the Y␯ (x). General Solution 196
Chapter 5 Review Questions and Problems 200
Summary of Chapter 5 201
CHAPTER 6 Laplace Transforms
203
6.1 Laplace Transform. Linearity. First Shifting Theorem (s-Shifting) 204
6.2 Transforms of Derivatives and Integrals. ODEs 211
6.3 Unit Step Function (Heaviside Function).
Second Shifting Theorem (t-Shifting) 217
6.4 Short Impulses. Dirac’s Delta Function. Partial Fractions 225
6.5 Convolution. Integral Equations 232
6.6 Differentiation and Integration of Transforms.
ODEs with Variable Coefficients 238
6.7 Systems of ODEs 242
6.8 Laplace Transform: General Formulas 248
6.9 Table of Laplace Transforms 249
Chapter 6 Review Questions and Problems 251
Summary of Chapter 6 253

PART B

Linear Algebra. Vector Calculus 255
CHAPTER 7

Linear Algebra: Matrices, Vectors, Determinants.
Linear Systems
256

7.1 Matrices, Vectors: Addition and Scalar Multiplication 257
7.2 Matrix Multiplication 263
7.3 Linear Systems of Equations. Gauss Elimination 272
7.4 Linear Independence. Rank of a Matrix. Vector Space 282
7.5 Solutions of Linear Systems: Existence, Uniqueness 288
7.6 For Reference: Second- and Third-Order Determinants 291
7.7 Determinants. Cramer’s Rule 293
7.8 Inverse of a Matrix. Gauss–Jordan Elimination 301
7.9 Vector Spaces, Inner Product Spaces. Linear Transformations. Optional 309
Chapter 7 Review Questions and Problems 318
Summary of Chapter 7 320
CHAPTER 8 Linear Algebra: Matrix Eigenvalue Problems
8.1 The Matrix Eigenvalue Problem.
Determining Eigenvalues and Eigenvectors 323
8.2 Some Applications of Eigenvalue Problems 329
8.3 Symmetric, Skew-Symmetric, and Orthogonal Matrices 334
8.4 Eigenbases. Diagonalization. Quadratic Forms 339
8.5 Complex Matrices and Forms. Optional 346
Chapter 8 Review Questions and Problems 352
Summary of Chapter 8 353

322

ftoc.qxd

11/4/10

11:48 AM

Page xvii

Contents

xvii

CHAPTER 9 Vector Differential Calculus. Grad, Div, Curl
354
9.1 Vectors in 2-Space and 3-Space 354
9.2 Inner Product (Dot Product) 361
9.3 Vector Product (Cross Product) 368
9.4 Vector and Scalar Functions and Their Fields. Vector Calculus: Derivatives 375
9.5 Curves. Arc Length. Curvature. Torsion 381
9.6 Calculus Review: Functions of Several Variables. Optional 392
9.7 Gradient of a Scalar Field. Directional Derivative 395
9.8 Divergence of a Vector Field 402
9.9 Curl of a Vector Field 406
Chapter 9 Review Questions and Problems 409
Summary of Chapter 9 410
CHAPTER 10 Vector Integral Calculus. Integral Theorems
10.1 Line Integrals 413
10.2 Path Independence of Line Integrals 419
10.3 Calculus Review: Double Integrals. Optional 426
10.4 Green’s Theorem in the Plane 433
10.5 Surfaces for Surface Integrals 439
10.6 Surface Integrals 443
10.7 Triple Integrals. Divergence Theorem of Gauss 452
10.8 Further Applications of the Divergence Theorem 458
10.9 Stokes’s Theorem 463
Chapter 10 Review Questions and Problems 469
Summary of Chapter 10 470

PART C

413

Fourier Analysis. Partial Differential Equations (PDEs) 473
CHAPTER 11 Fourier Analysis
474
11.1 Fourier Series 474
11.2 Arbitrary Period. Even and Odd Functions. Half-Range Expansions 483
11.3 Forced Oscillations 492
11.4 Approximation by Trigonometric Polynomials 495
11.5 Sturm–Liouville Problems. Orthogonal Functions 498
11.6 Orthogonal Series. Generalized Fourier Series 504
11.7 Fourier Integral 510
11.8 Fourier Cosine and Sine Transforms 518
11.9 Fourier Transform. Discrete and Fast Fourier Transforms 522
11.10 Tables of Transforms 534
Chapter 11 Review Questions and Problems 537
Summary of Chapter 11 538

540
CHAPTER 12 Partial Differential Equations (PDEs)
12.1 Basic Concepts of PDEs 540
12.2 Modeling: Vibrating String, Wave Equation 543
12.3 Solution by Separating Variables. Use of Fourier Series 545
12.4 D’Alembert’s Solution of the Wave Equation. Characteristics 553
12.5 Modeling: Heat Flow from a Body in Space. Heat Equation 557

ftoc.qxd

11/4/10

11:48 AM

xviii

Page xviii

Contents

12.6 Heat Equation: Solution by Fourier Series.
Steady Two-Dimensional Heat Problems. Dirichlet Problem 558
12.7 Heat Equation: Modeling Very Long Bars.
Solution by Fourier Integrals and Transforms 568
12.8 Modeling: Membrane, Two-Dimensional Wave Equation 575
12.9 Rectangular Membrane. Double Fourier Series 577
12.10 Laplacian in Polar Coordinates. Circular Membrane. Fourier–Bessel Series 585
12.11 Laplace’s Equation in Cylindrical and Spherical Coordinates. Potential 593
12.12 Solution of PDEs by Laplace Transforms 600
Chapter 12 Review Questions and Problems 603
Summary of Chapter 12 604

PART D

Complex Analysis 607
CHAPTER 13

Complex Numbers and Functions.
Complex Differentiation
608

13.1 Complex Numbers and Their Geometric Representation 608
13.2 Polar Form of Complex Numbers. Powers and Roots 613
13.3 Derivative. Analytic Function 619
13.4 Cauchy–Riemann Equations. Laplace’s Equation 625
13.5 Exponential Function 630
13.6 Trigonometric and Hyperbolic Functions. Euler’s Formula 633
13.7 Logarithm. General Power. Principal Value 636
Chapter 13 Review Questions and Problems 641
Summary of Chapter 13 641
CHAPTER 14 Complex Integration
643
14.1 Line Integral in the Complex Plane 643
14.2 Cauchy’s Integral Theorem 652
14.3 Cauchy’s Integral Formula 660
14.4 Derivatives of Analytic Functions 664
Chapter 14 Review Questions and Problems 668
Summary of Chapter 14 669
CHAPTER 15 Power Series, Taylor Series
671
15.1 Sequences, Series, Convergence Tests 671
15.2 Power Series 680
15.3 Functions Given by Power Series 685
15.4 Taylor and Maclaurin Series 690
15.5 Uniform Convergence. Optional 698
Chapter 15 Review Questions and Problems 706
Summary of Chapter 15 706
CHAPTER 16 Laurent Series. Residue Integration
16.1 Laurent Series 708
16.2 Singularities and Zeros. Infinity 715
16.3 Residue Integration Method 719
16.4 Residue Integration of Real Integrals 725
Chapter 16 Review Questions and Problems 733
Summary of Chapter 16 734

708

ftoc.qxd

11/4/10

11:48 AM

Page xix

Contents

xix

CHAPTER 17 Conformal Mapping
736
17.1 Geometry of Analytic Functions: Conformal Mapping 737
17.2 Linear Fractional Transformations (Möbius Transformations) 742
17.3 Special Linear Fractional Transformations 746
17.4 Conformal Mapping by Other Functions 750
17.5 Riemann Surfaces. Optional 754
Chapter 17 Review Questions and Problems 756
Summary of Chapter 17 757
CHAPTER 18 Complex Analysis and Potential Theory
18.1 Electrostatic Fields 759
18.2 Use of Conformal Mapping. Modeling 763
18.3 Heat Problems 767
18.4 Fluid Flow 771
18.5 Poisson’s Integral Formula for Potentials 777
18.6 General Properties of Harmonic Functions.
Uniqueness Theorem for the Dirichlet Problem 781
Chapter 18 Review Questions and Problems 785
Summary of Chapter 18 786

PART E

Numeric Analysis 787
Software 788
CHAPTER 19 Numerics in General
790
19.1 Introduction 790
19.2 Solution of Equations by Iteration 798
19.3 Interpolation 808
19.4 Spline Interpolation 820
19.5 Numeric Integration and Differentiation 827
Chapter 19 Review Questions and Problems 841
Summary of Chapter 19 842
CHAPTER 20 Numeric Linear Algebra
844
20.1 Linear Systems: Gauss Elimination 844
20.2 Linear Systems: LU-Factorization, Matrix Inversion 852
20.3 Linear Systems: Solution by Iteration 858
20.4 Linear Systems: Ill-Conditioning, Norms 864
20.5 Least Squares Method 872
20.6 Matrix Eigenvalue Problems: Introduction 876
20.7 Inclusion of Matrix Eigenvalues 879
20.8 Power Method for Eigenvalues 885
20.9 Tridiagonalization and QR-Factorization 888
Chapter 20 Review Questions and Problems 896
Summary of Chapter 20 898

900
CHAPTER 21 Numerics for ODEs and PDEs
21.1 Methods for First-Order ODEs 901
21.2 Multistep Methods 911
21.3 Methods for Systems and Higher Order ODEs 915

758

ftoc.qxd

11/4/10

11:48 AM

xx

Page xx

Contents

21.4 Methods for Elliptic PDEs 922
21.5 Neumann and Mixed Problems. Irregular Boundary 931
21.6 Methods for Parabolic PDEs 936
21.7 Method for Hyperbolic PDEs 942
Chapter 21 Review Questions and Problems 945
Summary of Chapter 21 946

PART F

Optimization, Graphs 949
CHAPTER 22 Unconstrained Optimization. Linear Programming
950
22.1 Basic Concepts. Unconstrained Optimization: Method of Steepest Descent 951
22.2 Linear Programming 954
22.3 Simplex Method 958
22.4 Simplex Method: Difficulties 962
Chapter 22 Review Questions and Problems 968
Summary of Chapter 22 969
CHAPTER 23 Graphs. Combinatorial Optimization
23.1 Graphs and Digraphs 970
23.2 Shortest Path Problems. Complexity 975
23.3 Bellman’s Principle. Dijkstra’s Algorithm 980
23.4 Shortest Spanning Trees: Greedy Algorithm 984
23.5 Shortest Spanning Trees: Prim’s Algorithm 988
23.6 Flows in Networks 991
23.7 Maximum Flow: Ford–Fulkerson Algorithm 998
23.8 Bipartite Graphs. Assignment Problems 1001
Chapter 23 Review Questions and Problems 1006
Summary of Chapter 23 1007

PART G

970

Probability, Statistics 1009
Software 1009
CHAPTER 24 Data Analysis. Probability Theory
1011
24.1 Data Representation. Average. Spread 1011
24.2 Experiments, Outcomes, Events 1015
24.3 Probability 1018
24.4 Permutations and Combinations 1024
24.5 Random Variables. Probability Distributions 1029
24.6 Mean and Variance of a Distribution 1035
24.7 Binomial, Poisson, and Hypergeometric Distributions 1039
24.8 Normal Distribution 1045
24.9 Distributions of Several Random Variables 1051
Chapter 24 Review Questions and Problems 1060
Summary of Chapter 24 1060
CHAPTER 25 Mathematical Statistics
25.1 Introduction. Random Sampling 1063
25.2 Point Estimation of Parameters 1065
25.3 Confidence Intervals 1068

1063

ftoc.qxd

11/4/10

11:48 AM

Page xxi

Contents

xxi

25.4 Testing Hypotheses. Decisions 1077
25.5 Quality Control 1087
25.6 Acceptance Sampling 1092
25.7 Goodness of Fit. ␹ 2-Test 1096
25.8 Nonparametric Tests 1100
25.9 Regression. Fitting Straight Lines. Correlation 1103
Chapter 25 Review Questions and Problems 1111
Summary of Chapter 25 1112
APPENDIX 1

References

A1

APPENDIX 2

Answers to Odd-Numbered Problems

APPENDIX 3 Auxiliary Material
A63
A3.1 Formulas for Special Functions A63
A3.2 Partial Derivatives A69
A3.3 Sequences and Series A72
A3.4 Grad, Div, Curl, ٌ 2 in Curvilinear Coordinates A74
APPENDIX 4

Additional Proofs

APPENDIX 5

Tables

INDEX

I1

PHOTO CREDITS

P1

A97

A77

A4

ftoc.qxd

11/4/10

11:48 AM

Page xxii

c01.qxd

7/30/10

8:14 PM

Page 1

PART

A

Ordinary
Differential
Equations (ODEs)
CHAPTER
CHAPTER
CHAPTER
CHAPTER
CHAPTER
CHAPTER

1
2
3
4
5
6

First-Order ODEs
Second-Order Linear ODEs
Higher Order Linear ODEs
Systems of ODEs. Phase Plane. Qualitative Methods
Series Solutions of ODEs. Special Functions
Laplace Transforms
Many physical laws and relations can be expressed mathematically in the form of differential
equations. Thus it is natural that this book opens with the study of differential equations and
their solutions. Indeed, many engineering problems appear as differential equations.
The main objectives of Part A are twofold: the study of ordinary differential equations
and their most important methods for solving them and the study of modeling.
Ordinary differential equations (ODEs) are differential equations that depend on a single
variable. The more difficult study of partial differential equations (PDEs), that is,
differential equations that depend on several variables, is covered in Part C.
Modeling is a crucial general process in engineering, physics, computer science, biology,
medicine, environmental science, chemistry, economics, and other fields that translates a
physical situation or some other observations into a “mathematical model.” Numerous
examples from engineering (e.g., mixing problem), physics (e.g., Newton’s law of cooling),
biology (e.g., Gompertz model), chemistry (e.g., radiocarbon dating), environmental science
(e.g., population control), etc. shall be given, whereby this process is explained in detail,
that is, how to set up the problems correctly in terms of differential equations.
For those interested in solving ODEs numerically on the computer, look at Secs. 21.1–21.3
of Chapter 21 of Part F, that is, numeric methods for ODEs. These sections are kept
independent by design of the other sections on numerics. This allows for the study of
numerics for ODEs directly after Chap. 1 or 2.
1

c01.qxd

7/30/10

8:14 PM

Page 2

CHAPTER

1

First-Order ODEs
Chapter 1 begins the study of ordinary differential equations (ODEs) by deriving them from
physical or other problems (modeling), solving them by standard mathematical methods,
and interpreting solutions and their graphs in terms of a given problem. The simplest ODEs
to be discussed are ODEs of the first order because they involve only the first derivative
of the unknown function and no higher derivatives. These unknown functions will usually
be denoted by y1x2 or y1t2 when the independent variable denotes time t. The chapter ends
with a study of the existence and uniqueness of solutions of ODEs in Sec. 1.7.
Understanding the basics of ODEs requires solving problems by hand (paper and pencil,
or typing on your computer, but first without the aid of a CAS). In doing so, you will
gain an important conceptual understanding and feel for the basic terms, such as ODEs,
direction field, and initial value problem. If you wish, you can use your Computer Algebra
System (CAS) for checking solutions.
COMMENT. Numerics for first-order ODEs can be studied immediately after this
chapter. See Secs. 21.1–21.2, which are independent of other sections on numerics.
Prerequisite: Integral calculus.
Sections that may be omitted in a shorter course: 1.6, 1.7.
References and Answers to Problems: App. 1 Part A, and App. 2.

1.1

Basic Concepts. Modeling
Physical
System

Mathematical
Model

Mathematical
Solution

Physical
Interpretation

Fig. 1. Modeling,
solving, interpreting

2

If we want to solve an engineering problem (usually of a physical nature), we first
have to formulate the problem as a mathematical expression in terms of variables,
functions, and equations. Such an expression is known as a mathematical model of the
given problem. The process of setting up a model, solving it mathematically, and
interpreting the result in physical or other terms is called mathematical modeling or,
briefly, modeling.
Modeling needs experience, which we shall gain by discussing various examples and
problems. (Your computer may often help you in solving but rarely in setting up models.)
Now many physical concepts, such as velocity and acceleration, are derivatives. Hence
a model is very often an equation containing derivatives of an unknown function. Such
a model is called a differential equation. Of course, we then want to find a solution (a
function that satisfies the equation), explore its properties, graph it, find values of it, and
interpret it in physical terms so that we can understand the behavior of the physical system
in our given problem. However, before we can turn to methods of solution, we must first
define some basic concepts needed throughout this chapter.

c01.qxd

7/30/10

8:14 PM

Page 3

SEC. 1.1 Basic Concepts. Modeling

3

y

h
Velocity
v
Water level h
Falling stone

Parachutist

y″ = g = const.
(Sec. 1.1)

mv′ = mg – bv
(Sec. 1.2)

Outflowing water
h′ = –k h
(Sec. 1.3)

2

y
R

(k)
C

E

t

L

y
m
Displacement y
Vibrating mass
on a spring
my″ + ky = 0
(Secs. 2.4, 2.8)

Beats of a vibrating
system
2

y″ + ω
w0 y = cos ω
wt, ω
w0 ≈ ω
w
(Sec. 2.8)

Current I in an
RLC circuit
LI″ + RI′ + 1 I = E′
C

(Sec. 2.9)

L

θ
y

Lotka–Volterra
predator–prey model

Deformation of a beam
iv

Pendulum

y′1 = ay1 – by1 y2

EIy = f(x)

Lθ″ + g sin θθ = 0

y′2 = ky1 y2 – ly2

(Sec. 3.3)

(Sec. 4.5)

(Sec. 4.5)

Fig. 2.

Some applications of differential equations

An ordinary differential equation (ODE) is an equation that contains one or several
derivatives of an unknown function, which we usually call y(x) (or sometimes y(t) if the
independent variable is time t). The equation may also contain y itself, known functions
of x (or t), and constants. For example,
(1)

y r ϭ cos x

(2)

y s ϩ 9y ϭ e؊2x

(3)

y r y t Ϫ 32 y r 2 ϭ 0

c01.qxd

7/30/10

8:14 PM

4

Page 4

CHAP. 1 First-Order ODEs

are ordinary differential equations (ODEs). Here, as in calculus, y r denotes dy>dx,
y s ϭ d 2y>dx 2, etc. The term ordinary distinguishes them from partial differential
equations (PDEs), which involve partial derivatives of an unknown function of two
or more variables. For instance, a PDE with unknown function u of two variables x
and y is
0 2u
0x

2

ϩ

0 2u
0y 2

ϭ 0.

PDEs have important engineering applications, but they are more complicated than ODEs;
they will be considered in Chap. 12.
An ODE is said to be of order n if the nth derivative of the unknown function y is the
highest derivative of y in the equation. The concept of order gives a useful classification
into ODEs of first order, second order, and so on. Thus, (1) is of first order, (2) of second
order, and (3) of third order.
In this chapter we shall consider first-order ODEs. Such equations contain only the
first derivative y r and may contain y and any given functions of x. Hence we can write
them as
(4)

F(x, y, y r ) ϭ 0

or often in the form
y r ϭ f (x, y).
This is called the explicit form, in contrast to the implicit form (4). For instance, the implicit
ODE x ؊3y r Ϫ 4y 2 ϭ 0 (where x 0) can be written explicitly as y r ϭ 4x 3y 2.

Concept of Solution
A function
y ϭ h(x)
is called a solution of a given ODE (4) on some open interval a Ͻ x Ͻ b if h(x) is
defined and differentiable throughout the interval and is such that the equation becomes
an identity if y and y r are replaced with h and h r , respectively. The curve (the graph) of
h is called a solution curve.
Here, open interval a Ͻ x Ͻ b means that the endpoints a and b are not regarded as
points belonging to the interval. Also, a Ͻ x Ͻ b includes infinite intervals Ϫϱ Ͻ x Ͻ b,
a Ͻ x Ͻ ϱ, Ϫϱ Ͻ x Ͻ ϱ (the real line) as special cases.

EXAMPLE 1

Verification of Solution
Verify that y ϭ c>x (c an arbitrary constant) is a solution of the ODE xy r ϭ Ϫy for all x 0. Indeed, differentiate
y ϭ c>x to get y r ϭ Ϫc>x 2. Multiply this by x, obtaining xy r ϭ Ϫc>x; thus, xy r ϭ Ϫy, the given ODE.
᭿

c01.qxd

7/30/10

8:14 PM

Page 5

SEC. 1.1 Basic Concepts. Modeling
EXAMPLE 2

5

Solution by Calculus. Solution Curves
The ODE y r ϭ dy>dx ϭ cos x can be solved directly by integration on both sides. Indeed, using calculus,
we obtain y ϭ ͐ cos x dx ϭ sin x ϩ c, where c is an arbitrary constant. This is a family of solutions. Each value
of c, for instance, 2.75 or 0 or Ϫ8, gives one of these curves. Figure 3 shows some of them, for c ϭ Ϫ3, Ϫ2,
Ϫ1, 0, 1, 2, 3, 4.
᭿
y

4

2

–π

π

0

x

2π

–2

–4

Fig. 3.

EXAMPLE 3

Solutions y ϭ sin x ϩ c of the ODE y r ϭ cos x

(A) Exponential Growth. (B) Exponential Decay
From calculus we know that y ϭ ce0.2t has the derivative
yr ϭ

dy
dt

ϭ 0.2e0.2t ϭ 0.2y.

Hence y is a solution of y r ϭ 0.2y (Fig. 4A). This ODE is of the form y r ϭ ky. With positive-constant k it can
model exponential growth, for instance, of colonies of bacteria or populations of animals. It also applies to
humans for small populations in a large country (e.g., the United States in early times) and is then known as
Malthus’s law.1 We shall say more about this topic in Sec. 1.5.
(B) Similarly, y r ϭ Ϫ0.2 (with a minus on the right) has the solution y ϭ ce؊0.2t, (Fig. 4B) modeling
exponential decay, as, for instance, of a radioactive substance (see Example 5).
᭿
y

y

40

2.5

2.0

30

1.5
20
1.0
10

0

0.5

0

2

4

6

8

10

12

14 t

Fig. 4A. Solutions of y r ϭ 0.2y
in Example 3 (exponential growth)

1

0

0

2

4

6

8

10

12

14 t

Fig. 4B. Solutions of y r ϭ Ϫ0.2y
in Example 3 (exponential decay)

Named after the English pioneer in classic economics, THOMAS ROBERT MALTHUS (1766–1834).

c01.qxd

7/30/10

8:14 PM

6

Page 6

CHAP. 1 First-Order ODEs

We see that each ODE in these examples has a solution that contains an arbitrary
constant c. Such a solution containing an arbitrary constant c is called a general solution
of the ODE.
(We shall see that c is sometimes not completely arbitrary but must be restricted to some
interval to avoid complex expressions in the solution.)
We shall develop methods that will give general solutions uniquely (perhaps except for
notation). Hence we shall say the general solution of a given ODE (instead of a general
solution).
Geometrically, the general solution of an ODE is a family of infinitely many solution
curves, one for each value of the constant c. If we choose a specific c (e.g., c ϭ 6.45 or 0
or Ϫ2.01) we obtain what is called a particular solution of the ODE. A particular solution
does not contain any arbitrary constants.
In most cases, general solutions exist, and every solution not containing an arbitrary
constant is obtained as a particular solution by assigning a suitable value to c. Exceptions
to these rules occur but are of minor interest in applications; see Prob. 16 in Problem
Set 1.1.

Initial Value Problem
In most cases the unique solution of a given problem, hence a particular solution, is
obtained from a general solution by an initial condition y(x 0) ϭ y0, with given values
x 0 and y0, that is used to determine a value of the arbitrary constant c. Geometrically
this condition means that the solution curve should pass through the point (x 0, y0)
in the xy-plane. An ODE, together with an initial condition, is called an initial value
problem. Thus, if the ODE is explicit, y r ϭ f (x, y), the initial value problem is of
the form
(5)
EXAMPLE 4

y r ϭ f (x, y),

y(x 0) ϭ y0.

Initial Value Problem
Solve the initial value problem
yr ϭ

dy
dx

ϭ 3y,

y(0) ϭ 5.7.

Solution. The general solution is y(x) ϭ ce3x; see Example 3. From this solution and the initial condition

we obtain y(0) ϭ ce0 ϭ c ϭ 5.7. Hence the initial value problem has the solution y(x) ϭ 5.7e3x. This is a
particular solution.
᭿

More on Modeling
The general importance of modeling to the engineer and physicist was emphasized at the
beginning of this section. We shall now consider a basic physical problem that will show
the details of the typical steps of modeling. Step 1: the transition from the physical situation
(the physical system) to its mathematical formulation (its mathematical model); Step 2:
the solution by a mathematical method; and Step 3: the physical interpretation of the result.
This may be the easiest way to obtain a first idea of the nature and purpose of differential
equations and their applications. Realize at the outset that your computer (your CAS)
may perhaps give you a hand in Step 2, but Steps 1 and 3 are basically your work.

c01.qxd

7/30/10

8:14 PM

Page 7

SEC. 1.1 Basic Concepts. Modeling

7

And Step 2 requires a solid knowledge and good understanding of solution methods
available to you—you have to choose the method for your work by hand or by the
computer. Keep this in mind, and always check computer results for errors (which may
arise, for instance, from false inputs).
EXAMPLE 5

Radioactivity. Exponential Decay
Given an amount of a radioactive substance, say, 0.5 g (gram), find the amount present at any later time.
Physical Information. Experiments show that at each instant a radioactive substance decomposes—and is thus
decaying in time—proportional to the amount of substance present.
Step 1. Setting up a mathematical model of the physical process. Denote by y(t) the amount of substance still
present at any time t. By the physical law, the time rate of change y r (t) ϭ dy>dt is proportional to y(t). This
gives the first-order ODE
dy

(6)

dt

ϭ Ϫky

where the constant k is positive, so that, because of the minus, we do get decay (as in [B] of Example 3).
The value of k is known from experiments for various radioactive substances (e.g., k ϭ 1.4 ؒ 10؊11 sec ؊1,
approximately, for radium 226
88 Ra).
Now the given initial amount is 0.5 g, and we can call the corresponding instant t ϭ 0. Then we have the
initial condition y(0) ϭ 0.5. This is the instant at which our observation of the process begins. It motivates
the term initial condition (which, however, is also used when the independent variable is not time or when
we choose a t other than t ϭ 0). Hence the mathematical model of the physical process is the initial value
problem
dy

(7)

dt

ϭ Ϫky,

y(0) ϭ 0.5.

Step 2. Mathematical solution. As in (B) of Example 3 we conclude that the ODE (6) models exponential decay
and has the general solution (with arbitrary constant c but definite given k)
y(t) ϭ ce؊kt.

(8)

We now determine c by using the initial condition. Since y(0) ϭ c from (8), this gives y(0) ϭ c ϭ 0.5. Hence
the particular solution governing our process is (cf. Fig. 5)
y(t) ϭ 0.5e؊kt

(9)

(k Ͼ 0).

Always check your result—it may involve human or computer errors! Verify by differentiation (chain rule!)
that your solution (9) satisfies (7) as well as y(0) ϭ 0.5:
dy
dt

ϭ Ϫ0.5ke؊kt ϭ Ϫk ؒ 0.5e؊kt ϭ Ϫky,

y(0) ϭ 0.5e0 ϭ 0.5.

Step 3. Interpretation of result. Formula (9) gives the amount of radioactive substance at time t. It starts from
᭿
the correct initial amount and decreases with time because k is positive. The limit of y as t : ϱ is zero.
y
0.5
0.4
0.3
0.2
0.1
0

0

0.5

1

1.5

2

2.5

3

Fig. 5. Radioactivity (Exponential decay,
y ϭ 0.5eϪkt, with k ϭ 1.5 as an example)

t

c01.qxd

7/30/10

8:15 PM

Page 8

8

CHAP. 1 First-Order ODEs

PROBLEM SET 1.1
1–8

CALCULUS

17–20

Solve the ODE by integration or by remembering a
differentiation formula.
1. y r ϩ 2 sin 2 px ϭ 0
2
2. y r ϩ xe؊x >2 ϭ 0
3. y r ϭ y
4. y r ϭ Ϫ1.5y
5. y r ϭ 4e؊x cos x
6. y s ϭ Ϫy
7. y r ϭ cosh 5.13x
8. y t ϭ e؊0.2x
9–15
VERIFICATION. INITIAL VALUE
PROBLEM (IVP)
(a) Verify that y is a solution of the ODE. (b) Determine
from y the particular solution of the IVP. (c) Graph the
solution of the IVP.
9. y r ϩ 4y ϭ 1.4, y ϭ ce؊4x ϩ 0.35, y(0) ϭ 2
2
10. y r ϩ 5xy ϭ 0, y ϭ ce؊2.5x , y(0) ϭ p
11. y r ϭ y ϩ ex, y ϭ (x ϩ c)ex, y(0) ϭ 12
12. yy r ϭ 4x, y 2 Ϫ 4x 2 ϭ c (y Ͼ 0), y(1) ϭ 4
1
13. y r ϭ y Ϫ y 2, y ϭ
, y(0) ϭ 0.25
1 ϩ ce؊x
14. y r tan x ϭ 2y Ϫ 8, y ϭ c sin 2 x ϩ 4, y(12 p) ϭ 0
15. Find two constant solutions of the ODE in Prob. 13 by
inspection.
16. Singular solution. An ODE may sometimes have an
additional solution that cannot be obtained from the
general solution and is then called a singular solution.
The ODE y r 2 Ϫ xy r ϩ y ϭ 0 is of this kind. Show
by differentiation and substitution that it has the
general solution y ϭ cx Ϫ c2 and the singular solution
y ϭ x 2>4. Explain Fig. 6.
y
3
2
1
–4

Fig. 6.

–2 –1
–2
–3
–4
–5

2

4 x

Particular solutions and singular
solution in Problem 16

MODELING, APPLICATIONS

These problems will give you a first impression of modeling.
Many more problems on modeling follow throughout this
chapter.
17. Half-life. The half-life measures exponential decay.
It is the time in which half of the given amount of
radioactive substance will disappear. What is the halflife of 226
88 Ra (in years) in Example 5?
18. Half-life. Radium 224
88 Ra has a half-life of about
3.6 days.
(a) Given 1 gram, how much will still be present after
1 day?
(b) After 1 year?
19. Free fall. In dropping a stone or an iron ball, air
resistance is practically negligible. Experiments
show that the acceleration of the motion is constant
(equal to g ϭ 9.80 m>sec2 ϭ 32 ft>sec 2, called the
acceleration of gravity). Model this as an ODE for
y(t), the distance fallen as a function of time t. If the
motion starts at time t ϭ 0 from rest (i.e., with velocity
v ϭ y r ϭ 0), show that you obtain the familiar law of
free fall
y ϭ 12 gt 2.
20. Exponential decay. Subsonic flight. The efficiency
of the engines of subsonic airplanes depends on air
pressure and is usually maximum near 35,000 ft.
Find the air pressure y(x) at this height. Physical
information. The rate of change y r (x) is proportional
to the pressure. At 18,000 ft it is half its value
y0 ϭ y(0) at sea level. Hint. Remember from calculus
that if y ϭ ekx, then y r ϭ kekx ϭ ky. Can you see
without calculation that the answer should be close
to y0>4?

c01.qxd

7/30/10

8:15 PM

Page 9

SEC. 1.2 Geometric Meaning of yЈ ϭ ƒ(x, y). Direction Fields, Euler’s Method

1.2

9

Geometric Meaning of y r ϭ f (x, y).
Direction Fields, Euler’s Method
A first-order ODE
y r ϭ f (x, y)

(1)

has a simple geometric interpretation. From calculus you know that the derivative y r (x) of
y(x) is the slope of y(x). Hence a solution curve of (1) that passes through a point (x 0, y0)
must have, at that point, the slope y r (x 0) equal to the value of f at that point; that is,
y r (x 0) ϭ f (x 0, y0).
Using this fact, we can develop graphic or numeric methods for obtaining approximate
solutions of ODEs (1). This will lead to a better conceptual understanding of an ODE (1).
Moreover, such methods are of practical importance since many ODEs have complicated
solution formulas or no solution formulas at all, whereby numeric methods are needed.
Graphic Method of Direction Fields. Practical Example Illustrated in Fig. 7. We
can show directions of solution curves of a given ODE (1) by drawing short straight-line
segments (lineal elements) in the xy-plane. This gives a direction field (or slope field)
into which you can then fit (approximate) solution curves. This may reveal typical
properties of the whole family of solutions.
Figure 7 shows a direction field for the ODE
yr ϭ y ϩ x

(2)

obtained by a CAS (Computer Algebra System) and some approximate solution curves
fitted in.
y
2

1

–2

–1.5

–1

–0.5

0.5

1 x

–1

–2

Fig. 7.

Direction field of y r ϭ y ϩ x, with three approximate solution
curves passing through (0, 1), (0, 0), (0, Ϫ1), respectively

c01.qxd

7/30/10

10

8:15 PM

Page 10

CHAP. 1 First-Order ODEs

If you have no CAS, first draw a few level curves f (x, y) ϭ const of f (x, y), then parallel
lineal elements along each such curve (which is also called an isocline, meaning a curve
of equal inclination), and finally draw approximation curves fit to the lineal elements.
We shall now illustrate how numeric methods work by applying the simplest numeric
method, that is Euler’s method, to an initial value problem involving ODE (2). First we
give a brief description of Euler’s method.

Numeric Method by Euler
Given an ODE (1) and an initial value y(x 0) ϭ y0, Euler’s method yields approximate
solution values at equidistant x-values x 0, x 1 ϭ x 0 ϩ h, x 2 ϭ x 0 ϩ 2h, Á , namely,
y1 ϭ y0 ϩ hf (x 0, y0)

(Fig. 8)

y2 ϭ y1 ϩ hf (x 1, y1),

etc.

In general,
yn ϭ ynϪ1 ϩ hf (x nϪ1, ynϪ1)
where the step h equals, e.g., 0.1 or 0.2 (as in Table 1.1) or a smaller value for greater
accuracy.
y
Solution curve
y(x1)

Error of y1

y1
hf(x0, y0)

y0

h
x0

Fig. 8.

x1

x

First Euler step, showing a solution curve, its tangent at (x0, y0),
step h and increment hf (x0, y0) in the formula for y1

Table 1.1 shows the computation of n ϭ 5 steps with step h ϭ 0.2 for the ODE (2) and
initial condition y(0) ϭ 0, corresponding to the middle curve in the direction field. We
shall solve the ODE exactly in Sec. 1.5. For the time being, verify that the initial value
problem has the solution y ϭ ex Ϫ x Ϫ 1. The solution curve and the values in Table 1.1
are shown in Fig. 9. These values are rather inaccurate. The errors y(x n) Ϫ yn are shown
in Table 1.1 as well as in Fig. 9. Decreasing h would improve the values, but would soon
require an impractical amount of computation. Much better methods of a similar nature
will be discussed in Sec. 21.1.

c01.qxd

7/30/10

8:15 PM

Page 11

SEC. 1.2 Geometric Meaning of yЈ ϭ ƒ(x, y). Direction Fields, Euler’s Method

11

Table 1.1. Euler method for y r ‫ ؍‬y ؉ x, y (0) ‫ ؍‬0 for
x ‫ ؍‬0, Á , 1.0 with step h ‫ ؍‬0.2
n

xn

yn

y(x n)

Error

0
1
2
3
4
5

0.0
0.2
0.4
0.6
0.8
1.0

0.000
0.000
0.04
0.128
0.274
0.488

0.000
0.021
0.092
0.222
0.426
0.718

0.000
0.021
0.052
0.094
0.152
0.230

y
0.7
0.5
0.3
0.1
0

Fig. 9.

0.2

0.4

0.6

0.8

1

x

Euler method: Approximate values in Table 1.1 and solution curve

PROBLEM SET 1.2
1–8

DIRECTION FIELDS, SOLUTION CURVES

Graph a direction field (by a CAS or by hand). In the field
graph several solution curves by hand, particularly those
passing through the given points (x, y).
1. y r ϭ 1 ϩ y 2, (14 p, 1)
2. yy r ϩ 4x ϭ 0, (1, 1), (0, 2)
3. y r ϭ 1 Ϫ y 2, (0, 0), (2, 12 )
4. y r ϭ 2y Ϫ y 2, (0, 0), (0, 1), (0, 2), (0, 3)
5. y r ϭ x Ϫ 1>y, (1, 12 )
6. y r ϭ sin 2 y, (0, Ϫ0.4), (0, 1)
7. y r ϭ ey>x, (2, 2), (3, 3)
8. y r ϭ Ϫ2xy, (0, 12 ), (0, 1), (0, 2)
9–10

ACCURACY OF DIRECTION FIELDS

Direction fields are very useful because they can give you
an impression of all solutions without solving the ODE,
which may be difficult or even impossible. To get a feel for
the accuracy of the method, graph a field, sketch solution
curves in it, and compare them with the exact solutions.
9. y r ϭ cos px
10. y r ϭ Ϫ5y 1>2 (Sol. 1y ϩ 52 x ϭ c)
11. Autonomous ODE. This means an ODE not showing
x (the independent variable) explicitly. (The ODEs in
Probs. 6 and 10 are autonomous.) What will the level
curves f (x, y) ϭ const (also called isoclines ϭ curves

of equal inclination) of an autonomous ODE look like?
Give reason.
12–15

MOTIONS

Model the motion of a body B on a straight line with
velocity as given, y(t) being the distance of B from a point
y ϭ 0 at time t. Graph a direction field of the model (the
ODE). In the field sketch the solution curve satisfying the
given initial condition.
12. Product of velocity times distance constant, equal to 2,
y(0) ϭ 2.
13. Distance ϭ Velocity ϫ Time,

y(1) ϭ 1

14. Square of the distance plus square of the velocity equal
to 1, initial distance 1> 12
15. Parachutist. Two forces act on a parachutist, the
attraction by the earth mg (m ϭ mass of person plus
equipment, g ϭ 9.8 m>sec2 the acceleration of gravity)
and the air resistance, assumed to be proportional to the
square of the velocity v(t). Using Newton’s second law
of motion (mass ϫ acceleration ϭ resultant of the forces),
set up a model (an ODE for v(t)). Graph a direction field
(choosing m and the constant of proportionality equal to 1).
Assume that the parachute opens when v ϭ 10 m>sec.
Graph the corresponding solution in the field. What is the
limiting velocity? Would the parachute still be sufficient
if the air resistance were only proportional to v(t)?

c01.qxd

7/30/10

8:15 PM

12

Page 12

CHAP. 1 First-Order ODEs

16. CAS PROJECT. Direction Fields. Discuss direction
fields as follows.
(a) Graph portions of the direction field of the ODE (2)
(see Fig. 7), for instance, Ϫ5 Ϲ x Ϲ 2, Ϫ1 Ϲ y Ϲ 5.
Explain what you have gained by this enlargement of
the portion of the field.
(b) Using implicit differentiation, find an ODE with
the general solution x 2 ϩ 9y 2 ϭ c (y Ͼ 0). Graph its
direction field. Does the field give the impression
that the solution curves may be semi-ellipses? Can you
do similar work for circles? Hyperbolas? Parabolas?
Other curves?
(c) Make a conjecture about the solutions of y r ϭ Ϫx>y
from the direction field.
(d) Graph the direction field of y r ϭ Ϫ12 y and some
solutions of your choice. How do they behave? Why
do they decrease for y Ͼ 0?

1.3

17–20

EULER’S METHOD

This is the simplest method to explain numerically solving
an ODE, more precisely, an initial value problem (IVP).
(More accurate methods based on the same principle are
explained in Sec. 21.1.) Using the method, to get a feel for
numerics as well as for the nature of IVPs, solve the IVP
numerically with a PC or a calculator, 10 steps. Graph the
computed values and the solution curve on the same
coordinate axes.
17. y r ϭ y,

y(0) ϭ 1,

h ϭ 0.1

18. y r ϭ y,

y(0) ϭ 1,

h ϭ 0.01

19. y r ϭ (y Ϫ x) , y(0) ϭ 0,
Sol. y ϭ x Ϫ tanh x

h ϭ 0.1

20. y r ϭ Ϫ5x 4y 2, y(0) ϭ 1,
Sol. y ϭ 1>(1 ϩ x)5

h ϭ 0.2

2

Separable ODEs. Modeling
Many practically useful ODEs can be reduced to the form
g(y) y r ϭ f (x)

(1)

by purely algebraic manipulations. Then we can integrate on both sides with respect to x,
obtaining

Ύ g(y) yrdx ϭ Ύ f (x) dx ϩ c.

(2)

On the left we can switch to y as the variable of integration. By calculus, y r dx ϭ dy, so that

Ύ g(y) dy ϭ Ύ f (x) dx ϩ c.

(3)

If f and g are continuous functions, the integrals in (3) exist, and by evaluating them we
obtain a general solution of (1). This method of solving ODEs is called the method of
separating variables, and (1) is called a separable equation, because in (3) the variables
are now separated: x appears only on the right and y only on the left.
EXAMPLE 1

Separable ODE
The ODE y r ϭ 1 ϩ y 2 is separable because it can be written
dy
1 ϩ y2

ϭ dx.

By integration,

arctan y ϭ x ϩ c

or

y ϭ tan (x ϩ c).

It is very important to introduce the constant of integration immediately when the integration is performed.
If we wrote arctan y ϭ x, then y ϭ tan x, and then introduced c, we would have obtained y ϭ tan x ϩ c, which
᭿
is not a solution (when c 0). Verify this.

c01.qxd

7/30/10

8:15 PM

Page 13

SEC. 1.3 Separable ODEs. Modeling
EXAMPLE 2

13

Separable ODE
The ODE y r ϭ (x ϩ 1)e؊xy 2 is separable; we obtain y ؊2 dy ϭ (x ϩ 1)e؊x dx.
By integration,

EXAMPLE 3

Ϫy ؊1 ϭ Ϫ(x ϩ 2)e؊x ϩ c,

yϭ

1
.
(x ϩ 2)eϪx Ϫ c

᭿

Initial Value Problem (IVP). Bell-Shaped Curve
Solve y r ϭ Ϫ2xy, y(0) ϭ 1.8.

Solution. By separation and integration,
dy
y

ϭ Ϫ2x dx,

ln y ϭ Ϫx 2 ϩ ෂ
c,

y ϭ ce؊x .
2

This is the general solution. From it and the initial condition, y(0) ϭ ce0 ϭ c ϭ 1.8. Hence the IVP has the
2
solution y ϭ 1.8e؊x . This is a particular solution, representing a bell-shaped curve (Fig. 10).
᭿

y

1

–2

–1

0

1

2 x

Fig. 10. Solution in Example 3 (bell-shaped curve)

Modeling
The importance of modeling was emphasized in Sec. 1.1, and separable equations yield
various useful models. Let us discuss this in terms of some typical examples.
EXAMPLE 4

Radiocarbon Dating2
In September 1991 the famous Iceman (Oetzi), a mummy from the Neolithic period of the Stone Age found in
the ice of the Oetztal Alps (hence the name “Oetzi”) in Southern Tyrolia near the Austrian–Italian border, caused
a scientific sensation. When did Oetzi approximately live and die if the ratio of carbon 146 C to carbon 126 C in
this mummy is 52.5% of that of a living organism?
Physical Information. In the atmosphere and in living organisms, the ratio of radioactive carbon 146 C (made
radioactive by cosmic rays) to ordinary carbon 126 C is constant. When an organism dies, its absorption of 146 C
by breathing and eating terminates. Hence one can estimate the age of a fossil by comparing the radioactive
carbon ratio in the fossil with that in the atmosphere. To do this, one needs to know the half-life of 146 C, which
is 5715 years (CRC Handbook of Chemistry and Physics, 83rd ed., Boca Raton: CRC Press, 2002, page 11–52,
line 9).
Modeling. Radioactive decay is governed by the ODE y r ϭ ky (see Sec. 1.1, Example 5). By
separation and integration (where t is time and y0 is the initial ratio of 146 C to 126 C)

Solution.

dy
y

2

ϭ k dt,

ln ƒ y ƒ ϭ kt ϩ c,

y ϭ y0 ekt

(y0 ϭ ec).

Method by WILLARD FRANK LIBBY (1908–1980), American chemist, who was awarded for this work
the 1960 Nobel Prize in chemistry.

c01.qxd

7/30/10

8:15 PM

14

Page 14

CHAP. 1 First-Order ODEs
Next we use the half-life H ϭ 5715 to determine k. When t ϭ H, half of the original substance is still present. Thus,
y0ekH ϭ 0.5y0,

ekH ϭ 0.5,

kϭ

0.693
ln 0.5
ϭϪ
ϭ Ϫ0.0001 213.
H
5715

Finally, we use the ratio 52.5% for determining the time t when Oetzi died (actually, was killed),
ekt ϭ e؊0.0001 213t ϭ 0.525,

tϭ

ln 0.525
ϭ 5312.
Ϫ0.0001 213

Answer:

About 5300 years ago.

Other methods show that radiocarbon dating values are usually too small. According to recent research, this is
due to a variation in that carbon ratio because of industrial pollution and other factors, such as nuclear testing. ᭿

EXAMPLE 5

Mixing Problem
Mixing problems occur quite frequently in chemical industry. We explain here how to solve the basic model
involving a single tank. The tank in Fig. 11 contains 1000 gal of water in which initially 100 lb of salt is dissolved.
Brine runs in at a rate of 10 gal> min, and each gallon contains 5 lb of dissoved salt. The mixture in the tank is
kept uniform by stirring. Brine runs out at 10 gal> min. Find the amount of salt in the tank at any time t.

Solution. Step 1. Setting up a model. Let y(t) denote the amount of salt in the tank at time t. Its time rate
of change is
y r ϭ Salt inflow rate Ϫ Salt outflow rate

Balance law.

5 lb times 10 gal gives an inflow of 50 lb of salt. Now, the outflow is 10 gal of brine. This is 10>1000 ϭ 0.01
(ϭ 1%) of the total brine content in the tank, hence 0.01 of the salt content y(t), that is, 0.01 y(t). Thus the
model is the ODE
y r ϭ 50 Ϫ 0.01y ϭ Ϫ0.01(y Ϫ 5000).

(4)

Step 2. Solution of the model. The ODE (4) is separable. Separation, integration, and taking exponents on both
sides gives
dy
y Ϫ 5000

ϭ Ϫ0.01 dt,

y Ϫ 5000 ϭ ce؊0.01t.

ln ƒ y Ϫ 5000 ƒ ϭ Ϫ0.01t ϩ c*,

Initially the tank contains 100 lb of salt. Hence y(0) ϭ 100 is the initial condition that will give the unique
solution. Substituting y ϭ 100 and t ϭ 0 in the last equation gives 100 Ϫ 5000 ϭ ce0 ϭ c. Hence c ϭ Ϫ4900.
Hence the amount of salt in the tank at time t is
y(t) ϭ 5000 Ϫ 4900e؊0.01t.

(5)

This function shows an exponential approach to the limit 5000 lb; see Fig. 11. Can you explain physically that
y(t) should increase with time? That its limit is 5000 lb? Can you see the limit directly from the ODE?
The model discussed becomes more realistic in problems on pollutants in lakes (see Problem Set 1.5, Prob. 35)
or drugs in organs. These types of problems are more difficult because the mixing may be imperfect and the flow
᭿
rates (in and out) may be different and known only very roughly.
y
5000
4000
3000
2000
1000
100
0

100

200

300

400

Salt content y(t)

Tank

Fig. 11. Mixing problem in Example 5

500

t

c01.qxd

7/30/10

8:15 PM

Page 15

SEC. 1.3 Separable ODEs. Modeling
EXAMPLE 6

15

Heating an Office Building (Newton’s Law of Cooling3)
Suppose that in winter the daytime temperature in a certain office building is maintained at 70°F. The heating
is shut off at 10 P.M. and turned on again at 6 A.M. On a certain day the temperature inside the building at 2 A.M.
was found to be 65°F. The outside temperature was 50°F at 10 P.M. and had dropped to 40°F by 6 A.M. What
was the temperature inside the building when the heat was turned on at 6 A.M.?
Physical information. Experiments show that the time rate of change of the temperature T of a body B (which
conducts heat well, for example, as a copper ball does) is proportional to the difference between T and the
temperature of the surrounding medium (Newton’s law of cooling).

Solution. Step 1. Setting up a model. Let T(t) be the temperature inside the building and TA the outside
temperature (assumed to be constant in Newton’s law). Then by Newton’s law,
dT
ϭ k(T Ϫ TA).
dt

(6)

Such experimental laws are derived under idealized assumptions that rarely hold exactly. However, even if a
model seems to fit the reality only poorly (as in the present case), it may still give valuable qualitative information.
To see how good a model is, the engineer will collect experimental data and compare them with calculations
from the model.
Step 2. General solution. We cannot solve (6) because we do not know TA, just that it varied between 50°F
and 40°F, so we follow the Golden Rule: If you cannot solve your problem, try to solve a simpler one. We
solve (6) with the unknown function TA replaced with the average of the two known values, or 45°F. For physical
reasons we may expect that this will give us a reasonable approximate value of T in the building at 6 A.M.
For constant TA ϭ 45 (or any other constant value) the ODE (6) is separable. Separation, integration, and
taking exponents gives the general solution
dT
ϭ k dt,
T Ϫ 45

ln ƒ T Ϫ 45 ƒ ϭ kt ϩ c*,

*

T(t) ϭ 45 ϩ cekt

(c ϭ ec ).

Step 3. Particular solution. We choose 10 P.M. to be t ϭ 0. Then the given initial condition is T(0) ϭ 70 and
yields a particular solution, call it Tp. By substitution,
T(0) ϭ 45 ϩ ce0 ϭ 70,

c ϭ 70 Ϫ 45 ϭ 25,

Tp(t) ϭ 45 ϩ 25ekt.

Step 4. Determination of k. We use T(4) ϭ 65, where t ϭ 4 is 2 A.M. Solving algebraically for k and inserting
k into Tp(t) gives (Fig. 12)
Tp(4) ϭ 45 ϩ 25e4k ϭ 65,

e4k ϭ 0.8,

k ϭ 14 ln 0.8 ϭ Ϫ0.056,

Tp(t) ϭ 45 ϩ 25e؊0.056t.

y
70
68
66
65
64
62
61
60

0

2

4

6

8

t

Fig. 12. Particular solution (temperature) in Example 6
3
Sir ISAAC NEWTON (1642–1727), great English physicist and mathematician, became a professor at
Cambridge in 1669 and Master of the Mint in 1699. He and the German mathematician and philosopher
GOTTFRIED WILHELM LEIBNIZ (1646–1716) invented (independently) the differential and integral calculus.
Newton discovered many basic physical laws and created the method of investigating physical problems by
means of calculus. His Philosophiae naturalis principia mathematica (Mathematical Principles of Natural
Philosophy, 1687) contains the development of classical mechanics. His work is of greatest importance to both
mathematics and physics.

c01.qxd

7/30/10

8:15 PM

16

Page 16

CHAP. 1 First-Order ODEs
Step 5. Answer and interpretation. 6 A.M. is t ϭ 8 (namely, 8 hours after 10 P.M.), and
Tp(8) ϭ 45 ϩ 25e؊0.056 ؒ 8 ϭ 613°F4.

᭿

Hence the temperature in the building dropped 9°F, a result that looks reasonable.

EXAMPLE 7

Leaking Tank. Outflow of Water Through a Hole (Torricelli’s Law)
This is another prototype engineering problem that leads to an ODE. It concerns the outflow of water from a
cylindrical tank with a hole at the bottom (Fig. 13). You are asked to find the height of the water in the tank at
any time if the tank has diameter 2 m, the hole has diameter 1 cm, and the initial height of the water when the
hole is opened is 2.25 m. When will the tank be empty?
Physical information. Under the influence of gravity the outflowing water has velocity
v(t) ϭ 0.600 22gh(t)

(7)

(Torricelli’s law4),

where h(t) is the height of the water above the hole at time t, and g ϭ 980 cm>sec2 ϭ 32.17 ft>sec2 is the
acceleration of gravity at the surface of the earth.

Solution. Step 1. Setting up the model. To get an equation, we relate the decrease in water level h(t) to the
outflow. The volume ¢V of the outflow during a short time ¢t is
¢V ϭ Av ¢t

(A ϭ Area of hole).

¢V must equal the change ¢V* of the volume of the water in the tank. Now
¢V* ϭ ϪB ¢h

(B ϭ Cross-sectional area of tank)

where ¢h (Ͼ 0) is the decrease of the height h(t) of the water. The minus sign appears because the volume of
the water in the tank decreases. Equating ¢V and ¢V* gives
ϪB ¢h ϭ Av ¢t.
We now express v according to Torricelli’s law and then let ¢t (the length of the time interval considered)
approach 0—this is a standard way of obtaining an ODE as a model. That is, we have
¢h
A
A
ϭ Ϫ v ϭ Ϫ 0.600 12gh(t)
¢t
B
B
and by letting ¢t : 0 we obtain the ODE
dh
A
ϭ Ϫ26.56 1h,
dt
B
where 26.56 ϭ 0.60022 ؒ 980. This is our model, a first-order ODE.
Step 2. General solution. Our ODE is separable. A>B is constant. Separation and integration gives
dh
A
ϭ Ϫ26.56 dt
B
1h

and

2 1h ϭ c* Ϫ 26.56

A
t.
B

Dividing by 2 and squaring gives h ϭ (c Ϫ 13.28At>B)2. Inserting 13.28A>B ϭ 13.28 ؒ 0.52p>1002p ϭ 0.000 332
yields the general solution
h(t) ϭ (c Ϫ 0.000 332t)2.

4
EVANGELISTA TORRICELLI (1608–1647), Italian physicist, pupil and successor of GALILEO GALILEI
(1564–1642) at Florence. The “contraction factor” 0.600 was introduced by J. C. BORDA in 1766 because the
stream has a smaller cross section than the area of the hole.

c01.qxd

7/30/10

8:15 PM

Page 17

SEC. 1.3 Separable ODEs. Modeling

17

Step 3. Particular solution. The initial height (the initial condition) is h(0) ϭ 225 cm. Substitution of t ϭ 0
and h ϭ 225 gives from the general solution c2 ϭ 225, c ϭ 15.00 and thus the particular solution (Fig. 13)
h p(t) ϭ (15.00 Ϫ 0.000 332t)2.
Step 4. Tank empty. h p(t) ϭ 0 if t ϭ 15.00>0.000 332 ϭ 45,181 c sec d ϭ 12.6 [hours].
Here you see distinctly the importance of the choice of units—we have been working with the cgs system,
in which time is measured in seconds! We used g ϭ 980 cm>sec2.

᭿

Step 5. Checking. Check the result.

h
250

2.00 m
Water level
at time t

200
150

2.25 m
h(t)

100
50

Outflowing
water

0

0

10000

30000

50000

t

Water level h(t) in tank

Tank

Fig. 13. Example 7. Outflow from a cylindrical tank (“leaking tank”).
Torricelli’s law

Extended Method: Reduction to Separable Form
Certain nonseparable ODEs can be made separable by transformations that introduce for
y a new unknown function. We discuss this technique for a class of ODEs of practical
importance, namely, for equations
y
yr ϭ f a b .
x

(8)

Here, f is any (differentiable) function of y>x, such as sin(y>x), (y>x)4, and so on. (Such
an ODE is sometimes called a homogeneous ODE, a term we shall not use but reserve
for a more important purpose in Sec. 1.5.)
The form of such an ODE suggests that we set y>x ϭ u; thus,
(9)

y ϭ ux

and by product differentiation

y r ϭ u r x ϩ u.

Substitution into y r ϭ f (y>x) then gives u r x ϩ u ϭ f (u) or u r x ϭ f (u) Ϫ u. We see that
if f (u) Ϫ u 0, this can be separated:
(10)

dx
du
ϭ .
x
f (u) Ϫ u

c01.qxd

7/30/10

8:15 PM

18

Page 18

CHAP. 1 First-Order ODEs
EXAMPLE 8

Reduction to Separable Form
Solve
2xyy r ϭ y 2 Ϫ x 2.

Solution. To get the usual explicit form, divide the given equation by 2xy,
yr ϭ

y2 Ϫ x 2
2xy

ϭ

y
2x

x
.
2y

Ϫ

Now substitute y and y r from (9) and then simplify by subtracting u on both sides,
urx ϩ u ϭ

u
1
Ϫ ,
2
2u

urx ϭ Ϫ

u
1
Ϫu 2 Ϫ 1
Ϫ
ϭ
.
2
2u
2u

You see that in the last equation you can now separate the variables,
2u du
1ϩu

2

ϭϪ

dx
.
x

By integration,

1
ln (1 ϩ u 2) ϭ Ϫln ƒ x ƒ ϩ c* ϭ ln ` ` ϩ c*.
x

Take exponents on both sides to get 1 ϩ u 2 ϭ c>x or 1 ϩ (y>x)2 ϭ c>x. Multiply the last equation by x 2 to
obtain (Fig. 14)
c 2
c2
Thus
x 2 ϩ y 2 ϭ cx.
ax Ϫ b ϩ y 2 ϭ .
2
4
This general solution represents a family of circles passing through the origin with centers on the x-axis.

᭿

y
4
2
–8

–4

4

8

x

–2
–4

Fig. 14. General solution (family of circles) in Example 8

PROBLEM SET 1.3
1. CAUTION! Constant of integration. Why is it
important to introduce the constant of integration
immediately when you integrate?
2–10

GENERAL SOLUTION

Find a general solution. Show the steps of derivation. Check
your answer by substitution.
2. y 3y r ϩ x 3 ϭ 0
3. y r ϭ sec 2 y
4. y r sin 2 px ϭ py cos 2 px
5. yy r ϩ 36x ϭ 0
6. y r ϭ e2xϪ1y 2
y
7. xy r ϭ y ϩ 2x 3 sin 2
(Set y>x ϭ u)
x
8. y r ϭ (y ϩ 4x)2 (Set y ϩ 4x ϭ v)
9. xy r ϭ y 2 ϩ y (Set y>x ϭ u)
10. xy r ϭ x ϩ y (Set y>x ϭ u)

11–17

INITIAL VALUE PROBLEMS (IVPS)

Solve the IVP. Show the steps of derivation, beginning with
the general solution.
11. xy r ϩ y ϭ 0,

y(4) ϭ 6

12. y r ϭ 1 ϩ 4y ,

y(1) ϭ 0

2

13. y r cosh x ϭ sin y,
2

14. dr>dt ϭ Ϫ2tr,
15. y r ϭ Ϫ4x>y,

2

y(0) ϭ 12 p

r(0) ϭ r0
y(2) ϭ 3

16. y r ϭ (x ϩ y Ϫ 2)2, y(0) ϭ 2
(Set v ϭ x ϩ y Ϫ 2)
17. xy r ϭ y ϩ 3x 4 cos 2 (y>x),
(Set y>x ϭ u)

y(1) ϭ 0

18. Particular solution. Introduce limits of integration in
(3) such that y obtained from (3) satisfies the initial
condition y(x 0) ϭ y0.

c01.qxd

7/30/10

8:15 PM

Page 19

SEC. 1.3 Separable ODEs. Modeling
19–36

MODELING, APPLICATIONS

19. Exponential growth. If the growth rate of the number
of bacteria at any time t is proportional to the number
present at t and doubles in 1 week, how many bacteria
can be expected after 2 weeks? After 4 weeks?
20. Another population model.
(a) If the birth rate and death rate of the number of
bacteria are proportional to the number of bacteria
present, what is the population as a function of time.
(b) What is the limiting situation for increasing time?
Interpret it.
21. Radiocarbon dating. What should be the 146 C content
(in percent of y0) of a fossilized tree that is claimed to
be 3000 years old? (See Example 4.)
22. Linear accelerators are used in physics for
accelerating charged particles. Suppose that an alpha
particle enters an accelerator and undergoes a constant
acceleration that increases the speed of the particle
from 10 3 m>sec to 10 4 m>sec in 10 ؊3 sec. Find the
acceleration a and the distance traveled during that
period of 10 ؊3 sec.
23. Boyle–Mariotte’s law for ideal gases.5 Experiments
show for a gas at low pressure p (and constant
temperature) the rate of change of the volume V(p)
equals ϪV>p. Solve the model.
24. Mixing problem. A tank contains 400 gal of brine
in which 100 lb of salt are dissolved. Fresh water runs
into the tank at a rate of 2 gal>min.The mixture, kept
practically uniform by stirring, runs out at the same
rate. How much salt will there be in the tank at the
end of 1 hour?
25. Newton’s law of cooling. A thermometer, reading
5°C, is brought into a room whose temperature is 22°C.
One minute later the thermometer reading is 12°C.
How long does it take until the reading is practically
22°C, say, 21.9°C?
26. Gompertz growth in tumors. The Gompertz model
is y r ϭ ϪAy ln y (A Ͼ 0), where y(t) is the mass of
tumor cells at time t. The model agrees well with
clinical observations. The declining growth rate with
increasing y Ͼ 1 corresponds to the fact that cells in
the interior of a tumor may die because of insufficient
oxygen and nutrients. Use the ODE to discuss the
growth and decline of solutions (tumors) and to find
constant solutions. Then solve the ODE.
27. Dryer. If a wet sheet in a dryer loses its moisture at
a rate proportional to its moisture content, and if it
loses half of its moisture during the first 10 min of

19
drying, when will it be practically dry, say, when will
it have lost 99% of its moisture? First guess, then
calculate.
28. Estimation. Could you see, practically without calculation, that the answer in Prob. 27 must lie between
60 and 70 min? Explain.
29. Alibi? Jack, arrested when leaving a bar, claims that
he has been inside for at least half an hour (which
would provide him with an alibi). The police check
the water temperature of his car (parked near the
entrance of the bar) at the instant of arrest and again
30 min later, obtaining the values 190°F and 110°F,
respectively. Do these results give Jack an alibi?
(Solve by inspection.)
30. Rocket. A rocket is shot straight up from the earth,
with a net acceleration (ϭ acceleration by the rocket
engine minus gravitational pullback) of 7t m>sec2
during the initial stage of flight until the engine cut out
at t ϭ 10 sec. How high will it go, air resistance
neglected?
31. Solution curves of y r ‫ ؍‬g1y>x2. Show that any
(nonvertical) straight line through the origin of the
xy-plane intersects all these curves of a given ODE at
the same angle.
32. Friction. If a body slides on a surface, it experiences
friction F (a force against the direction of motion).
Experiments show that ƒ F ƒ ϭ ␮ ƒ N ƒ (Coulomb’s6 law of
kinetic friction without lubrication), where N is the
normal force (force that holds the two surfaces together;
see Fig. 15) and the constant of proportionality ␮ is
called the coefficient of kinetic friction. In Fig. 15
assume that the body weighs 45 nt (about 10 lb; see
front cover for conversion). ␮ ϭ 0.20 (corresponding
to steel on steel), a ϭ 30°, the slide is 10 m long, the
initial velocity is zero, and air resistance is
negligible. Find the velocity of the body at the end
of the slide.

s(t)
Body
v(t)
N
α
W

Fig. 15. Problem 32

5
ROBERT BOYLE (1627–1691), English physicist and chemist, one of the founders of the Royal Society. EDME MARIOTTE (about
1620–1684), French physicist and prior of a monastry near Dijon. They found the law experimentally in 1662 and 1676, respectively.
6
CHARLES AUGUSTIN DE COULOMB (1736–1806), French physicist and engineer.

c01.qxd

7/30/10

20

8:15 PM

Page 20

CHAP. 1 First-Order ODEs

33. Rope. To tie a boat in a harbor, how many times
must a rope be wound around a bollard (a vertical
rough cylindrical post fixed on the ground) so that a
man holding one end of the rope can resist a force
exerted by the boat 1000 times greater than the man
can exert? First guess. Experiments show that the
change ¢S of the force S in a small portion of the
rope is proportional to S and to the small angle ¢␾
in Fig. 16. Take the proportionality constant 0.15.
The result should surprise you!
S

Small
portion
of rope
Δ␾
S + ΔS

Fig. 16. Problem 33
34. TEAM PROJECT. Family of Curves. A family of
curves can often be characterized as the general
solution of y r ϭ f (x, y).
(a) Show that for the circles with center at the origin
we get y r ϭ Ϫx>y.
(b) Graph some of the hyperbolas xy ϭ c. Find an
ODE for them.
(c) Find an ODE for the straight lines through the
origin.
(d) You will see that the product of the right sides of
the ODEs in (a) and (c) equals Ϫ1. Do you recognize

1.4

this as the condition for the two families to be
orthogonal (i.e., to intersect at right angles)? Do your
graphs confirm this?
(e) Sketch families of curves of your own choice and
find their ODEs. Can every family of curves be given
by an ODE?
35. CAS PROJECT. Graphing Solutions. A CAS can
usually graph solutions, even if they are integrals that
cannot be evaluated by the usual analytical methods of
calculus.
(a) Show2 this for the five initial value problems
y r ϭ e؊x , y(0) ϭ 0, Ϯ1, Ϯ2, graphing all five curves
on the same axes.
(b) Graph approximate solution curves, using the first
few terms of the Maclaurin series (obtained by termwise integration of that of y r ) and compare with the
exact curves.
(c) Repeat the work in (a) for another ODE and initial
conditions of your own choice, leading to an integral
that cannot be evaluated as indicated.
36. TEAM PROJECT. Torricelli’s Law. Suppose that
the tank in Example 7 is hemispherical, of radius R,
initially full of water, and has an outlet of 5 cm2 crosssectional area at the bottom. (Make a sketch.) Set
up the model for outflow. Indicate what portion of
your work in Example 7 you can use (so that it can
become part of the general method independent of the
shape of the tank). Find the time t to empty the tank
(a) for any R, (b) for R ϭ 1 m. Plot t as function of
R. Find the time when h ϭ R>2 (a) for any R, (b) for
R ϭ 1 m.

Exact ODEs. Integrating Factors
We recall from calculus that if a function u(x, y) has continuous partial derivatives, its
differential (also called its total differential) is
du ϭ

0u
0u
dx ϩ
dy.
0x
0y

From this it follows that if u(x, y) ϭ c ϭ const, then du ϭ 0.
For example, if u ϭ x ϩ x 2y 3 ϭ c, then
du ϭ (1 ϩ 2xy 3) dx ϩ 3x 2y 2 dy ϭ 0
or
yr ϭ

dy
1 ϩ 2xy 3
ϭϪ
,
dx
3x 2y 2

c01.qxd

7/30/10

8:15 PM

Page 21

SEC. 1.4 Exact ODEs. Integrating Factors

21

an ODE that we can solve by going backward. This idea leads to a powerful solution
method as follows.
A first-order ODE M(x, y) ϩ N(x, y)y r ϭ 0, written as (use dy ϭ y r dx as in Sec. 1.3)
M(x, y) dx ϩ N(x, y) dy ϭ 0

(1)

is called an exact differential equation if the differential form M(x, y) dx ϩ N(x, y) dy
is exact, that is, this form is the differential
du ϭ

(2)

0u
0u
dx ϩ
dy
0x
0y

of some function u(x, y). Then (1) can be written
du ϭ 0.
By integration we immediately obtain the general solution of (1) in the form
u(x, y) ϭ c.

(3)

This is called an implicit solution, in contrast to a solution y ϭ h(x) as defined in Sec.
1.1, which is also called an explicit solution, for distinction. Sometimes an implicit solution
can be converted to explicit form. (Do this for x 2 ϩ y 2 ϭ 1.) If this is not possible, your
CAS may graph a figure of the contour lines (3) of the function u(x, y) and help you in
understanding the solution.
Comparing (1) and (2), we see that (1) is an exact differential equation if there is some
function u(x, y) such that
(4)

(a)

0u
ϭ M,
0x

(b)

0u
ϭ N.
0y

From this we can derive a formula for checking whether (1) is exact or not, as follows.
Let M and N be continuous and have continuous first partial derivatives in a region in
the xy-plane whose boundary is a closed curve without self-intersections. Then by partial
differentiation of (4) (see App. 3.2 for notation),
0M
0 2u
ϭ
,
0y
0y 0x
0 2u
0N
ϭ
.
0x
0x 0y
By the assumption of continuity the two second partial derivaties are equal. Thus

(5)

0N
0M
ϭ
.
0y
0x

c01.qxd

7/30/10

8:15 PM

22

Page 22

CHAP. 1 First-Order ODEs

This condition is not only necessary but also sufficient for (1) to be an exact differential
equation. (We shall prove this in Sec. 10.2 in another context. Some calculus books, for
instance, [GenRef 12], also contain a proof.)
If (1) is exact, the function u(x, y) can be found by inspection or in the following
systematic way. From (4a) we have by integration with respect to x
uϭ

(6)

Ύ M dx ϩ k(y);

in this integration, y is to be regarded as a constant, and k(y) plays the role of a “constant”
of integration. To determine k(y), we derive 0u>0y from (6), use (4b) to get dk>dy, and
integrate dk>dy to get k. (See Example 1, below.)
Formula (6) was obtained from (4a). Instead of (4a) we may equally well use (4b).
Then, instead of (6), we first have by integration with respect to y
uϭ

(6*)

Ύ N dy ϩ l(x).

To determine l(x), we derive 0u>0x from (6*), use (4a) to get dl>dx, and integrate. We
illustrate all this by the following typical examples.

EXAMPLE 1

An Exact ODE
Solve
cos (x ϩ y) dx ϩ (3y 2 ϩ 2y ϩ cos (x ϩ y)) dy ϭ 0.

(7)

Solution. Step 1. Test for exactness. Our equation is of the form (1) with
M ϭ cos (x ϩ y),
N ϭ 3y 2 ϩ 2y ϩ cos (x ϩ y).
Thus
0M
ϭ Ϫsin (x ϩ y),
0y
0N
ϭ Ϫsin (x ϩ y).
0x
From this and (5) we see that (7) is exact.
Step 2. Implicit general solution. From (6) we obtain by integration
(8)

uϭ

Ύ M dx ϩ k(y) ϭ Ύ cos (x ϩ y) dx ϩ k(y) ϭ sin (x ϩ y) ϩ k(y).

To find k(y), we differentiate this formula with respect to y and use formula (4b), obtaining
0u
dk
ϭ cos (x ϩ y) ϩ
ϭ N ϭ 3y 2 ϩ 2y ϩ cos (x ϩ y).
0y
dy
Hence dk>dy ϭ 3y 2 ϩ 2y. By integration, k ϭ y 3 ϩ y 2 ϩ c*. Inserting this result into (8) and observing (3),
we obtain the answer
u(x, y) ϭ sin (x ϩ y) ϩ y 3 ϩ y 2 ϭ c.

c01.qxd

7/30/10

8:15 PM

Page 23

SEC. 1.4 Exact ODEs. Integrating Factors

23

Step 3. Checking an implicit solution. We can check by differentiating the implicit solution u(x, y) ϭ c
implicitly and see whether this leads to the given ODE (7):
(9)

du ϭ

0u
0u
dx ϩ
dy ϭ cos (x ϩ y) dx ϩ (cos (x ϩ y) ϩ 3y 2 ϩ 2y) dy ϭ 0.
0x
0y

᭿

This completes the check.

EXAMPLE 2

An Initial Value Problem
Solve the initial value problem
(cos y sinh x ϩ 1) dx Ϫ sin y cosh x dy ϭ 0,

(10)

y(1) ϭ 2.

Solution. You may verify that the given ODE is exact. We find u. For a change, let us use (6*),

Ύ

u ϭ Ϫ sin y cosh x dy ϩ l(x) ϭ cos y cosh x ϩ l(x).
From this, 0u> 0x ϭ cos y sinh x ϩ dl>dx ϭ M ϭ cos y sinh x ϩ 1. Hence dl>dx ϭ 1. By integration, l(x) ϭ x ϩ c*.
This gives the general solution u(x, y) ϭ cos y cosh x ϩ x ϭ c. From the initial condition, cos 2 cosh 1 ϩ 1 ϭ
0.358 ϭ c. Hence the answer is cos y cosh x ϩ x ϭ 0.358. Figure 17 shows the particular solutions for c ϭ 0, 0.358
(thicker curve), 1, 2, 3. Check that the answer satisfies the ODE. (Proceed as in Example 1.) Also check that the
initial condition is satisfied.
᭿
y
2.5
2.0
1.5
1.0
0.5

0

0.5

1.0

1.5

2.0

2.5

3.0

x

Fig. 17. Particular solutions in Example 2

EXAMPLE 3

WARNING! Breakdown in the Case of Nonexactness
The equation Ϫy dx ϩ x dy ϭ 0 is not exact because M ϭ Ϫy and N ϭ x, so that in (5), 0M> 0y ϭ Ϫ1 but
0N> 0x ϭ 1. Let us show that in such a case the present method does not work. From (6),
uϭ

Ύ M dx ϩ k(y) ϭ Ϫxy ϩ k(y),

hence

dk
0u
ϭ Ϫx ϩ .
0y
dy

Now, 0u> 0y should equal N ϭ x, by (4b). However, this is impossible because k(y) can depend only on y. Try
᭿
(6*); it will also fail. Solve the equation by another method that we have discussed.

Reduction to Exact Form. Integrating Factors
The ODE in Example 3 is Ϫy dx ϩ x dy ϭ 0. It is not exact. However, if we multiply it
by 1>x 2, we get an exact equation [check exactness by (5)!],
(11)

Ϫy dx ϩ x dy
x

2

ϭϪ

y
x

2

dx ϩ

y
1
dy ϭ d a b ϭ 0.
x
x

Integration of (11) then gives the general solution y>x ϭ c ϭ const.

c01.qxd

7/30/10

8:15 PM

24

Page 24

CHAP. 1 First-Order ODEs

This example gives the idea. All we did was to multiply a given nonexact equation, say,
P(x, y) dx ϩ Q(x, y) dy ϭ 0,

(12)

by a function F that, in general, will be a function of both x and y. The result was an equation
FP dx ϩ FQ dy ϭ 0

(13)

that is exact, so we can solve it as just discussed. Such a function F(x, y) is then called
an integrating factor of (12).
EXAMPLE 4

Integrating Factor
The integrating factor in (11) is F ϭ 1>x 2. Hence in this case the exact equation (13) is
FP dx ϩ FQ dy ϭ

Ϫy dx ϩ x dy
x

2

y
ϭ d a b ϭ 0.
x

Solution

y
x

ϭ c.

These are straight lines y ϭ cx through the origin. (Note that x ϭ 0 is also a solution of Ϫy dx ϩ x dy ϭ 0.)
It is remarkable that we can readily find other integrating factors for the equation Ϫy dx ϩ x dy ϭ 0, namely,
1>y 2, 1>(xy), and 1>(x 2 ϩ y 2), because
(14)

Ϫy dx ϩ x dy
y

2

x
ϭ d a b,
y

Ϫy dx ϩ x dy
xy

x
ϭ Ϫd aln b ,
y

Ϫy dx ϩ x dy
x ϩy
2

2

y
ϭ d aarctan b .
x

᭿

How to Find Integrating Factors
In simpler cases we may find integrating factors by inspection or perhaps after some trials,
keeping (14) in mind. In the general case, the idea is the following.
For M dx ϩ N dy ϭ 0 the exactness condition (5) is 0M>0y ϭ 0N>0x. Hence for (13),
FP dx ϩ FQ dy ϭ 0, the exactness condition is
0
0
(FP) ϭ
(FQ).
0y
0x

(15)

By the product rule, with subscripts denoting partial derivatives, this gives
FyP ϩ FPy ϭ FxQ ϩ FQ x.
In the general case, this would be complicated and useless. So we follow the Golden Rule:
If you cannot solve your problem, try to solve a simpler one—the result may be useful
(and may also help you later on). Hence we look for an integrating factor depending only
on one variable: fortunately, in many practical cases, there are such factors, as we shall
see. Thus, let F ϭ F(x). Then Fy ϭ 0, and Fx ϭ F r ϭ dF>dx, so that (15) becomes
FPy ϭ F r Q ϩ FQ x.
Dividing by FQ and reshuffling terms, we have
(16)

1 dF
ϭ R,
F dx

where

Rϭ

0Q
1 0P
a
Ϫ
b.
Q 0y
0x

c01.qxd

7/30/10

8:15 PM

Page 25

SEC. 1.4 Exact ODEs. Integrating Factors

25

This proves the following theorem.
THEOREM 1

Integrating Factor F (x)

If (12) is such that the right side R of (16) depends only on x, then (12) has an
integrating factor F ϭ F(x), which is obtained by integrating (16) and taking
exponents on both sides.

Ύ

F(x) ϭ exp R(x) dx.

(17)

Similarly, if F* ϭ F*(y), then instead of (16) we get
(18)

1 dF*
ϭ R*,
F* dy

1 0Q
0P
a
Ϫ
b
P 0x
0y

R* ϭ

where

and we have the companion
THEOREM 2

Integrating Factor F* (y)

If (12) is such that the right side R* of (18) depends only on y, then (12) has an
integrating factor F* ϭ F*(y), which is obtained from (18) in the form

Ύ

F*(y) ϭ exp R*(y) dy.

(19)

EXAMPLE 5

Application of Theorems 1 and 2. Initial Value Problem
Using Theorem 1 or 2, find an integrating factor and solve the initial value problem
(exϩy ϩ yey) dx ϩ (xey Ϫ 1) dy ϭ 0, y(0) ϭ Ϫ1

(20)

Solution. Step 1. Nonexactness. The exactness check fails:
0P
0 xϩy
ϭ
(e
ϩ yey) ϭ exϩy ϩ ey ϩ yey
0y
0y

0Q
but
0x

ϭ

0
(xey Ϫ 1) ϭ ey.
0x

Step 2. Integrating factor. General solution. Theorem 1 fails because R [the right side of (16)] depends on
both x and y.
Rϭ

0Q
1 0P
1
a
Ϫ
bϭ y
(exϩy ϩ ey ϩ yey Ϫ ey).
Q 0y
0x
xe Ϫ 1

Try Theorem 2. The right side of (18) is
R* ϭ

1 0Q
0P
1
(ey Ϫ exϩy Ϫ ey Ϫ yey) ϭ Ϫ1.
a
Ϫ
b ϭ xϩy
P 0x
0y
e
ϩ yey

Hence (19) gives the integrating factor F*(y) ϭ e؊y. From this result and (20) you get the exact equation
(ex ϩ y) dx ϩ (x Ϫ e؊y) dy ϭ 0.

c01.qxd

7/30/10

26

8:15 PM

Page 26

CHAP. 1 First-Order ODEs
Test for exactness; you will get 1 on both sides of the exactness condition. By integration, using (4a),
uϭ

Ύ (e

x

ϩ y) dx ϭ ex ϩ xy ϩ k(y).

Differentiate this with respect to y and use (4b) to get
dk
0u
ϭxϩ
ϭ N ϭ x Ϫ e؊y,
0y
dy

dk
ϭ Ϫe؊y,
dy

k ϭ e؊y ϩ c*.

Hence the general solution is
u(x, y) ϭ ex ϩ xy ϩ e؊y ϭ c.
Setp 3. Particular solution. The initial condition y(0) ϭ Ϫ1 gives u(0, Ϫ1) ϭ 1 ϩ 0 ϩ e ϭ 3.72. Hence the
answer is ex ϩ xy ϩ e؊y ϭ 1 ϩ e ϭ 3.72. Figure 18 shows several particular solutions obtained as level curves
of u(x, y) ϭ c, obtained by a CAS, a convenient way in cases in which it is impossible or difficult to cast a
solution into explicit form. Note the curve that (nearly) satisfies the initial condition.
Step 4. Checking. Check by substitution that the answer satisfies the given equation as well as the initial
condition.
᭿

y
3
2
1

–3

–2

–1

0

1

2

3

x

–1
–2
–3

Fig. 18.

Particular solutions in Example 5

PROBLEM SET 1.4
1–14

ODEs. INTEGRATING FACTORS

Test for exactness. If exact, solve. If not, use an integrating
factor as given or obtained by inspection or by the theorems
in the text. Also, if an initial condition is given, find the
corresponding particular solution.
1. 2xy dx ϩ x 2 dy ϭ 0
2. x 3dx ϩ y 3dy ϭ 0
3. sin x cos y dx ϩ cos x sin y dy ϭ 0
4. e3u(dr ϩ 3r du) ϭ 0
5. (x 2 ϩ y 2) dx Ϫ 2xy dy ϭ 0
6. 3(y ϩ 1) dx ϭ 2x dy, (y ϩ 1)x ؊4
7. 2x tan y dx ϩ sec 2 y dy ϭ 0

8. ex(cos y dx Ϫ sin y dy) ϭ 0
9. e2x(2 cos y dx Ϫ sin y dy) ϭ 0,

10. y dx ϩ 3y ϩ tan (x ϩ y)4 dy ϭ 0,

y(0) ϭ 0
cos (x ϩ y)

11. 2 cosh x cos y dx ϭ sinh x sin y dy
2

12. (2xy dx ϩ dy)ex ϭ 0,

y(0) ϭ 2

13. e؊y dx ϩ e؊x(Ϫe؊y ϩ 1) dy ϭ 0,
14. (a ϩ 1)y dx ϩ (b ϩ 1)x dy ϭ 0,
F ϭ x ay b

F ϭ exϩy
y(1) ϭ 1,

15. Exactness. Under what conditions for the constants a,
b, k, l is (ax ϩ by) dx ϩ (kx ϩ ly) dy ϭ 0 exact? Solve
the exact ODE.

c01.qxd

7/30/10

8:15 PM

Page 27

SEC. 1.5 Linear ODEs. Bernoulli Equation. Population Dynamics
16. TEAM PROJECT. Solution by Several Methods.
Show this as indicated. Compare the amount of work.
(a) ey(sinh x dx ϩ cosh x dy) ϭ 0 as an exact ODE
and by separation.
(b) (1 ϩ 2x) cos y dx ϩ dy>cos y ϭ 0 by Theorem 2
and by separation.
(c) (x 2 ϩ y 2) dx Ϫ 2xy dy ϭ 0 by Theorem 1 or 2 and
by separation with v ϭ y>x.
(d) 3x 2 y dx ϩ 4x 3 dy ϭ 0 by Theorems 1 and 2 and
by separation.
(e) Search the text and the problems for further ODEs
that can be solved by more than one of the methods
discussed so far. Make a list of these ODEs. Find
further cases of your own.
17. WRITING PROJECT. Working Backward.
Working backward from the solution to the problem
is useful in many areas. Euler, Lagrange, and other
great masters did it. To get additional insight into
the idea of integrating factors, start from a u(x, y) of
your choice, find du ϭ 0, destroy exactness by
division by some F(x, y), and see what ODE’s
solvable by integrating factors you can get. Can you
proceed systematically, beginning with the simplest
F(x, y)?

1.5

27

18. CAS PROJECT. Graphing Particular Solutions.
Graph particular solutions of the following ODE,
proceeding as explained.
(21)
dy Ϫ y 2 sin x dx ϭ 0.
(a) Show that (21) is not exact. Find an integrating
factor using either Theorem 1 or 2. Solve (21).
(b) Solve (21) by separating variables. Is this simpler
than (a)?
(c) Graph the seven particular solutions satisfying the
following initial conditions y(0) ϭ 1, y(p>2) ϭ Ϯ12 ,
Ϯ23 , Ϯ1 (see figure below).
(d) Which solution of (21) do we not get in (a) or (b)?
y
3
2
1
0

π

2π

3π

4π

x

–1
–2
–3

Particular solutions in CAS Project 18

Linear ODEs. Bernoulli Equation.
Population Dynamics
Linear ODEs or ODEs that can be transformed to linear form are models of various
phenomena, for instance, in physics, biology, population dynamics, and ecology, as we
shall see. A first-order ODE is said to be linear if it can be brought into the form
(1)

y r ϩ p(x)y ϭ r(x),

by algebra, and nonlinear if it cannot be brought into this form.
The defining feature of the linear ODE (1) is that it is linear in both the unknown
function y and its derivative y r ϭ dy>dx, whereas p and r may be any given functions of
x. If in an application the independent variable is time, we write t instead of x.
If the first term is f (x)y r (instead of y r ), divide the equation by f (x) to get the standard
form (1), with y r as the first term, which is practical.
For instance, y r cos x ϩ y sin x ϭ x is a linear ODE, and its standard form is
y r ϩ y tan x ϭ x sec x.
The function r(x) on the right may be a force, and the solution y(x) a displacement in
a motion or an electrical current or some other physical quantity. In engineering, r(x) is
frequently called the input, and y(x) is called the output or the response to the input (and,
if given, to the initial condition).

c01.qxd

7/30/10

28

8:15 PM

Page 28

CHAP. 1 First-Order ODEs

Homogeneous Linear ODE. We want to solve (1) in some interval a Ͻ x Ͻ b, call
it J, and we begin with the simpler special case that r(x) is zero for all x in J. (This is
sometimes written r(x) ϵ 0.) Then the ODE (1) becomes
y r ϩ p(x)y ϭ 0

(2)

and is called homogeneous. By separating variables and integrating we then obtain
dy
ϭ Ϫp(x) dx,
y

Ύ

ln ƒ y ƒ ϭ Ϫ p(x) dx ϩ c*.

thus

Taking exponents on both sides, we obtain the general solution of the homogeneous
ODE (2),
(3)

y(x) ϭ ce؊͐p(x) dx

(c ϭ Ϯec*

when

y ѥ 0);

here we may also choose c ϭ 0 and obtain the trivial solution y(x) ϭ 0 for all x in that
interval.
Nonhomogeneous Linear ODE. We now solve (1) in the case that r(x) in (1) is not
everywhere zero in the interval J considered. Then the ODE (1) is called nonhomogeneous.
It turns out that in this case, (1) has a pleasant property; namely, it has an integrating factor
depending only on x. We can find this factor F(x) by Theorem 1 in the previous section
or we can proceed directly, as follows. We multiply (1) by F(x), obtaining
(1*)

Fy r ϩ pFy ϭ rF.

The left side is the derivative (Fy) r ϭ F r y ϩ Fy r of the product Fy if
pFy ϭ F r y,

pF ϭ F r .

thus

By separating variables, dF>F ϭ p dx. By integration, writing h ϭ ͐ p dx,
ln ƒ F ƒ ϭ h ϭ

Ύ p dx,

F ϭ eh.

thus

With this F and h r ϭ p, Eq. (1*) becomes
ehy r ϩ h r ehy ϭ ehy r ϩ (eh) r y ϭ (ehy) r ϭ reh.
By integration,
ehy ϭ

Ύ e r dx ϩ c.
h

Dividing by eh, we obtain the desired solution formula
(4)

Ύ

y(x) ϭ e؊h a ehr dx ϩ cb,

hϭ

Ύ p(x) dx.

c01.qxd

7/30/10

8:15 PM

Page 29

SEC. 1.5 Linear ODEs. Bernoulli Equation. Population Dynamics

29

This reduces solving (1) to the generally simpler task of evaluating integrals. For ODEs
for which this is still difficult, you may have to use a numeric method for integrals from
Sec. 19.5 or for the ODE itself from Sec. 21.1. We mention that h has nothing to do with
h(x) in Sec. 1.1 and that the constant of integration in h does not matter; see Prob. 2.
The structure of (4) is interesting. The only quantity depending on a given initial
condition is c. Accordingly, writing (4) as a sum of two terms,

Ύ

y(x) ϭ e؊h ehr dx ϩ ce؊h,

(4*)
we see the following:
(5)

EXAMPLE 1

Total Output ϭ Response to the Input r ϩ Response to the Initial Data.

First-Order ODE, General Solution, Initial Value Problem
Solve the initial value problem
y r ϩ y tan x ϭ sin 2x,

y(0) ϭ 1.

Solution. Here p ϭ tan x, r ϭ sin 2x ϭ 2 sin x cos x, and
hϭ

Ύ p dx ϭ Ύ tan x dx ϭ ln ƒ sec x ƒ .

From this we see that in (4),
eh ϭ sec x,

e؊h ϭ cos x,

ehr ϭ (sec x)(2 sin x cos x) ϭ 2 sin x,

and the general solution of our equation is

Ύ

y(x) ϭ cos x a2 sin x dx ϩ cb ϭ c cos x Ϫ 2 cos2x.
From this and the initial condition, 1 ϭ c # 1 Ϫ 2 # 12; thus c ϭ 3 and the solution of our initial value problem
is y ϭ 3 cos x Ϫ 2 cos2 x. Here 3 cos x is the response to the initial data, and Ϫ2 cos2 x is the response to the
᭿
input sin 2x.

EXAMPLE 2

Electric Circuit
Model the RL-circuit in Fig. 19 and solve the resulting ODE for the current I(t) A (amperes), where t is
time. Assume that the circuit contains as an EMF E(t) (electromotive force) a battery of E ϭ 48 V (volts), which
is constant, a resistor of R ϭ 11 ⍀ (ohms), and an inductor of L ϭ 0.1 H (henrys), and that the current is initially
zero.

Physical Laws. A current I in the circuit causes a voltage drop RI across the resistor (Ohm’s law) and
a voltage drop LI r ϭ L dI>dt across the conductor, and the sum of these two voltage drops equals the EMF
(Kirchhoff’s Voltage Law, KVL).

Remark. In general, KVL states that “The voltage (the electromotive force EMF) impressed on a closed
loop is equal to the sum of the voltage drops across all the other elements of the loop.” For Kirchoff’s Current
Law (KCL) and historical information, see footnote 7 in Sec. 2.9.

Solution. According to these laws the model of the RL-circuit is LI r ϩ RI ϭ E(t), in standard form
(6)

Ir ϩ

E(t)
R
Iϭ
.
L
L

c01.qxd

7/30/10

8:15 PM

30

Page 30

CHAP. 1 First-Order ODEs
We can solve this linear ODE by (4) with x ϭ t, y ϭ I, p ϭ R>L, h ϭ (R>L)t, obtaining the general solution

Ύ

I ϭ e؊(R>L)t a e(

R>L)t

E(t)
dt ϩ c b.
L

By integration,
I ϭ e؊(R>L)t a

(7)

E e1R>L2t
E
ϩ cb ϭ ϩ ce؊(R>L)t.
L R>L
R

In our case, R>L ϭ 11>0.1 ϭ 110 and E(t) ϭ 48>0.1 ϭ 480 ϭ const; thus,
؊110t
I ϭ 48
.
11 ϩ ce

In modeling, one often gets better insight into the nature of a solution (and smaller roundoff errors) by inserting
given numeric data only near the end. Here, the general solution (7) shows that the current approaches the limit
E>R ϭ 48>11 faster the larger R>L is, in our case, R>L ϭ 11>0.1 ϭ 110, and the approach is very fast, from
below if I(0) Ͻ 48>11 or from above if I(0) Ͼ 48>11. If I(0) ϭ 48>11, the solution is constant (48/11 A). See
Fig. 19.
The initial value I(0) ϭ 0 gives I(0) ϭ E>R ϩ c ϭ 0, c ϭ ϪE>R and the particular solution
Iϭ

(8)

E
(1 Ϫ e؊(R>L)t),
R

thus

Iϭ

48
(1 Ϫ e؊110t).
11

᭿

I(t)
8
R = 11 ⍀
6

4
E = 48 V
2

0
L = 0.1 H
Circuit

0.01

0.02

0.03

0.04

0.05

t

Current I (t)

Fig. 19. RL-circuit

EXAMPLE 3

Hormone Level
Assume that the level of a certain hormone in the blood of a patient varies with time. Suppose that the time rate
of change is the difference between a sinusoidal input of a 24-hour period from the thyroid gland and a continuous
removal rate proportional to the level present. Set up a model for the hormone level in the blood and find its
general solution. Find the particular solution satisfying a suitable initial condition.

Solution. Step 1. Setting up a model. Let y(t) be the hormone level at time t. Then the removal rate is Ky(t).

The input rate is A ϩ B cos vt, where v ϭ 2p>24 ϭ p>12 and A is the average input rate; here A м B to make
the input rate nonnegative. The constants A, B, K can be determined from measurements. Hence the model is the
linear ODE
y r (t) ϭ In Ϫ Out ϭ A ϩ B cos vt Ϫ Ky(t),

thus

y r ϩ Ky ϭ A ϩ B cos vt.

The initial condition for a particular solution ypart is ypart(0) ϭ y0 with t ϭ 0 suitably chosen, for example,
6:00 A.M.
Step 2. General solution. In (4) we have p ϭ K ϭ const, h ϭ Kt, and r ϭ A ϩ B cos vt. Hence (4) gives the
general solution (evaluate ͐ eKt cos vt dt by integration by parts)

c01.qxd

7/30/10

8:15 PM

Page 31

SEC. 1.5 Linear ODEs. Bernoulli Equation. Population Dynamics

31

Ύ

y(t) ϭ e؊Kt eKt aA ϩ B cos vtb dt ϩ ce؊Kt
ϭ e؊KteKt c
ϭ

A
B
ϩ 2
aK cos vt ϩ v sin vtb d ϩ ce؊Kt
K
K ϩ v2

A
B
pt p
pt
ϩ 2
aK cos
ϩ
sin b ϩ ce؊Kt.
K
12
12
12
K ϩ (p>12)2

The last term decreases to 0 as t increases, practically after a short time and regardless of c (that is, of the initial
condition). The other part of y(t) is called the steady-state solution because it consists of constant and periodic
terms. The entire solution is called the transient-state solution because it models the transition from rest to the
steady state. These terms are used quite generally for physical and other systems whose behavior depends on time.
Step 3. Particular solution. Setting t ϭ 0 in y(t) and choosing y0 ϭ 0, we have
y(0) ϭ

A
B
u
ϩ 2
K ϩ c ϭ 0,
K
K ϩ (p>12)2 p

thus

cϭϪ

A
KB
Ϫ 2
.
K
K ϩ (p>12)2

Inserting this result into y(t), we obtain the particular solution
ypart(t) ϭ

A
B
pt p
pt
A
KB
ϩ 2
aK cos
ϩ
sin b Ϫ a ϩ 2
b e؊K
K
12
12
12
K
K ϩ (p>12)2
K ϩ (p>12)2

with the steady-state part as before. To plot ypart we must specify values for the constants, say, A ϭ B ϭ 1
and K ϭ 0.05. Figure 20 shows this solution. Notice that the transition period is relatively short (although
1
pt) ϭ
K is small), and the curve soon looks sinusoidal; this is the response to the input A ϩ B cos (12
1
1 ϩ cos (12
pt).
᭿
y
25
20
15
10
5
0

0

100

200

t

Fig. 20. Particular solution in Example 3

Reduction to Linear Form. Bernoulli Equation
Numerous applications can be modeled by ODEs that are nonlinear but can be transformed
to linear ODEs. One of the most useful ones of these is the Bernoulli equation7
(9)

y r ϩ p(x)y ϭ g(x)y a

(a any real number).

7
JAKOB BERNOULLI (1654–1705), Swiss mathematician, professor at Basel, also known for his contribution
to elasticity theory and mathematical probability. The method for solving Bernoulli’s equation was discovered by
Leibniz in 1696. Jakob Bernoulli’s students included his nephew NIKLAUS BERNOULLI (1687–1759), who
contributed to probability theory and infinite series, and his youngest brother JOHANN BERNOULLI (1667–1748),
who had profound influence on the development of calculus, became Jakob’s successor at Basel, and had among
his students GABRIEL CRAMER (see Sec. 7.7) and LEONHARD EULER (see Sec. 2.5). His son DANIEL
BERNOULLI (1700–1782) is known for his basic work in fluid flow and the kinetic theory of gases.

c01.qxd

7/30/10

8:15 PM

32

Page 32

CHAP. 1 First-Order ODEs

If a ϭ 0 or a ϭ 1, Equation (9) is linear. Otherwise it is nonlinear. Then we set
u(x) ϭ 3y(x)41؊a.
We differentiate this and substitute y r from (9), obtaining
u r ϭ (1 Ϫ a)y ؊ay r ϭ (1 Ϫ a)y ؊a(gy a Ϫ py).
Simplification gives
u r ϭ (1 Ϫ a)(g Ϫ py 1؊a),
where y 1؊a ϭ u on the right, so that we get the linear ODE
u r ϩ (1 Ϫ a)pu ϭ (1 Ϫ a)g.

(10)

For further ODEs reducible to linear form, see lnce’s classic [A11] listed in App. 1. See
also Team Project 30 in Problem Set 1.5.

EXAMPLE 4

Logistic Equation
Solve the following Bernoulli equation, known as the logistic equation (or Verhulst equation8):

y r ϭ Ay Ϫ By 2

(11)

Solution. Write (11) in the form (9), that is,
y r Ϫ Ay ϭ ϪBy 2
to see that a ϭ 2, so that u ϭ y 1؊a ϭ y ؊1. Differentiate this u and substitute y r from (11),
u r ϭ Ϫy ؊2y r ϭ Ϫy ؊2(Ay Ϫ By 2) ϭ B Ϫ Ay Ϫ1.
The last term is ϪAy ؊1 ϭ ϪAu. Hence we have obtained the linear ODE
u r ϩ Au ϭ B.
The general solution is [by (4)]
u ϭ ce؊At ϩ B>A.
Since u ϭ 1>y, this gives the general solution of (11),
(12)

yϭ

1
1
ϭ ؊At
u
ce
ϩ B>A

Directly from (11) we see that y ϵ 0 (y(t) ϭ 0 for all t) is also a solution.

8

(Fig. 21)

᭿

PIERRE-FRANÇOIS VERHULST, Belgian statistician, who introduced Eq. (8) as a model for human
population growth in 1838.

c01.qxd

7/30/10

8:15 PM

Page 33

SEC. 1.5 Linear ODEs. Bernoulli Equation. Population Dynamics

33

Population y
8
6
A
=4
B

2
0

1

2

3

4

Time t

Fig. 21. Logistic population model. Curves (9) in Example 4 with A>B ϭ 4

Population Dynamics
The logistic equation (11) plays an important role in population dynamics, a field
that models the evolution of populations of plants, animals, or humans over time t.
If B ϭ 0, then (11) is y r ϭ dy>dt ϭ Ay. In this case its solution (12) is y ϭ (1>c)eAt
and gives exponential growth, as for a small population in a large country (the
United States in early times!). This is called Malthus’s law. (See also Example 3 in
Sec. 1.1.)
The term ϪBy 2 in (11) is a “braking term” that prevents the population from growing
without bound. Indeed, if we write y r ϭ Ay 31 Ϫ (B>A)y4, we see that if y Ͻ A>B, then
y r Ͼ 0, so that an initially small population keeps growing as long as y Ͻ A>B. But if
y Ͼ A>B, then y r Ͻ 0 and the population is decreasing as long as y Ͼ A>B. The limit
is the same in both cases, namely, A>B. See Fig. 21.
We see that in the logistic equation (11) the independent variable t does not occur
explicitly. An ODE y r ϭ f (t, y) in which t does not occur explicitly is of the form
(13)

y r ϭ f (y)

and is called an autonomous ODE. Thus the logistic equation (11) is autonomous.
Equation (13) has constant solutions, called equilibrium solutions or equilibrium
points. These are determined by the zeros of f (y), because f (y) ϭ 0 gives y r ϭ 0 by
(13); hence y ϭ const. These zeros are known as critical points of (13). An
equilibrium solution is called stable if solutions close to it for some t remain close
to it for all further t. It is called unstable if solutions initially close to it do not remain
close to it as t increases. For instance, y ϭ 0 in Fig. 21 is an unstable equilibrium
solution, and y ϭ 4 is a stable one. Note that (11) has the critical points y ϭ 0 and
y ϭ A>B.

EXAMPLE 5

Stable and Unstable Equilibrium Solutions. “Phase Line Plot”
The ODE y r ϭ (y Ϫ 1)(y Ϫ 2) has the stable equilibrium solution y1 ϭ 1 and the unstable y2 ϭ 2, as the direction
field in Fig. 22 suggests. The values y1 and y2 are the zeros of the parabola f (y) ϭ (y Ϫ 1)(y Ϫ 2) in the figure.
Now, since the ODE is autonomous, we can “condense” the direction field to a “phase line plot” giving y1 and
y2, and the direction (upward or downward) of the arrows in the field, and thus giving information about the
stability or instability of the equilibrium solutions.
᭿

c01.qxd

7/30/10

34

8:15 PM

Page 34

CHAP. 1 First-Order ODEs
y(x)

y

3.0

2.0

2.5
1.5
y2

y2

2.0

1.0

1.5
y1

y1

1.0

0.5
0.5
y2

y1
–2

–1

0

1

2

(a)

Fig. 22.

x

0

(b)

0.5

1.0

1.5

2.0

2.5

3.0

x

(c)

Example 5. (A) Direction field. (B) “Phase line”. (C) Parabola f (y)

A few further population models will be discussed in the problem set. For some more
details of population dynamics, see C. W. Clark. Mathematical Bioeconomics: The
Mathematics of Conservation 3rd ed. Hoboken, NJ, Wiley, 2010.
Further applications of linear ODEs follow in the next section.

PROBLEM SET 1.5
1. CAUTION! Show that e؊ln x ϭ 1>x (not Ϫx) and
e؊ln(sec x) ϭ cos x.
2. Integration constant. Give a reason why in (4) you may
choose the constant of integration in ͐ p dx to be zero.

GENERAL SOLUTION. INITIAL VALUE
3–13
PROBLEMS
Find the general solution. If an initial condition is given,
find also the corresponding particular solution and graph or
sketch it. (Show the details of your work.)
3. y r Ϫ y ϭ 5.2
4. y r ϭ 2y Ϫ 4x
5. y r ϩ ky ϭ e؊kx
6. y r ϩ 2y ϭ 4 cos 2x, y(14 p) ϭ 3
7. xy r ϭ 2y ϩ x 3ex
8. y r ϩ y tan x ϭ e؊0.01x cos x, y(0) ϭ 0
9. y r ϩ y sin x ϭ ecos x, y(0) ϭ Ϫ2.5
10. y r cos x ϩ (3y Ϫ 1) sec x ϭ 0, y(14 p) ϭ 4>3
11. y r ϭ (y Ϫ 2) cot x
12. xy r ϩ 4y ϭ 8x 4, y(1) ϭ 2
13. y r ϭ 6(y Ϫ 2.5) tanh 1.5x

14. CAS EXPERIMENT. (a) Solve the ODE y r Ϫ y>x ϭ
Ϫx ؊1 cos (1>x). Find an initial condition for which the
arbitrary constant becomes zero. Graph the resulting
particular solution, experimenting to obtain a good
figure near x ϭ 0.
(b) Generalizing (a) from n ϭ 1 to arbitrary n, solve the
ODE y r Ϫ ny>x ϭ Ϫx n؊2 cos (1>x). Find an initial
condition as in (a) and experiment with the graph.
15–20

GENERAL PROPERTIES OF LINEAR ODEs

These properties are of practical and theoretical importance
because they enable us to obtain new solutions from given
ones. Thus in modeling, whenever possible, we prefer linear
ODEs over nonlinear ones, which have no similar properties.
Show that nonhomogeneous linear ODEs (1) and homogeneous linear ODEs (2) have the following properties.
Illustrate each property by a calculation for two or three
equations of your choice. Give proofs.
15. The sum y1 ϩ y2 of two solutions y1 and y2 of the
homogeneous equation (2) is a solution of (2), and so is
a scalar multiple ay1 for any constant a. These properties
are not true for (1)!

c01.qxd

7/30/10

10:01 PM

Page 35

SEC. 1.5 Linear ODEs. Bernoulli Equation. Population Dynamics
16. y ϭ 0 (that is, y(x) ϭ 0 for all x, also written y(x) ϵ 0)
is a solution of (2) [not of (1) if r(x) 0!], called the
trivial solution.
17. The sum of a solution of (1) and a solution of (2) is a
solution of (1).
18. The difference of two solutions of (1) is a solution of (2).
19. If y1 is a solution of (1), what can you say about cy1?
20. If y1 and y2 are solutions of y1r ϩ py1 ϭ r1 and
y2r ϩ py2 ϭ r2, respectively (with the same p!), what
can you say about the sum y1 ϩ y2?
21. Variation of parameter. Another method of obtaining
(4) results from the following idea. Write (3) as cy*,
where y* is the exponential function, which is a solution
of the homogeneous linear ODE y* r ϩ py* ϭ 0.
Replace the arbitrary constant c in (3) with a function
u to be determined so that the resulting function y ϭ uy*
is a solution of the nonhomogeneous linear ODE
y r ϩ py ϭ r.

(b) Show that y ϭ Y ϭ x is a solution of the ODE
y r Ϫ (2x 3 ϩ 1) y ϭ Ϫx 2y 2 Ϫ x 4 Ϫ x ϩ 1 and solve this
Riccati equation, showing the details.
(c) Solve the Clairaut equation y r 2 Ϫ xy r ϩ y ϭ 0 as
follows. Differentiate it with respect to x, obtaining
y s (2y r Ϫ x) ϭ 0. Then solve (A) y s ϭ 0 and (B)
2y r Ϫ x ϭ 0 separately and substitute the two solutions
(a) and (b) of (A) and (B) into the given ODE. Thus
obtain (a) a general solution (straight lines) and (b) a
parabola for which those lines (a) are tangents (Fig. 6
in Prob. Set 1.1); so (b) is the envelope of (a). Such a
solution (b) that cannot be obtained from a general
solution is called a singular solution.
(d) Show that the Clairaut equation (15) has as
solutions a family of straight lines y ϭ cx ϩ g(c) and
a singular solution determined by g r (s) ϭ Ϫx, where
s ϭ y r , that forms the envelope of that family.
31–40

22–28

NONLINEAR ODEs

Using a method of this section or separating variables, find
the general solution. If an initial condition is given, find
also the particular solution and sketch or graph it.
22. y r ϩ y ϭ y 2, y(0) ϭ Ϫ13
23. y r ϩ xy ϭ xy ؊1, y(0) ϭ 3
24. y r ϩ y ϭ Ϫx>y
25. y r ϭ 3.2y Ϫ 10y 2
26. y r ϭ (tan y)>(x Ϫ 1), y(0) ϭ 12 p
27. y r ϭ 1>(6ey Ϫ 2x)
28. 2xyy r ϩ (x Ϫ 1)y 2 ϭ x 2ex (Set y 2 ϭ z)
29. REPORT PROJECT. Transformation of ODEs.
We have transformed ODEs to separable form, to exact
form, and to linear form. The purpose of such
transformations is an extension of solution methods to
larger classes of ODEs. Describe the key idea of each
of these transformations and give three typical examples of your choice for each transformation. Show each
step (not just the transformed ODE).
30. TEAM PROJECT. Riccati Equation. Clairaut
Equation. Singular Solution.
A Riccati equation is of the form
(14)

y r ϩ p(x)y ϭ g(x)y 2 ϩ h(x).

A Clairaut equation is of the form
(15)

y ϭ xy r ϩ g(y r ).

(a) Apply the transformation y ϭ Y ϩ 1>u to the
Riccati equation (14), where Y is a solution of (14), and
obtain for u the linear ODE u r ϩ (2Yg Ϫ p)u ϭ Ϫg.
Explain the effect of the transformation by writing it
as y ϭ Y ϩ v, v ϭ 1>u.

35

MODELING. FURTHER APPLICATIONS

31. Newton’s law of cooling. If the temperature of a cake
is 300°F when it leaves the oven and is 200°F ten
minutes later, when will it be practically equal to the
room temperature of 60°F, say, when will it be 61°F?
32. Heating and cooling of a building. Heating and
cooling of a building can be modeled by the ODE
T r ϭ k 1(T Ϫ Ta) ϩ k 2(T Ϫ Tv) ϩ P,
where T ϭ T(t) is the temperature in the building at
time t, Ta the outside temperature, Tw the temperature
wanted in the building, and P the rate of increase of T
due to machines and people in the building, and k 1 and
k 2 are (negative) constants. Solve this ODE, assuming
P ϭ const, Tw ϭ const, and Ta varying sinusoidally
over 24 hours, say, Ta ϭ A Ϫ C cos(2 p>24)t. Discuss
the effect of each term of the equation on the solution.
33. Drug injection. Find and solve the model for drug
injection into the bloodstream if, beginning at t ϭ 0, a
constant amount A g> min is injected and the drug is
simultaneously removed at a rate proportional to the
amount of the drug present at time t.
34. Epidemics. A model for the spread of contagious
diseases is obtained by assuming that the rate of spread
is proportional to the number of contacts between
infected and noninfected persons, who are assumed to
move freely among each other. Set up the model. Find
the equilibrium solutions and indicate their stability or
instability. Solve the ODE. Find the limit of the
proportion of infected persons as t : ϱ and explain
what it means.
35. Lake Erie. Lake Erie has a water volume of about
450 km3 and a flow rate (in and out) of about 175 km2

c01.qxd

7/30/10

36

8:15 PM

Page 36

CHAP. 1 First-Order ODEs
per year. If at some instant the lake has pollution
concentration p ϭ 0.04 %, how long, approximately,
will it take to decrease it to p> 2, assuming that the
inflow is much cleaner, say, it has pollution
concentration p> 4, and the mixture is uniform (an
assumption that is only imperfectly true)? First guess.

36. Harvesting renewable resources. Fishing. Suppose
that the population y(t) of a certain kind of fish is given
by the logistic equation (11), and fish are caught at a
rate Hy proportional to y. Solve this so-called Schaefer
model. Find the equilibrium solutions y1 and y2 (Ͼ 0)
when H Ͻ A. The expression Y ϭ Hy2 is called
the equilibrium harvest or sustainable yield corresponding to H. Why?
37. Harvesting. In Prob. 36 find and graph the solution
satisfying y(0) ϭ 2 when (for simplicity) A ϭ B ϭ 1
and H ϭ 0.2. What is the limit? What does it mean?
What if there were no fishing?
38. Intermittent harvesting. In Prob. 36 assume that you
fish for 3 years, then fishing is banned for the next
3 years. Thereafter you start again. And so on. This is
called intermittent harvesting. Describe qualitatively
how the population will develop if intermitting is
continued periodically. Find and graph the solution for
the first 9 years, assuming that A ϭ B ϭ 1, H ϭ 0.2,
and y(0) ϭ 2.

1.6

y
2
1.8
1.6
1.4
1.2
1
0.8

0

Fig. 23.

2

4

6

8

t

Fish population in Problem 38

39. Extinction vs. unlimited growth. If in a population
y(t) the death rate is proportional to the population, and
the birth rate is proportional to the chance encounters
of meeting mates for reproduction, what will the model
be? Without solving, find out what will eventually
happen to a small initial population. To a large one.
Then solve the model.
40. Air circulation. In a room containing 20,000 ft 3 of air,
600 ft 3of fresh air flows in per minute, and the mixture
(made practically uniform by circulating fans) is
exhausted at a rate of 600 cubic feet per minute (cfm).
What is the amount of fresh air y(t) at any time if
y(0) ϭ 0? After what time will 90% of the air be fresh?

Orthogonal Trajectories. Optional
An important type of problem in physics or geometry is to find a family of curves that
intersects a given family of curves at right angles. The new curves are called orthogonal
trajectories of the given curves (and conversely). Examples are curves of equal
temperature (isotherms) and curves of heat flow, curves of equal altitude (contour lines)
on a map and curves of steepest descent on that map, curves of equal potential
(equipotential curves, curves of equal voltage—the ellipses in Fig. 24) and curves of
electric force (the parabolas in Fig. 24).
Here the angle of intersection between two curves is defined to be the angle between
the tangents of the curves at the intersection point. Orthogonal is another word for
perpendicular.
In many cases orthogonal trajectories can be found using ODEs. In general, if we
consider G(x, y, c) ϭ 0 to be a given family of curves in the xy-plane, then each value of
c gives a particular curve. Since c is one parameter, such a family is called a oneparameter family of curves.
In detail, let us explain this method by a family of ellipses
(1)

1
2

x 2 ϩ y2 ϭ c

(c Ͼ 0)

c01.qxd

7/30/10

8:15 PM

Page 37

SEC. 1.6 Orthogonal Trajectories. Optional

37

and illustrated in Fig. 24. We assume that this family of ellipses represents electric
equipotential curves between the two black ellipses (equipotential surfaces between two
elliptic cylinders in space, of which Fig. 24 shows a cross-section). We seek the
orthogonal trajectories, the curves of electric force. Equation (1) is a one-parameter family
with parameter c. Each value of c (Ͼ 0) corresponds to one of these ellipses.
Step 1. Find an ODE for which the given family is a general solution. Of course, this
ODE must no longer contain the parameter c. Differentiating (1), we have x ϩ 2yy r ϭ 0.
Hence the ODE of the given curves is
y r ϭ f (x, y) ϭ Ϫ

(2)

x
.
2y

y
4

6 x

–6

–4

Fig. 24. Electrostatic field between two ellipses (elliptic cylinders in space):
Elliptic equipotential curves (equipotential surfaces) and orthogonal
trajectories (parabolas)

Step 2.

Find an ODE for the orthogonal trajectories yෂ ϭ yෂ(x). This ODE is
ෂ
yr ϭ Ϫ

(3)

ෂ
2y
1
ϭ
ϩ
x
f (x, ෂ
y)

with the same f as in (2). Why? Well, a given curve passing through a point (x 0, y0) has
slope f (x 0, y0) at that point, by (2). The trajectory through (x 0, y0) has slope Ϫ1>f (x 0, y0)
by (3). The product of these slopes is Ϫ1, as we see. From calculus it is known that this
is the condition for orthogonality (perpendicularity) of two straight lines (the tangents at
(x 0, y0)), hence of the curve and its orthogonal trajectory at (x 0, y0).
Step 3.

Solve (3) by separating variables, integrating, and taking exponents:
d yෂ
dx
ϭ2 ,
x
yෂ

ln ƒ yෂ ƒ ϭ 2 ln x ϩ c,

ෂ
y ϭ c* x 2.

This is the family of orthogonal trajectories, the quadratic parabolas along which electrons
or other charged particles (of very small mass) would move in the electric field between
the black ellipses (elliptic cylinders).

c01.qxd

7/30/10

8:15 PM

38

Page 38

CHAP. 1 First-Order ODEs

PROBLEM SET 1.6
1–3

FAMILIES OF CURVES

Represent the given family of curves in the form
G(x, y; c) ϭ 0 and sketch some of the curves.
1. All ellipses with foci Ϫ3 and 3 on the x-axis.
2. All circles with centers on the cubic parabola y ϭ x 3
and passing through the origin (0, 0).
3. The catenaries obtained by translating the catenary
y ϭ cosh x in the direction of the straight line y ϭ x.
4–10

Fig. 25.

ORTHOGONAL TRAJECTORIES (OTs)

Sketch or graph some of the given curves. Guess what their
OTs may look like. Find these OTs.
4. y ϭ x 2 ϩ c
5. y ϭ cx
6. xy ϭ c

7. y ϭ c>x 2

8. y ϭ 2x ϩ c

9. y ϭ ce؊x

2

10. x 2 ϩ (y Ϫ c)2 ϭ c2
11–16

APPLICATIONS, EXTENSIONS

11. Electric field. Let the electric equipotential lines
(curves of constant potential) between two concentric
cylinders with the z-axis in space be given by
u(x, y) ϭ x 2 ϩ y 2 ϭ c (these are circular cylinders in
the xyz-space). Using the method in the text, find their
orthogonal trajectories (the curves of electric force).
12. Electric field. The lines of electric force of two opposite
charges of the same strength at (Ϫ1, 0) and (1, 0) are
the circles through (Ϫ1, 0) and (1, 0) . Show that these
circles are given by x 2 ϩ (y Ϫ c)2 ϭ 1 ϩ c2. Show
that the equipotential lines (which are orthogonal
trajectories of those circles) are the circles given by
(x ϩ c*)2 ϩ yෂ 2 ϭ c* 2 Ϫ 1 (dashed in Fig. 25).

1.7

Electric field in Problem 12

13. Temperature field. Let the isotherms (curves of
constant temperature) in a body in the upper half-plane
y Ͼ 0 be given by 4x 2 ϩ 9y 2 ϭ c. Find the orthogonal trajectories (the curves along which heat will
flow in regions filled with heat-conducting material and
free of heat sources or heat sinks).
14. Conic sections. Find the conditions under which
the orthogonal trajectories of families of ellipses
x 2>a 2 ϩ y 2>b 2 ϭ c are again conic sections. Illustrate
your result graphically by sketches or by using your
CAS. What happens if a : 0? If b : 0?
15. Cauchy–Riemann equations. Show that for a family
u(x, y) ϭ c ϭ const the orthogonal trajectories v(x, y) ϭ
c* ϭ const can be obtained from the following
Cauchy–Riemann equations (which are basic in
complex analysis in Chap. 13) and use them to find the
orthogonal trajectories of ex sin y ϭ const. (Here, subscripts denote partial derivatives.)
u x ϭ vy,

u y ϭ Ϫvx

16. Congruent OTs. If y r ϭ f (x) with f independent of y,
show that the curves of the corresponding family are
congruent, and so are their OTs.

Existence and Uniqueness of Solutions
for Initial Value Problems
The initial value problem
ƒ y r ƒ ϩ ƒ y ƒ ϭ 0,

y(0) ϭ 1

has no solution because y ϭ 0 (that is, y(x) ϭ 0 for all x) is the only solution of the ODE.
The initial value problem
y r ϭ 2x,

y(0) ϭ 1

c01.qxd

7/30/10

8:15 PM

Page 39

SEC. 1.7 Existence and Uniqueness of Solutions

39

has precisely one solution, namely, y ϭ x 2 ϩ 1. The initial value problem
xy r ϭ y Ϫ 1,

y(0) ϭ 1

has infinitely many solutions, namely, y ϭ 1 ϩ cx, where c is an arbitrary constant because
y(0) ϭ 1 for all c.
From these examples we see that an initial value problem
y r ϭ f (x, y),

(1)

y(x 0) ϭ y0

may have no solution, precisely one solution, or more than one solution. This fact leads
to the following two fundamental questions.
Problem of Existence

Under what conditions does an initial value problem of the form (1) have at least
one solution (hence one or several solutions)?
Problem of Uniqueness

Under what conditions does that problem have at most one solution (hence excluding
the case that is has more than one solution)?

Theorems that state such conditions are called existence theorems and uniqueness
theorems, respectively.
Of course, for our simple examples, we need no theorems because we can solve these
examples by inspection; however, for complicated ODEs such theorems may be of
considerable practical importance. Even when you are sure that your physical or other
system behaves uniquely, occasionally your model may be oversimplified and may not
give a faithful picture of reality.
THEOREM 1

Existence Theorem

Let the right side f (x, y) of the ODE in the initial value problem
(1)

y r ϭ f (x, y),

y(x 0) ϭ y0

be continuous at all points (x, y) in some rectangle
R: ƒ x Ϫ x 0 ƒ Ͻ a,

ƒ y Ϫ y0 ƒ Ͻ b

(Fig. 26)

and bounded in R; that is, there is a number K such that
(2)

ƒ f (x, y) ƒ Ϲ K

for all (x, y) in R.

Then the initial value problem (1) has at least one solution y(x). This solution exists
at least for all x in the subinterval ƒ x Ϫ x 0 ƒ Ͻ a of the interval ƒ x Ϫ x 0 ƒ Ͻ a;
here, a is the smaller of the two numbers a and b> K.

c01.qxd

7/30/10

8:15 PM

40

Page 40

CHAP. 1 First-Order ODEs
y
y0 + b

R
y0
y0 – b

x0 – a

Fig. 26.

x0

x0 + a

x

Rectangle R in the existence and uniqueness theorems

(Example of Boundedness. The function f (x, y) ϭ x 2 ϩ y 2 is bounded (with K ϭ 2) in the
square ƒ x ƒ Ͻ 1, ƒ y ƒ Ͻ 1. The function f (x, y) ϭ tan (x ϩ y) is not bounded for
ƒ x ϩ y ƒ Ͻ p>2. Explain!)
THEOREM 2

Uniqueness Theorem

Let f and its partial derivative fy ϭ 0f>0y be continuous for all (x, y) in the rectangle
R (Fig. 26) and bounded, say,
(3)

(a)

ƒ f (x, y) ƒ Ϲ K,

(b)

ƒ fy(x, y) ƒ Ϲ M

for all (x, y) in R.

Then the initial value problem (1) has at most one solution y(x). Thus, by Theorem 1,
the problem has precisely one solution. This solution exists at least for all x in that
subinterval ƒ x Ϫ x 0 ƒ Ͻ a.

Understanding These Theorems
These two theorems take care of almost all practical cases. Theorem 1 says that if f (x, y)
is continuous in some region in the xy-plane containing the point (x 0, y0), then the initial
value problem (1) has at least one solution.
Theorem 2 says that if, moreover, the partial derivative 0f>0y of f with respect to y
exists and is continuous in that region, then (1) can have at most one solution; hence, by
Theorem 1, it has precisely one solution.
Read again what you have just read—these are entirely new ideas in our discussion.
Proofs of these theorems are beyond the level of this book (see Ref. [A11] in App. 1);
however, the following remarks and examples may help you to a good understanding of
the theorems.
Since y r ϭ f (x, y), the condition (2) implies that ƒ y r ƒ Ϲ K; that is, the slope of any
solution curve y(x) in R is at least ϪK and at most K. Hence a solution curve that passes
through the point (x 0, y0) must lie in the colored region in Fig. 27 bounded by the lines
l 1 and l 2 whose slopes are ϪK and K, respectively. Depending on the form of R, two
different cases may arise. In the first case, shown in Fig. 27a, we have b>K м a and
therefore a ϭ a in the existence theorem, which then asserts that the solution exists for all
x between x 0 Ϫ a and x 0 ϩ a. In the second case, shown in Fig. 27b, we have b>K Ͻ a.
Therefore, a ϭ b>K Ͻ a, and all we can conclude from the theorems is that the solution

c01.qxd

7/30/10

8:15 PM

Page 41

SEC. 1.7 Existence and Uniqueness of Solutions

41

exists for all x between x 0 Ϫ b>K and x 0 ϩ b>K. For larger or smaller x’s the solution
curve may leave the rectangle R, and since nothing is assumed about f outside R, nothing
can be concluded about the solution for those larger or amaller x’s; that is, for such x’s
the solution may or may not exist—we don’t know.
y

y

y0 + b

R

l1

l1
y0 + b

y0

R

y0
y0 – b

l2

l2

α

y0 – b

α=a

α=a

α

a

a

x

x0

x0

(a)

x

(b)

Fig. 27. The condition (2) of the existence theorem. (a) First case. (b) Second case

Let us illustrate our discussion with a simple example. We shall see that our choice of
a rectangle R with a large base (a long x-interval) will lead to the case in Fig. 27b.
EXAMPLE 1

Choice of a Rectangle
Consider the initial value problem
y r ϭ 1 ϩ y 2,

y(0) ϭ 0

and take the rectangle R; ƒ x ƒ Ͻ 5, ƒ y ƒ Ͻ 3. Then a ϭ 5, b ϭ 3, and
ƒ f (x, y) ƒ ϭ ƒ 1 ϩ y 2 ƒ Ϲ K ϭ 10,
`

0f
0y

` ϭ 2 ƒ y ƒ Ϲ M ϭ 6,

aϭ

b
ϭ 0.3 Ͻ a.
K

Indeed, the solution of the problem is y ϭ tan x (see Sec. 1.3, Example 1). This solution is discontinuous at
Ϯp>2, and there is no continuous solution valid in the entire interval ƒ x ƒ Ͻ 5 from which we started.
᭿

The conditions in the two theorems are sufficient conditions rather than necessary ones,
and can be lessened. In particular, by the mean value theorem of differential calculus we
have
f (x, y2) Ϫ f (x, y1) ϭ (y2 Ϫ y1)

0f
`
0y yϭyෂ

y is a suitable value between y1
where (x, y1) and (x, y2) are assumed to be in R, and ෂ
and y2. From this and (3b) it follows that
(4)

ƒ f (x, y2) Ϫ f (x, y1) ƒ Ϲ M ƒ y2 Ϫ y1 ƒ .

c01.qxd

7/30/10

8:15 PM

42

Page 42

CHAP. 1 First-Order ODEs

It can be shown that (3b) may be replaced by the weaker condition (4), which is known
as a Lipschitz condition.9 However, continuity of f (x, y) is not enough to guarantee the
uniqueness of the solution. This may be illustrated by the following example.
EXAMPLE 2

Nonuniqueness
The initial value problem
yr ϭ 2 ƒ y ƒ .

y(0) ϭ 0

has the two solutions
yϭ0

y* ϭ e

and

x 2> 4 if
Ϫx 2>4 if

xм0
x Ͻ 0

although f (x, y) ϭ 2 ƒ y ƒ is continuous for all y. The Lipschitz condition (4) is violated in any region that includes
the line y ϭ 0, because for y1 ϭ 0 and positive y2 we have
(5)

ƒ f (x, y2) Ϫ f (x, y1) ƒ
ƒ y2 Ϫ y1 ƒ

ϭ

2y2
y2

ϭ

1
2y2

( 2y2 Ͼ 0)

,

and this can be made as large as we please by choosing y2 sufficiently small, whereas (4) requires that the
᭿
quotient on the left side of (5) should not exceed a fixed constant M.

PROBLEM SET 1.7
1. Linear ODE. If p and r in y r ϩ p(x)y ϭ r(x) are
continuous for all x in an interval ƒ x Ϫ x 0 ƒ Յ a, show
that f (x, y) in this ODE satisfies the conditions of our
present theorems, so that a corresponding initial value
problem has a unique solution. Do you actually need
these theorems for this ODE?
2. Existence? Does the initial value problem
(x Ϫ 2)y r ϭ y, y(2) ϭ 1 have a solution? Does your
result contradict our present theorems?
3. Vertical strip. If the assumptions of Theorems 1 and
2 are satisfied not merely in a rectangle but in a vertical
infinite strip ƒ x Ϫ x 0 ƒ Ͻ a, in what interval will the
solution of (1) exist?
4. Change of initial condition. What happens in Prob.
2 if you replace y(2) ϭ 1 with y(2) ϭ k?
5. Length of x-interval. In most cases the solution of an
initial value problem (1) exists in an x-interval larger than
that guaranteed by the present theorems. Show this fact
for y r ϭ 2y 2, y(1) ϭ 1 by finding the best possible a

9

(choosing b optimally) and comparing the result with the
actual solution.
6. CAS PROJECT. Picard Iteration. (a) Show that by
integrating the ODE in (1) and observing the initial
condition you obtain
x

(6)

y(x) ϭ y0 ϩ

Ύ f (t, y(t)) dt.
x0

This form (6) of (1) suggests Picard’s Iteration Method10
which is defined by
x

(7) yn(x) ϭ y0 ϩ

Ύ f (t, y

n؊1(t)

dt, n ϭ 1, 2, Á .

x0

It gives approximations y1, y2, y3, . . . of the unknown
solution y of (1). Indeed, you obtain y1 by substituting
y ϭ y0 on the right and integrating—this is the first
step—then y2 by substituting y ϭ y1 on the right and
integrating—this is the second step—and so on. Write

RUDOLF LIPSCHITZ (1832–1903), German mathematician. Lipschitz and similar conditions are important
in modern theories, for instance, in partial differential equations.
10
EMILE PICARD (1856–1941). French mathematician, also known for his important contributions to
complex analysis (see Sec. 16.2 for his famous theorem). Picard used his method to prove Theorems 1 and 2
as well as the convergence of the sequence (7) to the solution of (1). In precomputer times, the iteration was of
little practical value because of the integrations.

c01.qxd

7/30/10

8:15 PM

Page 43

Chapter 1 Review Questions and Problems
a program of the iteration that gives a printout of the
first approximations y0, y1, . . . , yN as well as their
graphs on common axes. Try your program on two
initial value problems of your own choice.
(b) Apply the iteration to y r ϭ x ϩ y, y(0) ϭ 0. Also
solve the problem exactly.
(c) Apply the iteration to y r ϭ 2y 2, y(0) ϭ 1. Also
solve the problem exactly.
(d) Find all solutions of y r ϭ 2 1y, y(1) ϭ 0. Which
of them does Picard’s iteration approximate?
(e) Experiment with the conjecture that Picard’s
iteration converges to the solution of the problem for
any initial choice of y in the integrand in (7) (leaving
y0 outside the integral as it is). Begin with a simple ODE
and see what happens. When you are reasonably sure,
take a slightly more complicated ODE and give it a try.

43
7. Maximum A. What is the largest possible a in
Example 1 in the text?
8. Lipschitz condition. Show that for a linear ODE
y r ϩ p(x)y ϭ r(x) with continuous p and r in
ƒ x Ϫ x 0 ƒ Ϲ a a Lipschitz condition holds. This is
remarkable because it means that for a linear ODE the
continuity of f (x, y) guarantees not only the existence
but also the uniqueness of the solution of an initial
value problem. (Of course, this also follows directly
from (4) in Sec. 1.5.)
9. Common points. Can two solution curves of the same
ODE have a common point in a rectangle in which the
assumptions of the present theorems are satisfied?
10. Three possible cases. Find all initial conditions such
that (x 2 Ϫ x)y r ϭ (2x Ϫ 1)y has no solution, precisely
one solution, and more than one solution.

CHAPTER 1 REVIEW QUESTIONS AND PROBLEMS
1. Explain the basic concepts ordinary and partial
differential equations (ODEs, PDEs), order, general
and particular solutions, initial value problems (IVPs).
Give examples.
2. What is a linear ODE? Why is it easier to solve than
a nonlinear ODE?
3. Does every first-order ODE have a solution? A solution
formula? Give examples.
4. What is a direction field? A numeric method for firstorder ODEs?
5. What is an exact ODE? Is f (x) dx ϩ g(y) dy ϭ 0
always exact?
6. Explain the idea of an integrating factor. Give two
examples.
7. What other solution methods did we consider in this
chapter?
8. Can an ODE sometimes be solved by several methods?
Give three examples.
9. What does modeling mean? Can a CAS solve a model
given by a first-order ODE? Can a CAS set up a model?
10. Give problems from mechanics, heat conduction, and
population dynamics that can be modeled by first-order
ODEs.
11–16

14. xy r ϭ y ϩ x 2
15. y r ϩ y ϭ 1.01 cos 10x
16. Solve y r ϭ y Ϫ y 2, y(0) ϭ 0.2 by Euler’s method
(10 steps, h ϭ 0.1). Solve exactly and compute the error.
17–21

GENERAL SOLUTION

Find the general solution. Indicate which method in this
chapter you are using. Show the details of your work.
17. y r ϩ 2.5y ϭ 1.6x
18. y r Ϫ 0.4y ϭ 29 sin x
19. 25yy r Ϫ 4x ϭ 0
20. y r ϭ ay ϩ by 2 (a 0)
21. (3xey ϩ 2y) dx ϩ (x 2ey ϩ x) dy ϭ 0
22–26

INITIAL VALUE PROBLEM (IVP)

Solve the IVP. Indicate the method used. Show the details
of your work.
2
22. y r ϩ 4xy ϭ eϪ2x , y(0) ϭ Ϫ4.3
23. y r ϭ 21 Ϫ y 2, y(0) ϭ 1> 12
24. y r ϩ 12 y ϭ y 3, y(0) ϭ 13
25. 3 sec y dx ϩ 13 sec x dy ϭ 0, y(0) ϭ 0
26. x sinh y dy ϭ cosh y dx, y(3) ϭ 0

DIRECTION FIELD: NUMERIC SOLUTION

Graph a direction field (by a CAS or by hand) and sketch
some solution curves. Solve the ODE exactly and compare.
In Prob. 16 use Euler’s method.
11. y r ϩ 2y ϭ 0
12. y r ϭ 1 Ϫ y 2
13. y r ϭ y Ϫ 4y 2

27–30

MODELING, APPLICATIONS

27. Exponential growth. If the growth rate of a culture
of bacteria is proportional to the number of bacteria
present and after 1 day is 1.25 times the original
number, within what interval of time will the number
of bacteria (a) double, (b) triple?

c01.qxd

7/30/10

8:15 PM

Page 44

44

CHAP. 1 First-Order ODEs

28. Mixing problem. The tank in Fig. 28 contains 80 lb
of salt dissolved in 500 gal of water. The inflow per
minute is 20 lb of salt dissolved in 20 gal of water. The
outflow is 20 gal> min of the uniform mixture. Find the
time when the salt content y(t) in the tank reaches 95%
of its limiting value (as t : ϱ ).

Fig. 28.

29. Half-life. If in a reactor, uranium 237
97 U loses 10% of
its weight within one day, what is its half-life? How
long would it take for 99% of the original amount to
disappear?
30. Newton’s law of cooling. A metal bar whose
temperature is 20°C is placed in boiling water. How
long does it take to heat the bar to practically 100°C,
say, to 99.9°C, if the temperature of the bar after 1 min
of heating is 51.5°C? First guess, then calculate.

Tank in Problem 28

SUMMARY OF CHAPTER

1

First-Order ODEs
This chapter concerns ordinary differential equations (ODEs) of first order and
their applications. These are equations of the form
(1)

F(x, y, y r ) ϭ 0

or in explicit form

y r ϭ f (x, y)

involving the derivative y r ϭ dy>dx of an unknown function y, given functions of
x, and, perhaps, y itself. If the independent variable x is time, we denote it by t.
In Sec. 1.1 we explained the basic concepts and the process of modeling, that is,
of expressing a physical or other problem in some mathematical form and solving
it. Then we discussed the method of direction fields (Sec. 1.2), solution methods
and models (Secs. 1.3–1.6), and, finally, ideas on existence and uniqueness of
solutions (Sec. 1.7).
A first-order ODE usually has a general solution, that is, a solution involving an
arbitrary constant, which we denote by c. In applications we usually have to find a
unique solution by determining a value of c from an initial condition y(x 0) ϭ y0.
Together with the ODE this is called an initial value problem
(2)

y r ϭ f (x, y),

y(x 0) ϭ y0

(x 0, y0 given numbers)

and its solution is a particular solution of the ODE. Geometrically, a general
solution represents a family of curves, which can be graphed by using direction
fields (Sec. 1.2). And each particular solution corresponds to one of these curves.
A separable ODE is one that we can put into the form
(3)

g(y) dy ϭ f (x) dx

(Sec. 1.3)

by algebraic manipulations (possibly combined with transformations, such as
y>x ϭ u) and solve by integrating on both sides.

c01.qxd

7/30/10

8:15 PM

Page 45

Summary of Chapter 1

45

An exact ODE is of the form
(4)

M(x, y) dx ϩ N(x, y) dy ϭ 0

(Sec. 1.4)

where M dx ϩ N dy is the differential
du ϭ u x dx ϩ u y dy
of a function u(x, y), so that from du ϭ 0 we immediately get the implicit general
solution u(x, y) ϭ c. This method extends to nonexact ODEs that can be made exact
by multiplying them by some function F(x, y,), called an integrating factor (Sec. 1.4).
Linear ODEs
(5)

y r ϩ p(x)y ϭ r(x)

are very important. Their solutions are given by the integral formula (4), Sec. 1.5.
Certain nonlinear ODEs can be transformed to linear form in terms of new variables.
This holds for the Bernoulli equation
y r ϩ p(x)y ϭ g(x)y a

(Sec. 1.5).

Applications and modeling are discussed throughout the chapter, in particular in
Secs. 1.1, 1.3, 1.5 (population dynamics, etc.), and 1.6 (trajectories).
Picard’s existence and uniqueness theorems are explained in Sec. 1.7 (and
Picard’s iteration in Problem Set 1.7).
Numeric methods for first-order ODEs can be studied in Secs. 21.1 and 21.2
immediately after this chapter, as indicated in the chapter opening.

c02.qxd

10/27/10

6:06 PM

Page 46

CHAPTER

2

Second-Order Linear ODEs
Many important applications in mechanical and electrical engineering, as shown in Secs.
2.4, 2.8, and 2.9, are modeled by linear ordinary differential equations (linear ODEs) of the
second order. Their theory is representative of all linear ODEs as is seen when compared
to linear ODEs of third and higher order, respectively. However, the solution formulas for
second-order linear ODEs are simpler than those of higher order, so it is a natural progression
to study ODEs of second order first in this chapter and then of higher order in Chap. 3.
Although ordinary differential equations (ODEs) can be grouped into linear and nonlinear
ODEs, nonlinear ODEs are difficult to solve in contrast to linear ODEs for which many
beautiful standard methods exist.
Chapter 2 includes the derivation of general and particular solutions, the latter in
connection with initial value problems.
For those interested in solution methods for Legendre’s, Bessel’s, and the hypergeometric
equations consult Chap. 5 and for Sturm–Liouville problems Chap. 11.
COMMENT. Numerics for second-order ODEs can be studied immediately after this
chapter. See Sec. 21.3, which is independent of other sections in Chaps. 19–21.
Prerequisite: Chap. 1, in particular, Sec. 1.5.
Sections that may be omitted in a shorter course: 2.3, 2.9, 2.10.
References and Answers to Problems: App. 1 Part A, and App. 2.

2.1

Homogeneous Linear ODEs of Second Order
We have already considered first-order linear ODEs (Sec. 1.5) and shall now define and
discuss linear ODEs of second order. These equations have important engineering
applications, especially in connection with mechanical and electrical vibrations (Secs. 2.4,
2.8, 2.9) as well as in wave motion, heat conduction, and other parts of physics, as we
shall see in Chap. 12.
A second-order ODE is called linear if it can be written
(1)

y s ϩ p(x)y r ϩ q(x)y ϭ r(x)

and nonlinear if it cannot be written in this form.
The distinctive feature of this equation is that it is linear in y and its derivatives, whereas
the functions p, q, and r on the right may be any given functions of x. If the equation
begins with, say, f (x)y s, then divide by f (x) to have the standard form (1) with y s as the
first term.
46

c02.qxd

10/27/10

6:06 PM

Page 47

SEC. 2.1 Homogeneous Linear ODEs of Second Order

47

The definitions of homogeneous and nonhomogenous second-order linear ODEs are
very similar to those of first-order ODEs discussed in Sec. 1.5. Indeed, if r(x) ϵ 0 (that
is, r(x) ϭ 0 for all x considered; read “r(x) is identically zero”), then (1) reduces to
y s ϩ p(x)y r ϩ q(x)y ϭ 0

(2)

and is called homogeneous. If r(x) [ 0, then (1) is called nonhomogeneous. This is
similar to Sec. 1.5.
An example of a nonhomogeneous linear ODE is
y s ϩ 25y ϭ e؊x cos x,
and a homogeneous linear ODE is
xy s ϩ y r ϩ xy ϭ 0,

written in standard form

1
y s ϩ x y r ϩ y ϭ 0.

Finally, an example of a nonlinear ODE is
y s y ϩ y r 2 ϭ 0.
The functions p and q in (1) and (2) are called the coefficients of the ODEs.
Solutions are defined similarly as for first-order ODEs in Chap. 1. A function
y ϭ h(x)
is called a solution of a (linear or nonlinear) second-order ODE on some open interval I
if h is defined and twice differentiable throughout that interval and is such that the ODE
becomes an identity if we replace the unknown y by h, the derivative y r by h r , and the
second derivative y s by h s . Examples are given below.

Homogeneous Linear ODEs: Superposition Principle
Sections 2.1–2.6 will be devoted to homogeneous linear ODEs (2) and the remaining
sections of the chapter to nonhomogeneous linear ODEs.
Linear ODEs have a rich solution structure. For the homogeneous equation the backbone
of this structure is the superposition principle or linearity principle, which says that we
can obtain further solutions from given ones by adding them or by multiplying them with
any constants. Of course, this is a great advantage of homogeneous linear ODEs. Let us
first discuss an example.

EXAMPLE 1

Homogeneous Linear ODEs: Superposition of Solutions
The functions y ϭ cos x and y ϭ sin x are solutions of the homogeneous linear ODE
ys ϩ y ϭ 0
for all x. We verify this by differentiation and substitution. We obtain (cos x) s ϭ Ϫcos x; hence
y s ϩ y ϭ (cos x) s ϩ cos x ϭ Ϫcos x ϩ cos x ϭ 0.

c02.qxd

10/27/10

6:06 PM

48

Page 48

CHAP. 2 Second-Order Linear ODEs
Similarly for y ϭ sin x (verify!). We can go an important step further. We multiply cos x by any constant, for
instance, 4.7, and sin x by, say, Ϫ2, and take the sum of the results, claiming that it is a solution. Indeed,
differentiation and substitution gives
(4.7 cos x Ϫ 2 sin x) s ϩ (4.7 cos x Ϫ 2 sin x) ϭ Ϫ4.7 cos x ϩ 2 sin x ϩ 4.7 cos x Ϫ 2 sin x ϭ 0.

᭿

In this example we have obtained from y1 (ϭ cos x) and y2 (ϭ sin x) a function of the form
y ϭ c1y1 ϩ c2y2

(3)

(c1, c2 arbitrary constants).

This is called a linear combination of y1 and y2. In terms of this concept we can now
formulate the result suggested by our example, often called the superposition principle
or linearity principle.
THEOREM 1

Fundamental Theorem for the Homogeneous Linear ODE (2)

For a homogeneous linear ODE (2), any linear combination of two solutions on an
open interval I is again a solution of (2) on I. In particular, for such an equation,
sums and constant multiples of solutions are again solutions.

PROOF

Let y1 and y2 be solutions of (2) on I. Then by substituting y ϭ c1 y1 ϩ c2 y2 and
its derivatives into (2), and using the familiar rule (c1 y1 ϩ c2 y2) r ϭ c1 y1r ϩ c2 y 2r , etc.,
we get
y s ϩ py r ϩ qy ϭ (c1 y1 ϩ c2 y2) s ϩ p(c1 y1 ϩ c2 y2) r ϩ q(c1 y1 ϩ c2 y2)
ϭ c1 y1s ϩ c2 y s2 ϩ p(c1 y1r ϩ c2 y2r ) ϩ q(c1 y1 ϩ c2 y2)
ϭ c1( y1s ϩ py1r ϩ qy1) ϩ c2(y2s ϩ py 2r ϩ qy2) ϭ 0,
since in the last line, ( Á ) ϭ 0 because y1 and y2 are solutions, by assumption. This shows
that y is a solution of (2) on I.
᭿
CAUTION! Don’t forget that this highly important theorem holds for homogeneous
linear ODEs only but does not hold for nonhomogeneous linear or nonlinear ODEs, as
the following two examples illustrate.

EXAMPLE 2

A Nonhomogeneous Linear ODE
Verify by substitution that the functions y ϭ 1 ϩ cos x and y ϭ 1 ϩ sin x are solutions of the nonhomogeneous
linear ODE
y s ϩ y ϭ 1,
but their sum is not a solution. Neither is, for instance, 2(1 ϩ cos x) or 5(1 ϩ sin x).

EXAMPLE 3

᭿

A Nonlinear ODE
Verify by substitution that the functions y ϭ x 2 and y ϭ 1 are solutions of the nonlinear ODE
y s y Ϫ xy r ϭ 0,
but their sum is not a solution. Neither is Ϫx 2, so you cannot even multiply by Ϫ1!

᭿

c02.qxd

10/27/10

6:06 PM

Page 49

SEC. 2.1 Homogeneous Linear ODEs of Second Order

49

Initial Value Problem. Basis. General Solution
Recall from Chap. 1 that for a first-order ODE, an initial value problem consists of the
ODE and one initial condition y(x 0) ϭ y0. The initial condition is used to determine the
arbitrary constant c in the general solution of the ODE. This results in a unique solution,
as we need it in most applications. That solution is called a particular solution of the
ODE. These ideas extend to second-order ODEs as follows.
For a second-order homogeneous linear ODE (2) an initial value problem consists of
(2) and two initial conditions
y(x 0) ϭ K 0,

(4)

y r (x 0) ϭ K 1.

These conditions prescribe given values K 0 and K 1 of the solution and its first derivative
(the slope of its curve) at the same given x ϭ x 0 in the open interval considered.
The conditions (4) are used to determine the two arbitrary constants c1 and c2 in a
general solution
y ϭ c1 y1 ϩ c2 y2

(5)

of the ODE; here, y1 and y2 are suitable solutions of the ODE, with “suitable” to be
explained after the next example. This results in a unique solution, passing through the
point (x 0, K 0) with K 1 as the tangent direction (the slope) at that point. That solution is
called a particular solution of the ODE (2).
EXAMPLE 4

Initial Value Problem
Solve the initial value problem
y s ϩ y ϭ 0,

y(0) ϭ 3.0,

y r (0) ϭ Ϫ0.5.

Solution. Step 1. General solution. The functions cos x and sin x are solutions of the ODE (by Example 1),
and we take
y

y ϭ c1 cos x ϩ c2 sin x.

3
2

This will turn out to be a general solution as defined below.

1

Step 2. Particular solution. We need the derivative y r ϭ Ϫc1 sin x ϩ c2 cos x. From this and the
initial values we obtain, since cos 0 ϭ 1 and sin 0 ϭ 0,

0

2

4

6

8

10

–1
–2

x

y(0) ϭ c1 ϭ 3.0

and

y r (0) ϭ c2 ϭ Ϫ0.5.

This gives as the solution of our initial value problem the particular solution

–3

Fig. 29. Particular solution
and initial tangent in
Example 4

y ϭ 3.0 cos x Ϫ 0.5 sin x.
Figure 29 shows that at x ϭ 0 it has the value 3.0 and the slope Ϫ0.5, so that its tangent intersects
᭿
the x-axis at x ϭ 3.0>0.5 ϭ 6.0 . (The scales on the axes differ!)

Observation. Our choice of y1 and y2 was general enough to satisfy both initial
conditions. Now let us take instead two proportional solutions y1 ϭ cos x and y2 ϭ k cos x,
so that y1/y2 ϭ 1/k ϭ const. Then we can write y ϭ c1 y1 ϩ c2 y2 in the form
y ϭ c1 cos x ϩ c2(k cos x) ϭ C cos x

where

C ϭ c1 ϩ c2k.

c02.qxd

10/27/10

6:06 PM

50

Page 50

CHAP. 2 Second-Order Linear ODEs

Hence we are no longer able to satisfy two initial conditions with only one arbitrary
constant C. Consequently, in defining the concept of a general solution, we must exclude
proportionality. And we see at the same time why the concept of a general solution is of
importance in connection with initial value problems.

DEFINITION

General Solution, Basis, Particular Solution

A general solution of an ODE (2) on an open interval I is a solution (5) in which
y1 and y2 are solutions of (2) on I that are not proportional, and c1 and c2 are arbitrary
constants. These y1, y2 are called a basis (or a fundamental system) of solutions
of (2) on I.
A particular solution of (2) on I is obtained if we assign specific values to c1
and c2 in (5).

For the definition of an interval see Sec. 1.1. Furthermore, as usual, y1 and y2 are called
proportional on I if for all x on I,
(6)

(a)

y1 ϭ ky2

or

(b)

y2 ϭ ly1

where k and l are numbers, zero or not. (Note that (a) implies (b) if and only if k 0).
Actually, we can reformulate our definition of a basis by using a concept of general
importance. Namely, two functions y1 and y2 are called linearly independent on an
interval I where they are defined if
(7)

k 1y1(x) ϩ k 2y2(x) ϭ 0

everywhere on I implies

k 1 ϭ 0 and k 2 ϭ 0.

And y1 and y2 are called linearly dependent on I if (7) also holds for some constants k 1,
k 2 not both zero. Then, if k 1 0 or k 2 0, we can divide and see that y1 and y2 are
proportional,
y1 ϭ Ϫ

k2
y2
k1

or

y2 ϭ Ϫ

k1
y1.
k2

In contrast, in the case of linear independence these functions are not proportional because
then we cannot divide in (7). This gives the following

DEFINITION

Basis (Reformulated)

A basis of solutions of (2) on an open interval I is a pair of linearly independent
solutions of (2) on I.

If the coefficients p and q of (2) are continuous on some open interval I, then (2) has a
general solution. It yields the unique solution of any initial value problem (2), (4). It
includes all solutions of (2) on I; hence (2) has no singular solutions (solutions not
obtainable from of a general solution; see also Problem Set 1.1). All this will be shown
in Sec. 2.6.

c02.qxd

10/27/10

6:06 PM

Page 51

SEC. 2.1 Homogeneous Linear ODEs of Second Order
EXAMPLE 5

51

Basis, General Solution, Particular Solution
cos x and sin x in Example 4 form a basis of solutions of the ODE y s ϩ y ϭ 0 for all x because their
quotient is cot x const (or tan x const). Hence y ϭ c1 cos x ϩ c2 sin x is a general solution. The solution
y ϭ 3.0 cos x Ϫ 0.5 sin x of the initial value problem is a particular solution.
᭿

EXAMPLE 6

Basis, General Solution, Particular Solution
Verify by substitution that y1 ϭ ex and y2 ϭ e؊x are solutions of the ODE y s Ϫ y ϭ 0. Then solve the initial
value problem
y s Ϫ y ϭ 0,

y(0) ϭ 6,

y r (0) ϭ Ϫ2.

Solution. (ex) s Ϫ ex ϭ 0 and (e؊x) s Ϫ e؊x ϭ 0 show that ex and e؊x are solutions. They are not
proportional, ex/e؊x ϭ e2x const. Hence ex, e؊x form a basis for all x. We now write down the corresponding
general solution and its derivative and equate their values at 0 to the given initial conditions,
y ϭ c1ex ϩ c2e؊x,

y r ϭ c1ex Ϫ c2e؊x,

y(0) ϭ c1 ϩ c2 ϭ 6,

y r (0) ϭ c1 Ϫ c2 ϭ Ϫ2.

By addition and subtraction, c1 ϭ 2, c2 ϭ 4, so that the answer is y ϭ 2ex ϩ 4e؊x. This is the particular solution
satisfying the two initial conditions.
᭿

Find a Basis if One Solution Is Known.
Reduction of Order
It happens quite often that one solution can be found by inspection or in some other way.
Then a second linearly independent solution can be obtained by solving a first-order ODE.
This is called the method of reduction of order.1 We first show how this method works
in an example and then in general.
EXAMPLE 7

Reduction of Order if a Solution Is Known. Basis
Find a basis of solutions of the ODE
(x 2 Ϫ x)y s Ϫ xy r ϩ y ϭ 0.

Solution. Inspection shows that y1 ϭ x is a solution because y1r ϭ 1 and y s1 ϭ 0, so that the first term
vanishes identically and the second and third terms cancel. The idea of the method is to substitute
y ϭ uy1 ϭ ux,

y r ϭ u r x ϩ u,

y s ϭ u s x ϩ 2u r

into the ODE. This gives
(x 2 Ϫ x)(u s x ϩ 2u r ) Ϫ x(u r x ϩ u) ϩ ux ϭ 0.
ux and –xu cancel and we are left with the following ODE, which we divide by x, order, and simplify,
(x 2 Ϫ x)(u s x ϩ 2u r ) Ϫ x 2u r ϭ 0,

(x 2 Ϫ x)u s ϩ (x Ϫ 2)u r ϭ 0.

This ODE is of first order in v ϭ u r , namely, (x 2 Ϫ x)v r ϩ (x Ϫ 2)v ϭ 0. Separation of variables and integration
gives
dv
1
2
xϪ2
dx ϭ a
Ϫ b dx,
ϭϪ 2
x Ϫx
xϪ1
v
x
1

ln ƒ v ƒ ϭ ln ƒ x Ϫ 1 ƒ Ϫ 2 ln ƒ x ƒ ϭ ln

ƒx Ϫ 1ƒ
.
x2

Credited to the great mathematician JOSEPH LOUIS LAGRANGE (1736–1813), who was born in Turin,
of French extraction, got his first professorship when he was 19 (at the Military Academy of Turin), became
director of the mathematical section of the Berlin Academy in 1766, and moved to Paris in 1787. His important
major work was in the calculus of variations, celestial mechanics, general mechanics (Mécanique analytique,
Paris, 1788), differential equations, approximation theory, algebra, and number theory.

c02.qxd

10/27/10

52

6:06 PM

Page 52

CHAP. 2 Second-Order Linear ODEs
We need no constant of integration because we want to obtain a particular solution; similarly in the next
integration. Taking exponents and integrating again, we obtain
vϭ

xϪ1
1
1
ϭ Ϫ 2,
x
x2
x

uϭ

Ύ v dx ϭ ln ƒ x ƒ ϩ x ,
1

hence

y2 ϭ ux ϭ x ln ƒ x ƒ ϩ 1.

Since y1 ϭ x and y2 ϭ x ln ƒ x ƒ ϩ 1 are linearly independent (their quotient is not constant), we have obtained
a basis of solutions, valid for all positive x.
᭿

In this example we applied reduction of order to a homogeneous linear ODE [see (2)]
y s ϩ p(x)y r ϩ q(x)y ϭ 0.
Note that we now take the ODE in standard form, with y s, not f (x)y s—this is essential
in applying our subsequent formulas. We assume a solution y1 of (2), on an open interval
I, to be known and want to find a basis. For this we need a second linearly independent
solution y2 of (2) on I. To get y2, we substitute
y ϭ y2 ϭ uy1,

y r ϭ y2r ϭ u r y1 ϩ uy1r ,

y s ϭ y2s ϭ u s y1 ϩ 2u r y1r ϩ uy s1

into (2). This gives
(8)

u s y1 ϩ 2u r y1r ϩ uy s1 ϩ p(u r y1 ϩ uy1r ) ϩ quy1 ϭ 0.

Collecting terms in u s, u r, and u, we have
u s y1 ϩ u r (2y1r ϩ py1) ϩ u(y1s ϩ py 1r ϩ qy1) ϭ 0.
Now comes the main point. Since y1 is a solution of (2), the expression in the last
parentheses is zero. Hence u is gone, and we are left with an ODE in u r and u s . We divide
this remaining ODE by y1 and set u r ϭ U, u s ϭ U r,
us ϩ ur

2y1r ϩ py1
ϭ 0,
y1

2y 1r
U r ϩ a y ϩ pb U ϭ 0.
1

thus

This is the desired first-order ODE, the reduced ODE. Separation of variables and
integration gives
2y1r
dU
ϭ Ϫa
ϩ pb dx
y1
U

and

ln ƒ U ƒ ϭ Ϫ2 ln ƒ y1 ƒ Ϫ

Ύ p dx.

By taking exponents we finally obtain
(9)

Uϭ

1 ؊͐p dx
e
.
y 21

Here U ϭ u r, so that u ϭ ͐ U dx. Hence the desired second solution is

Ύ

y2 ϭ y1u ϭ y1 U dx.
The quotient y2 /y1 ϭ u ϭ ͐ U dx cannot be constant (since U Ͼ 0), so that y1 and y2 form
a basis of solutions.

c02.qxd

11/9/10

7:21 PM

Page 53

SEC. 2.2 Homogeneous Linear ODEs with Constant Coefficients

53

PROBLEM SET 2.1
REDUCTION OF ORDER is important because it
gives a simpler ODE. A general second-order ODE
F (x, y, y r , y s ) ϭ 0, linear or not, can be reduced to first
order if y does not occur explicitly (Prob. 1) or if x does not
occur explicitly (Prob. 2) or if the ODE is homogeneous
linear and we know a solution (see the text).
1. Reduction. Show that F (x, y r, y s ) ϭ 0 can be
reduced to first order in z ϭ y r (from which y follows
by integration). Give two examples of your own.
2. Reduction. Show that F ( y, y r, y s ) ϭ 0 can be
reduced to a first-order ODE with y as the independent
variable and y s ϭ (dz/dy)z, where z ϭ y r; derive this
by the chain rule. Give two examples.
3–10

REDUCTION OF ORDER

Reduce to first order and solve, showing each step in detail.
3. y s ϩ y r ϭ 0
4. 2xy s ϭ 3y r
5. yy s ϭ 3y r 2
6. xy s ϩ 2y r ϩ xy ϭ 0, y1 ϭ (cos x)/x
7. y s ϩ y r 3 sin y ϭ 0
8. y s ϭ 1 ϩ y r 2
9. x 2y s Ϫ 5xy r ϩ 9y ϭ 0, y1 ϭ x 3
10. y s ϩ (1 ϩ 1/y)y r 2 ϭ 0
11–14

APPLICATIONS OF REDUCIBLE ODEs

11. Curve. Find the curve through the origin in the
xy-plane which satisfies y s ϭ 2y r and whose tangent
at the origin has slope 1.
12. Hanging cable. It can be shown that the curve y(x)
of an inextensible flexible homogeneous cable hanging
between two fixed points is obtained by solving

2.2

y s ϭ k 21 ϩ y r 2, where the constant k depends on the
weight. This curve is called catenary (from Latin
catena = the chain). Find and graph y(x), assuming that
k ϭ 1 and those fixed points are (Ϫ1, 0) and (1, 0) in
a vertical xy-plane.
13. Motion. If, in the motion of a small body on a
straight line, the sum of velocity and acceleration equals
a positive constant, how will the distance y(t) depend
on the initial velocity and position?
14. Motion. In a straight-line motion, let the velocity be
the reciprocal of the acceleration. Find the distance y(t)
for arbitrary initial position and velocity.
15–19

GENERAL SOLUTION. INITIAL VALUE
PROBLEM (IVP)

(More in the next set.) (a) Verify that the given functions
are linearly independent and form a basis of solutions of
the given ODE. (b) Solve the IVP. Graph or sketch the
solution.
15. 4y s ϩ 25y ϭ 0, y(0) ϭ 3.0, y r (0) ϭ Ϫ2.5,
cos 2.5x, sin 2.5x
16. y s ϩ 0.6y r ϩ 0.09y ϭ 0, y(0) ϭ 2.2, y r (0) ϭ 0.14,
e؊0.3x, xe؊0.3x
17. 4x 2y s Ϫ 3y ϭ 0, y(1) ϭ Ϫ3, y r (1) ϭ 0,
x 3>2, x ؊1>2
18. x 2y s Ϫ xy r ϩ y ϭ 0, y(1) ϭ 4.3, y r (1) ϭ 0.5,
x, x ln x
19. y s ϩ 2y r ϩ 2y ϭ 0, y(0) ϭ 0, y r (0) ϭ 15,
e؊x cos x, e؊x sin x
20. CAS PROJECT. Linear Independence. Write a
program for testing linear independence and dependence. Try it out on some of the problems in this and
the next problem set and on examples of your own.

Homogeneous Linear ODEs
with Constant Coefficients
We shall now consider second-order homogeneous linear ODEs whose coefficients a and
b are constant,
(1)

y s ϩ ay r ϩ by ϭ 0.

These equations have important applications in mechanical and electrical vibrations, as
we shall see in Secs. 2.4, 2.8, and 2.9.
To solve (1), we recall from Sec. 1.5 that the solution of the first-order linear ODE with
a constant coefficient k
y r ϩ ky ϭ 0

c02.qxd

10/27/10

54

6:06 PM

Page 54

CHAP. 2 Second-Order Linear ODEs

is an exponential function y ϭ ce؊kx. This gives us the idea to try as a solution of (1) the
function
y ϭ elx.

(2)
Substituting (2) and its derivatives
y r ϭ lelx

and

y s ϭ l2elx

into our equation (1), we obtain
(l2 ϩ al ϩ b)elx ϭ 0.
Hence if l is a solution of the important characteristic equation (or auxiliary equation)
(3)

l2 ϩ al ϩ b ϭ 0

then the exponential function (2) is a solution of the ODE (1). Now from algebra we recall
that the roots of this quadratic equation (3) are
(4)

l1 ϭ 12 AϪa ϩ 2a 2 Ϫ 4b B ,

l2 ϭ 12 AϪa Ϫ 2a 2 Ϫ 4b B .

(3) and (4) will be basic because our derivation shows that the functions
(5)

y1 ϭ el1x

and

y2 ϭ el2x

are solutions of (1). Verify this by substituting (5) into (1).
From algebra we further know that the quadratic equation (3) may have three kinds of
roots, depending on the sign of the discriminant a 2 Ϫ 4b, namely,

(Case I)
Two real roots if a 2 Ϫ 4b Ͼ 0,
(Case II) A real double root if a 2 Ϫ 4b ϭ 0,
(Case III) Complex conjugate roots if a 2 Ϫ 4b Ͻ 0.

Case I. Two Distinct Real-Roots l1 and l2
In this case, a basis of solutions of (1) on any interval is
y1 ϭ el1x

and

y2 ϭ el2x

because y1 and y2 are defined (and real) for all x and their quotient is not constant. The
corresponding general solution is
(6)

y ϭ c1el1x ϩ c2el2x.

c02.qxd

10/27/10

6:06 PM

Page 55

SEC. 2.2 Homogeneous Linear ODEs with Constant Coefficients
EXAMPLE 1

55

General Solution in the Case of Distinct Real Roots
We can now solve y s Ϫ y ϭ 0 in Example 6 of Sec. 2.1 systematically. The characteristic equation is
l2 Ϫ 1 ϭ 0. Its roots are l1 ϭ 1 and l2 ϭ Ϫ1. Hence a basis of solutions is ex and e؊x and gives the same
general solution as before,

᭿

y ϭ c1ex ϩ c2e؊x.

EXAMPLE 2

Initial Value Problem in the Case of Distinct Real Roots
Solve the initial value problem
y s ϩ y r Ϫ 2y ϭ 0,

y(0) ϭ 4,

y r (0) ϭ Ϫ5.

Solution. Step 1. General solution. The characteristic equation is
l2 ϩ l Ϫ 2 ϭ 0.
Its roots are
l1 ϭ 12 (Ϫ1 ϩ 19 ) ϭ 1

and

l2 ϭ 12 (Ϫ1 Ϫ 19) ϭ Ϫ2

so that we obtain the general solution
y ϭ c1ex ϩ c2e؊2x.
Step 2. Particular solution. Since y r (x) ϭ c1ex Ϫ 2c2e؊2x, we obtain from the general solution and the initial
conditions
y(0) ϭ c1 ϩ c2 ϭ 4,
y r (0) ϭ c1 Ϫ 2c2 ϭ Ϫ5.
Hence c1 ϭ 1 and c2 ϭ 3. This gives the answer y ϭ ex ϩ 3e؊2x. Figure 30 shows that the curve begins at
y ϭ 4 with a negative slope (Ϫ5, but note that the axes have different scales!), in agreement with the initial
conditions.
᭿
y
8
6
4
2
0
0

0.5

1

1.5

2

x

Fig. 30. Solution in Example 2

Case II. Real Double Root l ϭ Ϫa/2
If the discriminant a 2 Ϫ 4b is zero, we see directly from (4) that we get only one root,
l ϭ l1 ϭ l2 ϭ Ϫa/2, hence only one solution,
y1 ϭ e؊(a/2)x.
To obtain a second independent solution y2 (needed for a basis), we use the method of
reduction of order discussed in the last section, setting y2 ϭ uy1. Substituting this and its
derivatives y r2 ϭ u r y1 ϩ uy 1r and y s2 into (1), we first have
(u sy1 ϩ 2u r y 1r ϩ uy s1) ϩ a(u r y1 ϩ uy 1r ) ϩ buy1 ϭ 0.

c02.qxd

10/27/10

6:06 PM

56

Page 56

CHAP. 2 Second-Order Linear ODEs

Collecting terms in u s, u r, and u, as in the last section, we obtain
u s y1 ϩ u r (2y 1r ϩ ay1) ϩ u(y s1 ϩ ay 1r ϩ by1) ϭ 0.
The expression in the last parentheses is zero, since y1 is a solution of (1). The expression
in the first parentheses is zero, too, since
2y 1r ϭ Ϫae؊ax/2 ϭ Ϫay1.
We are thus left with u s y1 ϭ 0. Hence u s ϭ 0. By two integrations, u ϭ c1x ϩ c2. To
get a second independent solution y2 ϭ uy1, we can simply choose c1 ϭ 1, c2 ϭ 0 and
take u ϭ x. Then y2 ϭ xy1. Since these solutions are not proportional, they form a basis.
Hence in the case of a double root of (3) a basis of solutions of (1) on any interval is
e؊ax/2,

xe؊ax/2.

The corresponding general solution is
y ϭ (c1 ϩ c2x)e؊ax/2.

(7)

WARNING! If l is a simple root of (4), then (c1 ϩ c2x)elx with c2
of (1).
EXAMPLE 3

0 is not a solution

General Solution in the Case of a Double Root
The characteristic equation of the ODE y s ϩ 6y r ϩ 9y ϭ 0 is l2 ϩ 6l ϩ 9 ϭ (l ϩ 3)2 ϭ 0. It has the double
root l ϭ Ϫ3. Hence a basis is e؊3x and xe؊3x. The corresponding general solution is y ϭ (c1 ϩ c2x)e؊3x. ᭿

EXAMPLE 4

Initial Value Problem in the Case of a Double Root
Solve the initial value problem
y s ϩ y r ϩ 0.25y ϭ 0,

y(0) ϭ 3.0,

y r (0) ϭ Ϫ3.5.

Solution. The characteristic equation is l ϩ l ϩ 0.25 ϭ (l ϩ 0.5) 2 ϭ 0. It has the double root l ϭ Ϫ0.5.
2

This gives the general solution
y ϭ (c1 ϩ c2x)e؊0.5x.
We need its derivative
y r ϭ c2e؊0.5x Ϫ 0.5(c1 ϩ c2x)e؊0.5x.
From this and the initial conditions we obtain
y(0) ϭ c1 ϭ 3.0,

y r (0) ϭ c2 Ϫ 0.5c1 ϭ 3.5;

The particular solution of the initial value problem is y ϭ (3 Ϫ 2x)e

c2 ϭ Ϫ2.

hence
؊0.5x

. See Fig. 31.

y
3
2
1
0

2

4

6

8

10

12

–1

Fig. 31. Solution in Example 4

14

x

᭿

c02.qxd

10/27/10

6:06 PM

Page 57

SEC. 2.2 Homogeneous Linear ODEs with Constant Coefficients

57

Case III. Complex Roots Ϫ21 a ϩ iv and Ϫ21 a Ϫ iv
This case occurs if the discriminant a 2 Ϫ 4b of the characteristic equation (3) is negative.
In this case, the roots of (3) are the complex l ϭ Ϫ 12 a Ϯ iv that give the complex solutions
of the ODE (1). However, we will show that we can obtain a basis of real solutions
(8)

y1 ϭ e؊ax/2 cos vx,

y2 ϭ e؊ax/2 sin vx

(v Ͼ 0)

where v2 ϭ b Ϫ 14 a 2. It can be verified by substitution that these are solutions in the
present case. We shall derive them systematically after the two examples by using the
complex exponential function. They form a basis on any interval since their quotient
cot vx is not constant. Hence a real general solution in Case III is
y ϭ e؊ax/2 (A cos vx ϩ B sin vx)

(9)
EXAMPLE 5

(A, B arbitrary).

Complex Roots. Initial Value Problem
Solve the initial value problem
y s ϩ 0.4y r ϩ 9.04y ϭ 0,

y(0) ϭ 0,

y r (0) ϭ 3.

Solution. Step 1. General solution. The characteristic equation is l2 ϩ 0.4l ϩ 9.04 ϭ 0. It has the roots
Ϫ0.2 Ϯ 3i. Hence v ϭ 3, and a general solution (9) is

y ϭ e؊0.2x (A cos 3x ϩ B sin 3x).
Step 2. Particular solution. The first initial condition gives y(0) ϭ A ϭ 0. The remaining expression is
y ϭ Be؊0.2x sin 3x. We need the derivative (chain rule!)
y r ϭ B(Ϫ0.2e؊0.2x sin 3x ϩ 3e؊0.2x cos 3x).
From this and the second initial condition we obtain y r (0) ϭ 3B ϭ 3. Hence B ϭ 1. Our solution is
y ϭ e؊0.2x sin 3x.
Figure 32 shows y and the curves of e؊0.2x and Ϫe؊0.2x (dashed), between which the curve of y oscillates.
Such “damped vibrations” (with x ϭ t being time) have important mechanical and electrical applications, as we
shall soon see (in Sec. 2.4).
᭿
y
1.0
0.5

0

5

10

15

20

25

30

x

–0.5
–1.0

Fig. 32.

EXAMPLE 6

Solution in Example 5

Complex Roots
A general solution of the ODE
y s ϩ v2y ϭ 0

(v constant, not zero)

is
y ϭ A cos vx ϩ B sin vx.
With v ϭ 1 this confirms Example 4 in Sec. 2.1.

᭿

c02.qxd

10/27/10

58

6:06 PM

Page 58

CHAP. 2 Second-Order Linear ODEs

Summary of Cases I–III
Case

Roots of (2)

Basis of (1)

General Solution of (1)

I

Distinct real
l1, l2

el1x, el2x

y ϭ c1el1x ϩ c2el2x

II

Real double root
l ϭ Ϫ12 a

e؊ax>2, xe؊ax>2

y ϭ (c1 ϩ c2x)e؊ax>2

III

Complex conjugate
l1 ϭ Ϫ12 a ϩ iv,
l2 ϭ Ϫ12 a Ϫ iv

e؊ax>2 cos vx

y ϭ e؊ax>2(A cos vx ϩ B sin vx)

e

؊ax>2

sin vx

It is very interesting that in applications to mechanical systems or electrical circuits,
these three cases correspond to three different forms of motion or flows of current,
respectively. We shall discuss this basic relation between theory and practice in detail in
Sec. 2.4 (and again in Sec. 2.8).

Derivation in Case III. Complex Exponential Function
If verification of the solutions in (8) satisfies you, skip the systematic derivation of these
real solutions from the complex solutions by means of the complex exponential function
ez of a complex variable z ϭ r ϩ it. We write r ϩ it, not x ϩ iy because x and y occur
in the ODE. The definition of ez in terms of the real functions er, cos t, and sin t is
(10)

ez ϭ erϩit ϭ ereit ϭ er(cos t ϩ i sin t).

This is motivated as follows. For real z ϭ r, hence t ϭ 0, cos 0 ϭ 1, sin 0 ϭ 0, we get
the real exponential function er. It can be shown that ez1ϩz2 ϭ ez1ez2, just as in real. (Proof
in Sec. 13.5.) Finally, if we use the Maclaurin series of ez with z ϭ it as well as
i 2 ϭ Ϫ1, i 3 ϭ Ϫi, i 4 ϭ 1, etc., and reorder the terms as shown (this is permissible, as
can be proved), we obtain the series
eit ϭ 1 ϩ it ϩ
ϭ1Ϫ

(it)2
(it)3
(it)4
(it) 5 Á
ϩ
ϩ
ϩ
ϩ
2!
3!
4!
5!

t2
t4
t3
t5
ϩ
Ϫ ϩ Á ϩ i at Ϫ
ϩ
Ϫ ϩ Áb
2!
4!
3!
5!

ϭ cos t ϩ i sin t.
(Look up these real series in your calculus book if necessary.) We see that we have obtained
the formula
(11)

eit ϭ cos t ϩ i sin t,

called the Euler formula. Multiplication by er gives (10).

c02.qxd

10/27/10

6:06 PM

Page 59

SEC. 2.2 Homogeneous Linear ODEs with Constant Coefficients

59

For later use we note that e؊it ϭ cos (Ϫt) ϩ i sin (Ϫt) ϭ cos t Ϫ i sin t, so that by
addition and subtraction of this and (11),
cos t ϭ 12 (eit ϩ e؊it),

(12)

sin t ϭ

1 it
(e Ϫ e؊it).
2i

After these comments on the definition (10), let us now turn to Case III.
In Case III the radicand a 2 Ϫ 4b in (4) is negative. Hence 4b Ϫ a 2 is positive and,
using 1Ϫ1 ϭ i, we obtain in (4)
1
2
2 2a

Ϫ 4b ϭ 12 2Ϫ(4b Ϫ a 2) ϭ 2Ϫ(b Ϫ 14 a 2) ϭ i 2b Ϫ 14 a 2 ϭ iv

with v defined as in (8). Hence in (4),
l1 ϭ 12 a ϩ iv

and, similarly,

l2 ϭ 12 a Ϫ iv.

Using (10) with r ϭ Ϫ12 ax and t ϭ vx, we thus obtain
el1x ϭ e؊(a/2)xϩivx ϭ e؊(a/2)x(cos vx ϩ i sin vx)
el2x ϭ e؊(a/2)xϪivx ϭ e؊(a/2)x(cos vx Ϫ i sin vx).
We now add these two lines and multiply the result by 12. This gives y1 as in (8). Then
we subtract the second line from the first and multiply the result by 1/(2i). This gives y2
as in (8). These results obtained by addition and multiplication by constants are again
solutions, as follows from the superposition principle in Sec. 2.1. This concludes the
derivation of these real solutions in Case III.

PROBLEM SET 2.2
1–15

GENERAL SOLUTION

Find a general solution. Check your answer by substitution.
ODEs of this kind have important applications to be
discussed in Secs. 2.4, 2.7, and 2.9.
1. 4y s Ϫ 25y ϭ 0
2. y s ϩ 36y ϭ 0
3. y s ϩ 6y r ϩ 8.96y ϭ 0
4. y s ϩ 4y r ϩ (p2 ϩ 4)y ϭ 0
5. y s ϩ 2py r ϩ p2y ϭ 0
6. 10y s Ϫ 32y r ϩ 25.6y ϭ 0
7. y s ϩ 4.5y r ϭ 0
8. y s ϩ y r ϩ 3.25y ϭ 0
9. y s ϩ 1.8y r Ϫ 2.08y ϭ 0
10. 100y s ϩ 240y r ϩ (196p2 ϩ 144)y ϭ 0
11. 4y s Ϫ 4y r Ϫ 3y ϭ 0
12. y s ϩ 9y r ϩ 20y ϭ 0
13. 9y s Ϫ 30y r ϩ 25y ϭ 0

14. y s ϩ 2k 2y r ϩ k 4y ϭ 0
15. y s ϩ 0.54y r ϩ (0.0729 ϩ p)y ϭ 0
16–20

FIND AN ODE

y s ϩ ay r ϩ by ϭ 0 for the given basis.
16. e2.6x, e؊4.3x
17. e؊25x, xe؊25x
18. cos 2px, sin 2px
19. e(؊2ϩi)x, e(؊2؊i)x
؊3.1x
؊3.1x
20. e
cos 2.1x, e
sin 2.1x
21–30

INITIAL VALUES PROBLEMS

Solve the IVP. Check that your answer satisfies the ODE as
well as the initial conditions. Show the details of your work.
21. y s ϩ 25y ϭ 0, y(0) ϭ 4.6, y r (0) ϭ Ϫ1.2
22. The ODE in Prob. 4, y(12) ϭ 1, y r (12) ϭ Ϫ2
23. y s ϩ y r Ϫ 6y ϭ 0, y(0) ϭ 10, y r (0) ϭ 0
24. 4y s Ϫ 4y r Ϫ 3y ϭ 0, y(Ϫ2) ϭ e, y r (Ϫ2) ϭ Ϫe>2
25. y s Ϫ y ϭ 0, y(0) ϭ 2, y r (0) ϭ Ϫ2
26. y s Ϫ k 2y ϭ 0 (k 0), y(0) ϭ 1, y r (0) ϭ 1

c02.qxd

10/27/10

60

6:06 PM

Page 60

CHAP. 2 Second-Order Linear ODEs

27. The ODE in Prob. 5,
y(0) ϭ 4.5, y r (0) ϭ Ϫ4.5p Ϫ 1 ϭ 13.137
28. 8y s Ϫ 2y r Ϫ y ϭ 0, y(0) ϭ Ϫ0.2, y r (0) ϭ Ϫ0.325
29. The ODE in Prob. 15, y(0) ϭ 0, y r (0) ϭ 1
30. 9y s Ϫ 30y r ϩ 25y ϭ 0, y(0) ϭ 3.3, y r (0) ϭ 10.0
31–36
LINEAR INDEPENDENCE is of basic importance, in this chapter, in connection with general solutions,
as explained in the text. Are the following functions linearly
independent on the given interval? Show the details of your
work.
31.
32.
33.
34.
35.
36.
37.

ekx, xekx, any interval
eax, e؊ax, x Ͼ 0
x 2, x 2 ln x, x Ͼ 1
ln x, ln (x 3), x Ͼ 1
sin 2x, cos x sin x, x Ͻ 0
e؊x cos 12 x, 0, Ϫ1 Ϲ x Ϲ 1
Instability. Solve y s Ϫ y ϭ 0 for the initial conditions
y(0) ϭ 1, y r (0) ϭ Ϫ1. Then change the initial conditions
to y(0) ϭ 1.001, y r (0) ϭ Ϫ0.999 and explain why this
small change of 0.001 at t ϭ 0 causes a large change later,

2.3

e.g., 22 at t ϭ 10. This is instability: a small initial
difference in setting a quantity (a current, for instance) becomes larger and larger with time t. This is
undesirable.
38. TEAM PROJECT. General Properties of Solutions
(a) Coefficient formulas. Show how a and b in (1)
can be expressed in terms of l1 and l2. Explain how
these formulas can be used in constructing equations
for given bases.
(b) Root zero. Solve y s ϩ 4y r ϭ 0 (i) by the present
method, and (ii) by reduction to first order. Can you
explain why the result must be the same in both
cases? Can you do the same for a general ODE
y s ϩ ay r ϭ 0?
(c) Double root. Verify directly that xelx with l ϭ
Ϫa>2 is a solution of (1) in the case of a double root.
Verify and explain why y ϭ e؊2x is a solution of
y s Ϫ y r Ϫ 6y ϭ 0 but xeϪ2x is not.
(d) Limits. Double roots should be limiting cases of
distinct roots l1, l2 as, say, l2 : l1. Experiment with
this idea. (Remember l’Hôpital’s rule from calculus.)
Can you arrive at xel1x? Give it a try.

Differential Operators. Optional
This short section can be omitted without interrupting the flow of ideas. It will not be
used subsequently, except for the notations Dy, D 2 y, etc. to stand for y r , y s , etc.
Operational calculus means the technique and application of operators. Here, an
operator is a transformation that transforms a function into another function. Hence
differential calculus involves an operator, the differential operator D, which
transforms a (differentiable) function into its derivative. In operator notation we write
d
D ϭ dx
and
(1)

Dy ϭ y r ϭ

dy
.
dx

Similarly, for the higher derivatives we write D 2y ϭ D(Dy) ϭ y s , and so on. For example,
D sin ϭ cos, D 2 sin ϭ Ϫsin, etc.
For a homogeneous linear ODE y s ϩ ay r ϩ by ϭ 0 with constant coefficients we can
now introduce the second-order differential operator
L ϭ P(D) ϭ D 2 ϩ aD ϩ bI,
where I is the identity operator defined by Iy ϭ y. Then we can write that ODE as
(2)

Ly ϭ P(D)y ϭ (D 2 ϩ aD ϩ bI)y ϭ 0.

c02.qxd

10/27/10

6:06 PM

Page 61

SEC. 2.3 Differential Operators. Optional

61

P suggests “polynomial.” L is a linear operator. By definition this means that if Ly and
Lw exist (this is the case if y and w are twice differentiable), then L(cy ϩ kw) exists for
any constants c and k, and
L(cy ϩ kw) ϭ cLy ϩ kLw.
Let us show that from (2) we reach agreement with the results in Sec. 2.2. Since
(Del)(x) ϭ lelx and (D 2el)(x) ϭ l2elx, we obtain
Lel(x) ϭ P(D)el(x) ϭ (D 2 ϩ aD ϩ bI)el(x)

(3)

ϭ (l2 ϩ al ϩ b)elx ϭ P(l)elx ϭ 0.
This confirms our result of Sec. 2.2 that elx is a solution of the ODE (2) if and only if l
is a solution of the characteristic equation P(l) ϭ 0.
P(l) is a polynomial in the usual sense of algebra. If we replace l by the operator D,
we obtain the “operator polynomial” P(D). The point of this operational calculus is that
P(D) can be treated just like an algebraic quantity. In particular, we can factor it.
EXAMPLE 1

Factorization, Solution of an ODE
Factor P(D) ϭ D 2 Ϫ 3D Ϫ 40I and solve P(D)y ϭ 0.

Solution. D 2 Ϫ 3D Ϫ 40I ϭ (D Ϫ 8I )(D ϩ 5I ) because I 2 ϭ I. Now (D Ϫ 8I)y ϭ y r Ϫ 8y ϭ 0 has the
solution y1 ϭ e8x. Similarly, the solution of (D ϩ 5I )y ϭ 0 is y2 ϭ e؊5x. This is a basis of P(D)y ϭ 0 on any
interval. From the factorization we obtain the ODE, as expected,
(D Ϫ 8I )(D ϩ 5I )y ϭ (D Ϫ 8I )(y r ϩ 5y) ϭ D(y r ϩ 5y) Ϫ 8(y r ϩ 5y)
ϭ y s ϩ 5y r Ϫ 8y r Ϫ 40y ϭ y s Ϫ 3 r Ϫ 40y ϭ 0.
Verify that this agrees with the result of our method in Sec. 2.2. This is not unexpected because we factored
᭿
P(D) in the same way as the characteristic polynomial P(l) ϭ l2 Ϫ 3l Ϫ 40.

It was essential that L in (2) had constant coefficients. Extension of operator methods to
variable-coefficient ODEs is more difficult and will not be considered here.
If operational methods were limited to the simple situations illustrated in this section,
it would perhaps not be worth mentioning. Actually, the power of the operator approach
appears in more complicated engineering problems, as we shall see in Chap. 6.

PROBLEM SET 2.3
1–5

APPLICATION OF DIFFERENTIAL
OPERATORS

Apply the given operator to the given functions. Show all
steps in detail.
1. D 2 ϩ 2D; cosh 2x, e؊x ϩ e2x, cos x
2. D Ϫ 3I; 3x 2 ϩ 3x, 3e3x, cos 4x Ϫ sin 4x
3. (D Ϫ 2I )2; e2x, xe2x, e؊2x
4. (D ϩ 6I )2; 6x ϩ sin 6x, xe؊6x
5. (D Ϫ 2I )(D ϩ 3I );

e2x, xe2x, e؊3x

6–12

GENERAL SOLUTION

Factor as in the text and solve.
6. (D 2 ϩ 4.00D ϩ 3.36I )y ϭ 0
7. (4D 2 Ϫ I )y ϭ 0
8. (D 2 ϩ 3I )y ϭ 0
9. (D 2 Ϫ 4.20D ϩ 4.41I )y ϭ 0
10. (D 2 ϩ 4.80D ϩ 5.76I )y ϭ 0
11. (D 2 Ϫ 4.00D ϩ 3.84I )y ϭ 0
12. (D 2 ϩ 3.0D ϩ 2.5I )y ϭ 0

c02.qxd

10/27/10

62

6:06 PM

Page 62

CHAP. 2 Second-Order Linear ODEs

13. Linear operator. Illustrate the linearity of L in (2) by
taking c ϭ 4, k ϭ Ϫ6, y ϭ e2x, and w ϭ cos 2x.
Prove that L is linear.
14. Double root. If D 2 ϩ aD ϩ bI has distinct roots
␮ and l, show that a particular solution is
y ϭ (e␮x Ϫ elx)>(␮ Ϫ l). Obtain from this a solution
xelx by letting ␮ : l and applying l’Hôpital’s rule.

2.4

15. Definition of linearity. Show that the definition of
linearity in the text is equivalent to the following. If
L[ y] and L[w] exist, then L[ y ϩ w] exists and L[cy]
and L[kw] exist for all constants c and k, and
L[ y ϩ w] ϭ L[ y] ϩ L[w] as well as L[cy] ϭ cL[ y]
and L[kw] ϭ kL[w].

Modeling of Free Oscillations
of a Mass–Spring System
Linear ODEs with constant coefficients have important applications in mechanics, as we
show in this section as well as in Sec. 2.8, and in electrical circuits as we show in Sec. 2.9.
In this section we model and solve a basic mechanical system consisting of a mass on an
elastic spring (a so-called “mass–spring system,” Fig. 33), which moves up and down.

Setting Up the Model
We take an ordinary coil spring that resists extension as well as compression. We suspend
it vertically from a fixed support and attach a body at its lower end, for instance, an iron
ball, as shown in Fig. 33. We let y ϭ 0 denote the position of the ball when the system
is at rest (Fig. 33b). Furthermore, we choose the downward direction as positive, thus
regarding downward forces as positive and upward forces as negative.

Unstretched
spring

s0
(y = 0)
y
System at
rest

(a)

Fig. 33.

(b)

System in
motion
(c)

Mechanical mass–spring system

We now let the ball move, as follows. We pull it down by an amount y Ͼ 0 (Fig. 33c).
This causes a spring force
(1)

F1 ϭ Ϫky

(Hooke’s law2)

proportional to the stretch y, with k (Ͼ 0) called the spring constant. The minus sign
indicates that F1 points upward, against the displacement. It is a restoring force: It wants
to restore the system, that is, to pull it back to y ϭ 0. Stiff springs have large k.
2

ROBERT HOOKE (1635–1703), English physicist, a forerunner of Newton with respect to the law of
gravitation.

c02.qxd

10/27/10

6:06 PM

Page 63

SEC. 2.4 Modeling of Free Oscillations of a Mass–Spring System

63

Note that an additional force ϪF0 is present in the spring, caused by stretching it in
fastening the ball, but F0 has no effect on the motion because it is in equilibrium with
the weight W of the ball, ϪF0 ϭ W ϭ mg, where g ϭ 980 cm>sec2 ϭ 9.8 m>sec2 ϭ
32.17 ft>sec2 is the constant of gravity at the Earth’s surface (not to be confused with
the universal gravitational constant G ϭ gR2>M ϭ 6.67 # 10؊11 nt m2>kg 2, which we
shall not need; here R ϭ 6.37 # 106 m and M ϭ 5.98 # 1024 kg are the Earth’s radius and
mass, respectively).
The motion of our mass–spring system is determined by Newton’s second law
Mass ϫ Acceleration ϭ my s ϭ Force

(2)

where y s ϭ d 2y>dt 2 and “Force” is the resultant of all the forces acting on the ball. (For
systems of units, see the inside of the front cover.)

ODE of the Undamped System
Every system has damping. Otherwise it would keep moving forever. But if the damping
is small and the motion of the system is considered over a relatively short time, we
may disregard damping. Then Newton’s law with F ϭ ϪF1 gives the model
my s ϭ ϪF1 ϭ Ϫky; thus
my s ϩ ky ϭ 0.

(3)

This is a homogeneous linear ODE with constant coefficients. A general solution is
obtained as in Sec. 2.2, namely (see Example 6 in Sec. 2.2)
y(t) ϭ A cos v0t ϩ B sin v0t

(4)

v0 ϭ

k
.
m
B

This motion is called a harmonic oscillation (Fig. 34). Its frequency is f ϭ v0>2p Hertz3
(ϭ cycles>sec) because cos and sin in (4) have the period 2p>v0. The frequency f is called
the natural frequency of the system. (We write v0 to reserve v for Sec. 2.8.)
y

2
1

t
3

1 Positive
2 Zero
3 Negative

Initial velocity

Fig. 34. Typical harmonic oscillations (4) and (4*) with the same y(0) ϭ A and
different initial velocities y r (0) ϭ v0 B, positive 1 , zero 2 , negative 3
3
HEINRICH HERTZ (1857–1894), German physicist, who discovered electromagnetic waves, as the basis
of wireless communication developed by GUGLIELMO MARCONI (1874–1937), Italian physicist (Nobel prize
in 1909).

c02.qxd

10/27/10

6:06 PM

64

Page 64

CHAP. 2 Second-Order Linear ODEs

An alternative representation of (4), which shows the physical characteristics of amplitude
and phase shift of (4), is
y(t) ϭ C cos (v0t Ϫ d)

(4*)

with C ϭ 2A2 ϩ B 2 and phase angle d, where tan d ϭ B>A. This follows from the
addition formula (6) in App. 3.1.
EXAMPLE 1

Harmonic Oscillation of an Undamped Mass–Spring System
If a mass–spring system with an iron ball of weight W ϭ 98 nt (about 22 lb) can be regarded as undamped, and
the spring is such that the ball stretches it 1.09 m (about 43 in.), how many cycles per minute will the system
execute? What will its motion be if we pull the ball down from rest by 16 cm (about 6 in.) and let it start with
zero initial velocity?

Solution. Hooke’s law (1) with W as the force and 1.09 meter as the stretch gives W ϭ 1.09k; thus

k ϭ W>1.09 ϭ 98>1.09 ϭ 90 [kg>sec2] ϭ 90 [nt>meter]. The mass is m ϭ W>g ϭ 98>9.8 ϭ 10 [kg]. This
gives the frequency v0>(2p) ϭ 2k>m>(2p) ϭ 3>(2p) ϭ 0.48 [Hz] ϭ 29 [cycles>min].
From (4) and the initial conditions, y(0) ϭ A ϭ 0.16 [meter] and y r (0) ϭ v0B ϭ 0. Hence the motion is
y(t) ϭ 0.16 cos 3t [meter]

or

0.52 cos 3t [ft]

(Fig. 35).

If you have a chance of experimenting with a mass–spring system, don’t miss it. You will be surprised about
the good agreement between theory and experiment, usually within a fraction of one percent if you measure
᭿
carefully.
y
0.2
0.1
0

2

–0.1
–0.2

Fig. 35.

4

6

8

10

t

Harmonic oscillation in Example 1

ODE of the Damped System
To our model my s ϭ Ϫky we now add a damping force
F2 ϭ Ϫcy r ,
k

Spring

obtaining my s ϭ Ϫky Ϫ cy r ; thus the ODE of the damped mass–spring system is
(5)

m
c

Ball
Dashpot

Fig. 36.
Damped system

my s ϩ cy r ϩ ky ϭ 0.

(Fig. 36)

Physically this can be done by connecting the ball to a dashpot; see Fig. 36. We assume
this damping force to be proportional to the velocity y r ϭ dy>dt. This is generally a good
approximation for small velocities.

c02.qxd

10/27/10

6:06 PM

Page 65

SEC. 2.4 Modeling of Free Oscillations of a Mass–Spring System

65

The constant c is called the damping constant. Let us show that c is positive. Indeed,
the damping force F2 ϭ Ϫcy r acts against the motion; hence for a downward motion we
have y r Ͼ 0 which for positive c makes F negative (an upward force), as it should be.
Similarly, for an upward motion we have y r Ͻ 0 which, for c Ͼ 0 makes F2 positive (a
downward force).
The ODE (5) is homogeneous linear and has constant coefficients. Hence we can solve
it by the method in Sec. 2.2. The characteristic equation is (divide (5) by m)

c
k
l2 ϩ m l ϩ m ϭ 0.

By the usual formula for the roots of a quadratic equation we obtain, as in Sec. 2.2,

(6) l1 ϭ Ϫa ϩ b, l2 ϭ Ϫa Ϫ b, where

aϭ

c
2m

and

bϭ

1
2c2 Ϫ 4mk.
2m

It is now interesting that depending on the amount of damping present—whether a lot of
damping, a medium amount of damping or little damping—three types of motions occur,
respectively:
Case I.

c2 Ͼ 4mk.

Distinct real roots l1, l2.

(Overdamping)

Case II.

c2 ϭ 4mk.

A real double root.

(Critical damping)

Complex conjugate roots.

(Underdamping)

Case III. c2 Ͻ 4mk .

They correspond to the three Cases I, II, III in Sec. 2.2.

Discussion of the Three Cases
Case I. Overdamping
If the damping constant c is so large that c2 Ͼ 4mk, then l1 and l2 are distinct real roots.
In this case the corresponding general solution of (5) is

(7)

y(t) ϭ c1e؊(a؊b)t ϩ c2e؊(a؉b)t.

We see that in this case, damping takes out energy so quickly that the body does not
oscillate. For t Ͼ 0 both exponents in (7) are negative because a Ͼ 0, b Ͼ 0, and
b2 ϭ a2 Ϫ k>m Ͻ a2. Hence both terms in (7) approach zero as t : ϱ . Practically
speaking, after a sufficiently long time the mass will be at rest at the static equilibrium
position (y ϭ 0). Figure 37 shows (7) for some typical initial conditions.

c02.qxd

10/27/10

6:06 PM

66

Page 66

CHAP. 2 Second-Order Linear ODEs
y
y
1

t

1

2

2

3
3
t
(a)

(b)
1 Positive
2 Zero
3 Negative

Initial velocity

Fig. 37. Typical motions (7) in the overdamped case
(a) Positive initial displacement
(b) Negative initial displacement

Case II. Critical Damping
Critical damping is the border case between nonoscillatory motions (Case I) and oscillations
(Case III). It occurs if the characteristic equation has a double root, that is, if c2 ϭ 4mk,
so that b ϭ 0, l1 ϭ l2 ϭ Ϫa. Then the corresponding general solution of (5) is

y(t) ϭ (c1 ϩ c2t)e؊at.

(8)

This solution can pass through the equilibrium position y ϭ 0 at most once because e؊at
is never zero and c1 ϩ c2t can have at most one positive zero. If both c1 and c2 are positive
(or both negative), it has no positive zero, so that y does not pass through 0 at all. Figure 38
shows typical forms of (8). Note that they look almost like those in the previous figure.

y

1
2

3
t
1 Positive
2 Zero
3 Negative

Fig. 38.

Initial velocity

Critical damping [see (8)]

c02.qxd

10/27/10

6:06 PM

Page 67

SEC. 2.4 Modeling of Free Oscillations of a Mass–Spring System

67

Case III. Underdamping
This is the most interesting case. It occurs if the damping constant c is so small that
c2 Ͻ 4mk. Then b in (6) is no longer real but pure imaginary, say,
(9)

b ϭ iv*

where

v* ϭ

c2
1
k
24mk Ϫ c2 ϭ
Ϫ
4m 2
2m
Bm

(Ͼ0).

(We now write v* to reserve v for driving and electromotive forces in Secs. 2.8 and 2.9.)
The roots of the characteristic equation are now complex conjugates,
l1 ϭ Ϫa ϩ iv*,

l2 ϭ Ϫa Ϫ iv*

with a ϭ c>(2m), as given in (6). Hence the corresponding general solution is
(10)

y(t) ϭ e؊at(A cos v*t ϩ B sin v*t) ϭ Ce؊at cos (v*t Ϫ d)

where C 2 ϭ A2 ϩ B 2 and tan d ϭ B>A, as in (4*).
This represents damped oscillations. Their curve lies between the dashed curves
y ϭ Ce؊at and y ϭ ϪCe؊at in Fig. 39, touching them when v*t Ϫ d is an integer multiple
of p because these are the points at which cos (v*t Ϫ d) equals 1 or Ϫ1.
The frequency is v*>(2p) Hz (hertz, cycles/sec). From (9) we see that the smaller
c (Ͼ0) is, the larger is v* and the more rapid the oscillations become. If c approaches 0,
then v* approaches v0 ϭ 2k>m, giving the harmonic oscillation (4), whose frequency
v0>(2p) is the natural frequency of the system.
y
–α t

Ce

t
–α t

–Ce

Fig. 39.

EXAMPLE 2

Damped oscillation in Case III [see (10)]

The Three Cases of Damped Motion
How does the motion in Example 1 change if we change the damping constant c from one to another of the
following three values, with y(0) ϭ 0.16 and y r (0) ϭ 0 as before?
(I) c ϭ 100 kg>sec,

(II) c ϭ 60 kg>sec,

(III) c ϭ 10 kg>sec.

Solution. It is interesting to see how the behavior of the system changes due to the effect of the damping,
which takes energy from the system, so that the oscillations decrease in amplitude (Case III) or even disappear
(Cases II and I).
(I) With m ϭ 10 and k ϭ 90, as in Example 1, the model is the initial value problem
10y s ϩ 100y r ϩ 90y ϭ 0,

y(0) ϭ 0.16 [meter],

y r (0) ϭ 0.

c02.qxd

10/27/10

68

6:06 PM

Page 68

CHAP. 2 Second-Order Linear ODEs
The characteristic equation is 10l2 ϩ 100l ϩ 90 ϭ 10(l ϩ 9)(l ϩ 1) ϭ 0. It has the roots Ϫ9 and Ϫ1. This
gives the general solution
y ϭ c1e؊9t ϩ c2e؊t.

We also need

y r ϭ Ϫ9c1e؊9t Ϫ c2e؊t.

The initial conditions give c1 ϩ c2 ϭ 0.16, Ϫ9c1 Ϫ c2 ϭ 0. The solution is c1 ϭ Ϫ0.02, c2 ϭ 0.18. Hence in
the overdamped case the solution is
y ϭ Ϫ0.02e؊9t ϩ 0.18e؊t.
It approaches 0 as t : ϱ . The approach is rapid; after a few seconds the solution is practically 0, that is, the
iron ball is at rest.
(II) The model is as before, with c ϭ 60 instead of 100. The characteristic equation now has the form
10l2 ϩ 60l ϩ 90 ϭ 10(l ϩ 3) 2 ϭ 0. It has the double root Ϫ3. Hence the corresponding general solution is
y ϭ (c1 ϩ c2t)e؊3t.

We also need

y r ϭ (c2 Ϫ 3c1 Ϫ 3c2t)e؊3t.

The initial conditions give y(0) ϭ c1 ϭ 0.16, y r (0) ϭ c2 Ϫ 3c1 ϭ 0, c2 ϭ 0.48. Hence in the critical case the
solution is
y ϭ (0.16 ϩ 0.48t)e؊3t.
It is always positive and decreases to 0 in a monotone fashion.
(III) The model now is 10y s ϩ 10y r ϩ 90y ϭ 0. Since c ϭ 10 is smaller than the critical c, we shall get
oscillations. The characteristic equation is 10l2 ϩ 10l ϩ 90 ϭ 10[(l ϩ 12 ) 2 ϩ 9 Ϫ 14 ] ϭ 0. It has the complex
roots [see (4) in Sec. 2.2 with a ϭ 1 and b ϭ 9]
l ϭ Ϫ0.5 Ϯ 20.52 Ϫ 9 ϭ Ϫ0.5 Ϯ 2.96i.
This gives the general solution
y ϭ e؊0.5t(A cos 2.96t ϩ B sin 2.96t).
Thus y(0) ϭ A ϭ 0.16. We also need the derivative
y r ϭ e؊0.5t(Ϫ0.5A cos 2.96t Ϫ 0.5B sin 2.96t Ϫ 2.96A sin 2.96t ϩ 2.96B cos 2.96t).
Hence y r (0) ϭ Ϫ0.5A ϩ 2.96B ϭ 0, B ϭ 0.5A>2.96 ϭ 0.027. This gives the solution
y ϭ e؊0.5t(0.16 cos 2.96t ϩ 0.027 sin 2.96t) ϭ 0.162e؊0.5t cos (2.96t Ϫ 0.17).
We see that these damped oscillations have a smaller frequency than the harmonic oscillations in Example 1 by
about 1% (since 2.96 is smaller than 3.00 by about 1% ). Their amplitude goes to zero. See Fig. 40.
᭿
y
0.15
0.1
0.05
0

2

4

6

8

10

t

–0.05
–0.1

Fig. 40. The three solutions in Example 2

This section concerned free motions of mass–spring systems. Their models are homogeneous linear ODEs. Nonhomogeneous linear ODEs will arise as models of forced
motions, that is, motions under the influence of a “driving force.” We shall study them
in Sec. 2.8, after we have learned how to solve those ODEs.

c02.qxd

10/27/10

6:06 PM

Page 69

SEC. 2.4 Modeling of Free Oscillations of a Mass–Spring System

69

PROBLEM SET 2.4
1–10

HARMONIC OSCILLATIONS
(UNDAMPED MOTION)

1. Initial value problem. Find the harmonic motion (4)
that starts from y0 with initial velocity v0. Graph or
sketch the solutions for v0 ϭ p, y0 ϭ 1, and various
v0 of your choice on common axes. At what t-values
do all these curves intersect? Why?
2. Frequency. If a weight of 20 nt (about 4.5 lb) stretches
a certain spring by 2 cm, what will the frequency of the
corresponding harmonic oscillation be? The period?
3. Frequency. How does the frequency of the harmonic
oscillation change if we (i) double the mass, (ii) take
a spring of twice the modulus? First find qualitative
answers by physics, then look at formulas.
4. Initial velocity. Could you make a harmonic oscillation
move faster by giving the body a greater initial push?
5. Springs in parallel. What are the frequencies of
vibration of a body of mass m ϭ 5 kg (i) on a spring
of modulus k 1 ϭ 20 nt>m, (ii) on a spring of modulus
k 2 ϭ 45 nt>m, (iii) on the two springs in parallel? See
Fig. 41.

The cylindrical buoy of diameter 60 cm in Fig. 43 is
floating in water with its axis vertical. When depressed
downward in the water and released, it vibrates with
period 2 sec. What is its weight?

Water
level

Fig. 43. Buoy (Problem 8)
9. Vibration of water in a tube. If 1 liter of water (about
1.06 US quart) is vibrating up and down under the
influence of gravitation in a U-shaped tube of diameter
2 cm (Fig. 44), what is the frequency? Neglect friction.
First guess.

y
( y = 0)

Fig. 44. Tube (Problem 9)

Fig. 41. Parallel springs (Problem 5)
6. Spring in series. If a body hangs on a spring s1 of
modulus k 1 ϭ 8, which in turn hangs on a spring s2
of modulus k 2 ϭ 12, what is the modulus k of this
combination of springs?
7. Pendulum. Find the frequency of oscillation of a
pendulum of length L (Fig. 42), neglecting air
resistance and the weight of the rod, and assuming u
to be so small that sin u practically equals u.

L

θ

Body of
mass m

10. TEAM PROJECT. Harmonic Motions of Similar
Models. The unifying power of mathematical methods results to a large extent from the fact that different
physical (or other) systems may have the same or very
similar models. Illustrate this for the following three
systems
(a) Pendulum clock. A clock has a 1-meter pendulum.
The clock ticks once for each time the pendulum
completes a full swing, returning to its original position.
How many times a minute does the clock tick?
(b) Flat spring (Fig. 45). The harmonic oscillations
of a flat spring with a body attached at one end and
horizontally clamped at the other are also governed by
(3). Find its motions, assuming that the body weighs
8 nt (about 1.8 lb), the system has its static equilibrium
1 cm below the horizontal line, and we let it start from
this position with initial velocity 10 cm/sec.

Fig. 42. Pendulum (Problem 7)
8. Archimedian principle. This principle states that the
buoyancy force equals the weight of the water
displaced by the body (partly or totally submerged).

y

Fig. 45. Flat spring

c02.qxd

10/27/10

70

6:06 PM

Page 70

CHAP. 2 Second-Order Linear ODEs
(c) Torsional vibrations (Fig. 46). Undamped
torsional vibrations (rotations back and forth) of a
wheel attached to an elastic thin rod or wire are
governed by the equation I0u s ϩ Ku ϭ 0, where u
is the angle measured from the state of equilibrium.
Solve this equation for K>I0 ϭ 13.69 sec؊2, initial
angle 30°(ϭ 0.5235 rad) and initial angular velocity
20° sec؊1 (ϭ 0.349 rad # sec؊1).

θ

Fig. 46. Torsional vibrations

11–20

DAMPED MOTION

11. Overdamping. Show that for (7) to satisfy initial conditions y(0) ϭ y0 and v(0) ϭ v0 we must have c1 ϭ
[(1 ϩ a>b)y0 ϩ v0>b]>2 and c2 ϭ [(1 Ϫ a>b)y0 Ϫ
v0>b]>2.
12. Overdamping. Show that in the overdamped case, the
body can pass through y ϭ 0 at most once (Fig. 37).
13. Initial value problem. Find the critical motion (8)
that starts from y0 with initial velocity v0. Graph
solution curves for a ϭ 1, y0 ϭ 1 and several v0 such
that (i) the curve does not intersect the t-axis, (ii) it
intersects it at t ϭ 1, 2, . . . , 5, respectively.
14. Shock absorber. What is the smallest value of the
damping constant of a shock absorber in the suspension of a wheel of a car (consisting of a spring and an
absorber) that will provide (theoretically) an oscillationfree ride if the mass of the car is 2000 kg and the spring
constant equals 4500 kg>sec 2?
15. Frequency. Find an approximation formula for v* in
terms of v0 by applying the binomial theorem in (9)
and retaining only the first two terms. How good is the
approximation in Example 2, III?
16. Maxima. Show that the maxima of an underdamped
motion occur at equidistant t-values and find the
distance.

equals ¢ ϭ 2pa>v*. Find ¢ for the solutions of
y s ϩ 2y r ϩ 5y ϭ 0.
19. Damping constant. Consider an underdamped motion
of a body of mass m ϭ 0.5 kg. If the time between two
consecutive maxima is 3 sec and the maximum
amplitude decreases to 12 its initial value after 10 cycles,
what is the damping constant of the system?
20. CAS PROJECT. Transition Between Cases I, II,
III. Study this transition in terms of graphs of typical
solutions. (Cf. Fig. 47.)
(a) Avoiding unnecessary generality is part of good
modeling. Show that the initial value problems (A)
and (B),
(A) y s ϩ cy r ϩ y ϭ 0,

y(0) ϭ 1,

y r (0) ϭ 0

(B) the same with different c and y r (0) ϭ Ϫ2 (instead
of 0), will give practically as much information as a
problem with other m, k, y(0), y r (0).
(b) Consider (A). Choose suitable values of c,
perhaps better ones than in Fig. 47, for the transition
from Case III to II and I. Guess c for the curves in the
figure.
(c) Time to go to rest. Theoretically, this time is
infinite (why?). Practically, the system is at rest when
its motion has become very small, say, less than 0.1%
of the initial displacement (this choice being up to us),
that is in our case,
(11)

ƒ y(t) ƒ Ͻ 0.001

for all t greater than some t 1.

In engineering constructions, damping can often be
varied without too much trouble. Experimenting with
your graphs, find empirically a relation between t 1
and c.
(d) Solve (A) analytically. Give a reason why the
solution c of y(t 2) ϭ Ϫ0.001, with t 2 the solution of
y r (t) ϭ 0, will give you the best possible c satisfying
(11).
(e) Consider (B) empirically as in (a) and (b). What
is the main difference between (B) and (A)?

y
1

17. Underdamping. Determine the values of t corresponding to the maxima and minima of the oscillation
y(t) ϭ e؊t sin t. Check your result by graphing y(t).

0.5

18. Logarithmic decrement. Show that the ratio of
two consecutive maximum amplitudes of a damped
oscillation (10) is constant, and the natural logarithm
of this ratio called the logarithmic decrement,

– 0.5

2

4

6

8

–1

Fig. 47. CAS Project 20

10

t

c02.qxd

10/27/10

6:06 PM

Page 71

SEC. 2.5 Euler–Cauchy Equations

2.5

71

Euler–Cauchy Equations
Euler–Cauchy equations4 are ODEs of the form
x 2y s ϩ axy r ϩ by ϭ 0

(1)

with given constants a and b and unknown function y(x). We substitute
y ϭ x m,

y r ϭ mx m؊1,

y s ϭ m(m Ϫ 1)x m؊2

into (1). This gives
x 2m(m Ϫ 1)x mϪ2 ϩ axmx mϪ1 ϩ bx m ϭ 0
and we now see that y ϭ x m was a rather natural choice because we have obtained a common factor x m. Dropping it, we have the auxiliary equation m(m Ϫ 1) ϩ am ϩ b ϭ 0 or
(2)

m 2 ϩ (a Ϫ 1)m ϩ b ϭ 0.

(Note: a Ϫ 1, not a.)

Hence y ϭ x m is a solution of (1) if and only if m is a root of (2). The roots of (2) are
(3) m 1 ϭ 12 (1 Ϫ a) ϩ 214 (1 Ϫ a)2 Ϫ b,

m 2 ϭ 12 (1 Ϫ a) Ϫ 214 (1 Ϫ a)2 Ϫ b.

Case I. Real different roots m 1 and m 2 give two real solutions
y1(x) ϭ x m1

y2(x) ϭ x m2.

and

These are linearly independent since their quotient is not constant. Hence they constitute
a basis of solutions of (1) for all x for which they are real. The corresponding general
solution for all these x is
(4)

EXAMPLE 1

y ϭ c1x m1 ϩ c2x m2

(c1, c2 arbitrary).

General Solution in the Case of Different Real Roots
The Euler–Cauchy equation x 2y s ϩ 1.5xy r Ϫ 0.5y ϭ 0 has the auxiliary equation m 2 ϩ 0.5m Ϫ 0.5 ϭ 0. The
roots are 0.5 and Ϫ1. Hence a basis of solutions for all positive x is y1 ϭ x 0.5 and y2 ϭ 1>x and gives the general
solution
y ϭ c1 1x ϩ

4

c2
x

(x Ͼ 0).

᭿

LEONHARD EULER (1707–1783) was an enormously creative Swiss mathematician. He made
fundamental contributions to almost all branches of mathematics and its application to physics. His important
books on algebra and calculus contain numerous basic results of his own research. The great French
mathematician AUGUSTIN LOUIS CAUCHY (1789–1857) is the father of modern analysis. He is the creator
of complex analysis and had great influence on ODEs, PDEs, infinite series, elasticity theory, and optics.

c02.qxd

10/27/10

6:06 PM

72

Page 72

CHAP. 2 Second-Order Linear ODEs

Case II. A real double root m 1 ϭ 12 (1 Ϫ a) occurs if and only if b ϭ 14 (a Ϫ 1)2 because
then (2) becomes [m ϩ 12 (a Ϫ 1)]2, as can be readily verified. Then a solution is
y1 ϭ x (1؊a)>2, and (1) is of the form
(5)

x 2y s ϩ axy r ϩ 14 (1 Ϫ a)2y ϭ 0

ys ϩ

or

(1 Ϫ a)2
a
y ϭ 0.
yr ϩ
x
4x 2

A second linearly independent solution can be obtained by the method of reduction of
order from Sec. 2.1, as follows. Starting from y2 ϭ uy1, we obtain for u the expression
(9) Sec. 2.1, namely,

Ύ

u ϭ U dx

Uϭ

where

Ύ

1
exp aϪ p dxb .
y 12

From (5) in standard form (second ODE) we see that p ϭ a>x (not ax; this is essential!).
Hence exp ͐ (Ϫp dx) ϭ exp (Ϫa ln x) ϭ exp (ln x ؊a) ϭ 1>x a. Division by y 12 ϭ x 1Ϫ a
gives U ϭ 1>x, so that u ϭ ln x by integration. Thus, y2 ϭ uy1 ϭ y1 ln x, and y1 and y2
are linearly independent since their quotient is not constant. The general solution
corresponding to this basis is
(6)

EXAMPLE 2

y ϭ (c1 ϩ c2 ln x) x m,

m ϭ 12 (1 Ϫ a).

General Solution in the Case of a Double Root
The Euler–Cauchy equation x 2y s Ϫ 5xy r ϩ 9y ϭ 0 has the auxiliary equation m 2 Ϫ 6m ϩ 9 ϭ 0. It has the
double root m ϭ 3, so that a general solution for all positive x is
y ϭ (c1 ϩ c2 ln x) x 3.

᭿

Case III. Complex conjugate roots are of minor practical importance, and we discuss
the derivation of real solutions from complex ones just in terms of a typical example.
EXAMPLE 3

Real General Solution in the Case of Complex Roots
The Euler–Cauchy equation x 2y s ϩ 0.6xy r ϩ 16.04y ϭ 0 has the auxiliary equation m 2 Ϫ 0.4m ϩ 16.04 ϭ 0.
The roots are complex conjugate, m 1 ϭ 0.2 ϩ 4i and m 2 ϭ 0.2 Ϫ 4i, where i ϭ 1Ϫ1. We now use the trick
of writing x ϭ eln x and obtain
x m1 ϭ x 0.2ϩ4i ϭ x 0.2(eln x)4i ϭ x 0.2e(4 ln x)i,
x m2 ϭ x 0.2؊4i ϭ x 0.2(eln x)؊4i ϭ x 0.2e؊(4 ln x)i.
Next we apply Euler’s formula (11) in Sec. 2.2 with t ϭ 4 ln x to these two formulas. This gives
x m1 ϭ x 0.2[cos (4 ln x) ϩ i sin (4 ln x)],
x m2 ϭ x 0.2[cos (4 ln x) Ϫ i sin (4 ln x)].
We now add these two formulas, so that the sine drops out, and divide the result by 2. Then we subtract the
second formula from the first, so that the cosine drops out, and divide the result by 2i. This yields
x 0.2 cos (4 ln x)

and

x 0.2 sin (4 ln x)

respectively. By the superposition principle in Sec. 2.2 these are solutions of the Euler–Cauchy equation (1).
Since their quotient cot (4 ln x) is not constant, they are linearly independent. Hence they form a basis of solutions,
and the corresponding real general solution for all positive x is
(8)

y ϭ x 0.2[A cos (4 ln x) ϩ B sin (4 ln x)].

c02.qxd

10/27/10

6:06 PM

Page 73

SEC. 2.5 Euler–Cauchy Equations

73

Figure 48 shows typical solution curves in the three cases discussed, in particular the real basis functions in
Examples 1 and 3.
᭿

y

y

x 1.5

3.0

x1
2.0
x 0.5
1.0

x –0.5

x –1.5
0

1

2

x 0.5

0
–0.5
–1.0
–1.5

x –1

x

Case I: Real roots

y

x ln x

1.5
1.0
0.5
0.4

1.5
1.0
0.5

ln x
x –0.5 ln x
x –1.5 ln x
2 x

1 1.4

0
–0.5
–1.0
–1.5

Case II: Double root

x 0.2 sin (4 ln x)

0.4

1 1.4

2

x

x 0.2 cos (4 ln x)

Case III: Complex roots

Fig. 48. Euler–Cauchy equations

EXAMPLE 4

Boundary Value Problem. Electric Potential Field Between Two Concentric Spheres
Find the electrostatic potential v ϭ v(r) between two concentric spheres of radii r1 ϭ 5 cm and r2 ϭ 10 cm
kept at potentials v1 ϭ 110 V and v2 ϭ 0, respectively.
Physical Information. v(r) is a solution of the Euler–Cauchy equation rv s ϩ 2v r ϭ 0, where v r ϭ dv>dr.

Solution. The auxiliary equation is m2 ϩ m ϭ 0. It has the roots 0 and Ϫ1. This gives the general solution
v(r) ϭ c1 ϩ c2>r. From the “boundary conditions” (the potentials on the spheres) we obtain
v(5) ϭ c1 ϩ

c2
ϭ 110.
5

v(10) ϭ c1 ϩ

c2
ϭ 0.
10

By subtraction, c2>10 ϭ 110, c2 ϭ 1100. From the second equation, c1 ϭ Ϫc2>10 ϭ Ϫ110. Answer:
v(r) ϭ Ϫ110 ϩ 1100>r V. Figure 49 shows that the potential is not a straight line, as it would be for a potential
between two parallel plates. For example, on the sphere of radius 7.5 cm it is not 110>2 ϭ 55 V, but considerably
᭿
less. (What is it?)

v
100
80
60
40
20
0

5

6

7

8

9

10

r

Fig. 49. Potential v(r) in Example 4

PROBLEM SET 2.5
1. Double root. Verify directly by substitution that
x (1؊a)>2 ln x is a solution of (1) if (2) has a double root,
but x m1 ln x and x m2 ln x are not solutions of (1) if the
roots m1 and m2 of (2) are different.
2–11

GENERAL SOLUTION

Find a real general solution. Show the details of your work.
2. x 2y s Ϫ 20y ϭ 0
3. 5x 2y s ϩ 23xy r ϩ 16.2y ϭ 0

4.
5.
6.
7.
8.
9.
10.
11.

xy s ϩ 2y r ϭ 0
4x 2y s ϩ 5y ϭ 0
x 2y s ϩ 0.7xy r Ϫ 0.1y ϭ 0
(x 2D 2 Ϫ 4xD ϩ 6I)y ϭ C
(x 2D 2 Ϫ 3xD ϩ 4I)y ϭ 0
(x 2D 2 Ϫ 0.2xD ϩ 0.36I)y ϭ 0
(x 2D 2 Ϫ xD ϩ 5I)y ϭ 0
(x 2D 2 Ϫ 3xD ϩ 10I)y ϭ 0

c02.qxd

10/27/10

6:06 PM

74
12–19

Page 74

CHAP. 2 Second-Order Linear ODEs

INITIAL VALUE PROBLEM

Solve and graph the solution. Show the details of your work.
12. x 2y s Ϫ 4xy r ϩ 6y ϭ 0, y(1) ϭ 0.4, y r (1) ϭ 0
13. x 2y s ϩ 3xy r ϩ 0.75y ϭ 0, y(1) ϭ 1,
y r (1) ϭ Ϫ1.5
14. x 2y s ϩ xy r ϩ 9y ϭ 0, y(1) ϭ 0, y r (1) ϭ 2.5
15. x 2y s ϩ 3xy r ϩ y ϭ 0, y(1) ϭ 3.6, y r (1) ϭ 0.4
16. (x 2D 2 Ϫ 3xD ϩ 4I )y ϭ 0, y(1) ϭ Ϫp, y r (1) ϭ 2p
17. (x 2D 2 ϩ xD ϩ I )y ϭ 0, y(1) ϭ 1, y r (1) ϭ 1
18. (9x 2D 2 ϩ 3xD ϩ I )y ϭ 0, y(1) ϭ 1, y r (1) ϭ 0
19. (x 2D 2 Ϫ xD Ϫ 15I )y ϭ 0, y(1) ϭ 0.1,
y r (1) ϭ Ϫ4.5

2.6

20. TEAM PROJECT. Double Root
(a) Derive a second linearly independent solution of
(1) by reduction of order; but instead of using (9), Sec.
2.1, perform all steps directly for the present ODE (1).
(b) Obtain x m ln x by considering the solutions x m
and x mϩs of a suitable Euler–Cauchy equation and
letting s : 0.
(c) Verify by substitution that x m ln x, m ϭ (1 Ϫ a)>2,
is a solution in the critical case.
(d) Transform the Euler–Cauchy equation (1) into
an ODE with constant coefficients by setting
x ϭ et (x Ͼ 0).
(e) Obtain a second linearly independent solution of
the Euler–Cauchy equation in the “critical case” from
that of a constant-coefficient ODE.

Existence and Uniqueness
of Solutions. Wronskian
In this section we shall discuss the general theory of homogeneous linear ODEs
(1)

y s ϩ p(x)y r ϩ q(x)y ϭ 0

with continuous, but otherwise arbitrary, variable coefficients p and q. This will concern
the existence and form of a general solution of (1) as well as the uniqueness of the solution
of initial value problems consisting of such an ODE and two initial conditions
(2)

y(x 0) ϭ K 0,

y r (x 0) ϭ K 1

with given x 0, K 0, and K 1.
The two main results will be Theorem 1, stating that such an initial value problem
always has a solution which is unique, and Theorem 4, stating that a general solution
(3)

y ϭ c1y1 ϩ c2y2

(c1, c2 arbitrary)

includes all solutions. Hence linear ODEs with continuous coefficients have no “singular
solutions” (solutions not obtainable from a general solution).
Clearly, no such theory was needed for constant-coefficient or Euler–Cauchy equations
because everything resulted explicitly from our calculations.
Central to our present discussion is the following theorem.
THEOREM 1

Existence and Uniqueness Theorem for Initial Value Problems

If p(x) and q(x) are continuous functions on some open interval I (see Sec. 1.1) and
x0 is in I, then the initial value problem consisting of (1) and (2) has a unique
solution y(x) on the interval I.

c02.qxd

10/27/10

6:06 PM

Page 75

SEC. 2.6 Existence and Uniqueness of Solutions. Wronskian

75

The proof of existence uses the same prerequisites as the existence proof in Sec. 1.7
and will not be presented here; it can be found in Ref. [A11] listed in App. 1. Uniqueness
proofs are usually simpler than existence proofs. But for Theorem 1, even the uniqueness
proof is long, and we give it as an additional proof in App. 4.

Linear Independence of Solutions
Remember from Sec. 2.1 that a general solution on an open interval I is made up from a
basis y1, y2 on I, that is, from a pair of linearly independent solutions on I. Here we call
y1, y2 linearly independent on I if the equation
k1y1(x) ϩ k 2y2(x) ϭ 0

(4)

on I

implies

k1 ϭ 0, k 2 ϭ 0.

We call y1, y2 linearly dependent on I if this equation also holds for constants k 1, k 2
not both 0. In this case, and only in this case, y1 and y2 are proportional on I, that is (see
Sec. 2.1),
(a) y1 ϭ ky2

(5)

or

(b) y2 ϭ ly1

for all on I.

For our discussion the following criterion of linear independence and dependence of
solutions will be helpful.

THEOREM 2

Linear Dependence and Independence of Solutions

Let the ODE (1) have continuous coefficients p(x) and q(x) on an open interval I.
Then two solutions y1 and y2 of (1) on I are linearly dependent on I if and only if
their “Wronskian”
(6)

W(y1, y2) ϭ y1y2r Ϫ y2y1r

is 0 at some x 0 in I. Furthermore, if W ϭ 0 at an x ϭ x 0 in I, then W ϭ 0 on I;
hence, if there is an x 1 in I at which W is not 0, then y1, y2 are linearly independent
on I.

PROOF

(a) Let y1 and y2 be linearly dependent on I. Then (5a) or (5b) holds on I. If (5a) holds,
then
W(y1, y2) ϭ y1y2r Ϫ y2y1r ϭ ky2y2r Ϫ y2ky2r ϭ 0.
Similarly if (5b) holds.
(b) Conversely, we let W( y1, y2) ϭ 0 for some x ϭ x 0 and show that this implies linear
dependence of y1 and y2 on I. We consider the linear system of equations in the unknowns
k 1, k 2
(7)

k 1 y1(x 0) ϩ k 2 y2(x 0) ϭ 0
k 1 y1r (x 0) ϩ k 2 y2r (x 0) ϭ 0.

c02.qxd

10/27/10

76

6:06 PM

Page 76

CHAP. 2 Second-Order Linear ODEs

To eliminate k 2, multiply the first equation by y 2r and the second by Ϫy2 and add the
resulting equations. This gives
k 1y1(x 0)y2r (x 0) Ϫ k 1y1r (x 0)y2(x 0) ϭ k 1W( y1(x 0), y2(x 0)) ϭ 0.
Similarly, to eliminate k 1, multiply the first equation by Ϫy1r and the second by y1 and
add the resulting equations. This gives
k 2W( y1(x 0), y2(x 0)) ϭ 0.
If W were not 0 at x 0, we could divide by W and conclude that k 1 ϭ k 2 ϭ 0. Since W is
0, division is not possible, and the system has a solution for which k 1 and k 2 are not both
0. Using these numbers k 1, k 2, we introduce the function
y(x) ϭ k 1y1(x) ϩ k 2y2(x).
Since (1) is homogeneous linear, Fundamental Theorem 1 in Sec. 2.1 (the superposition
principle) implies that this function is a solution of (1) on I. From (7) we see that it satisfies
the initial conditions y(x 0) ϭ 0, y r (x 0) ϭ 0. Now another solution of (1) satisfying the
same initial conditions is y* ϵ 0. Since the coefficients p and q of (1) are continuous,
Theorem 1 applies and gives uniqueness, that is, y ϵ y*, written out
k 1y1 ϩ k 2y2 ϵ 0

on I.

Now since k 1 and k 2 are not both zero, this means linear dependence of y1, y2 on I.
(c) We prove the last statement of the theorem. If W(x 0) ϭ 0 at an x 0 in I, we have
linear dependence of y1, y2 on I by part (b), hence W ϵ 0 by part (a) of this proof. Hence
in the case of linear dependence it cannot happen that W(x 1) 0 at an x 1 in I. If it does
happen, it thus implies linear independence as claimed.
᭿
For calculations, the following formulas are often simpler than (6).
(6*) W( y1, y2) ϭ (a)

y2 r
a y b y 12
1

( y1

0)

or

(b)

y1 r
Ϫa y b y 22
2

( y2

0).

These formulas follow from the quotient rule of differentiation.
Remark. Determinants. Students familiar with second-order determinants may have
noticed that
W( y1, y2) ϭ `

y1

y2

y1r

y 2r

` ϭ y1y 2r Ϫ y2y1r .

This determinant is called the Wronski determinant5 or, briefly, the Wronskian, of two
solutions y1 and y2 of (1), as has already been mentioned in (6). Note that its four entries
occupy the same positions as in the linear system (7).
5

Introduced by WRONSKI (JOSEF MARIA HÖNE, 1776–1853), Polish mathematician.

c02.qxd

10/27/10

6:06 PM

Page 77

SEC. 2.6 Existence and Uniqueness of Solutions. Wronskian
EXAMPLE 1

77

Illustration of Theorem 2
The functions y1 ϭ cos vx and y2 ϭ sin vx are solutions of y s ϩ v2y ϭ 0. Their Wronskian is
W(cos vx, sin vx) ϭ `

cos vx

sin vx

Ϫv sin vx

v cos vx

` ϭ y1y2r Ϫ y2y1r ϭ v cos2 vx ϩ v sin2 vx ϭ v.

Theorem 2 shows that these solutions are linearly independent if and only if v 0. Of course, we can see
this directly from the quotient y2>y1 ϭ tan vx. For v ϭ 0 we have y2 ϭ 0, which implies linear dependence
᭿
(why?).

EXAMPLE 2

Illustration of Theorem 2 for a Double Root
A general solution of y s Ϫ 2y r ϩ y ϭ 0 on any interval is y ϭ (c1 ϩ c2x)ex. (Verify!). The corresponding
Wronskian is not 0, which shows linear independence of ex and xex on any interval. Namely,
W(x, xex) ϭ `

ex

xex

ex

(x ϩ 1)ex

` ϭ (x ϩ 1)e2x Ϫ xe2x ϭ e2x

0.

᭿

A General Solution of (1) Includes All Solutions
This will be our second main result, as announced at the beginning. Let us start with
existence.

THEOREM 3

Existence of a General Solution

If p(x) and q(x) are continuous on an open interval I, then (1) has a general solution
on I.

PROOF

By Theorem 1, the ODE (1) has a solution y1(x) on I satisfying the initial conditions
y1(x 0) ϭ 1,

y1r (x 0) ϭ 0

and a solution y2(x) on I satisfying the initial conditions
y2(x 0) ϭ 0,

y2r (x 0) ϭ 1.

The Wronskian of these two solutions has at x ϭ x 0 the value
W( y1(0), y2(0)) ϭ y1(x 0)y2r (x 0) Ϫ y2(x 0)y1r (x 0) ϭ 1.
Hence, by Theorem 2, these solutions are linearly independent on I. They form a basis of
solutions of (1) on I, and y ϭ c1y1 ϩ c2y2 with arbitrary c1, c2 is a general solution of (1)
on I, whose existence we wanted to prove.
᭿
˛

c02.qxd

10/27/10

6:06 PM

78

Page 78

CHAP. 2 Second-Order Linear ODEs

We finally show that a general solution is as general as it can possibly be.

THEOREM 4

A General Solution Includes All Solutions

If the ODE (1) has continuous coefficients p(x) and q(x) on some open interval I,
then every solution y ϭ Y(x) of (1) on I is of the form
Y(x) ϭ C1y1(x) ϩ C2y2(x)

(8)

where y1, y2 is any basis of solutions of (1) on I and C1, C2 are suitable constants.
Hence (1) does not have singular solutions (that is, solutions not obtainable from
a general solution).

PROOF

Let y ϭ Y(x) be any solution of (1) on I. Now, by Theorem 3 the ODE (1) has a general
solution
y(x) ϭ c1y1(x) ϩ c2y2(x)

(9)

on I. We have to find suitable values of c1, c2 such that y(x) ϭ Y(x) on I. We choose any
x 0 in I and show first that we can find values of c1, c2 such that we reach agreement at
x 0, that is, y(x 0) ϭ Y(x 0) and y r (x 0) ϭ Y r (x 0). Written out in terms of (9), this becomes
(10)

(a)

c1y1(x 0) ϩ c2y2(x 0) ϭ Y(x 0)

(b) c1y1r (x 0) ϩ c2y2r (x 0) ϭ Y r (x 0).

We determine the unknowns c1 and c2. To eliminate c2, we multiply (10a) by y2r (x 0) and
(10b) by Ϫy2(x 0) and add the resulting equations. This gives an equation for c1. Then we
multiply (10a) by Ϫy1r (x 0) and (10b) by y1(x 0) and add the resulting equations. This gives
an equation for c2. These new equations are as follows, where we take the values of
y1, y1r , y2, y2r , Y, Y r at x 0.
c1( y1y2r Ϫ y2y1r ) ϭ c1W( y1, y2) ϭ Yy2r Ϫ y2Y r
c2( y1y2r Ϫ y2y1r ) ϭ c2W( y1, y2) ϭ y1Y r Ϫ Yy1r .
Since y1, y2 is a basis, the Wronskian W in these equations is not 0, and we can solve for
c1 and c2. We call the (unique) solution c1 ϭ C1, c2 ϭ C2. By substituting it into (9) we
obtain from (9) the particular solution
y*(x) ϭ C1y1(x) ϩ C2 y2(x).
Now since C1, C2 is a solution of (10), we see from (10) that
y*(x 0) ϭ Y(x 0),

y* r (x 0) ϭ Y r (x 0).

From the uniqueness stated in Theorem 1 this implies that y* and Y must be equal
everywhere on I, and the proof is complete.
᭿

c02.qxd

10/27/10

6:06 PM

Page 79

SEC. 2.7 Nonhomogeneous ODEs

79

Reflecting on this section, we note that homogeneous linear ODEs with continuous variable
coefficients have a conceptually and structurally rather transparent existence and uniqueness
theory of solutions. Important in itself, this theory will also provide the foundation for our
study of nonhomogeneous linear ODEs, whose theory and engineering applications form
the content of the remaining four sections of this chapter.

PROBLEM SET 2.6
1. Derive (6*) from (6).
2–8

BASIS OF SOLUTIONS. WRONSKIAN

Find the Wronskian. Show linear independence by using
quotients and confirm it by Theorem 2.
2. e4.0x, e؊1.5x
3. e؊0.4x, e؊2.6x
4. x, 1>x
5. x 3, x 2
6. e؊x cos vx, e؊x sin vx
7. cosh ax, sinh ax
8. x k cos (ln x), x k sin (ln x)
9–15

ODE FOR GIVEN BASIS. WRONSKIAN. IVP

(a) Find a second-order homogeneous linear ODE for
which the given functions are solutions. (b) Show linear
independence by the Wronskian. (c) Solve the initial value
problem.
9. cos 5x, sin 5x, y(0) ϭ 3, y r (0) ϭ Ϫ5
10. x m1, x m2, y(1) ϭ Ϫ2, y r (1) ϭ 2m 1 Ϫ 4m 2
11. e؊2.5x cos 0.3x, e؊2.5x sin 0.3x, y(0) ϭ 3,
y r (0) ϭ Ϫ7.5
12. x 2, x 2 ln x, y(1) ϭ 4, y r (1) ϭ 6
13. 1, eϪ2x, y(0) ϭ 1, y r (0) ϭ Ϫ1
14. eϪkx cos px, eϪkx sin px, y(0) ϭ 1,
y r (0) ϭ Ϫk Ϫ p
15. cosh 1.8x, sinh 1.8x, y(0) ϭ 14.20, y r (0) ϭ 16.38

2.7

16. TEAM PROJECT. Consequences of the Present
Theory. This concerns some noteworthy general
properties of solutions. Assume that the coefficients p
and q of the ODE (1) are continuous on some open
interval I, to which the subsequent statements refer.
(a) Solve y s Ϫ y ϭ 0 (a) by exponential functions,
(b) by hyperbolic functions. How are the constants in
the corresponding general solutions related?
(b) Prove that the solutions of a basis cannot be 0 at
the same point.
(c) Prove that the solutions of a basis cannot have a
maximum or minimum at the same point.
(d) Why is it likely that formulas of the form (6*)
should exist?
(e) Sketch y1(x) ϭ x 3 if x м 0 and 0 if x Ͻ 0,
y2(x) ϭ 0 if x м 0 and x 3 if x Ͻ 0. Show linear
independence on Ϫ1 Ͻ x Ͻ 1. What is their
Wronskian? What Euler–Cauchy equation do y1, y2
satisfy? Is there a contradiction to Theorem 2?
(f) Prove Abel’s formula6
W( y1(x), y2(x)) ϭ c exp c Ϫ

x

Ύ p(t) dt d
x0

where c ϭ W(y1(x 0), y2(x 0)). Apply it to Prob. 6. Hint:
Write (1) for y1 and for y2. Eliminate q algebraically
from these two ODEs, obtaining a first-order linear
ODE. Solve it.

Nonhomogeneous ODEs
We now advance from homogeneous to nonhomogeneous linear ODEs.
Consider the second-order nonhomogeneous linear ODE
(1)

y s ϩ p(x)y r ϩ q(x)y ϭ r(x)

where r(x) [ 0. We shall see that a “general solution” of (1) is the sum of a general
solution of the corresponding homogeneous ODE
6

NIELS HENRIK ABEL (1802–1829), Norwegian mathematician.

c02.qxd

10/27/10

6:06 PM

80

Page 80

CHAP. 2 Second-Order Linear ODEs

y s ϩ p(x)y r ϩ q(x)y ϭ 0

(2)

and a “particular solution” of (1). These two new terms “general solution of (1)” and
“particular solution of (1)” are defined as follows.
DEFINITION

General Solution, Particular Solution

A general solution of the nonhomogeneous ODE (1) on an open interval I is a
solution of the form
(3)

y(x) ϭ yh(x) ϩ yp1x2;

here, yh ϭ c1y1 ϩ c2y2 is a general solution of the homogeneous ODE (2) on I and
yp is any solution of (1) on I containing no arbitrary constants.
A particular solution of (1) on I is a solution obtained from (3) by assigning
specific values to the arbitrary constants c1 and c2 in yh.
Our task is now twofold, first to justify these definitions and then to develop a method
for finding a solution yp of (1).
Accordingly, we first show that a general solution as just defined satisfies (1) and that
the solutions of (1) and (2) are related in a very simple way.
THEOREM 1

Relations of Solutions of (1) to Those of (2)

(a) The sum of a solution y of (1) on some open interval I and a solution ~y of
(2) on I is a solution of (1) on I. In particular, (3) is a solution of (1) on I.
(b) The difference of two solutions of (1) on I is a solution of (2) on I.

PROOF

(a) Let L[y] denote the left side of (1). Then for any solutions y of (1) and ~y of (2) on I,
L[ y ϩ ~y ] ϭ L[ y] ϩ L[ ~y ] ϭ r ϩ 0 ϭ r.
(b) For any solutions y and y* of (1) on I we have L[ y Ϫ y*] ϭ L[ y] Ϫ L[ y*] ϭ
r Ϫ r ϭ 0.
᭿
Now for homogeneous ODEs (2) we know that general solutions include all solutions.
We show that the same is true for nonhomogeneous ODEs (1).

THEOREM 2

A General Solution of a Nonhomogeneous ODE Includes All Solutions

If the coefficients p(x), q(x), and the function r(x) in (1) are continuous on some
open interval I, then every solution of (1) on I is obtained by assigning suitable
values to the arbitrary constants c1 and c2 in a general solution (3) of (1) on I.
PROOF

Let y* be any solution of (1) on I and x 0 any x in I. Let (3) be any general solution of
(1) on I. This solution exists. Indeed, yh ϭ c1y1 ϩ c2y2 exists by Theorem 3 in Sec. 2.6

c02.qxd

10/27/10

6:06 PM

Page 81

SEC. 2.7 Nonhomogeneous ODEs

81

because of the continuity assumption, and yp exists according to a construction to be
shown in Sec. 2.10. Now, by Theorem 1(b) just proved, the difference Y ϭ y* Ϫ yp is a
solution of (2) on I. At x 0 we have
Y1x 02 ϭ y*1x 02 Ϫ yp(x 0).

Y r 1x 02 ϭ y* r 1x 02 Ϫ ypr 1x 02.

Theorem 1 in Sec. 2.6 implies that for these conditions, as for any other initial conditions
in I, there exists a unique particular solution of (2) obtained by assigning suitable values
᭿
to c1, c2 in yh. From this and y* ϭ Y ϩ yp the statement follows.

Method of Undetermined Coefficients
Our discussion suggests the following. To solve the nonhomogeneous ODE (1) or an initial
value problem for (1), we have to solve the homogeneous ODE (2) and find any solution
yp of (1), so that we obtain a general solution (3) of (1).
How can we find a solution yp of (1)? One method is the so-called method of
undetermined coefficients. It is much simpler than another, more general, method (given
in Sec. 2.10). Since it applies to models of vibrational systems and electric circuits to be
shown in the next two sections, it is frequently used in engineering.
More precisely, the method of undetermined coefficients is suitable for linear ODEs
with constant coefficients a and b
(4)

y s ϩ ay r ϩ by ϭ r(x)

when r (x) is an exponential function, a power of x, a cosine or sine, or sums or products
of such functions. These functions have derivatives similar to r (x) itself. This gives the
idea. We choose a form for yp similar to r (x), but with unknown coefficients to be
determined by substituting that yp and its derivatives into the ODE. Table 2.1 on p. 82
shows the choice of yp for practically important forms of r (x). Corresponding rules are
as follows.
Choice Rules for the Method of Undetermined Coefficients

(a) Basic Rule. If r (x) in (4) is one of the functions in the first column in
Table 2.1, choose yp in the same line and determine its undetermined
coefficients by substituting yp and its derivatives into (4).
(b) Modification Rule. If a term in your choice for yp happens to be a
solution of the homogeneous ODE corresponding to (4), multiply this term
by x (or by x 2 if this solution corresponds to a double root of the
characteristic equation of the homogeneous ODE).
(c) Sum Rule. If r (x) is a sum of functions in the first column of Table 2.1,
choose for yp the sum of the functions in the corresponding lines of the
second column.
The Basic Rule applies when r (x) is a single term. The Modification Rule helps in the
indicated case, and to recognize such a case, we have to solve the homogeneous ODE
first. The Sum Rule follows by noting that the sum of two solutions of (1) with r ϭ r1
and r ϭ r2 (and the same left side!) is a solution of (1) with r ϭ r1 ϩ r2. (Verify!)

c02.qxd

10/27/10

6:06 PM

82

Page 82

CHAP. 2 Second-Order Linear ODEs

The method is self-correcting. A false choice for yp or one with too few terms will lead
to a contradiction. A choice with too many terms will give a correct result, with superfluous
coefficients coming out zero.
Let us illustrate Rules (a)–(c) by the typical Examples 1–3.
Table 2.1

Method of Undetermined Coefficients

Term in r (x)

Choice for yp(x)

kegx
kx n (n ϭ 0, 1, Á )
k cos vx
k sin vx
keax cos vx
keax sin vx

EXAMPLE 1

Cegx
K nx n ϩ K nϪ1x nϪ1 ϩ Á ϩ K 1x ϩ K 0
f K cos vx ϩ M sin vx
f eax(K cos vx ϩ M sin vx)

Application of the Basic Rule (a)
Solve the initial value problem
(5)

y s ϩ y ϭ 0.001x 2,

y(0) ϭ 0,

y r (0) ϭ 1.5.

Solution. Step 1. General solution of the homogeneous ODE. The ODE y s ϩ y ϭ 0 has the general solution
yh ϭ A cos x ϩ B sin x.
Step 2. Solution yp of the nonhomogeneous ODE. We first try yp ϭ Kx 2. Then y sp ϭ 2K. By substitution,
2K ϩ Kx 2 ϭ 0.001x 2. For this to hold for all x, the coefficient of each power of x (x 2 and x 0) must be the same
on both sides; thus K ϭ 0.001 and 2K ϭ 0, a contradiction.
The second line in Table 2.1 suggests the choice
yp ϭ K 2 x 2 ϩ K 1x ϩ K 0.

Then

y sp ϩ yp ϭ 2K 2 ϩ K 2x 2 ϩ K 1x ϩ K 0 ϭ 0.001x 2.

Equating the coefficients of x 2, x, x 0 on both sides, we have K 2 ϭ 0.001, K 1 ϭ 0, 2K 2 ϩ K 0 ϭ 0. Hence
K 0 ϭ Ϫ2K 2 ϭ Ϫ0.002. This gives yp ϭ 0.001x 2 Ϫ 0.002, and
y ϭ yh ϩ yp ϭ A cos x ϩ B sin x ϩ 0.001x 2 Ϫ 0.002.
Step 3. Solution of the initial value problem. Setting x ϭ 0 and using the first initial condition gives
y(0) ϭ A Ϫ 0.002 ϭ 0, hence A ϭ 0.002. By differentiation and from the second initial condition,
y r ϭ yhr ϩ ypr ϭ ϪA sin x ϩ B cos x ϩ 0.002x

y r (0) ϭ B ϭ 1.5.

and

This gives the answer (Fig. 50)
y ϭ 0.002 cos x ϩ 1.5 sin x ϩ 0.001x 2 Ϫ 0.002.
Figure 50 shows y as well as the quadratic parabola yp about which y is oscillating, practically like a sine curve
since the cosine term is smaller by a factor of about 1>1000.
᭿
y
2
1
0
–1

10

Fig. 50.

20

30

40

Solution in Example 1

x

c02.qxd

10/27/10

6:06 PM

Page 83

SEC. 2.7 Nonhomogeneous ODEs
EXAMPLE 2

83

Application of the Modification Rule (b)
Solve the initial value problem
y s ϩ 3y r ϩ 2.25y ϭ Ϫ10e؊1.5x,

(6)

y(0) ϭ 1,

y r (0) ϭ 0.

Solution. Step 1. General solution of the homogeneous ODE. The characteristic equation of the homogeneous
ODE is l2 ϩ 3l ϩ 2.25 ϭ (l ϩ 1.5)2 ϭ 0. Hence the homogeneous ODE has the general solution
yh ϭ (c1 ϩ c2x)e؊1.5x.
˛

Step 2. Solution yp of the nonhomogeneous ODE. The function e؊1.5x on the right would normally require
the choice Ce؊1.5x. But we see from yh that this function is a solution of the homogeneous ODE, which
corresponds to a double root of the characteristic equation. Hence, according to the Modification Rule we have
to multiply our choice function by x 2. That is, we choose
yp ϭ Cx 2e؊1.5x.

Then

ypr ϭ C(2x Ϫ 1.5x 2)e؊1.5x,

y sp ϭ C(2 Ϫ 3x Ϫ 3x ϩ 2.25x 2)e؊1.5x.

We substitute these expressions into the given ODE and omit the factor e؊1.5x. This yields
C(2 Ϫ 6x ϩ 2.25x 2) ϩ 3C(2x Ϫ 1.5x 2) ϩ 2.25Cx 2 ϭ Ϫ10.
Comparing the coefficients of x 2, x, x 0 gives 0 ϭ 0, 0 ϭ 0, 2C ϭ Ϫ10, hence C ϭ Ϫ5. This gives the solution
yp ϭ Ϫ5x 2e؊1.5x. Hence the given ODE has the general solution
y ϭ yh ϩ yp ϭ (c1 ϩ c2x)e؊1.5x Ϫ 5x 2e؊1.5x.
Step 3. Solution of the initial value problem. Setting x ϭ 0 in y and using the first initial condition, we obtain
y(0) ϭ c1 ϭ 1. Differentiation of y gives
y r ϭ (c2 Ϫ 1.5c1 Ϫ 1.5c2x)e؊1.5x Ϫ 10xe؊1.5x ϩ 7.5x 2e؊1.5x.
From this and the second initial condition we have y r (0) ϭ c2 Ϫ 1.5c1 ϭ 0. Hence c2 ϭ 1.5c1 ϭ 1.5. This gives
the answer (Fig. 51)
y ϭ (1 ϩ 1.5x)e؊1.5x Ϫ 5x 2e؊1.5x ϭ (1 ϩ 1.5x Ϫ 5x 2)e؊1.5x.
The curve begins with a horizontal tangent, crosses the x-axis at x ϭ 0.6217 (where 1 ϩ 1.5x Ϫ 5x 2 ϭ 0) and
approaches the axis from below as x increases.
᭿
y
1.0
0.5
0

1

2

3

4

5

x

–0.5
–1.0

Fig. 51. Solution in Example 2

EXAMPLE 3

Application of the Sum Rule (c)
Solve the initial value problem
(7)

y s ϩ 2y r ϩ 0.75y ϭ 2 cos x Ϫ 0.25 sin x ϩ 0.09x,

y(0) ϭ 2.78,

y r (0) ϭ Ϫ0.43.

Solution. Step 1. General solution of the homogeneous ODE. The characteristic equation of the homogeneous
ODE is
l2 ϩ 2l ϩ 0.75 ϭ (l ϩ 12 ) (l ϩ 32 ) ϭ 0
which gives the general solution yh ϭ c1e؊x>2 ϩ c2e؊3x>2.

c02.qxd

10/27/10

84

6:06 PM

Page 84

CHAP. 2 Second-Order Linear ODEs
Step 2. Particular solution of the nonhomogeneous ODE. We write yp ϭ yp1 ϩ yp2 and, following Table 2.1,
(C) and (B),
yp1 ϭ K cos x ϩ M sin x

and

yp2 ϭ K 1x ϩ K 0.

Differentiation gives yp1
r ϭ ϪK sin x ϩ M cos x, yp1
s ϭ ϪK cos x Ϫ M sin x and yp2
r ϭ 1, ysp2 ϭ 0. Substitution
of yp1 into the ODE in (7) gives, by comparing the cosine and sine terms,
ϪK ϩ 2M ϩ 0.75K ϭ 2,

ϪM Ϫ 2K ϩ 0.75M ϭ Ϫ0.25,

hence K ϭ 0 and M ϭ 1. Substituting yp2 into the ODE in (7) and comparing the x- and x 0-terms gives
0.75K 1 ϭ 0.09, 2K 1 ϩ 0.75K 0 ϭ 0,

thus

K 1 ϭ 0.12, K 0 ϭ Ϫ0.32.

Hence a general solution of the ODE in (7) is
y ϭ c1e؊x>2 ϩ c2e؊3x>2 ϩ sin x ϩ 0.12x Ϫ 0.32.
Step 3. Solution of the initial value problem. From y, y r and the initial conditions we obtain
y r (0) ϭ Ϫ12 c1 Ϫ 32 c2 ϩ 1 ϩ 0.12 ϭ Ϫ0.4.

y(0) ϭ c1 ϩ c2 Ϫ 0.32 ϭ 2.78,

Hence c1 ϭ 3.1, c2 ϭ 0. This gives the solution of the IVP (Fig. 52)

᭿

y ϭ 3.1e؊x>2 ϩ sin x ϩ 0.12x Ϫ 0.32.
y
3
2.5
2
1.5
1
0.5
0

2

4

6

8

10 12 14 16 18 20

x

–0.5

Fig. 52.

Solution in Example 3

Stability. The following is important. If (and only if) all the roots of the characteristic
equation of the homogeneous ODE y s ϩ ay r ϩ by ϭ 0 in (4) are negative, or have a negative
real part, then a general solution yh of this ODE goes to 0 as x : ϱ , so that the “transient
solution” y ϭ yh ϩ yp of (4) approaches the “steady-state solution” yp. In this case the
nonhomogeneous ODE and the physical or other system modeled by the ODE are called
stable; otherwise they are called unstable. For instance, the ODE in Example 1 is unstable.
Applications follow in the next two sections.

PROBLEM SET 2.7
1–10

NONHOMOGENEOUS LINEAR ODEs:
GENERAL SOLUTION

Find a (real) general solution. State which rule you are
using. Show each step of your work.
1. y s ϩ 5y r ϩ 4y ϭ 10e؊3x

2.
3.
4.
5.
6.

10y s ϩ 50y r ϩ 57.6y ϭ cos x
y s ϩ 3y r ϩ 2y ϭ 12x 2
y s Ϫ 9y ϭ 18 cos px
y s ϩ 4y r ϩ 4y ϭ e؊x cos x
y s ϩ y r ϩ (p2 ϩ 14)y ϭ e؊x>2 sin p x

c02.qxd

10/27/10

6:06 PM

Page 85

SEC. 2.8 Modeling: Forced Oscillations. Resonance
7.
8.
9.
10.

(D 2 ϩ 2D ϩ 34 I )y ϭ 3ex ϩ 92 x
(3D 2 ϩ 27I )y ϭ 3 cos x ϩ cos 3x
(D 2 Ϫ 16I )y ϭ 9.6e4x ϩ 30ex
(D 2 ϩ 2D ϩ I )y ϭ 2x sin x

11–18

NONHOMOGENEOUS LINEAR
ODEs: IVPs

Solve the initial value problem. State which rule you are
using. Show each step of your calculation in detail.
11. y s ϩ 3y ϭ 18x 2, y(0) ϭ Ϫ3, y r (0) ϭ 0
12. y s ϩ 4y ϭ Ϫ12 sin 2x, y(0) ϭ 1.8, y r (0) ϭ 5.0
13. 8y s Ϫ 6y r ϩ y ϭ 6 cosh x, y(0) ϭ 0.2,
y r (0) ϭ 0.05
14. y s ϩ 4y r ϩ 4y ϭ e؊2x sin 2x, y(0) ϭ 1,
y r (0) ϭ Ϫ1.5
15. (x 2D 2 Ϫ 3xD ϩ 3I )y ϭ 3 ln x Ϫ 4,
y(1) ϭ 0, y r (1) ϭ 1; yp ϭ ln x
16. (D 2 Ϫ 2D)y ϭ 6e2x Ϫ 4e؊2x, y(0) ϭ Ϫ1, y r (0) ϭ 6
17. (D 2 ϩ 0.2D ϩ 0.26I)y ϭ 1.22e0.5x, y(0) ϭ 3.5,
y r (0) ϭ 0.35

2.8

85
18. (D 2 ϩ 2D ϩ 10I)y ϭ 17 sin x Ϫ 37 sin 3x,
y(0) ϭ 6.6, y r (0) ϭ Ϫ2.2
19. CAS PROJECT. Structure of Solutions of Initial
Value Problems. Using the present method, find,
graph, and discuss the solutions y of initial value
problems of your own choice. Explore effects on
solutions caused by changes of initial conditions.
Graph yp, y, y Ϫ yp separately, to see the separate
effects. Find a problem in which (a) the part of y
resulting from yh decreases to zero, (b) increases,
(c) is not present in the answer y. Study a problem with
y(0) ϭ 0, y r (0) ϭ 0. Consider a problem in which
you need the Modification Rule (a) for a simple root,
(b) for a double root. Make sure that your problems
cover all three Cases I, II, III (see Sec. 2.2).
20. TEAM PROJECT. Extensions of the Method of
Undetermined Coefficients. (a) Extend the method
to products of the function in Table 2.1, (b) Extend
the method to Euler–Cauchy equations. Comment on
the practical significance of such extensions.

Modeling: Forced Oscillations. Resonance
In Sec. 2.4 we considered vertical motions of a mass–spring system (vibration of a mass
m on an elastic spring, as in Figs. 33 and 53) and modeled it by the homogeneous linear
ODE
(1)

my s ϩ cy r ϩ ky ϭ 0.

Here y(t) as a function of time t is the displacement of the body of mass m from rest.
The mass–spring system of Sec. 2.4 exhibited only free motion. This means no external
forces (outside forces) but only internal forces controlled the motion. The internal forces
are forces within the system. They are the force of inertia my s , the damping force cy r
(if c Ͼ 0), and the spring force ky, a restoring force.

k

m

Spring

Mass

r(t)

c

Dashpot

Fig. 53.

Mass on a spring

c02.qxd

10/27/10

86

6:06 PM

Page 86

CHAP. 2 Second-Order Linear ODEs

We now extend our model by including an additional force, that is, the external force
r(t), on the right. Then we have
(2*)

my s ϩ cy r ϩ ky ϭ r(t).

Mechanically this means that at each instant t the resultant of the internal forces is in
equilibrium with r(t). The resulting motion is called a forced motion with forcing function
r(t), which is also known as input or driving force, and the solution y(t) to be obtained
is called the output or the response of the system to the driving force.
Of special interest are periodic external forces, and we shall consider a driving force
of the form
r(t) ϭ F0 cos vt

(F0 Ͼ 0, v Ͼ 0).

Then we have the nonhomogeneous ODE
(2)

my s ϩ cy r ϩ ky ϭ F0 cos vt.

Its solution will reveal facts that are fundamental in engineering mathematics and allow
us to model resonance.

Solving the Nonhomogeneous ODE (2)
From Sec. 2.7 we know that a general solution of (2) is the sum of a general solution yh
of the homogeneous ODE (1) plus any solution yp of (2). To find yp, we use the method
of undetermined coefficients (Sec. 2.7), starting from
(3)

yp(t) ϭ a cos vt ϩ b sin vt.

By differentiating this function (chain rule!) we obtain
ypr ϭ Ϫva sin vt ϩ vb cos vt,
y sp ϭ Ϫv2a cos vt Ϫ v2b sin vt.
Substituting yp, ypr , and y sp into (2) and collecting the cosine and the sine terms, we get
[(k Ϫ mv2)a ϩ vcb] cos vt ϩ [Ϫvca ϩ (k Ϫ mv2)b] sin vt ϭ F0 cos vt.
The cosine terms on both sides must be equal, and the coefficient of the sine term
on the left must be zero since there is no sine term on the right. This gives the two
equations

(4)

(k Ϫ mv2)a ϩ
Ϫvca

vcb

ϭ F0

ϩ (k Ϫ mv2)b ϭ 0

c02.qxd

10/27/10

6:06 PM

Page 87

SEC. 2.8 Modeling: Forced Oscillations. Resonance

87

for determining the unknown coefficients a and b. This is a linear system. We can solve
it by elimination. To eliminate b, multiply the first equation by k Ϫ mv2 and the second
by Ϫvc and add the results, obtaining
(k Ϫ mv2)2a ϩ v2c2a ϭ F0(k Ϫ mv2).
Similarly, to eliminate a, multiply (the first equation by vc and the second by k Ϫ mv2
and add to get
v2c2b ϩ (k Ϫ mv2)2b ϭ F0vc.
If the factor (k Ϫ mv2)2 ϩ v2c2 is not zero, we can divide by this factor and solve for a
and b,
a ϭ F0

k Ϫ mv2
,
(k Ϫ mv2)2 ϩ v2c2

b ϭ F0

vc
.
(k Ϫ mv2)2 ϩ v2c2

If we set 2k>m ϭ v0 (Ͼ 0) as in Sec. 2.4, then k ϭ mv20 and we obtain
(5)

a ϭ F0

m(v20 Ϫ v2)
m 2(v20 Ϫ v2)2 ϩ v2c2

,

b ϭ F0

vc
.
m 2(v20 Ϫ v2)2 ϩ v2c2

We thus obtain the general solution of the nonhomogeneous ODE (2) in the form
y(t) ϭ yh(t) ϩ yp(t).

(6)

Here yh is a general solution of the homogeneous ODE (1) and yp is given by (3) with
coefficients (5).
We shall now discuss the behavior of the mechanical system, distinguishing between
the two cases c ϭ 0 (no damping) and c Ͼ 0 (damping). These cases will correspond to
two basically different types of output.

Case 1. Undamped Forced Oscillations. Resonance
If the damping of the physical system is so small that its effect can be neglected over the
time interval considered, we can set c ϭ 0. Then (5) reduces to a ϭ F0>[m(v20 Ϫ v2)]
and b ϭ 0. Hence (3) becomes (use v02 ϭ k>m)
(7)

yp(t) ϭ

F0
m(v20

Ϫv )
2

cos vt ϭ

F0
k[1 Ϫ (v>v0)2]

cos vt.

Here we must assume that v2 v02; physically, the frequency v>(2p) [cycles>sec] of
the driving force is different from the natural frequency v0>(2p) of the system, which is
the frequency of the free undamped motion [see (4) in Sec. 2.4]. From (7) and from (4*)
in Sec. 2.4 we have the general solution of the “undamped system”
(8)

y(t) ϭ C cos (v0t Ϫ d) ϩ

F0
m(v20

Ϫ v2)

cos vt.

We see that this output is a superposition of two harmonic oscillations of the frequencies
just mentioned.

c02.qxd

10/27/10

88

6:06 PM

Page 88

CHAP. 2 Second-Order Linear ODEs

Resonance. We discuss (7). We see that the maximum amplitude of yp is (put cos vt ϭ 1)
(9)

a0 ϭ

F0
k

r

where

rϭ

1
.
1 Ϫ (v>v0)2

a0 depends on v and v0. If v : v0, then r and a0 tend to infinity. This excitation of large
oscillations by matching input and natural frequencies (v ϭ v0) is called resonance. r is
called the resonance factor (Fig. 54), and from (9) we see that r>k ϭ a0>F0 is the ratio
of the amplitudes of the particular solution yp and of the input F0 cos vt. We shall see
later in this section that resonance is of basic importance in the study of vibrating systems.
In the case of resonance the nonhomogeneous ODE (2) becomes
F0
y s ϩ v20 y ϭ m cos v0t.

(10)

Then (7) is no longer valid, and, from the Modification Rule in Sec. 2.7, we conclude that
a particular solution of (10) is of the form
yp(t) ϭ t(a cos v0t ϩ b sin v0t).
ρ

1

ω0

ω

Fig. 54. Resonance factor r(v)

By substituting this into (10) we find a ϭ 0 and b ϭ F0>(2mv0). Hence (Fig. 55)
yp(t) ϭ

(11)

F0
t sin v0t.
2mv0

yp

t

Fig. 55.

Particular solution in the case of resonance

We see that, because of the factor t, the amplitude of the vibration becomes larger and
larger. Practically speaking, systems with very little damping may undergo large vibrations

c02.qxd

10/27/10

6:06 PM

Page 89

SEC. 2.8 Modeling: Forced Oscillations. Resonance

89

that can destroy the system. We shall return to this practical aspect of resonance later in
this section.
Beats. Another interesting and highly important type of oscillation is obtained if v is
close to v0. Take, for example, the particular solution [see (8)]
y(t) ϭ

(12)

F0
m(v20 Ϫ v2)

(cos vt Ϫ cos v0t)

(v

v0).

Using (12) in App. 3.1, we may write this as
y(t) ϭ

2F0
m(v20 Ϫ v2)

sin a

v0 ϩ v
2

tb sin a

v0 Ϫ v
2

tb .

Since v is close to v0, the difference v0 Ϫ v is small. Hence the period of the last sine
function is large, and we obtain an oscillation of the type shown in Fig. 56, the dashed
curve resulting from the first sine factor. This is what musicians are listening to when
they tune their instruments.

y

t

Fig. 56.

Forced undamped oscillation when the difference of the input
and natural frequencies is small (“beats”)

Case 2. Damped Forced Oscillations
If the damping of the mass–spring system is not negligibly small, we have c Ͼ 0 and
a damping term cy r in (1) and (2). Then the general solution yh of the homogeneous
ODE (1) approaches zero as t goes to infinity, as we know from Sec. 2.4. Practically,
it is zero after a sufficiently long time. Hence the “transient solution” (6) of (2),
given by y ϭ yh ϩ yp, approaches the “steady-state solution” yp. This proves the
following.

THEOREM 1

Steady-State Solution

After a sufficiently long time the output of a damped vibrating system under a purely
sinusoidal driving force [see (2)] will practically be a harmonic oscillation whose
frequency is that of the input.

c02.qxd

10/27/10

90

6:06 PM

Page 90

CHAP. 2 Second-Order Linear ODEs

Amplitude of the Steady-State Solution. Practical Resonance
Whereas in the undamped case the amplitude of yp approaches infinity as v approaches
v0, this will not happen in the damped case. In this case the amplitude will always be
finite. But it may have a maximum for some v depending on the damping constant c.
This may be called practical resonance. It is of great importance because if c is not too
large, then some input may excite oscillations large enough to damage or even destroy
the system. Such cases happened, in particular in earlier times when less was known about
resonance. Machines, cars, ships, airplanes, bridges, and high-rising buildings are vibrating
mechanical systems, and it is sometimes rather difficult to find constructions that are
completely free of undesired resonance effects, caused, for instance, by an engine or by
strong winds.
To study the amplitude of yp as a function of v, we write (3) in the form
yp(t) ϭ C* cos (vt Ϫ h).

(13)

C* is called the amplitude of yp and h the phase angle or phase lag because it measures
the lag of the output behind the input. According to (5), these quantities are
C*(v) ϭ 2a 2 ϩ b 2 ϭ

F0
2m

2

(14)
tan h (v) ϭ

(v20

Ϫ v2)2 ϩ v2c2

,

b
vc
ϭ
.
2
a
m(v0 Ϫ v2)

Let us see whether C*(v) has a maximum and, if so, find its location and then its size.
We denote the radicand in the second root in C* by R. Equating the derivative of C* to
zero, we obtain
dC*
1
ϭ F0 aϪ RϪ3>2 b [2m 2(v20 Ϫ v2)(Ϫ2v) ϩ 2vc2].
dv
2
The expression in the brackets [. . .] is zero if
(15)

c2 ϭ 2m 2(v20 Ϫ v2)

(v20 ϭ k>m).

By reshuffling terms we have
2m 2v2 ϭ 2m 2v02 Ϫ c2 ϭ 2mk Ϫ c2.
The right side of this equation becomes negative if c2 Ͼ 2mk, so that then (15) has no
real solution and C* decreases monotone as v increases, as the lowest curve in Fig. 57
shows. If c is smaller, c2 Ͻ 2mk, then (15) has a real solution v ϭ vmax, where
(15*)

v2max ϭ v20 Ϫ

c2
.
2m 2

From (15*) we see that this solution increases as c decreases and approaches v0 as c
approaches zero. See also Fig. 57.

c02.qxd

10/27/10

6:06 PM

Page 91

SEC. 2.8 Modeling: Forced Oscillations. Resonance

91

The size of C*(vmax) is obtained from (14), with v2 ϭ v2max given by (15*). For this
v we obtain in the second radicand in (14) from (15*)
2

m 2(v20 Ϫ v2max)2 ϭ

c4
4m 2

and

v2max c2 ϭ av20 Ϫ

c2
b c2.
2m 2

The sum of the right sides of these two formulas is
(c4 ϩ 4m 2v20c2 Ϫ 2c4)>(4m 2) ϭ c2(4m 2v20 Ϫ c2)>(4m 2).
Substitution into (14) gives
C*(vmax) ϭ

(16)

2mF0
c24m 2v20 Ϫ c2

.

We see that C*(vmax) is always finite when c Ͼ 0. Furthermore, since the expression
c24m 2v20 Ϫ c4 ϭ c2(4mk Ϫ c2)
in the denominator of (16) decreases monotone to zero as c2 (Ͻ2mk) goes to zero, the maximum
amplitude (16) increases monotone to infinity, in agreement with our result in Case 1. Figure 57
shows the amplification C*>F0 (ratio of the amplitudes of output and input) as a function of
v for m ϭ 1, k ϭ 1, hence v0 ϭ 1, and various values of the damping constant c.
Figure 58 shows the phase angle (the lag of the output behind the input), which is less
than p>2 when v Ͻ v0, and greater than p>2 for v Ͼ v0.
C*
F0

η
π

4
c=

c=0
c = 1/2
c=1
c=2

1
_
4

3
2

c=

π
__
2

1
_
2

c=

1
c=
0
0

1

2
1

2

0
0

ω

Fig. 57. Amplification C*>F0 as a function of
v for m ϭ 1, k ϭ 1, and various values of the
damping constant c

1

2

ω

Fig. 58. Phase lag h as a function of v for
m ϭ 1, k ϭ 1, thus v0 ϭ 1, and various values
of the damping constant c

PROBLEM SET 2.8
1. WRITING REPORT. Free and Forced Vibrations.
Write a condensed report of 2–3 pages on the most
important similarities and differences of free and forced
vibrations, with examples of your own. No proofs.
2. Which of Probs. 1–18 in Sec. 2.7 (with x ϭ time t)
can be models of mass–spring systems with a harmonic
oscillation as steady-state solution?

3–7

STEADY-STATE SOLUTIONS

Find the steady-state motion of the mass–spring system
modeled by the ODE. Show the details of your work.
3. y s ϩ 6y r ϩ 8y ϭ 42.5 cos 2t
4. y s ϩ 2.5y r ϩ 10y ϭ Ϫ13.6 sin 4t
5. (D 2 ϩ D ϩ 4.25I )y ϭ 22.1 cos 4.5t

c02.qxd

10/27/10

6:06 PM

92

Page 92

CHAP. 2 Second-Order Linear ODEs

6. (D 2 ϩ 4D ϩ 3I )y ϭ cos t ϩ 13 cos 3t
7. (4D 2 ϩ 12D ϩ 9I )y ϭ 225 Ϫ 75 sin 3t

TRANSIENT SOLUTIONS

8–15

Find the transient motion of the mass–spring system
modeled by the ODE. Show the details of your work.
8. 2y s ϩ 4y r ϩ 6.5y ϭ 4 sin 1.5t

24. Gun barrel. Solve y s ϩ y ϭ 1 Ϫ t 2> p2 if 0 Ϲ
t Ϲ p and 0 if t : ϱ ; here, y(0) ϭ 0, y r (0) ϭ 0. This
models an undamped system on which a force F acts
during some interval of time (see Fig. 59), for instance,
the force on a gun barrel when a shell is fired, the barrel
being braked by heavy springs (and then damped by a
dashpot, which we disregard for simplicity). Hint: At p
both y and y r must be continuous.

9. y s ϩ 3y r ϩ 3.25y ϭ 3 cos t Ϫ 1.5 sin t

m=1

F

10. y s ϩ 16y ϭ 56 cos 4t

k=1
1

F = 1 – t2/π2

11. (D 2 ϩ 2I )y ϭ cos 12t ϩ sin 12t

π

12. (D ϩ 2D ϩ 5I )y ϭ 4 cos t ϩ 8 sin t

F=0
t

2

13. (D 2 ϩ I )y ϭ cos vt, v2

1

14. (D 2 ϩ I )y ϭ 5e؊t cos t
15. (D 2 ϩ 4D ϩ 8I )y ϭ 2 cos 2t ϩ sin 2t

INITIAL VALUE PROBLEMS

16–20

Find the motion of the mass–spring system modeled by the
ODE and the initial conditions. Sketch or graph the solution
curve. In addition, sketch or graph the curve of y Ϫ yp to
see when the system practically reaches the steady state.
16. y s ϩ 25y ϭ 24 sin t, y(0) ϭ 1,
1
3

y r (0) ϭ 1

Fig. 59.

25. CAS EXPERIMENT. Undamped Vibrations.
(a) Solve the initial value problem y s ϩ y ϭ cos vt,
v2 1, y(0) ϭ 0, y r (0) ϭ 0. Show that the solution
can be written
y (t) ϭ

2
sin [12 (1 ϩ v)t] sin [12 (1 Ϫ v)t].
1 Ϫ v2

(b) Experiment with the solution by changing v to
see the change of the curves from those for small
v (Ͼ 0) to beats, to resonance, and to large values of
v (see Fig. 60).

1
5

17. (D ϩ 4I)y ϭ sin t ϩ sin 3t ϩ sin 5t,
3
y(0) ϭ 0, y r (0) ϭ 35
2

Problem 24

1

18. (D 2 ϩ 8D ϩ 17I )y ϭ 474.5 sin 0.5t, y(0) ϭ Ϫ5.4,
y r (0) ϭ 9.4
19. (D 2 ϩ 2D ϩ 2I )y ϭ e؊t>2 sin 12 t,
y r (0) ϭ 1

10π

y(0) ϭ 0,

20. (D 2 ϩ 5I )y ϭ cos pt Ϫ sin pt, y(0) ϭ 0, y r (0) ϭ 0

ω = 0.2

21. Beats. Derive the formula after (12) from (12). Can
we have beats in a damped system?
10

22. Beats. Solve y s ϩ 25y ϭ 99 cos 4.9t, y(0) ϭ 2,
y r (0) ϭ 0. How does the graph of the solution change
if you change (a) y(0), (b) the frequency of the driving
force?
23. TEAM EXPERIMENT. Practical Resonance.
(a) Derive, in detail, the crucial formula (16).
(b) By considering dC*>dc show that C*(vmax) increases as c (Ϲ 12mk) decreases.
(c) Illustrate practical resonance with an ODE of your
own in which you vary c, and sketch or graph
corresponding curves as in Fig. 57.
(d) Take your ODE with c fixed and an input of two
terms, one with frequency close to the practical
resonance frequency and the other not. Discuss and
sketch or graph the output.
(e) Give other applications (not in the book) in which
resonance is important.

20π

–1

20π
–10
ω = 0.9

0.04

10π
–0.04

ω=6

Fig. 60.

Typical solution curves in CAS Experiment 25

c02.qxd

10/27/10

6:06 PM

Page 93

SEC. 2.9 Modeling: Electric Circuits

2.9

93

Modeling: Electric Circuits
Designing good models is a task the computer cannot do. Hence setting up models has
become an important task in modern applied mathematics. The best way to gain experience
in successful modeling is to carefully examine the modeling process in various fields and
applications. Accordingly, modeling electric circuits will be profitable for all students,
not just for electrical engineers and computer scientists.
Figure 61 shows an RLC-circuit, as it occurs as a basic building block of large electric
networks in computers and elsewhere. An RLC-circuit is obtained from an RL-circuit by
adding a capacitor. Recall Example 2 on the RL-circuit in Sec. 1.5: The model of the
RL-circuit is LI r ϩ RI ϭ E(t). It was obtained by KVL (Kirchhoff’s Voltage Law)7 by
equating the voltage drops across the resistor and the inductor to the EMF (electromotive
force). Hence we obtain the model of the RLC-circuit simply by adding the voltage drop
Q> C across the capacitor. Here, C F (farads) is the capacitance of the capacitor. Q coulombs
is the charge on the capacitor, related to the current by
I(t) ϭ

dQ
,
dt

equivalently

Ύ

Q(t) ϭ I(t) dt.

See also Fig. 62. Assuming a sinusoidal EMF as in Fig. 61, we thus have the model of
the RLC-circuit
C

R

L

E(t) = E0 sin ω
ωt

Fig. 61. RLC-circuit

Name

Symbol

Notation

Unit

Voltage Drop
RI
dI
L
dt
Q/C

Ohm’s Resistor

R

Ohm’s Resistance

ohms (⍀)

Inductor

L

Inductance

henrys (H)

Capacitor

C

Capacitance

farads (F)

Fig. 62.

Elements in an RLC-circuit

7
GUSTAV ROBERT KIRCHHOFF (1824–1887), German physicist. Later we shall also need Kirchhoff’s
Current Law (KCL):
At any point of a circuit, the sum of the inflowing currents is equal to the sum of the outflowing currents.
The units of measurement of electrical quantities are named after ANDRÉ MARIE AMPÈRE (1775–1836),
French physicist, CHARLES AUGUSTIN DE COULOMB (1736–1806), French physicist and engineer,
MICHAEL FARADAY (1791–1867), English physicist, JOSEPH HENRY (1797–1878), American physicist,
GEORG SIMON OHM (1789–1854), German physicist, and ALESSANDRO VOLTA (1745–1827), Italian
physicist.

c02.qxd

10/27/10

94

6:06 PM

Page 94

CHAP. 2 Second-Order Linear ODEs

(1 r )

LI r ϩ RI ϩ

Ύ

1
I dt ϭ E(t) ϭ E 0 sin vt.
C

This is an “integro-differential equation.” To get rid of the integral, we differentiate (1 r )
with respect to t, obtaining
(1)

LI s ϩ RI r ϩ

1
I ϭ E r (t) ϭ E 0v cos vt.
C

This shows that the current in an RLC-circuit is obtained as the solution of this
nonhomogeneous second-order ODE (1) with constant coefficients.
In connection with initial value problems, we shall occasionally use
(1 s )

LQ s ϩ RQ s ϩ

1
Q ϭ E(t),
C

obtained from (1 r ) and I ϭ Q r .

Solving the ODE (1) for the Current in an RLC-Circuit
A general solution of (1) is the sum I ϭ Ih ϩ Ip, where Ih is a general solution of the
homogeneous ODE corresponding to (1) and Ip is a particular solution of (1). We first
determine Ip by the method of undetermined coefficients, proceeding as in the previous
section. We substitute
(2)

Ip ϭ a cos vt ϩ b sin vt
Ipr ϭ v(Ϫa sin vt ϩ b cos vt)
Ips ϭ v2(Ϫa cos vt Ϫ b sin vt)

into (1). Then we collect the cosine terms and equate them to E 0v cos vt on the right,
and we equate the sine terms to zero because there is no sine term on the right,
Lv2(Ϫa) ϩ Rvb ϩ a>C ϭ E 0v

(Cosine terms)

Lv2(Ϫb) ϩ Rv(Ϫa) ϩ b>C ϭ 0

(Sine terms).

Before solving this system for a and b, we first introduce a combination of L and C, called
the reactance

(3)

S ϭ vL Ϫ

1
.
vC

Dividing the previous two equations by v, ordering them, and substituting S gives
ϪSa ϩ Rb ϭ E 0
ϪRa Ϫ Sb ϭ 0.

c02.qxd

10/27/10

6:06 PM

Page 95

SEC. 2.9 Modeling: Electric Circuits

95

We now eliminate b by multiplying the first equation by S and the second by R, and
adding. Then we eliminate a by multiplying the first equation by R and the second by
ϪS, and adding. This gives
Ϫ(S 2 ϩ R2)a ϭ E 0 S,

(R2 ϩ S 2)b ϭ E 0 R.

We can solve for a and b,
aϭ

(4)

ϪE 0 S
R ϩS
2

2

,

bϭ

E0 R
R ϩ S2
2

.

Equation (2) with coefficients a and b given by (4) is the desired particular solution Ip of
the nonhomogeneous ODE (1) governing the current I in an RLC-circuit with sinusoidal
electromotive force.
Using (4), we can write Ip in terms of “physically visible” quantities, namely, amplitude
I0 and phase lag u of the current behind the EMF, that is,
Ip(t) ϭ I0 sin (vt Ϫ u)

(5)
where [see (14) in App. A3.1]

I0 ϭ 2a 2 ϩ b 2 ϭ

E0
2R ϩ S
2

2

,

tan u ϭ Ϫ

a
b

ϭ

S

.

R

The quantity 2R2 ϩ S 2 is called the impedance. Our formula shows that the impedance
equals the ratio E 0>I0. This is somewhat analogous to E>I ϭ R (Ohm’s law) and, because
of this analogy, the impedance is also known as the apparent resistance.
A general solution of the homogeneous equation corresponding to (1) is
Ih ϭ c1el1t ϩ c2el2t
where l1 and l2 are the roots of the characteristic equation
l2 ϩ

R
1
lϩ
ϭ 0.
L
LC

We can write these roots in the form l1 ϭ Ϫa ϩ b and l2 ϭ Ϫa Ϫ b, where
aϭ

R
,
2L

bϭ

R2
1
1
4L
Ϫ
ϭ
R2 Ϫ
.
2
LC
2L B
C
B 4L

Now in an actual circuit, R is never zero (hence R Ͼ 0). From this it follows that Ih
approaches zero, theoretically as t : ϱ , but practically after a relatively short time. Hence
the transient current I ϭ Ih ϩ Ip tends to the steady-state current Ip, and after some time
the output will practically be a harmonic oscillation, which is given by (5) and whose
frequency is that of the input (of the electromotive force).

c02.qxd

10/27/10

6:06 PM

96

Page 96

CHAP. 2 Second-Order Linear ODEs
EXAMPLE 1

RLC-Circuit
Find the current I(t) in an RLC-circuit with R ϭ 11 ⍀ (ohms), L ϭ 0.1 H (henry), C ϭ 10؊2 F (farad), which
is connected to a source of EMF E(t) ϭ 110 sin (60 # 2pt) ϭ 110 sin 377 t (hence 60 Hz ϭ 60 cycles> sec, the
usual in the U.S. and Canada; in Europe it would be 220 V and 50 Hz). Assume that current and capacitor
charge are 0 when t ϭ 0.

Solution. Step 1. General solution of the homogeneous ODE. Substituting R, L, C and the derivative E r (t)
into (1), we obtain
0.1I s ϩ 11I r ϩ 100I ϭ 110 # 377 cos 377t.
Hence the homogeneous ODE is 0.1I s ϩ 11I r ϩ 100I ϭ 0. Its characteristic equation is
0.1l2 ϩ 11l ϩ 100 ϭ 0.
The roots are l1 ϭ Ϫ10 and l2 ϭ Ϫ100. The corresponding general solution of the homogeneous ODE is
Ih(t) ϭ c1e؊10t ϩ c2e؊100t.
Step 2. Particular solution Ip of (1). We calculate the reactance S ϭ 37.7 Ϫ 0.3 ϭ 37.4 and the steady-state
current
Ip(t) ϭ a cos 377t ϩ b sin 377t
with coefficients obtained from (4) (and rounded)
aϭ

Ϫ110 # 37.4
11 ϩ 37.4
2

2

ϭ Ϫ2.71,

bϭ

110 # 11
112 ϩ 37.42

ϭ 0.796.

Hence in our present case, a general solution of the nonhomogeneous ODE (1) is
(6)

I(t) ϭ c1e؊10t ϩ c2e؊100t Ϫ 2.71 cos 377t ϩ 0.796 sin 377t.

Step 3. Particular solution satisfying the initial conditions. How to use Q(0) ϭ 0? We finally determine c1
and c2 from the in initial conditions I(0) ϭ 0 and Q(0) ϭ 0. From the first condition and (6) we have
(7)

I(0) ϭ c1 ϩ c2 Ϫ 2.71 ϭ 0,

c2 ϭ 2.71 Ϫ c1.

hence

We turn to Q(0) ϭ 0. The integral in (1 r ) equals ͐ I dt ϭ Q(t); see near the beginning of this section. Hence for
t ϭ 0, Eq. (1 r ) becomes
LI r (0) ϩ R # 0 ϭ 0,

so that

I r (0) ϭ 0.

Differentiating (6) and setting t ϭ 0, we thus obtain
I r (0) ϭ Ϫ10c1 Ϫ 100c2 ϩ 0 ϩ 0.796 # 377 ϭ 0,

hence by (7),

Ϫ10c1 ϭ 100(2.71 Ϫ c1) Ϫ 300.1.

The solution of this and (7) is c1 ϭ Ϫ0.323, c2 ϭ 3.033. Hence the answer is
I(t) ϭ Ϫ0.323e؊10t ϩ 3.033e؊100t Ϫ 2.71 cos 377t ϩ 0.796 sin 377t .
You may get slightly different values depending on the rounding. Figure 63 shows I(t) as well as Ip(t), which
practically coincide, except for a very short time near t ϭ 0 because the exponential terms go to zero very rapidly.
Thus after a very short time the current will practically execute harmonic oscillations of the input frequency
60 Hz ϭ 60 cycles> sec. Its maximum amplitude and phase lag can be seen from (5), which here takes the form
Ip(t) ϭ 2.824 sin (377t Ϫ 1.29).

᭿

c02.qxd

10/27/10

6:06 PM

Page 97

SEC. 2.9 Modeling: Electric Circuits

97
y
I(t)

3
2
1

0

0.01

0.02

0.03

0.04

0.05

t

–1
–2
–3

Fig. 63.

Transient (upper curve) and steady-state currents in Example 1

Analogy of Electrical and Mechanical Quantities
Entirely different physical or other systems may have the same mathematical model.
For instance, we have seen this from the various applications of the ODE y r ϭ ky in
Chap. 1. Another impressive demonstration of this unifying power of mathematics is
given by the ODE (1) for an electric RLC-circuit and the ODE (2) in the last section for
a mass–spring system. Both equations
LI s ϩ RI r ϩ

1
I ϭ E 0v cos vt
C

and

my s ϩ cy r ϩ ky ϭ F0 cos vt

are of the same form. Table 2.2 shows the analogy between the various quantities involved.
The inductance L corresponds to the mass m and, indeed, an inductor opposes a change
in current, having an “inertia effect” similar to that of a mass. The resistance R corresponds
to the damping constant c, and a resistor causes loss of energy, just as a damping dashpot
does. And so on.
This analogy is strictly quantitative in the sense that to a given mechanical system we
can construct an electric circuit whose current will give the exact values of the displacement
in the mechanical system when suitable scale factors are introduced.
The practical importance of this analogy is almost obvious. The analogy may be used
for constructing an “electrical model” of a given mechanical model, resulting in substantial
savings of time and money because electric circuits are easy to assemble, and electric
quantities can be measured much more quickly and accurately than mechanical ones.
Table 2.2 Analogy of Electrical and Mechanical Quantities
Electrical System
Inductance L
Resistance R
Reciprocal 1> C of capacitance
Derivative E 0v cos vt of
}
electromotive force
Current I(t)

Mechanical System
Mass m
Damping constant c
Spring modulus k
Driving force F0 cos vt
Displacement y(t)

c02.qxd

10/27/10

6:06 PM

Page 98

98

CHAP. 2 Second-Order Linear ODEs

Related to this analogy are transducers, devices that convert changes in a mechanical
quantity (for instance, in a displacement) into changes in an electrical quantity that can
be monitored; see Ref. [GenRef11] in App. 1.

PROBLEM SET 2.9
1–6

RLC-CIRCUITS: SPECIAL CASES

1. RC-Circuit. Model the RC-circuit in Fig. 64. Find the
current due to a constant E.

4. RL-Circuit. Solve Prob. 3 when E ϭ E 0 sin vt and R,
L, E 0, and are arbitrary. Sketch a typical solution.
Current I(t)
2

R

1.5
1
E(t)

0.5

4π

C

Fig. 64. RC-circuit

Fig. 68.

c

Typical current I ϭ e؊0.1t ϩ sin (t Ϫ 41 p)
in Problem 4

5. LC-Circuit. This is an RLC-circuit with negligibly
small R (analog of an undamped mass–spring system).
Find the current when L ϭ 0.5 H, C ϭ 0.005 F, and
E ϭ sin t V, assuming zero initial current and charge.

t

Current 1 in Problem 1

2. RC-Circuit. Solve Prob. 1 when E ϭ E 0 sin vt and
R, C, E 0, and v are arbitrary.
3. RL-Circuit. Model the RL-circuit in Fig. 66. Find a
general solution when R, L, E are any constants. Graph
or sketch solutions when L ϭ 0.25 H, R ϭ 10 ⍀, and
E ϭ 48 V.

C

L

E(t)

Fig. 69.

R

LC-circuit

6. LC-Circuit. Find the current when L ϭ 0.5 H,
C ϭ 0.005 F, E ϭ 2t 2 V, and initial current and charge
zero.

E(t)

7–18
L

Fig. 66.

t

–1

Current I(t)

Fig. 65.

12π

8π

–0.5

GENERAL RLC-CIRCUITS

7. Tuning. In tuning a stereo system to a radio station,
we adjust the tuning control (turn a knob) that changes
C (or perhaps L) in an RLC-circuit so that the amplitude
of the steady-state current (5) becomes maximum. For
what C will this happen?

RL-circuit

Current I(t)
5
4
3

8–14
Find the steady-state current in the RLC-circuit
in Fig. 61 for the given data. Show the details of your work.

2
1
0

0.02

0.04

0.06

0.08

0.1

Fig. 67. Currents in Problem 3

t

8. R ϭ 4 ⍀, L ϭ 0.5 H, C ϭ 0.1 F, E ϭ 500 sin 2t V
9. R ϭ 4 ⍀, L ϭ 0.1 H, C ϭ 0.05 F, E ϭ 110 V
1
10. R ϭ 2 ⍀, L ϭ 1 H, C ϭ 20
F, E ϭ 157 sin 3t V

c02.qxd

10/27/10

6:06 PM

Page 99

SEC. 2.10 Solution by Variation of Parameters

99

1
11. R ϭ 12 ⍀, L ϭ 0.4 H, C ϭ 80
F,
E ϭ 220 sin 10t V

12. R ϭ 0.2 ⍀, L ϭ 0.1 H, C ϭ 2 F, E ϭ 220 sin 314t V

# 10؊3 F,
13. R ϭ 12, L ϭ 1.2 H, C ϭ 20
3
E ϭ 12,000 sin 25t V
14. Prove the claim in the text that if R 0 (hence R Ͼ 0),
then the transient current approaches Ip as t : ϱ.
15. Cases of damping. What are the conditions for an
RLC-circuit to be (I) overdamped, (II) critically damped,
(III) underdamped? What is the critical resistance Rcrit
(the analog of the critical damping constant 2 1mk)?
16–18
Solve the initial value problem for the RLCcircuit in Fig. 61 with the given data, assuming zero initial
current and charge. Graph or sketch the solution. Show the
details of your work.

2.10

16. R ϭ 8 ⍀, L ϭ 0.2 H, C ϭ 12.5 # 10؊3 F,
E ϭ 100 sin 10t V
17. R ϭ 6 ⍀, L ϭ 1 H, C ϭ 0.04 F,
E ϭ 600 (cos t ϩ 4 sin t) V
18. R ϭ 18 ⍀, L ϭ 1 H, C ϭ 12.5 # 10؊3 F,
E ϭ 820 cos 10t V
19. WRITING REPORT. Mechanic-Electric Analogy.
Explain Table 2.2 in a 1–2 page report with examples,
e.g., the analog (with L ϭ 1 H) of a mass–spring system
of mass 5 kg, damping constant 10 kg>sec, spring constant
60 kg>sec2, and driving force 220 cos 10t kg>sec.
~
~
20. Complex Solution Method. Solve LI s ϩ RI r ϩ
~
ivt
I >C ϭ E 0e , i ϭ 1Ϫ1, by substituting Ip ϭ Keivt
(K unknown) and its derivatives and taking the real
~
part Ip of the solution I p . Show agreement with (2), (4).
ivt
Hint: Use (11) e ϭ cos vt ϩ i sin vt; cf. Sec. 2.2,
and i 2 ϭ Ϫ1.

Solution by Variation of Parameters
We continue our discussion of nonhomogeneous linear ODEs, that is
(1)

y s ϩ p(x)y r ϩ q(x)y ϭ r (x).

In Sec. 2.6 we have seen that a general solution of (1) is the sum of a general solution yh
of the corresponding homogeneous ODE and any particular solution yp of (1). To obtain yp
when r (x) is not too complicated, we can often use the method of undetermined coefficients,
as we have shown in Sec. 2.7 and applied to basic engineering models in Secs. 2.8 and 2.9.
However, since this method is restricted to functions r (x) whose derivatives are of a form
similar to r (x) itself (powers, exponential functions, etc.), it is desirable to have a method valid
for more general ODEs (1), which we shall now develop. It is called the method of variation
of parameters and is credited to Lagrange (Sec. 2.1). Here p, q, r in (1) may be variable
(given functions of x), but we assume that they are continuous on some open interval I.
Lagrange’s method gives a particular solution yp of (1) on I in the form
(2)

yp(x) ϭ Ϫy1

Ύ W dx ϩ y Ύ W dx
y2r

y1r

2

where y1, y2 form a basis of solutions of the corresponding homogeneous ODE
(3)

y s ϩ p(x)y r ϩ q(x)y ϭ 0

on I, and W is the Wronskian of y1, y2,
(4)

W ϭ y1y2r Ϫ y2y1r

(see Sec. 2.6).

CAUTION! The solution formula (2) is obtained under the assumption that the ODE
is written in standard form, with y s as the first term as shown in (1). If it starts with
f (x)y s , divide first by f (x).

c02.qxd

10/27/10

6:06 PM

100

Page 100

CHAP. 2 Second-Order Linear ODEs

The integration in (2) may often cause difficulties, and so may the determination of
y1, y 2 if (1) has variable coefficients. If you have a choice, use the previous method. It is
simpler. Before deriving (2) let us work an example for which you do need the new
method. (Try otherwise.)
EXAMPLE 1

Method of Variation of Parameters
Solve the nonhomogeneous ODE
y s ϩ y ϭ sec x ϭ

1
.
cos x

Solution. A basis of solutions of the homogeneous ODE on any interval is y1 ϭ cos x, y2 ϭ sin x. This gives
the Wronskian
W( y1, y2) ϭ cos x cos x Ϫ sin x (Ϫsin x) ϭ 1.
From (2), choosing zero constants of integration, we get the particular solution of the given ODE

Ύ

Ύ

yp ϭ Ϫcos x sin x sec x dx ϩ sin x cos x sec x dx
ϭ cos x ln ƒ cos x ƒ ϩ x sin x

(Fig. 70)

Figure 70 shows yp and its first term, which is small, so that x sin x essentially determines the shape of the curve
of yp. (Recall from Sec. 2.8 that we have seen x sin x in connection with resonance, except for notation.) From
yp and the general solution yh ϭ c1y1 ϩ c2y2 of the homogeneous ODE we obtain the answer
y ϭ yh ϩ yp ϭ (c1 ϩ ln ƒ cos x ƒ ) cos x ϩ (c2 ϩ x) sin x.
Had we included integration constants Ϫc1, c2 in (2), then (2) would have given the additional
c1 cos x ϩ c2 sin x ϭ c1y1 ϩ c2y2, that is, a general solution of the given ODE directly from (2). This will
᭿
always be the case.
y
10

5

0

2

4

6

8 10 12 x

–5

–10

Fig. 70. Particular solution yp and its first term in Example 1

Idea of the Method. Derivation of (2)
What idea did Lagrange have? What gave the method the name? Where do we use the
continuity assumptions?
The idea is to start from a general solution
yh(x) ϭ c1y1(x) ϩ c2y2(x)

c02.qxd

10/27/10

6:06 PM

Page 101

SEC. 2.10 Solution by Variation of Parameters

101

of the homogeneous ODE (3) on an open interval I and to replace the constants (“the
parameters”) c1 and c2 by functions u(x) and v(x); this suggests the name of the method.
We shall determine u and v so that the resulting function
yp(x) ϭ u(x)y1(x) ϩ v(x)y2(x)

(5)

is a particular solution of the nonhomogeneous ODE (1). Note that yh exists by Theorem
3 in Sec. 2.6 because of the continuity of p and q on I. (The continuity of r will be used
later.)
We determine u and v by substituting (5) and its derivatives into (1). Differentiating (5),
we obtain
ypr ϭ u r y1 ϩ uy1r ϩ v r y2 ϩ vy2r .
Now yp must satisfy (1). This is one condition for two functions u and v. It seems plausible
that we may impose a second condition. Indeed, our calculation will show that we can
determine u and v such that yp satisfies (1) and u and v satisfy as a second condition the
equation
u r y1 ϩ v r y2 ϭ 0.

(6)

This reduces the first derivative ypr to the simpler form
ypr ϭ uy1r ϩ vy2r .

(7)
Differentiating (7), we obtain

yps ϭ u r y1r ϩ uy1s ϩ v r y2r ϩ vy2s .

(8)

We now substitute yp and its derivatives according to (5), (7), (8) into (1). Collecting
terms in u and terms in v, we obtain
u( y1s ϩ py1r ϩ qy1) ϩ v( y2s ϩ py2r ϩ qy2) ϩ u r y1r ϩ v r y2r ϭ r.
Since y1 and y2 are solutions of the homogeneous ODE (3), this reduces to
(9a)

u r y1r ϩ v r y2r ϭ r.

Equation (6) is
(9b)

u r y1 ϩ v r y2 ϭ 0.

This is a linear system of two algebraic equations for the unknown functions u r and v r .
We can solve it by elimination as follows (or by Cramer’s rule in Sec. 7.6). To eliminate
v r , we multiply (9a) by Ϫy2 and (9b) by y2r and add, obtaining
u r (y1y2r Ϫ y2y1r ) ϭ Ϫy2r,

thus

u r W ϭ Ϫy2r.

Here, W is the Wronskian (4) of y1, y2. To eliminate u r we multiply (9a) by y1, and (9b)
by Ϫy1r and add, obtaining

c02.qxd

10/27/10

102

6:06 PM

Page 102

CHAP. 2 Second-Order Linear ODEs

v r (y1y 2r Ϫ y2y1r ) ϭ Ϫy1r,
Since y1, y 2 form a basis, we have W
(10)

v r W ϭ y1r.

thus

0 (by Theorem 2 in Sec. 2.6) and can divide by W,

ur ϭ Ϫ

y2r
,
W

vr ϭ

y1r
.
W

By integration,

Ύ W dx,

uϭϪ

y2r

vϭ

Ύ W dx.
y1r

These integrals exist because r (x) is continuous. Inserting them into (5) gives (2) and
completes the derivation.
᭿

PROBLEM SET 2.10
1–13

GENERAL SOLUTION

Solve the given nonhomogeneous linear ODE by variation
of parameters or undetermined coefficients. Show the
details of your work.
1. y s ϩ 9y ϭ sec 3x
2. y s ϩ 9y ϭ csc 3x
3. x 2y s Ϫ 2xy r ϩ 2y ϭ x 3 sin x
4. y s Ϫ 4y r ϩ 5y ϭ e2x csc x
5. y s ϩ y ϭ cos x Ϫ sin x
6. (D 2 ϩ 6D ϩ 9I )y ϭ 16e؊3x>(x 2 ϩ 1)
7. (D 2 Ϫ 4D ϩ 4I )y ϭ 6e2x>x 4
8. (D 2 ϩ 4I )y ϭ cosh 2x
9. (D 2 Ϫ 2D ϩ I )y ϭ 35x 3>2ex
10. (D 2 ϩ 2D ϩ 2I )y ϭ 4e؊x sec3 x

11.
12.
13.
14.

(x 2D 2 Ϫ 4xD ϩ 6I )y ϭ 21x ؊4
(D 2 Ϫ I )y ϭ 1>cosh x
(x 2D 2 ϩ xD Ϫ 9I )y ϭ 48x 5
TEAM PROJECT. Comparison of Methods. Invention. The undetermined-coefficient method should be
used whenever possible because it is simpler. Compare
it with the present method as follows.
(a) Solve y s ϩ 4y r ϩ 3y ϭ 65 cos 2x by both methods,
showing all details, and compare.
(b) Solve y s Ϫ 2y r ϩ y ϭ r1 ϩ r2, r1 ϭ 35x 3>2ex r2 ϭ
x 2 by applying each method to a suitable function on
the right.
(c) Experiment to invent an undetermined-coefficient
method for nonhomogeneous Euler–Cauchy equations.

CHAPTER 2 REVIEW QUESTIONS AND PROBLEMS
1. Why are linear ODEs preferable to nonlinear ones in
modeling?
2. What does an initial value problem of a second-order
ODE look like? Why must you have a general solution
to solve it?
3. By what methods can you get a general solution of a
nonhomogeneous ODE from a general solution of a
homogeneous one?
4. Describe applications of ODEs in mechanical systems.
What are the electrical analogs of the latter?
5. What is resonance? How can you remove undesirable
resonance of a construction, such as a bridge, a ship,
or a machine?
6. What do you know about existence and uniqueness of
solutions of linear second-order ODEs?

7–18

GENERAL SOLUTION

Find a general solution. Show the details of your calculation.
7. 4y s ϩ 32y r ϩ 63y ϭ 0
8. y s ϩ y r Ϫ 12y ϭ 0
9. y s ϩ 6y r ϩ 34y ϭ 0
10. y s ϩ 0.20y r ϩ 0.17y ϭ 0
11. (100D 2 Ϫ 160D ϩ 64I )y ϭ 0
12. (D 2 ϩ 4pD ϩ 4p2I )y ϭ 0
13. (x 2D 2 ϩ 2xD Ϫ 12I )y ϭ 0
14. (x 2D 2 ϩ xD Ϫ 9I )y ϭ 0
15. (2D 2 Ϫ 3D Ϫ 2I )y ϭ 13 Ϫ 2x 2
16. (D 2 ϩ 2D ϩ 2I )y ϭ 3e؊x cos 2x
17. (4D 2 Ϫ 12D ϩ 9I )y ϭ 2e1.5x
18. yy s ϭ 2y r 2

c02.qxd

10/27/10

6:06 PM

Page 103

Summary of Chapter 2
19–22

103

INITIAL VALUE PROBLEMS

Solve the problem, showing the details of your work.
Sketch or graph the solution.
19. y s ϩ 16y ϭ 17ex, y(0) ϭ 6, y r (0) ϭ Ϫ2
20. y s Ϫ 3y r ϩ 2y ϭ 10 sin x, y(0) ϭ 1, y r (0) ϭ Ϫ6
21. (x 2D 2 ϩ xD Ϫ I )y ϭ 16x 3, y(1) ϭ Ϫ1, y r (1) ϭ 1
22. (x 2D 2 ϩ 15xD ϩ 49I )y ϭ 0, y(1) ϭ 2,
y r (1) ϭ Ϫ11
23–30

27. Find an electrical analog of the mass–spring system
with mass 4 kg, spring constant 10 kg>sec2, damping
constant 20 kg> sec, and driving force 100 sin 4t nt.
28. Find the motion of the mass–spring system in Fig. 72
with mass 0.125 kg, damping 0, spring constant
1.125 kg>sec2, and driving force cos t Ϫ 4 sin t nt, assuming zero initial displacement and velocity. For what
frequency of the driving force would you get resonance?

APPLICATIONS

23. Find the steady-state current in the RLC-circuit in Fig. 71
when R ϭ 2 k⍀ (2000 ⍀), L ϭ 1 H, C ϭ 4 # 10؊3 F, and
E ϭ 110 sin 415t V (66 cycles> sec).
24. Find a general solution of the homogeneous linear
ODE corresponding to the ODE in Prob. 23.
25. Find the steady-state current in the RLC-circuit
in Fig. 71 when R ϭ 50 ⍀, L ϭ 30 H, C ϭ 0.025 F,
E ϭ 200 sin 4t V.
C

m
c

Spring

Mass
Dashpot

Fig. 72. Mass–spring system
29. Show that the system in Fig. 72 with m ϭ 4, c ϭ 0,
k ϭ 36, and driving force 61 cos 3.1t exhibits beats.
Hint: Choose zero initial conditions.

L

R

k

E(t )

Fig. 71. RLC-circuit
26. Find the current in the RLC-circuit in Fig. 71
when R ϭ 40 ⍀, L ϭ 0.4 H, C ϭ 10؊4 F, E ϭ
220 sin 314t V (50 cycles> sec).

SUMMARY OF CHAPTER

30. In Fig. 72, let m ϭ 1 kg, c ϭ 4 kg> sec, k ϭ 24 kg>sec2,
and r(t) ϭ 10 cos vt nt. Determine w such that you
get the steady-state vibration of maximum possible
amplitude. Determine this amplitude. Then find the
general solution with this v and check whether the results
are in agreement.

2

Second-Order Linear ODEs
Second-order linear ODEs are particularly important in applications, for instance,
in mechanics (Secs. 2.4, 2.8) and electrical engineering (Sec. 2.9). A second-order
ODE is called linear if it can be written
(1)

y s ϩ p(x)y r ϩ q(x)y ϭ r (x)

(Sec. 2.1).

(If the first term is, say, f (x)y s , divide by f (x) to get the “standard form” (1) with
y s as the first term.) Equation (1) is called homogeneous if r (x) is zero for all x
considered, usually in some open interval; this is written r (x) ϵ 0. Then
(2)

y s ϩ p(x)y r ϩ q(x)y ϭ 0.

Equation (1) is called nonhomogeneous if r (x) [ 0 (meaning r (x) is not zero for
some x considered).

c02.qxd

10/27/10

104

6:06 PM

Page 104

CHAP. 2 Second-Order Linear ODEs

For the homogeneous ODE (2) we have the important superposition principle (Sec.
2.1) that a linear combination y ϭ ky1 ϩ ly2 of two solutions y1, y2 is again a solution.
Two linearly independent solutions y1, y2 of (2) on an open interval I form a basis
(or fundamental system) of solutions on I. and y ϭ c1y1 ϩ c2y2 with arbitrary
constants c1, c2 a general solution of (2) on I. From it we obtain a particular
solution if we specify numeric values (numbers) for c1 and c2, usually by prescribing
two initial conditions
y(x 0) ϭ K 0,

(3)

y r (x 0) ϭ K 1

(x 0, K 0, K 1 given numbers; Sec. 2.1).

(2) and (3) together form an initial value problem. Similarly for (1) and (3).
For a nonhomogeneous ODE (1) a general solution is of the form
y ϭ yh ϩ yp

(4)

(Sec. 2.7).

Here yh is a general solution of (2) and yp is a particular solution of (1). Such a yp
can be determined by a general method (variation of parameters, Sec. 2.10) or in
many practical cases by the method of undetermined coefficients. The latter applies
when (1) has constant coefficients p and q, and r (x) is a power of x, sine, cosine,
etc. (Sec. 2.7). Then we write (1) as
y s ϩ ay r ϩ by ϭ r (x)

(5)

(Sec. 2.7).

The corresponding homogeneous ODE y r ϩ ay r ϩ by ϭ 0 has solutions y ϭ elx,
where l is a root of
l2 ϩ al ϩ b ϭ 0.

(6)

Hence there are three cases (Sec. 2.2):
Case
I
II
III

Type of Roots

General Solution

Distinct real l1, l2
Double Ϫ12 a
Complex Ϫ12 a Ϯ iv*

y ϭ c1el1x ϩ c2el2x
y ϭ (c1 ϩ c2x)e Ϫax>2
y ϭ e؊ax>2(A cos v*x ϩ B sin v*x)

Here v* is used since v is needed in driving forces.
Important applications of (5) in mechanical and electrical engineering in connection
with vibrations and resonance are discussed in Secs. 2.4, 2.7, and 2.8.
Another large class of ODEs solvable “algebraically” consists of the Euler–Cauchy
equations
(7)

x 2y s ϩ axy r ϩ by ϭ 0

(Sec. 2.5).

These have solutions of the form y ϭ x m, where m is a solution of the auxiliary equation
(8)

m 2 ϩ (a Ϫ 1)m ϩ b ϭ 0.

Existence and uniqueness of solutions of (1) and (2) is discussed in Secs. 2.6
and 2.7, and reduction of order in Sec. 2.1.

c03.qxd

10/27/10

6:20 PM

Page 105

CHAPTER

3

Higher Order Linear ODEs
The concepts and methods of solving linear ODEs of order n ϭ 2 extend nicely to linear
ODEs of higher order n, that is, n ϭ 3, 4, etc. This shows that the theory explained in
Chap. 2 for second-order linear ODEs is attractive, since it can be extended in a
straightforward way to arbitrary n. We do so in this chapter and notice that the formulas
become more involved, the variety of roots of the characteristic equation (in Sec. 3.2)
becomes much larger with increasing n, and the Wronskian plays a more prominent role.
The concepts and methods of solving second-order linear ODEs extend readily to linear
ODEs of higher order.
This chapter follows Chap. 2 naturally, since the results of Chap. 2 can be readily
extended to that of Chap. 3.
Prerequisite: Secs. 2.1, 2.2, 2.6, 2.7, 2.10.
References and Answers to Problems: App. 1 Part A, and App. 2.

3.1

Homogeneous Linear ODEs
Recall from Sec. 1.1 that an ODE is of nth order if the nth derivative y (n) ϭ d ny>dx n of
the unknown function y(x) is the highest occurring derivative. Thus the ODE is of the form
F (x, y, y r , Á , y (n)) ϭ 0
where lower order derivatives and y itself may or may not occur. Such an ODE is called
linear if it can be written
(1)

y (n) ϩ pn؊1(x)y (n؊1) ϩ Á ϩ p1(x)y r ϩ p0(x)y ϭ r (x).

(For n ϭ 2 this is (1) in Sec. 2.1 with p1 ϭ p and p0 ϭ q.) The coefficients p0, Á , pn؊1
and the function r on the right are any given functions of x, and y is unknown. y (n) has
coefficient 1. We call this the standard form. (If you have pn(x)y (n), divide by pn(x)
to get this form.) An nth-order ODE that cannot be written in the form (1) is called
nonlinear.
If r (x) is identically zero, r (x) ϵ 0 (zero for all x considered, usually in some open
interval I), then (1) becomes
(2)

y (n) ϩ pn؊1(x)y (n؊1) ϩ Á ϩ p1(x)y r ϩ p0(x)y ϭ 0

105

c03.qxd

10/27/10

6:20 PM

106

Page 106

CHAP. 3 Higher Order Linear ODEs

and is called homogeneous. If r (x) is not identically zero, then the ODE is called
nonhomogeneous. This is as in Sec. 2.1.
A solution of an nth-order (linear or nonlinear) ODE on some open interval I is a
function y ϭ h(x) that is defined and n times differentiable on I and is such that the ODE
becomes an identity if we replace the unknown function y and its derivatives by h and its
corresponding derivatives.
Sections 3.1–3.2 will be devoted to homogeneous linear ODEs and Section 3.3 to
nonhomogeneous linear ODEs.

Homogeneous Linear ODE: Superposition Principle,
General Solution
The basic superposition or linearity principle of Sec. 2.1 extends to nth order
homogeneous linear ODEs as follows.
THEOREM 1

Fundamental Theorem for the Homogeneous Linear ODE (2)

For a homogeneous linear ODE (2), sums and constant multiples of solutions on
some open interval I are again solutions on I. (This does not hold for a
nonhomogeneous or nonlinear ODE!)

The proof is a simple generalization of that in Sec. 2.1 and we leave it to the student.
Our further discussion parallels and extends that for second-order ODEs in Sec. 2.1.
So we next define a general solution of (2), which will require an extension of linear
independence from 2 to n functions.
DEFINITION

General Solution, Basis, Particular Solution

A general solution of (2) on an open interval I is a solution of (2) on I of the form
(3)

y(x) ϭ c1 y1(x) ϩ Á ϩ cn yn(x)

(c1, Á , cn arbitrary)

where y1, Á , yn is a basis (or fundamental system) of solutions of (2) on I; that
is, these solutions are linearly independent on I, as defined below.
A particular solution of (2) on I is obtained if we assign specific values to the
n constants c1, Á , cn in (3).
DEFINITION

Linear Independence and Dependence

Consider n functions y1(x), Á , yn(x) defined on some interval I.
These functions are called linearly independent on I if the equation
(4)

k1 y1(x) ϩ Á ϩ k n yn(x) ϭ 0

on I

implies that all k1, Á , k n are zero. These functions are called linearly dependent
on I if this equation also holds on I for some k1, Á , k n not all zero.

c03.qxd

10/27/10

6:20 PM

Page 107

SEC. 3.1 Homogeneous Linear ODEs

107

If and only if y1, Á , yn are linearly dependent on I, we can express (at least) one of
these functions on I as a “linear combination” of the other n Ϫ 1 functions, that is, as a
sum of those functions, each multiplied by a constant (zero or not). This motivates the
term “linearly dependent.” For instance, if (4) holds with k 1 0, we can divide by k 1
and express y1 as the linear combination
y1 ϭ Ϫ

1
(k 2 y2 ϩ Á ϩ k n yn).
k1

Note that when n ϭ 2, these concepts reduce to those defined in Sec. 2.1.
EXAMPLE 1

Linear Dependence
Show that the functions y1 ϭ x 2, y2 ϭ 5x, y3 ϭ 2x are linearly dependent on any interval.

᭿

Solution. y2 ϭ 0y1 ϩ 2.5y3. This proves linear dependence on any interval.
EXAMPLE 2

Linear Independence
Show that y1 ϭ x, y2 ϭ x 2, y3 ϭ x 3 are linearly independent on any interval, for instance, on Ϫ1 Ϲ x Ϲ 2.

Solution. Equation (4) is k 1x ϩ k 2x 2 ϩ k 3x 3 ϭ 0. Taking (a) x ϭ Ϫ1, (b) x ϭ 1, (c) x ϭ 2, we get
(a) Ϫk 1 ϩ k 2 Ϫ k 3 ϭ 0,

(b) k 1 ϩ k 2 ϩ k 3 ϭ 0,

(c) 2k 1 ϩ 4k 2 ϩ 8k 3 ϭ 0.

k 2 ϭ 0 from (a) ϩ (b). Then k 3 ϭ 0 from (c) Ϫ2(b). Then k 1 ϭ 0 from (b). This proves linear independence.
A better method for testing linear independence of solutions of ODEs will soon be explained.
᭿

EXAMPLE 3

General Solution. Basis
Solve the fourth-order ODE
y iv Ϫ 5y s ϩ 4y ϭ 0

(where y iv ϭ d 4y>dx 4).

Solution. As in Sec. 2.2 we substitute y ϭ elx. Omitting the common factor elx, we obtain the characteristic
equation
l4 Ϫ 5l2 ϩ 4 ϭ 0.
This is a quadratic equation in ␮ ϭ l2, namely,
␮2 Ϫ 5␮ ϩ 4 ϭ (␮ Ϫ 1)(␮ Ϫ 4) ϭ 0.
The roots are ␮ ϭ 1 and 4. Hence l ϭ Ϫ2, Ϫ1, 1, 2. This gives four solutions. A general solution on any
interval is
y ϭ c1e؊2x ϩ c2e؊x ϩ c3ex ϩ c4e2x
provided those four solutions are linearly independent. This is true but will be shown later.

Initial Value Problem. Existence and Uniqueness
An initial value problem for the ODE (2) consists of (2) and n initial conditions
(5)

y(x 0) ϭ K 0,

y r (x 0) ϭ K 1,

Á,

y (n؊1)(x 0) ϭ K n؊1

with given x 0 in the open interval I considered, and given K 0, Á , K n؊1.

᭿

c03.qxd

10/27/10

6:20 PM

108

Page 108

CHAP. 3 Higher Order Linear ODEs

In extension of the existence and uniqueness theorem in Sec. 2.6 we now have the
following.
THEOREM 2

Existence and Uniqueness Theorem for Initial Value Problems

If the coefficients p0(x), Á , pn؊1(x) of (2) are continuous on some open interval I
and x 0 is in I, then the initial value problem (2), (5) has a unique solution y(x) on I.
Existence is proved in Ref. [A11] in App. 1. Uniqueness can be proved by a slight
generalization of the uniqueness proof at the beginning of App. 4.
EXAMPLE 4

Initial Value Problem for a Third-Order Euler–Cauchy Equation
Solve the following initial value problem on any open interval I on the positive x-axis containing x ϭ 1.
x 3y t Ϫ 3x 2y s ϩ 6xy r Ϫ 6y ϭ 0,

y(1) ϭ 2,

y r (1) ϭ 1,

y s (1) ϭ Ϫ4.

Solution. Step 1. General solution. As in Sec. 2.5 we try y ϭ x m. By differentiation and substitution,
m(m Ϫ 1)(m Ϫ 2)x m Ϫ 3m(m Ϫ 1)x m ϩ 6mx m Ϫ 6x m ϭ 0.
Dropping x m and ordering gives m 3 Ϫ 6m 2 ϩ 11m Ϫ 6 ϭ 0. If we can guess the root m ϭ 1. We can divide
by m Ϫ 1 and find the other roots 2 and 3, thus obtaining the solutions x, x 2, x 3, which are linearly independent
on I (see Example 2). [In general one shall need a root-finding method, such as Newton’s (Sec. 19.2), also
available in a CAS (Computer Algebra System).] Hence a general solution is
y ϭ c1x ϩ c2 x 2 ϩ c3 x 3
valid on any interval I, even when it includes x ϭ 0 where the coefficients of the ODE divided by x 3 (to have
the standard form) are not continuous.
Step 2. Particular solution. The derivatives are y r ϭ c1 ϩ 2c2 x ϩ 3c3 x 2 and y s ϭ 2c2 ϩ 6c3 x. From this, and
y and the initial conditions, we get by setting x ϭ 1
(a) y(1) ϭ c1 ϩ c2 ϩ c3 ϭ

2

(b) y r (1) ϭ c1 ϩ 2c2 ϩ 3c3 ϭ

1

(c) y s (1) ϭ

2c2 ϩ 6c3 ϭ Ϫ4.

This is solved by Cramer’s rule (Sec. 7.6), or by elimination, which is simple, as follows. (b) Ϫ (a) gives
(d) c2 ϩ 2c3 ϭ Ϫ1. Then (c) Ϫ 2(d) gives c3 ϭ Ϫ1. Then (c) gives c2 ϭ 1. Finally c1 ϭ 2 from (a).
᭿
Answer: y ϭ 2x ϩ x 2 Ϫ x 3.

Linear Independence of Solutions. Wronskian
Linear independence of solutions is crucial for obtaining general solutions. Although it can
often be seen by inspection, it would be good to have a criterion for it. Now Theorem 2
in Sec. 2.6 extends from order n ϭ 2 to any n. This extended criterion uses the Wronskian
W of n solutions y1, Á , yn defined as the nth-order determinant

(6)

W(y1, Á , yn) ϭ 5

y1

y2

Á

yn

y1r

y2r

Á

ynr

#

#

Á

#

Á

y (n؊1)
n

y (n؊1)
y (n؊1)
1
2

5.

c03.qxd

10/27/10

6:20 PM

Page 109

SEC. 3.1 Homogeneous Linear ODEs

109

Note that W depends on x since y1, Á , yn do. The criterion states that these solutions
form a basis if and only if W is not zero; more precisely:
THEOREM 3

Linear Dependence and Independence of Solutions

Let the ODE (2) have continuous coefficients p0(x), Á , pn؊1(x) on an open interval
I. Then n solutions y1, Á , yn of (2) on I are linearly dependent on I if and only if their
Wronskian is zero for some x ϭ x 0 in I. Furthermore, if W is zero for x ϭ x 0, then W
is identically zero on I. Hence if there is an x 1 in I at which W is not zero, then y1, Á , yn
are linearly independent on I, so that they form a basis of solutions of (2) on I.
PROOF

(a) Let y1, Á , yn be linearly dependent solutions of (2) on I. Then, by definition, there
are constants k 1, Á , k n not all zero, such that for all x in I,
k 1 y1 ϩ Á ϩ k n yn ϭ 0.

(7)

By n Ϫ 1 differentiations of (7) we obtain for all x in I
k 1 y1r ϩ Á ϩ k n ynr

ϭ0

.
.
.

(8)

k 1y (n؊1)
ϩ Á ϩ k ny (n؊1)
ϭ 0.
1
n
(7), (8) is a homogeneous linear system of algebraic equations with a nontrivial solution
k 1, Á , k n. Hence its coefficient determinant must be zero for every x on I, by Cramer’s
theorem (Sec. 7.7). But that determinant is the Wronskian W, as we see from (6). Hence
W is zero for every x on I.
(b) Conversely, if W is zero at an x 0 in I, then the system (7), (8) with x ϭ x 0 has a
solution k 1*, Á , k n*, not all zero, by the same theorem. With these constants we define
the solution y* ϭ k 1*y1 ϩ Á ϩ k n* yn of (2) on I. By (7), (8) this solution satisfies the
initial conditions y*(x 0) ϭ 0, Á , y*(n؊1)(x 0) ϭ 0. But another solution satisfying the
same conditions is y ϵ 0. Hence y* ϵ y by Theorem 2, which applies since the coefficients
of (2) are continuous. Together, y* ϭ k 1*y1 ϩ Á ϩ k n* yn ϵ 0 on I. This means linear
dependence of y1, Á , yn on I.
(c) If W is zero at an x 0 in I, we have linear dependence by (b) and then W ϵ 0 by (a).
Hence if W is not zero at an x 1 in I, the solutions y1, Á , yn must be linearly independent
on I.
᭿
EXAMPLE 5

Basis, Wronskian
We can now prove that in Example 3 we do have a basis. In evaluating W, pull out the exponential functions
columnwise. In the result, subtract Column 1 from Columns 2, 3, 4 (without changing Column 1). Then expand by
Row 1. In the resulting third-order determinant, subtract Column 1 from Column 2 and expand the result by Row 2:

Wϭ6

e؊2x

e؊x

ex

e2x

1

1

1

1

Ϫ2e؊2x

Ϫe؊x

ex

2e2x

Ϫ2

Ϫ1

1

2

4e؊2x

e؊x

ex

4e2x

4

1

1

4

Ϫ8e؊2x

Ϫe؊x

ex

8e2x

Ϫ8

Ϫ1

1

8

6

ϭ6

6

1

3

ϭ 3 Ϫ3

Ϫ3

7

9

4
0 3 ϭ 72.
16

᭿

c03.qxd

10/27/10

6:20 PM

110

Page 110

CHAP. 3 Higher Order Linear ODEs

A General Solution of (2) Includes All Solutions
Let us first show that general solutions always exist. Indeed, Theorem 3 in Sec. 2.6 extends
as follows.
THEOREM 4

Existence of a General Solution

If the coefficients p0(x), Á , pn؊1(x) of (2) are continuous on some open interval I,
then (2) has a general solution on I.
PROOF

We choose any fixed x 0 in I. By Theorem 2 the ODE (2) has n solutions y1, Á , yn, where
yj satisfies initial conditions (5) with K j؊1 ϭ 1 and all other K’s equal to zero. Their
Wronskian at x 0 equals 1. For instance, when n ϭ 3, then y1(x 0) ϭ 1, y2r (x 0) ϭ 1,
y3s (x 0) ϭ 1, and the other initial values are zero. Thus, as claimed,
y1(x 0)

y2(x 0)

y3(x 0)

1

0

0

W( y1(x 0), y2(x 0), y3(x 0)) ϭ 4 y1r (x 0)

y2r (x 0)

y3r (x 0) 4 ϭ 4 0

1

0 4 ϭ 1.

y1s (x 0)

y2s (x 0)

y3s (x 0)

0

1

0

Hence for any n those solutions y1, Á , yn are linearly independent on I, by Theorem 3.
They form a basis on I, and y ϭ c1 y1 ϩ Á ϩ cn yn is a general solution of (2) on I. ᭿
We can now prove the basic property that, from a general solution of (2), every solution
of (2) can be obtained by choosing suitable values of the arbitrary constants. Hence an
nth-order linear ODE has no singular solutions, that is, solutions that cannot be obtained
from a general solution.
THEOREM 5

General Solution Includes All Solutions

If the ODE (2) has continuous coefficients p0(x), Á , pn؊1(x) on some open interval
I, then every solution y ϭ Y(x) of (2) on I is of the form
(9)

Y(x) ϭ C1 y1(x) ϩ Á ϩ Cn yn(x)

where y1, Á , yn is a basis of solutions of (2) on I and C1, Á , Cn are suitable constants.
PROOF

Let Y be a given solution and y ϭ c1 y1 ϩ Á ϩ cn yn a general solution of (2) on I. We
choose any fixed x 0 in I and show that we can find constants c1, Á , cn for which y and
its first n Ϫ 1 derivatives agree with Y and its corresponding derivatives at x 0. That is,
we should have at x ϭ x 0

(10)

c1 y1 ϩ Á ϩ

cn yn

ϭY

c1 y1r ϩ Á ϩ

cn ynr

ϭ Yr

.
.
.
c1 y (n؊1)
ϩ Á ϩ cn y (n؊1)
ϭ Y (n؊1).
1
n

But this is a linear system of equations in the unknowns c1, Á , cn. Its coefficient
determinant is the Wronskian W of y1, Á , yn at x 0. Since y1, Á , yn form a basis, they

c03.qxd

10/27/10

6:20 PM

Page 111

SEC. 3.2 Homogeneous Linear ODEs with Constant Coefficients

111

are linearly independent, so that W is not zero by Theorem 3. Hence (10) has a unique
solution c1 ϭ C1, Á , cn ϭ Cn (by Cramer’s theorem in Sec. 7.7). With these values we
obtain the particular solution
y*(x) ϭ C1 y1(x) ϩ Á ϩ Cn yn(x)
on I. Equation (10) shows that y* and its first n Ϫ 1 derivatives agree at x 0 with Y and
its corresponding derivatives. That is, y* and Y satisfy, at x 0, the same initial conditions.
The uniqueness theorem (Theorem 2) now implies that y* ϵ Y on I. This proves the
theorem.
᭿
This completes our theory of the homogeneous linear ODE (2). Note that for n ϭ 2 it is
identical with that in Sec. 2.6. This had to be expected.

PROBLEM SET 3.1
1–6

BASES: TYPICAL EXAMPLES

To get a feel for higher order ODEs, show that the given
functions are solutions and form a basis on any interval.
Use Wronskians. In Prob. 6, x Ͼ 0,
1. 1, x, x 2, x 3, y iv ϭ 0
2. ex, e؊x, e2x, y t Ϫ 2y s Ϫ y r ϩ 2y ϭ 0
3. cos x, sin x, x cos x, x sin x, y iv ϩ 2y s ϩ y ϭ 0
4. e؊4x, xe؊4x, x 2e؊4x, y t ϩ 12y s ϩ 48y r ϩ 64y ϭ 0
5. 1, e؊x cos 2x, e؊x sin 2x, y t ϩ 2y s ϩ 5y r ϭ 0
6. 1, x 2, x 4, x 2y t Ϫ 3xy s ϩ 3y r ϭ 0
7. TEAM PROJECT. General Properties of Solutions
of Linear ODEs. These properties are important in
obtaining new solutions from given ones. Therefore
extend Team Project 38 in Sec. 2.2 to nth-order ODEs.
Explore statements on sums and multiples of solutions
of (1) and (2) systematically and with proofs.
Recognize clearly that no new ideas are needed in this
extension from n ϭ 2 to general n.
8–15

LINEAR INDEPENDENCE

Are the given functions linearly independent or dependent
on the half-axis x Ն 0? Give reason.
8. x 2, 1>x 2, 0
9. tan x, cot x, 1

3.2

10. e2x, xe2x, x 2e2x

11. ex cos x, ex sin x, ex

12. sin2 x, cos2 x, cos 2x

13. sin x, cos x, sin 2x

2

2

14. cos x, sin x, 2p

15. cosh 2x, sinh 2x, e2x

16. TEAM PROJECT. Linear Independence and
Dependence. (a) Investigate the given question about
a set S of functions on an interval I. Give an example.
Prove your answer.
(1) If S contains the zero function, can S be linearly
independent?
(2) If S is linearly independent on a subinterval J of I,
is it linearly independent on I?
(3) If S is linearly dependent on a subinterval J of I,
is it linearly dependent on I?
(4) If S is linearly independent on I, is it linearly
independent on a subinterval J?
(5) If S is linearly dependent on I, is it linearly
independent on a subinterval J?
(6) If S is linearly dependent on I, and if T contains S,
is T linearly dependent on I?
(b) In what cases can you use the Wronskian for
testing linear independence? By what other means can
you perform such a test?

Homogeneous Linear ODEs
with Constant Coefficients
We proceed along the lines of Sec. 2.2, and generalize the results from n ϭ 2 to arbitrary n.
We want to solve an nth-order homogeneous linear ODE with constant coefficients,
written as
(1)

y (n) ϩ an؊1 y (n؊1) ϩ Á ϩ a1 y r ϩ a0y ϭ 0

c03.qxd

10/27/10

6:20 PM

112

Page 112

CHAP. 3 Higher Order Linear ODEs

where y (n) ϭ d ny>dx n, etc. As in Sec. 2.2, we substitute y ϭ elx to obtain the characteristic
equation
(2)

l(n) ϩ an؊1l(n؊1) ϩ Á ϩ a1l ϩ a0y ϭ 0

of (1). If l is a root of (2), then y ϭ elx is a solution of (1). To find these roots, you may
need a numeric method, such as Newton’s in Sec. 19.2, also available on the usual CASs.
For general n there are more cases than for n ϭ 2. We can have distinct real roots, simple
complex roots, multiple roots, and multiple complex roots, respectively. This will be shown
next and illustrated by examples.

Distinct Real Roots
If all the n roots l1, Á , ln of (2) are real and different, then the n solutions
(3)

y1 ϭ el1x,

yn ϭ elnx.

Á,

constitute a basis for all x. The corresponding general solution of (1) is
(4)

y ϭ c1el1x ϩ Á ϩ cnelnx.

Indeed, the solutions in (3) are linearly independent, as we shall see after the example.
EXAMPLE 1

Distinct Real Roots
Solve the ODE y t Ϫ 2y s Ϫ y r ϩ 2y ϭ 0.

Solution. The characteristic equation is l3 Ϫ 2l2 Ϫ l ϩ 2 ϭ 0. It has the roots Ϫ1, 1, 2; if you find one
of them by inspection, you can obtain the other two roots by solving a quadratic equation (explain!). The
corresponding general solution (4) is y ϭ c1e؊x ϩ c2ex ϩ c3e2x.
᭿

Linear Independence of (3). Students familiar with nth-order determinants may verify
that, by pulling out all exponential functions from the columns and denoting their product
by E ϭ exp [l1 ϩ Á ϩ ln)x], the Wronskian of the solutions in (3) becomes

(5)

el1x

el2x

Á

elnx

l1el1x

l2el2x

Á

lnelnx

W ϭ 7 l21el1x

l22el2x

Á

l2nelnx 7

#

#

Á

#

ln؊1
el1x
1

ln؊1
el2x
2

Á

ln؊1
elnx
n

1

1

Á

1

l1

l2

Á

ln

ϭ E 7 l21

l22

Á

l2n 7 .

#

#

Á

#

ln؊1
1

ln؊1
2

Á

ln؊1
n

c03.qxd

10/27/10

6:20 PM

Page 113

SEC. 3.2 Homogeneous Linear ODEs with Constant Coefficients

113

The exponential function E is never zero. Hence W ϭ 0 if and only if the determinant on
the right is zero. This is a so-called Vandermonde or Cauchy determinant.1 It can be
shown that it equals
(Ϫ1)n(n؊1)>2V

(6)

where V is the product of all factors lj Ϫ lk with j Ͻ k (Ϲ n); for instance, when n ϭ 3
we get ϪV ϭ Ϫ(l1 Ϫ l2)(l1 Ϫ l3)(l2 Ϫ l3). This shows that the Wronskian is not zero
if and only if all the n roots of (2) are different and thus gives the following.
THEOREM 1

Basis

Solutions y1 ϭ el1x, Á , yn ϭ elnx of (1) (with any real or complex lj’s) form a
basis of solutions of (1) on any open interval if and only if all n roots of (2) are
different.
Actually, Theorem 1 is an important special case of our more general result obtained
from (5) and (6):
THEOREM 2

Linear Independence

Any number of solutions of (1) of the form elx are linearly independent on an open
interval I if and only if the corresponding l are all different.

Simple Complex Roots
If complex roots occur, they must occur in conjugate pairs since the coefficients of (1)
are real. Thus, if l ϭ g ϩ iv is a simple root of (2), so is the conjugate l ϭ g Ϫ iv, and
two corresponding linearly independent solutions are (as in Sec. 2.2, except for notation)
y1 ϭ egx cos vx,
EXAMPLE 2

y2 ϭ egx sin vx.

Simple Complex Roots. Initial Value Problem
Solve the initial value problem
y t Ϫ y s ϩ 100y r Ϫ 100y ϭ 0,

y(0) ϭ 4,

y r (0) ϭ 11,

y s (0) ϭ Ϫ299.

Solution. The characteristic equation is l3 Ϫ l2 ϩ 100l Ϫ 100 ϭ 0. It has the root 1, as can perhaps be
seen by inspection. Then division by l Ϫ 1 shows that the other roots are Ϯ10i. Hence a general solution and
its derivatives (obtained by differentiation) are
y ϭ c1ex ϩ A cos 10x ϩ B sin 10x,
y r ϭ c1ex Ϫ 10A sin 10x ϩ 10B cos 10x,
y s ϭ c1ex Ϫ 100A cos 10x Ϫ 100B sin 10x.

1

ALEXANDRE THÉOPHILE VANDERMONDE (1735–1796), French mathematician, who worked on
solution of equations by determinants. For CAUCHY see footnote 4, in Sec. 2.5.

c03.qxd

10/27/10

6:20 PM

114

Page 114

CHAP. 3 Higher Order Linear ODEs
From this and the initial conditions we obtain, by setting x ϭ 0,
(a) c1 ϩ A ϭ 4,

(b) c1 ϩ 10B ϭ 11,

(c) c1 Ϫ 100A ϭ Ϫ299.

We solve this system for the unknowns A, B, c1. Equation (a) minus Equation (c) gives 101A ϭ 303, A ϭ 3.
Then c1 ϭ 1 from (a) and B ϭ 1 from (b). The solution is (Fig. 73)
y ϭ ex ϩ 3 cos 10x ϩ sin 10x.
This gives the solution curve, which oscillates about ex (dashed in Fig. 73).

᭿

y
20

10
4
0

0

1

2

3

x

Fig. 73. Solution in Example 2

Multiple Real Roots
If a real double root occurs, say, l1 ϭ l2, then y1 ϭ y2 in (3), and we take y1 and xy1 as
corresponding linearly independent solutions. This is as in Sec. 2.2.
More generally, if l is a real root of order m, then m corresponding linearly independent
solutions are
(7)

elx,

xelx,

x 2elx,

Á , x m؊1elx.

We derive these solutions after the next example and indicate how to prove their linear
independence.

EXAMPLE 3

Real Double and Triple Roots
Solve the ODE y v Ϫ 3y iv ϩ 3y t Ϫ y s ϭ 0.

Solution. The characteristic equation l5 Ϫ 3l4 ϩ 3l3 Ϫ l2 ϭ 0 has the roots l1 ϭ l2 ϭ 0, and l3 ϭ l4 ϭ

l5 ϭ 1, and the answer is
(8)

y ϭ c1 ϩ c2 x ϩ (c3 ϩ c4 x ϩ c5 x 2)ex.

Derivation of (7). We write the left side of (1) as
L[ y] ϭ y (n) ϩ an؊1 y (n؊1) ϩ Á ϩ a0y.
Let y ϭ elx. Then by performing the differentiations we have
L[elx] ϭ (ln ϩ an؊1ln؊1 ϩ Á ϩ a0)elx.

᭿

c03.qxd

10/27/10

6:20 PM

Page 115

SEC. 3.2 Homogeneous Linear ODEs with Constant Coefficients

115

Now let l1 be a root of mth order of the polynomial on the right, where m Ϲ n. For m Ͻ n
let lm؉1, Á , ln be the other roots, all different from l1. Writing the polynomial in
product form, we then have
L[elx] ϭ (l Ϫ l1)mh(l)elx
with h(l) ϭ 1 if m ϭ n, and h(l) ϭ (l Ϫ lmϩ1) Á (l Ϫ ln) if m Ͻ n. Now comes the
key idea: We differentiate on both sides with respect to l,
(9)

0
0
L[elx] ϭ m(l Ϫ l1)m؊1h(l)elx ϩ (l Ϫ l1)m
[h(l)elx].
0l
0l

The differentiations with respect to x and l are independent and the resulting derivatives
are continuous, so that we can interchange their order on the left:
(10)

0
0 lx
L[elx] ϭ L c
e d ϭ L[xelx].
0l
0l

The right side of (9) is zero for l ϭ l1 because of the factors l Ϫ l1 (and m м 2 since
we have a multiple root!). Hence L[xel1x] ϭ 0 by (9) and (10). This proves that xel1x is
a solution of (1).
We can repeat this step and produce x 2el1x, Á , x m؊1el1x by another m Ϫ 2 such
differentiations with respect to l. Going one step further would no longer give zero on the
right because the lowest power of l Ϫ l1 would then be (l Ϫ l1)0, multiplied by m!h(l)
and h(l1) 0 because h(l) has no factors l Ϫ l1; so we get precisely the solutions in (7).
We finally show that the solutions (7) are linearly independent. For a specific n this
can be seen by calculating their Wronskian, which turns out to be nonzero. For arbitrary
m we can pull out the exponential functions from the Wronskian. This gives (elx)m ϭ elmx
times a determinant which by “row operations” can be reduced to the Wronskian of 1,
x, Á , x m؊1. The latter is constant and different from zero (equal to 1!2! Á (m Ϫ 1)!).
These functions are solutions of the ODE y (m) ϭ 0, so that linear independence follows
from Theroem 3 in Sec. 3.1.

Multiple Complex Roots
In this case, real solutions are obtained as for complex simple roots above. Consequently,
if l ϭ g ϩ iv is a complex double root, so is the conjugate l ϭ g Ϫ iv. Corresponding
linearly independent solutions are
(11)

egx cos vx,

egx sin vx,

xegx cos vx,

xegx sin vx.

The first two of these result from elx and elx as before, and the second two from xelx
and xelx in the same fashion. Obviously, the corresponding general solution is
(12)

y ϭ egx[(A1 ϩ A2x) cos vx ϩ (B1 ϩ B2x) sin vx].

For complex triple roots (which hardly ever occur in applications), one would obtain
two more solutions x 2egx cos vx, x 2egx sin vx, and so on.

c03.qxd

10/27/10

116

6:20 PM

Page 116

CHAP. 3 Higher Order Linear ODEs

PROBLEM SET 3.2
1–6

GENERAL SOLUTION

Solve the given ODE. Show the details of your work.
1. y t ϩ 25y r ϭ 0
2. y iv ϩ 2y s ϩ y ϭ 0
3. y iv ϩ 4y s ϭ 0
4. (D 3 Ϫ D 2 Ϫ D ϩ I ) y ϭ 0
5. (D 4 ϩ 10D 2 ϩ 9I ) y ϭ 0
6. (D 5 ϩ 8D 3 ϩ 16D) y ϭ 0
7–13

INITIAL VALUE PROBLEM

Solve the IVP by a CAS, giving a general solution and the
particular solution and its graph.
7. y t ϩ 3.2y s ϩ 4.81y r ϭ 0, y(0) ϭ 3.4, y r(0) ϭ Ϫ4.6,
y s (0) ϭ 9.91
8. y t ϩ 7.5y s ϩ 14.25y r Ϫ 9.125y ϭ 0, y(0) ϭ 10.05,
y r (0) ϭ Ϫ54.975, y s (0) ϭ 257.5125
9. 4y t ϩ 8y s ϩ 41y r ϩ 37y ϭ 0, y(0) ϭ 9,
y r (0) ϭ Ϫ6.5, y s (0) ϭ Ϫ39.75
10. y iv ϩ 4y ϭ 0, y(0) ϭ 12, y r (0) ϭ Ϫ 32, y s (0) ϭ 52,
y t (0) ϭ Ϫ72
11. y iv Ϫ 9y s Ϫ 400y ϭ 0, y(0) ϭ 0, y r (0) ϭ 0,
y s (0) ϭ 41, y t (0) ϭ 0
12. y v Ϫ 5y t ϩ 4y r ϭ 0, y(0) ϭ 3, y r (0) ϭ Ϫ5,
y s (0) ϭ 11, y t (0) ϭ Ϫ23, y iv(0) ϭ 47

3.3

13. y iv ϩ 0.45y t Ϫ 0.165y s ϩ 0.0045y r Ϫ 0.00175y ϭ 0,
y(0) ϭ 17.4, y r (0) ϭ Ϫ2.82, y s (0) ϭ 2.0485,
y t (0) ϭ Ϫ1.458675
14. PROJECT. Reduction of Order. This is of practical
interest since a single solution of an ODE can often be
guessed. For second order, see Example 7 in Sec. 2.1.
(a) How could you reduce the order of a linear
constant-coefficient ODE if a solution is known?
(b) Extend the method to a variable-coefficient ODE
y t ϩ p2(x)y s ϩ p1(x)y r ϩ p0(x)y ϭ 0.
Assuming a solution y1 to be known, show that another
solution is y2(x) ϭ u(x)y1(x) with u(x) ϭ ͐ z(x) dx and
z obtained by solving
y1z s ϩ (3y1r ϩ p2 y1)z r ϩ (3y1s ϩ 2p2 y1r ϩ p1 y1)z ϭ 0.
(c) Reduce
x 3y t Ϫ 3x 2y s ϩ (6 Ϫ x 2)xy r Ϫ (6 Ϫ x 2)y ϭ 0,
using y1 ϭ x (perhaps obtainable by inspection).
15. CAS EXPERIMENT. Reduction of Order. Starting
with a basis, find third-order linear ODEs with variable
coefficients for which the reduction to second order
turns out to be relatively simple.

Nonhomogeneous Linear ODEs
We now turn from homogeneous to nonhomogeneous linear ODEs of nth order. We write
them in standard form
(1)

y (n) ϩ pn؊1(x)y (n؊1) ϩ Á ϩ p1(x)y r ϩ p0(x)y ϭ r (x)

with y (n) ϭ d ny>dx n as the first term, and r (x) [ 0. As for second-order ODEs, a general
solution of (1) on an open interval I of the x-axis is of the form
(2)

y(x) ϭ yh(x) ϩ yp(x).

Here yh(x) ϭ c1 y1(x) ϩ Á ϩ cn yn(x) is a general solution of the corresponding
homogeneous ODE
(3)

y (n) ϩ pn؊1(x)y (n؊1) ϩ Á ϩ p1(x)y r ϩ p0(x)y ϭ 0

on I. Also, yp is any solution of (1) on I containing no arbitrary constants. If (1) has
continuous coefficients and a continuous r (x) on I, then a general solution of (1) exists
and includes all solutions. Thus (1) has no singular solutions.

c03.qxd

10/27/10

6:20 PM

Page 117

SEC. 3.3 Nonhomogeneous Linear ODEs

117

An initial value problem for (1) consists of (1) and n initial conditions
y(x 0) ϭ K 0,

(4)

y r (x 0) ϭ K 1,

y (n؊1)(x 0) ϭ K n؊1

Á,

with x 0 in I. Under those continuity assumptions it has a unique solution. The ideas of
proof are the same as those for n ϭ 2 in Sec. 2.7.

Method of Undetermined Coefficients
Equation (2) shows that for solving (1) we have to determine a particular solution of (1).
For a constant-coefficient equation
y (n) ϩ an؊1 y (n؊1) ϩ Á ϩ a1 y r ϩ a0y ϭ r (x)

(5)

(a0, Á , an؊1 constant) and special r (x) as in Sec. 2.7, such a yp(x) can be determined by
the method of undetermined coefficients, as in Sec. 2.7, using the following rules.
(A) Basic Rule as in Sec. 2.7.
(B) Modification Rule. If a term in your choice for yp(x) is a solution of the
homogeneous equation (3), then multiply this term by x k, where k is the smallest
positive integer such that this term times x k is not a solution of (3).
(C) Sum Rule as in Sec. 2.7.
The practical application of the method is the same as that in Sec. 2.7. It suffices to
illustrate the typical steps of solving an initial value problem and, in particular, the new
Modification Rule, which includes the old Modification Rule as a particular case (with
k ϭ 1 or 2). We shall see that the technicalities are the same as for n ϭ 2, except perhaps
for the more involved determination of the constants.
EXAMPLE 1

Initial Value Problem. Modification Rule
Solve the initial value problem
(6)

y t ϩ 3y s ϩ 3y r ϩ y ϭ 30e؊x,

y(0) ϭ 3,

y r (0) ϭ Ϫ3,

y s (0) ϭ Ϫ47.

Solution. Step 1. The characteristic equation is l3 ϩ 3l2 ϩ 3l ϩ 1 ϭ (l ϩ 1)3 ϭ 0. It has the triple root
l ϭ Ϫ1. Hence a general solution of the homogeneous ODE is

yh ϭ c1e؊x ϩ c2 xe؊x ϩ c3 x 2e؊x
ϭ (c1 ϩ c2 x ϩ c3 x 2)e؊x.
Step 2. If we try yp ϭ Ce؊x, we get ϪC ϩ 3C Ϫ 3C ϩ C ϭ 30, which has no solution. Try Cxe؊x and
Cx 2e؊x. The Modification Rule calls for
yp ϭ Cx 3e؊x.
Then

ypr ϭ C(3x 2 Ϫ x 3)e؊x,
yps ϭ C(6x Ϫ 6x 2 ϩ x 3)e؊x,
ypt ϭ C(6 Ϫ 18x ϩ 9x 2 Ϫ x 3)e؊x.

c03.qxd

10/27/10

118

6:20 PM

Page 118

CHAP. 3 Higher Order Linear ODEs
Substitution of these expressions into (6) and omission of the common factor e؊x gives
C(6 Ϫ 18x ϩ 9x 2 Ϫ x 3) ϩ 3C(6x Ϫ 6x 2 ϩ x 3) ϩ 3C(3x 2 Ϫ x 3) ϩ Cx 3 ϭ 30.
The linear, quadratic, and cubic terms drop out, and 6C ϭ 30. Hence C ϭ 5. This gives yp ϭ 5x 3e؊x.
Step 3. We now write down y ϭ yh ϩ yp, the general solution of the given ODE. From it we find c1 by the
first initial condition. We insert the value, differentiate, and determine c2 from the second initial condition, insert
the value, and finally determine c3 from y s (0) and the third initial condition:
y ϭ yh ϩ yp ϭ (c1 ϩ c2x ϩ c3x 2)e؊x ϩ 5x 3e؊x,

y(0) ϭ c1 ϭ 3

y r ϭ [Ϫ3 ϩ c2 ϩ (Ϫc2 ϩ 2c3)x ϩ (15 Ϫ c3)x 2 Ϫ 5x 3]e؊x,

y r (0) ϭ Ϫ3 ϩ c2 ϭ Ϫ3,

c2 ϭ 0

y s ϭ [3 ϩ 2c3 ϩ (30 Ϫ 4c3)x ϩ (Ϫ30 ϩ c3)x 2 ϩ 5x 3]e؊x,

y s (0) ϭ 3 ϩ 2c3 ϭ Ϫ47,

c3 ϭ Ϫ25.

Hence the answer to our problem is (Fig. 73)
y ϭ (3 Ϫ 25x 2)e؊x ϩ 5x 3e؊x.
The curve of y begins at (0, 3) with a negative slope, as expected from the initial values, and approaches zero
as x : ϱ. The dashed curve in Fig. 74 is yp.
᭿

y
5

0

5

10

x

–5

Fig. 74. y and yp (dashed) in Example 1

Method of Variation of Parameters
The method of variation of parameters (see Sec. 2.10) also extends to arbitrary order n.
It gives a particular solution yp for the nonhomogeneous equation (1) (in standard form
with y (n) as the first term!) by the formula
n

yp(x) ϭ a yk(x)
kϭ1

(7)
ϭ y1(x)

Ύ W(x) r (x) dx
Wk(x)

Ύ W(x) r (x) dx ϩ Á ϩ y (x) Ύ W(x) r (x) dx
W1(x)

Wn(x)

n

on an open interval I on which the coefficients of (1) and r (x) are continuous. In (7) the
functions y1, Á , yn form a basis of the homogeneous ODE (3), with Wronskian W, and
Wj ( j ϭ 1, Á , n) is obtained from W by replacing the jth column of W by the column
[0 0 Á 0 1]T. Thus, when n ϭ 2, this becomes identical with (2) in Sec. 2.10,
Wϭ `

y1

y2

y1r

y2r

`,

W1 ϭ `

0

y2

1

y2r

` ϭ Ϫy2,

W2 ϭ `

y1

0

y1r

1

` ϭ y1.

c03.qxd

10/27/10

6:20 PM

Page 119

SEC. 3.3 Nonhomogeneous Linear ODEs

119

The proof of (7) uses an extension of the idea of the proof of (2) in Sec. 2.10 and can
be found in Ref [A11] listed in App. 1.
EXAMPLE 2

Variation of Parameters. Nonhomogeneous Euler–Cauchy Equation
Solve the nonhomogeneous Euler–Cauchy equation
x 3y t Ϫ 3x 2y s ϩ 6xy r Ϫ 6y ϭ x 4 ln x

(x Ͼ 0).

Solution. Step 1. General solution of the homogeneous ODE. Substitution of y ϭ x m and the derivatives
into the homogeneous ODE and deletion of the factor x m gives

m(m Ϫ 1)(m Ϫ 2) Ϫ 3m(m Ϫ 1) ϩ 6m Ϫ 6 ϭ 0.
The roots are 1, 2, 3 and give as a basis
y1 ϭ x,

y2 ϭ x 2,

y3 ϭ x 3.

Hence the corresponding general solution of the homogeneous ODE is
yh ϭ c1x ϩ c2x 2 ϩ c3x 3.
Step 2. Determinants needed in (7). These are
x

x2

x3

Wϭ31

2x

3x 2 3 ϭ 2x 3

0

2

6x

0

x2

x3

W1 ϭ 4 0

2x

3x 2 4 ϭ x 4

1

2

6x

x

0

x3

W2 ϭ 4 1

0

3x 2 4 ϭ Ϫ2x 3

0

1

6x

x

x2

0

W3 ϭ 4 1

2x

0 4 ϭ x 2.

0

2

1

Step 3. Integration. In (7) we also need the right side r (x) of our ODE in standard form, obtained by division
of the given equation by the coefficient x 3 of y t ; thus, r (x) ϭ (x 4 ln x)>x 3 ϭ x ln x. In (7) we have the simple
quotients W1>W ϭ x>2, W2>W ϭ Ϫ1, W3>W ϭ 1>(2x). Hence (7) becomes
yp ϭ x
ϭ

Ύ 2 x ln x dx Ϫ x Ύ x ln x dx ϩ x Ύ 2x x ln x dx
x

2

3

1

x x3
x3
x2
x2
x3
a ln x Ϫ b Ϫ x 2 a ln x Ϫ b ϩ
(x ln x Ϫ x).
2 3
9
2
4
2

Simplification gives yp ϭ 16 x 4 (ln x Ϫ

11
6 ).

Hence the answer is

y ϭ yh ϩ yp ϭ c1x ϩ c2 x 2 ϩ c3 x 3 ϩ 16 x 4 (ln x Ϫ 11
6 ).
Figure 75 shows yp. Can you explain the shape of this curve? Its behavior near x ϭ 0? The occurrence of a minimum?
᭿
Its rapid increase? Why would the method of undetermined coefficients not have given the solution?

c03.qxd

10/27/10

6:20 PM

120

Page 120

CHAP. 3 Higher Order Linear ODEs
y
30
20
10
0

x

10

5

–10
–20

Fig. 75. Particular solution yp of the nonhomogeneous
Euler–Cauchy equation in Example 2

Application: Elastic Beams
Whereas second-order ODEs have various applications, of which we have discussed some
of the more important ones, higher order ODEs have much fewer engineering applications.
An important fourth-order ODE governs the bending of elastic beams, such as wooden or
iron girders in a building or a bridge.
A related application of vibration of beams does not fit in here since it leads to PDEs
and will therefore be discussed in Sec. 12.3.
EXAMPLE 3

Bending of an Elastic Beam under a Load
We consider a beam B of length L and constant (e.g., rectangular) cross section and homogeneous elastic
material (e.g., steel); see Fig. 76. We assume that under its own weight the beam is bent so little that it is
practically straight. If we apply a load to B in a vertical plane through the axis of symmetry (the x-axis in
Fig. 76), B is bent. Its axis is curved into the so-called elastic curve C (or deflection curve). It is shown in
elasticity theory that the bending moment M(x) is proportional to the curvature k(x) of C. We assume the bending
to be small, so that the deflection y(x) and its derivative y r (x) (determining the tangent direction of C) are small.
Then, by calculus, k ϭ y s >(1 ϩ y r 2)3>2 Ϸ y s . Hence
M(x) ϭ EIy s (x).
EI is the constant of proportionality. E is Young’s modulus of elasticity of the material of the beam. I is the
moment of inertia of the cross section about the (horizontal) z-axis in Fig. 76.
Elasticity theory shows further that M s (x) ϭ f (x), where f (x) is the load per unit length. Together,
EIy iv ϭ f (x).

(8)

x
L
y

z
Undeformed beam

x

y

z

Deformed beam
under uniform load
(simply supported)

Fig. 76. Elastic beam

c03.qxd

10/27/10

6:20 PM

Page 121

SEC. 3.3 Nonhomogeneous Linear ODEs

121

In applications the most important supports and corresponding boundary conditions are as follows and shown
in Fig. 77.
(A) Simply supported

y ϭ y s ϭ 0 at x ϭ 0 and L

(B) Clamped at both ends

y ϭ y r ϭ 0 at x ϭ 0 and L

(C) Clamped at x ϭ 0, free at x ϭ L

y(0) ϭ y r (0) ϭ 0, y s (L) ϭ y t (L) ϭ 0.

The boundary condition y ϭ 0 means no displacement at that point, y r ϭ 0 means a horizontal tangent, y s ϭ 0
means no bending moment, and y t ϭ 0 means no shear force.
Let us apply this to the uniformly loaded simply supported beam in Fig. 76. The load is f (x) ϵ f0 ϭ const.
Then (8) is
f0
(9)
y iv ϭ k,
kϭ .
EI
This can be solved simply by calculus. Two integrations give
k 2
x ϩ c1x ϩ c2.
2

ys ϭ

y s (0) ϭ 0 gives c2 ϭ 0. Then y s (L) ϭ L (12 kL ϩ c1) ϭ 0, c1 ϭ ϪkL>2 (since L
ys ϭ

0). Hence

k 2
(x Ϫ Lx).
2

Integrating this twice, we obtain
yϭ

k 1 4 L 3
a x Ϫ x ϩ c3 x ϩ c4 b
2 12
6

with c4 ϭ 0 from y(0) ϭ 0. Then
y(L) ϭ

kL L3
L3
a Ϫ
ϩ c3 b ϭ 0,
2 12
6

c3 ϭ

L3
.
12

Inserting the expression for k, we obtain as our solution
yϭ

f0
24EI

(x 4 Ϫ 2L x 3 ϩ L3x).

Since the boundary conditions at both ends are the same, we expect the deflection y(x) to be “symmetric” with
respect to L>2, that is, y(x) ϭ y(L Ϫ x). Verify this directly or set x ϭ u ϩ L>2 and show that y becomes an
even function of u,
yϭ

f0
24EI

au 2 Ϫ

1 2
5
L b au 2 Ϫ L2 b .
4
4

From this we can see that the maximum deflection in the middle at u ϭ 0 (x ϭ L>2) is 5f0L4>(16 # 24EI). Recall
that the positive direction points downward.
᭿
x
(A) Simply supported
x=0

x=L

(B) Clamped at both
ends
x=0

x=0

x=L

x=L

(C) Clamped at the left
end, free at the
right end

Fig. 77. Supports of a beam

c03.qxd

10/27/10

122

6:20 PM

Page 122

CHAP. 3 Higher Order Linear ODEs

PROBLEM SET 3.3
1–7
GENERAL SOLUTION
Solve the following ODEs, showing the details of your
work.
1. y t ϩ 3y s ϩ 3y r ϩ y ϭ ex Ϫ x Ϫ 1
2. y t ϩ 2y s Ϫ y r Ϫ 2y ϭ 1 Ϫ 4x 3
3. (D 4 ϩ 10D 2 ϩ 9I ) y ϭ 6.5 sinh 2x
4. (D 3 ϩ 3D 2 Ϫ 5D Ϫ 39I )y ϭ Ϫ300 cos x
5. (x 3D 3 ϩ x 2D 2 Ϫ 2xD ϩ 2I )y ϭ x ؊2
6. (D 3 ϩ 4D)y ϭ sin x
7. (D 3 Ϫ 9D 2 ϩ 27D Ϫ 27I )y ϭ 27 sin 3x
8–13
INITIAL VALUE PROBLEM
Solve the given IVP, showing the details of your work.
8. y iv Ϫ 5y s ϩ 4y ϭ 10e؊3x, y(0) ϭ 1, y r (0) ϭ 0,
y s (0) ϭ 0, y t (0) ϭ 0
9. y iv ϩ 5y s ϩ 4y ϭ 90 sin 4x, y(0) ϭ 1, y r (0) ϭ 2,
y s (0) ϭ Ϫ1, y t (0) ϭ Ϫ32
10. x 3y t ϩ xy r Ϫ y ϭ x 2, y(1) ϭ 1, y r (1) ϭ 3,
y s (1) ϭ 14
11. (D 3 Ϫ 2D 2 Ϫ 3D)y ϭ 74e؊3x sin x, y(0) ϭ Ϫ1.4,
y r (0) ϭ 3.2, y s (0) ϭ Ϫ5.2
12. (D 3 Ϫ 2D 2 Ϫ 9D ϩ 18I )y ϭ e2x, y(0) ϭ 4.5,
y r (0) ϭ 8.8, y s (0) ϭ 17.2

13. (D 3 Ϫ 4D)y ϭ 10 cos x ϩ 5 sin x, y(0) ϭ 3,
y r (0) ϭ Ϫ2, y s (0) ϭ Ϫ1
14. CAS EXPERIMENT. Undetermined Coefficients.
Since variation of parameters is generally complicated,
it seems worthwhile to try to extend the other method.
Find out experimentally for what ODEs this is possible
and for what not. Hint: Work backward, solving ODEs
with a CAS and then looking whether the solution
could be obtained by undetermined coefficients. For
example, consider
y t Ϫ 3y s ϩ 3y r Ϫ y ϭ x 1>2ex
and
x 3y t ϩ x 2y s Ϫ 2xy r ϩ 2y ϭ x 3 ln x.
15. WRITING REPORT. Comparison of Methods. Write
a report on the method of undetermined coefficients and
the method of variation of parameters, discussing and
comparing the advantages and disadvantages of each
method. Illustrate your findings with typical examples.
Try to show that the method of undetermined coefficients,
say, for a third-order ODE with constant coefficients and
an exponential function on the right, can be derived from
the method of variation of parameters.

CHAPTER 3 REVIEW QUESTIONS AND PROBLEMS
1. What is the superposition or linearity principle? For
what nth-order ODEs does it hold?
2. List some other basic theorems that extend from
second-order to nth-order ODEs.
3. If you know a general solution of a homogeneous linear
ODE, what do you need to obtain from it a general
solution of a corresponding nonhomogeneous linear
ODE?
4. What form does an initial value problem for an nthorder linear ODE have?
5. What is the Wronskian? What is it used for?
6–15
GENERAL SOLUTION
Solve the given ODE. Show the details of your work.
6. y iv Ϫ 3y s Ϫ 4y ϭ 0
7. y t ϩ 4y s ϩ 13y r ϭ 0
8. y t Ϫ 4y s Ϫ y r ϩ 4y ϭ 30e2x
9. (D 4 Ϫ 16I )y ϭ Ϫ15 cosh x
10. x 2y t ϩ 3xy s Ϫ 2y r ϭ 0

11. y t ϩ 4.5y s ϩ 6.75y r ϩ 3.375y ϭ 0
12. (D 3 Ϫ D)y ϭ sinh 0.8x
13. (D 3 ϩ 6D 2 ϩ 12D ϩ 8I )y ϭ 8x 2
14. (D 4 Ϫ 13D 2 ϩ 36I )y ϭ 12ex
15. 4x 3y t ϩ 3xy r Ϫ 3y ϭ 10

INITIAL VALUE PROBLEM
16–20
Solve the IVP. Show the details of your work.
16. (D 3 Ϫ D 2 Ϫ D ϩ I )y ϭ 0, y(0) ϭ 0, Dy(0) ϭ 1,
D 2y(0) ϭ 0
17. y t ϩ 5y s ϩ 24y r ϩ 20y ϭ x, y(0) ϭ 1.94,
y r (0) ϭ Ϫ3.95, y s ϭ Ϫ24
18. (D 4 Ϫ 26D 2 ϩ 25I )y ϭ 50(x ϩ 1)2, y(0) ϭ 12.16,
Dy(0) ϭ Ϫ6, D 2y(0) ϭ 34, D 3y(0) ϭ Ϫ130
19. (D 3 ϩ 9D 2 ϩ 23D ϩ 15I )y ϭ 12exp(Ϫ4x),
y(0) ϭ 9, Dy(0) ϭ Ϫ41, D 2y(0) ϭ 189
20. (D 3 ϩ 3D 2 ϩ 3D ϩ I )y ϭ 8 sin x, y(0) ϭ Ϫ1,
y r (0) ϭ Ϫ3, y s (0) ϭ 5

c03.qxd

10/27/10

6:20 PM

Page 123

Summary of Chapter 3

123

SUMMARY OF CHAPTER

3

Higher Order Linear ODEs
Compare with the similar Summary of Chap. 2 (the case n ‫ ؍‬2).
Chapter 3 extends Chap. 2 from order n ϭ 2 to arbitrary order n. An nth-order
linear ODE is an ODE that can be written
(1)

y (n) ϩ pn؊1(x)y (n؊1) ϩ Á ϩ p1(x)y r ϩ p0(x)y ϭ r (x)

with y (n) ϭ d ny>dx n as the first term; we again call this the standard form. Equation
(1) is called homogeneous if r (x) ϵ 0 on a given open interval I considered,
nonhomogeneous if r (x) [ 0 on I. For the homogeneous ODE
(2)

y (n) ϩ pn؊1(x)y (n؊1) ϩ Á ϩ p1(x)y r ϩ p0(x)y ϭ 0

the superposition principle (Sec. 3.1) holds, just as in the case n ϭ 2. A basis or
fundamental system of solutions of (2) on I consists of n linearly independent
solutions y1, Á , yn of (2) on I. A general solution of (2) on I is a linear combination
of these,
(3)

y ϭ c1 y1 ϩ Á ϩ cn yn

(c1, Á , cn arbitrary constants).

A general solution of the nonhomogeneous ODE (1) on I is of the form
y ϭ yh ϩ yp

(4)

(Sec. 3.3).

Here, yp is a particular solution of (1) and is obtained by two methods (undetermined
coefficients or variation of parameters) explained in Sec. 3.3.
An initial value problem for (1) or (2) consists of one of these ODEs and n
initial conditions (Secs. 3.1, 3.3)
(5)

y(x 0) ϭ K 0,

y r (x 0) ϭ K 1,

Á,

y (n؊1)(x 0) ϭ K n؊1

with given x 0 in I and given K 0, Á , K n؊1. If p0, Á , pn؊1, r are continuous on I,
then general solutions of (1) and (2) on I exist, and initial value problems (1), (5)
or (2), (5) have a unique solution.

c04.qxd

10/27/10

9:32 PM

Page 124

CHAPTER

4

Systems of ODEs. Phase Plane.
Qualitative Methods
Tying in with Chap. 3, we present another method of solving higher order ODEs in
Sec. 4.1. This converts any nth-order ODE into a system of n first-order ODEs. We also
show some applications. Moreover, in the same section we solve systems of first-order
ODEs that occur directly in applications, that is, not derived from an nth-order ODE but
dictated by the application such as two tanks in mixing problems and two circuits in
electrical networks. (The elementary aspects of vectors and matrices needed in this chapter
are reviewed in Sec. 4.0 and are probably familiar to most students.)
In Sec. 4.3 we introduce a totally different way of looking at systems of ODEs. The
method consists of examining the general behavior of whole families of solutions of ODEs
in the phase plane, and aptly is called the phase plane method. It gives information on the
stability of solutions. (Stability of a physical system is desirable and means roughly that a
small change at some instant causes only a small change in the behavior of the system at
later times.) This approach to systems of ODEs is a qualitative method because it depends
only on the nature of the ODEs and does not require the actual solutions. This can be very
useful because it is often difficult or even impossible to solve systems of ODEs. In contrast,
the approach of actually solving a system is known as a quantitative method.
The phase plane method has many applications in control theory, circuit theory,
population dynamics and so on. Its use in linear systems is discussed in Secs. 4.3, 4.4,
and 4.6 and its even more important use in nonlinear systems is discussed in Sec. 4.5 with
applications to the pendulum equation and the Lokta–Volterra population model. The
chapter closes with a discussion of nonhomogeneous linear systems of ODEs.
NOTATION. We continue to denote unknown functions by y; thus, y1(t), y2(t)—
analogous to Chaps. 1–3. (Note that some authors use x for functions, x 1(t), x 2(t) when
dealing with systems of ODEs.)
Prerequisite: Chap. 2.
References and Answers to Problems: App. 1 Part A, and App. 2.

4.0

For Reference:
Basics of Matrices and Vectors
For clarity and simplicity of notation, we use matrices and vectors in our discussion
of linear systems of ODEs. We need only a few elementary facts (and not the bulk of
the material of Chaps. 7 and 8). Most students will very likely be already familiar

124

c04.qxd

10/27/10

9:32 PM

Page 125

SEC. 4.0 For Reference: Basics of Matrices and Vectors

125

with these facts. Thus this section is for reference only. Begin with Sec. 4.1 and consult
4.0 as needed.
Most of our linear systems will consist of two linear ODEs in two unknown functions
y1(t), y2(t),
(1)

y1r ϭ a11y1 ϩ a12y2,

y1r ϭ Ϫ5y1 ϩ 2y2

for example,

y2r ϭ a21y1 ϩ a22y2,

y2r ϭ 13y1 ϩ 12 y2

(perhaps with additional given functions g1(t), g2(t) on the right in the two ODEs).
Similarly, a linear system of n first-order ODEs in n unknown functions y1(t), Á , yn(t)
is of the form
y1r ϭ a11y1 ϩ a12y2 ϩ Á ϩ a1nyn
y2r ϭ a21y1 ϩ a22y2 ϩ Á ϩ a2nyn

(2)

.............................
ynr ϭ an1y1 ϩ an2y2 ϩ Á ϩ annyn
(perhaps with an additional given function on the right in each ODE).

Some Definitions and Terms
Matrices. In (1) the (constant or variable) coefficients form a 2 ؋ 2 matrix A, that is,
an array
(3)

A ϭ [ajk] ϭ

c

a11

a12

a21

a22

d,

Aϭ

for example,

c

Ϫ5

2

13

1
2

d.

Similarly, the coefficients in (2) form an n ؋ n matrix

(4)

A ϭ [ajk] ϭ E

a11

a12

Á

a1n

a21

a22

Á

a2n

#

#

Á

#

an1

an2

Á

ann

U.

The a11, a12, Á are called entries, the horizontal lines rows, and the vertical lines columns.
Thus, in (3) the first row is [a11 a12], the second row is [a21 a22], and the first and
second columns are

ca d
a11
21

ca d.
a12

and

22

In the “double subscript notation” for entries, the first subscript denotes the row and the
second the column in which the entry stands. Similarly in (4). The main diagonal is the
diagonal a11 a22 Á ann in (4), hence a11 a22 in (3).

c04.qxd

10/27/10

126

9:32 PM

Page 126

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods

We shall need only square matrices, that is, matrices with the same number of rows
and columns, as in (3) and (4).
Vectors. A column vector x with n components x 1, Á , x n is of the form
x1
xϭE

x2

U,

thus if n ϭ 2,

xϭ

o

c

x1
x2

d.

xn
Similarly, a row vector v is of the form
v ϭ [v1

thus if n ϭ 2, then

vn],

Á

v ϭ [v1

v2].

Calculations with Matrices and Vectors
Equality. Two n ϫ n matrices are equal if and only if corresponding entries are equal.
Thus for n ϭ 2, let
Aϭ

c

a11

a12

a21

a22

d

Bϭ

and

c

b11

b12

b21

b22

d.

Then A ϭ B if and only if
a11 ϭ b11,

a12 ϭ b12

a21 ϭ b21,

a22 ϭ b22.

Two column vectors (or two row vectors) are equal if and only if they both have n
components and corresponding components are equal. Thus, let
vϭ

c d
v1

xϭ

and

v2

c d.
x1

vϭx

Then

x2

if and only if

v1 ϭ x 1
v2 ϭ x 2.

Addition is performed by adding corresponding entries (or components); here, matrices
must both be n ϫ n, and vectors must both have the same number of components. Thus
for n ϭ 2,
(5)

AϩBϭ

c

a11 ϩ b11

a12 ϩ b12

a21 ϩ b21

a22 ϩ b22

d,

vϩxϭ

c

v1 ϩ x 1
v2 ϩ x 2

d.

Scalar multiplication (multiplication by a number c) is performed by multiplying each
entry (or component) by c. For example, if
Aϭ

c

9
Ϫ2

3
0

d,

then

Ϫ7A ϭ

c

Ϫ63 Ϫ21
14

0

d.

c04.qxd

10/27/10

9:32 PM

Page 127

SEC. 4.0 For Reference: Basics of Matrices and Vectors

If

c

vϭ

0.4
Ϫ13

127

d,

c

10v ϭ

then

4
Ϫ130

d.

Matrix Multiplication. The product C ϭ AB (in this order) of two n ϫ n matrices
A ϭ [ajk] and B ϭ [bjk] is the n ϫ n matrix C ϭ [cjk] with entries
j ϭ 1, Á , n

n

cjk ϭ a ajmbmk

(6)

k ϭ 1, Á , n,

mϭ1

that is, multiply each entry in the jth row of A by the corresponding entry in the kth column
of B and then add these n products. One says briefly that this is a “multiplication of rows
into columns.” For example,

c

9

3

Ϫ2

0

dc

1

Ϫ4

2

5

d

ϭ

c

ϭ

c

9ؒ1ϩ3ؒ2

9 ؒ (Ϫ4) ϩ 3 ؒ 5

Ϫ2 ؒ 1 ϩ 0 ؒ 2

(Ϫ2) ؒ (Ϫ4) ϩ 0 ؒ 5

15

Ϫ21

Ϫ2

8

d.

CAUTION! Matrix multiplication is not commutative, AB
example,

c

1

Ϫ4

2

5

dc

9

3

Ϫ2

0

d

d,

BA in general. In our

ϭ

c

1 ؒ 9 ϩ (Ϫ4) ؒ (Ϫ2)

1 ؒ 3 ϩ (Ϫ4) ؒ 0

2 ؒ 9 ϩ 5 ؒ (Ϫ2)

2ؒ3ϩ5ؒ0

ϭ

c

17

3

8

6

d

d.

Multiplication of an n ϫ n matrix A by a vector x with n components is defined by the
same rule: v ϭ Ax is the vector with the n components
n

vj ϭ a ajmxm

j ϭ 1, Á , n.

mϭ1

For example,

c

12

7

Ϫ8

3

dc d
x1
x2

ϭ

c

12x 1 ϩ 7x 2
Ϫ8x 1 ϩ 3x 2

d.

Systems of ODEs as Vector Equations
Differentiation. The derivative of a matrix (or vector) with variable entries (or
components) is obtained by differentiating each entry (or component). Thus, if
y(t) ϭ

c

y1(t)
y2(t)

d

ϭ

c

e؊2t
sin t

d,

then

y r (t) ϭ

c

y1r (t)
y2r (t)

d

ϭ

c

Ϫ2e؊2t
cos t

d.

c04.qxd

10/27/10

128

9:32 PM

Page 128

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods

Using matrix multiplication and differentiation, we can now write (1) as
(7)

yr ϭ

c d
y1r
y2r

ϭ Ay ϭ

c

a11

a12

a21

a22

d c d,
y1

e.g., y r ϭ

y2

c

d c d.
1

Ϫ5

2

y1

13

2

y2

Similarly for (2) by means of an n ϫ n matrix A and a column vector y with n components,
namely, y r ϭ Ay. The vector equation (7) is equivalent to two equations for the
components, and these are precisely the two ODEs in (1).

Some Further Operations and Terms
Transposition is the operation of writing columns as rows and conversely and is indicated
by T. Thus the transpose AT of the 2 ϫ 2 matrix
Aϭ

c

a11

a12

a21

a22

d

ϭ

c

Ϫ5
13

d
1

2

is

c

AT ϭ

2

a11

a21

a12

a22

d

ϭ

c

Ϫ5

13

2

1
2

d.

The transpose of a column vector, say,
vϭ

c d,
v1

is a row vector,

v2

v T ϭ [v1

v2],

and conversely.
Inverse of a Matrix. The n ϫ n unit matrix I is the n ϫ n matrix with main diagonal
1, 1, Á , 1 and all other entries zero. If, for a given n ϫ n matrix A, there is an n ϫ n
matrix B such that AB ϭ BA ϭ I, then A is called nonsingular and B is called the inverse
of A and is denoted by A؊1; thus
AA؊1 ϭ A؊1A ϭ I.

(8)

The inverse exists if the determinant det A of A is not zero.
If A has no inverse, it is called singular. For n ϭ 2,
(9)

A؊1 ϭ

a22
1
c
det A Ϫa21

Ϫa12
a11

d,

where the determinant of A is
(10)

det A ϭ 2

a11

a12

a21

a22

2 ϭ a11a22 Ϫ a12a21.

(For general n, see Sec. 7.7, but this will not be needed in this chapter.)
Linear Independence. r given vectors v (1), Á , v (r) with n components are called a
linearly independent set or, more briefly, linearly independent, if
(11)

c1v (1) ϩ Á ϩ crv (r) ϭ 0

c04.qxd

10/27/10

9:32 PM

Page 129

SEC. 4.0 For Reference: Basics of Matrices and Vectors

129

implies that all scalars c1, Á , cr must be zero; here, 0 denotes the zero vector, whose n
components are all zero. If (11) also holds for scalars not all zero (so that at least one of
these scalars is not zero), then these vectors are called a linearly dependent set or, briefly,
linearly dependent, because then at least one of them can be expressed as a linear
combination of the others; that is, if, for instance, c1 0 in (11), then we can obtain
1
v (1) ϭ Ϫ c (c2v (2) ϩ Á ϩ crv (r)).
1

Eigenvalues, Eigenvectors
Eigenvalues and eigenvectors will be very important in this chapter (and, as a matter of
fact, throughout mathematics).
Let A ϭ [ajk] be an n ϫ n matrix. Consider the equation
Ax ϭ lx

(12)

where l is a scalar (a real or complex number) to be determined and x is a vector to be
determined. Now, for every l, a solution is x ϭ 0. A scalar l such that (12) holds for
some vector x 0 is called an eigenvalue of A, and this vector is called an eigenvector
of A corresponding to this eigenvalue l.
We can write (12) as Ax Ϫ lx ϭ 0 or
(A Ϫ lI)x ϭ 0.

(13)

These are n linear algebraic equations in the n unknowns x 1, Á , x n (the components
of x). For these equations to have a solution x 0, the determinant of the coefficient
matrix A Ϫ lI must be zero. This is proved as a basic fact in linear algebra (Theorem 4
in Sec. 7.7). In this chapter we need this only for n ϭ 2. Then (13) is

c

(14)

a11 Ϫ l

a12

a21

a22 Ϫ l

dc d
x1
x2

ϭ

c d;
0
0

in components,
(14*)

(a11 Ϫ l)x 1 ϩ
a21 x 1

a12 x 2

ϭ0

ϩ (a22 Ϫ l)x 2 ϭ 0.

Now A Ϫ lI is singular if and only if its determinant det (A Ϫ lI), called the characteristic
determinant of A (also for general n), is zero. This gives
det (A Ϫ lI) ϭ 2
(15)

a11 Ϫ l

a12

a21

a22 Ϫ l

2

ϭ (a11 Ϫ l)(a22 Ϫ l) Ϫ a12a21
ϭ l2 Ϫ (a11 ϩ a22)l ϩ a11a22 Ϫ a12a21 ϭ 0.

c04.qxd

10/27/10

9:32 PM

130

Page 130

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods

This quadratic equation in l is called the characteristic equation of A. Its solutions are
the eigenvalues l1 and l2 of A. First determine these. Then use (14*) with l ϭ l1 to
determine an eigenvector x (1) of A corresponding to l1. Finally use (14*) with l ϭ l2
to find an eigenvector x (2) of A corresponding to l2. Note that if x is an eigenvector of
A, so is kx with any k 0.
EXAMPLE 1

Eigenvalue Problem
Find the eigenvalues and eigenvectors of the matrix
Aϭ

(16)

c

Ϫ4.0

4.0

Ϫ1.6

1.2

d

Solution. The characteristic equation is the quadratic equation
det ƒ A Ϫ lI ƒ ϭ 2

Ϫ4 Ϫ l

4

Ϫ1.6

1.2 Ϫ l

2 ϭ l2 ϩ 2.8l ϩ 1.6 ϭ 0.

It has the solutions l1 ϭ Ϫ2 and l2 ϭ Ϫ0.8. These are the eigenvalues of A.
Eigenvectors are obtained from (14*). For l ϭ l1 ϭ Ϫ2 we have from (14*)
(Ϫ4.0 ϩ 2.0)x 1 ϩ
Ϫ1.6x 1

4.0x 2

ϭ0

ϩ (1.2 ϩ 2.0)x 2 ϭ 0.

A solution of the first equation is x 1 ϭ 2, x 2 ϭ 1. This also satisfies the second equation. (Why?) Hence an
eigenvector of A corresponding to l1 ϭ Ϫ2.0 is
(17)

x (1) ϭ

c d.
2

Similarly,

1

x (2) ϭ

c

1
0.8

d

is an eigenvector of A corresponding to l2 ϭ Ϫ0.8, as obtained from (14*) with l ϭ l2. Verify this.

4.1

᭿

Systems of ODEs as Models
in Engineering Applications
We show how systems of ODEs are of practical importance as follows. We first illustrate
how systems of ODEs can serve as models in various applications. Then we show how a
higher order ODE (with the highest derivative standing alone on one side) can be reduced
to a first-order system.

EXAMPLE 1

Mixing Problem Involving Two Tanks
A mixing problem involving a single tank is modeled by a single ODE, and you may first review the
corresponding Example 3 in Sec. 1.3 because the principle of modeling will be the same for two tanks. The
model will be a system of two first-order ODEs.
Tank T1 and T2 in Fig. 78 contain initially 100 gal of water each. In T1 the water is pure, whereas 150 lb of
fertilizer are dissolved in T2. By circulating liquid at a rate of 2 gal>min and stirring (to keep the mixture uniform)
the amounts of fertilizer y1(t) in T1 and y2(t) in T2 change with time t. How long should we let the liquid circulate
so that T1 will contain at least half as much fertilizer as there will be left in T2?

c04.qxd

10/27/10

9:32 PM

Page 131

SEC. 4.1 Systems of ODEs as Models in Engineering Applications

131
y(t)
150
y2(t)

100

2 gal/min

75
T1

T2

2 gal/min

50

0
0

System of tanks

Fig. 78.

y1(t)

27.5

50

100

t

Fertilizer content in Tanks T1 (lower curve) and T2

Solution. Step 1. Setting up the model. As for a single tank, the time rate of change y1r (t) of y1(t) equals
inflow minus outflow. Similarly for tank T2. From Fig. 78 we see that
y1r ϭ Inflow>min Ϫ Outflow>min ϭ

y2r ϭ Inflow>min Ϫ Outflow>min ϭ

2
100
2
100

y2 Ϫ

y1 Ϫ

2
100
2
100

y1

(Tank T1)

y2

(Tank T2).

Hence the mathematical model of our mixture problem is the system of first-order ODEs
y1r ϭ Ϫ0.02y1 ϩ 0.02y2

(Tank T1)

y2r ϭ

(Tank T2).

As a vector equation with column vector y ϭ

y r ϭ Ay,

0.02y1 Ϫ 0.02y2

c d
y1

and matrix A this becomes

y2

where

Aϭ

c

Ϫ0.02

0.02

0.02

Ϫ0.02

d.

Step 2. General solution. As for a single equation, we try an exponential function of t,
y ϭ xelt.

(1)

Then

y r ϭ lxelt ϭ Axelt.

Dividing the last equation lxelt ϭ Axelt by elt and interchanging the left and right sides, we obtain
Ax ϭ lx.
We need nontrivial solutions (solutions that are not identically zero). Hence we have to look for eigenvalues
and eigenvectors of A. The eigenvalues are the solutions of the characteristic equation
(2)

det (A Ϫ lI) ϭ 2

Ϫ0.02 Ϫ l

0.02

0.02

Ϫ0.02 Ϫ l

2 ϭ (Ϫ0.02 Ϫ l)2 Ϫ 0.022 ϭ l(l ϩ 0.04) ϭ 0.

We see that l1 ϭ 0 (which can very well happen—don’t get mixed up—it is eigenvectors that must not be zero)
and l2 ϭ Ϫ0.04. Eigenvectors are obtained from (14*) in Sec. 4.0 with l ϭ 0 and l ϭ Ϫ0.04. For our present
A this gives [we need only the first equation in (14*)]
Ϫ0.02x 1 ϩ 0.02x 2 ϭ 0

and

(Ϫ0.02 ϩ 0.04)x 1 ϩ 0.02x 2 ϭ 0,

c04.qxd

10/27/10

9:32 PM

132

Page 132

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods
respectively. Hence x 1 ϭ x 2 and x 1 ϭ Ϫx 2, respectively, and we can take x 1 ϭ x 2 ϭ 1 and x 1 ϭ Ϫx 2 ϭ 1.
This gives two eigenvectors corresponding to l1 ϭ 0 and l2 ϭ Ϫ0.04, respectively, namely,

c d
1

x (1) ϭ

x (2) ϭ

and

1

c

1
Ϫ1

d.

From (1) and the superposition principle (which continues to hold for systems of homogeneous linear ODEs)
we thus obtain a solution
y ϭ c1x (1)el1t ϩ c2x (2)el2t ϭ c1 c

(3)

1
1

d

ϩ c2 c

1
Ϫ1

d e؊0.04t

where c1 and c2 are arbitrary constants. Later we shall call this a general solution.
Step 3. Use of initial conditions. The initial conditions are y1(0) ϭ 0 (no fertilizer in tank T1) and y2(0) ϭ 150.
From this and (3) with t ϭ 0 we obtain
y(0) ϭ c1 c

1
1

d

ϩ c2 c

1
Ϫ1

d

ϭ

c

c1 ϩ c2
c1 Ϫ c2

d

ϭ

c

0
150

d.

In components this is c1 ϩ c2 ϭ 0, c1 Ϫ c2 ϭ 150. The solution is c1 ϭ 75, c2 ϭ Ϫ75. This gives the answer
y ϭ 75x (1) Ϫ 75x (2)e؊0.04t ϭ 75 c

1
1

d

Ϫ 75 c

1
Ϫ1

d e؊0.04t.

In components,
y1 ϭ 75 Ϫ 75e؊0.04t
y2 ϭ 75 ϩ 75e

(Tank T1, lower curve)

؊0.04t

(Tank T2, upper curve).

Figure 78 shows the exponential increase of y1 and the exponential decrease of y2 to the common limit 75 lb.
Did you expect this for physical reasons? Can you physically explain why the curves look “symmetric”? Would
the limit change if T1 initially contained 100 lb of fertilizer and T2 contained 50 lb?
Step 4. Answer. T1 contains half the fertilizer amount of T2 if it contains 1>3 of the total amount, that is,
50 lb. Thus
y1 ϭ 75 Ϫ 75e؊0.04t ϭ 50,

e؊0.04t ϭ 13 ,

t ϭ (ln 3)>0.04 ϭ 27.5.

᭿

Hence the fluid should circulate for at least about half an hour.

EXAMPLE 2

Electrical Network
Find the currents I1(t) and I2(t) in the network in Fig. 79. Assume all currents and charges to be zero at t ϭ 0,
the instant when the switch is closed.
L = 1 henry
Switch
t=0

I1

I1

C = 0.25 farad
I2

I2

R1 = 4 ohms
E = 12 volts

I1

I2
R2 = 6 ohms

Fig. 79. Electrical network in Example 2

Solution. Step 1. Setting up the mathematical model. The model of this network is obtained from
Kirchhoff’s Voltage Law, as in Sec. 2.9 (where we considered single circuits). Let I1(t) and I2(t) be the currents

c04.qxd

10/27/10

9:32 PM

Page 133

SEC. 4.1 Systems of ODEs as Models in Engineering Applications

133

in the left and right loops, respectively. In the left loop, the voltage drops are LI1r ϭ I1r [V] over the inductor
and R1(I1 Ϫ I2) ϭ 4(I1 Ϫ I2) [V] over the resistor, the difference because I1 and I2 flow through the resistor in
opposite directions. By Kirchhoff’s Voltage Law the sum of these drops equals the voltage of the battery; that
is, I1r ϩ 4(I1 Ϫ I2) ϭ 12, hence
I1r ϭ Ϫ4I1 ϩ 4I2 ϩ 12.

(4a)

In the right loop, the voltage drops are R2I2 ϭ 6I2 [V] and R1(I2 Ϫ I1) ϭ 4(I2 Ϫ I1) [V] over the resistors and
(I>C) ͐ I2 dt ϭ 4 ͐ I2 dt [V] over the capacitor, and their sum is zero,
6I2 ϩ 4(I2 Ϫ I1) ϩ 4

ΎI

2

dt ϭ 0

10I2 Ϫ 4I1 ϩ 4

or

ΎI

2

dt ϭ 0.

Division by 10 and differentiation gives I2r Ϫ 0.4I1r ϩ 0.4I2 ϭ 0.
To simplify the solution process, we first get rid of 0.4I1r , which by (4a) equals 0.4(Ϫ4I1 ϩ 4I2 ϩ 12).
Substitution into the present ODE gives
I2r ϭ 0.4I1r Ϫ 0.4I2 ϭ 0.4(Ϫ4I1 ϩ 4I2 ϩ 12) Ϫ 0.4I2
and by simplification
I2r ϭ Ϫ1.6I1 ϩ 1.2I2 ϩ 4.8.

(4b)

In matrix form, (4) is (we write J since I is the unit matrix)
(5)

J r ϭ AJ ϩ g,

Jϭ

where

c d,
I1

Aϭ

I2

c

Ϫ4.0

4.0

Ϫ1.6

1.2

d,

gϭ

c

12.0
4.8

d.

Step 2. Solving (5). Because of the vector g this is a nonhomogeneous system, and we try to proceed as for a
single ODE, solving first the homogeneous system J r ϭ AJ (thus J r Ϫ AJ ϭ 0) by substituting J ϭ xelt. This
gives
J r ϭ lxelt ϭ Axelt,

hence

Ax ϭ lx.

Hence, to obtain a nontrivial solution, we again need the eigenvalues and eigenvectors. For the present matrix
A they are derived in Example 1 in Sec. 4.0:
l1 ϭ Ϫ2,

x (1) ϭ

c d;
2
1

l2 ϭ Ϫ0.8,

x (2) ϭ

c

1
0.8

d.

Hence a “general solution” of the homogeneous system is
Jh ϭ c1x (1)e؊2t ϩ c2x (2)e؊0.8t.
For a particular solution of the nonhomogeneous system (5), since g is constant, we try a constant column
vector Jp ϭ a with components a1, a2. Then Jpr ϭ 0, and substitution into (5) gives Aa ϩ g ϭ 0; in components,
Ϫ4.0a1 ϩ 4.0a2 ϩ 12.0 ϭ 0
Ϫ1.6a1 ϩ 1.2a2 ϩ 4.8 ϭ 0.
The solution is a1 ϭ 3, a2 ϭ 0; thus a ϭ
(6)

c d . Hence
3
0

J ϭ Jh ϩ Jp ϭ c1x (1)e؊2t ϩ c2x (2)e؊0.8t ϩ a;

in components,
I1 ϭ 2c1e؊2t ϩ

c2e؊0.8t ϩ 3

I2 ϭ c1e؊2t ϩ 0.8c2e؊0.8t.

c04.qxd

10/27/10

134

9:32 PM

Page 134

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods
The initial conditions give
I1(0) ϭ 2c1 ϩ

c2 ϩ 3 ϭ 0

I2(0) ϭ c1 ϩ 0.8c2

ϭ 0.

Hence c1 ϭ Ϫ4 and c2 ϭ 5. As the solution of our problem we thus obtain
J ϭ Ϫ4x (1)e؊2t ϩ 5x (2)e؊0.8t ϩ a.

(7)
In components (Fig. 80b),

I1 ϭ Ϫ8e؊2t ϩ 5e؊0.8t ϩ 3
I2 ϭ Ϫ4e؊2t ϩ 4e؊0.8t.
Now comes an important idea, on which we shall elaborate further, beginning in Sec. 4.3. Figure 80a shows
I1(t) and I2(t) as two separate curves. Figure 80b shows these two currents as a single curve [I1(t), I2(t)] in the
I1I2-plane. This is a parametric representation with time t as the parameter. It is often important to know in
which sense such a curve is traced. This can be indicated by an arrow in the sense of increasing t, as is shown.
The I1I2-plane is called the phase plane of our system (5), and the curve in Fig. 80b is called a trajectory. We
shall see that such “phase plane representations” are far more important than graphs as in Fig. 80a because
they will give a much better qualitative overall impression of the general behavior of whole families of solutions,
not merely of one solution as in the present case.
᭿
I2

I(t )

1.5

I1(t)

4
3

1

2
1
0
0

0.5

I2(t)
1

2

3

4

5

(a) Currents I1
(upper curve)
and I2

Fig. 80.

t

0
0

1

2

3

4

5

I1

(b) Trajectory [I1(t), I2(t)]
in the I1I2-plane
(the “phase plane”)

Currents in Example 2

Remark. In both examples, by growing the dimension of the problem (from one tank to
two tanks or one circuit to two circuits) we also increased the number of ODEs (from one
ODE to two ODEs). This “growth” in the problem being reflected by an “increase” in the
mathematical model is attractive and affirms the quality of our mathematical modeling and
theory.

Conversion of an nth-Order ODE to a System
We show that an nth-order ODE of the general form (8) (see Theorem 1) can be converted
to a system of n first-order ODEs. This is practically and theoretically important—
practically because it permits the study and solution of single ODEs by methods for
systems, and theoretically because it opens a way of including the theory of higher order
ODEs into that of first-order systems. This conversion is another reason for the importance
of systems, in addition to their use as models in various basic applications. The idea of
the conversion is simple and straightforward, as follows.

c04.qxd

10/27/10

9:32 PM

Page 135

SEC. 4.1 Systems of ODEs as Models in Engineering Applications

THEOREM 1

135

Conversion of an ODE

An nth-order ODE
y (n) ϭ F(t, y, y r , Á , y (n؊1))

(8)

can be converted to a system of n first-order ODEs by setting
y1 ϭ y, y2 ϭ y r ,

(9)

y3 ϭ y s , Á , yn ϭ y (n؊1).

This system is of the form
y 1r ϭ y2
y r2 ϭ y3
(10)

.

o
ynr ؊1 ϭ yn
y rn ϭ F(t, y1, y2, Á , yn).

PROOF
EXAMPLE 3

The first n Ϫ 1 of these n ODEs follows immediately from (9) by differentiation. Also,
y rn ϭ y (n) by (9), so that the last equation in (10) results from the given ODE (8).
᭿
Mass on a Spring
To gain confidence in the conversion method, let us apply it to an old friend of ours, modeling the free motions
of a mass on a spring (see Sec. 2.4)
my s ϩ cy r ϩ ky ϭ 0

or

ys ϭ Ϫ

c
k
y r Ϫ y.
m
m

For this ODE (8) the system (10) is linear and homogeneous,
y1r ϭ y2
y2r ϭ Ϫ
Setting y ϭ

k
c
y Ϫ y2.
m 1
m

c d , we get in matrix form
y1
y2

0
y r ϭ Ay ϭ D

k
Ϫ
m

1

y1
cT c d.
y2
Ϫ
m

The characteristic equation is

det (A Ϫ lI) ϭ 4

Ϫl

1

k
Ϫ
m

c
Ϫ Ϫl
m

c

k

2
4 ϭ l ϩ m l ϩ m ϭ 0.

c04.qxd

10/27/10

136

9:32 PM

Page 136

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods
It agrees with that in Sec. 2.4. For an illustrative computation, let m ϭ 1, c ϭ 2, and k ϭ 0.75. Then
l2 ϩ 2l ϩ 0.75 ϭ (l ϩ 0.5)(l ϩ 1.5) ϭ 0.
This gives the eigenvalues l1 ϭ Ϫ0.5 and l2 ϭ Ϫ1.5. Eigenvectors follow from the first equation in A Ϫ lI ϭ 0,
which is Ϫlx 1 ϩ x 2 ϭ 0. For l1 this gives 0.5x 1 ϩ x 2 ϭ 0, say, x 1 ϭ 2, x 2 ϭ Ϫ1. For l2 ϭ Ϫ1.5 it gives
1.5x 1 ϩ x 2 ϭ 0, say, x 1 ϭ 1, x 2 ϭ Ϫ1.5. These eigenvectors
x (1) ϭ

c

2
Ϫ1

d,

x (2) ϭ

c

1
Ϫ1.5

d

give

y ϭ c1

c

2
Ϫ1

d e؊0.5t ϩ c2 c

1
Ϫ1.5

d e؊1.5t.

This vector solution has the first component
y ϭ y1 ϭ 2c1e؊0.5t ϩ c2e؊1.5t
which is the expected solution. The second component is its derivative
y2 ϭ y1r ϭ y r ϭ Ϫc1e؊0.5t Ϫ 1.5c2e؊1.5t.

᭿

PROBLEM SET 4.1
1–6

MIXING PROBLEMS

1. Find out, without calculation, whether doubling the
flow rate in Example 1 has the same effect as halfing
the tank sizes. (Give a reason.)
2. What happens in Example 1 if we replace T1 by a tank
containing 200 gal of water and 150 lb of fertilizer
dissolved in it?
3. Derive the eigenvectors in Example 1 without consulting
this book.
4. In Example 1 find a “general solution” for any ratio
a ϭ (flow rate)>(tank size), tank sizes being equal.
Comment on the result.
5. If you extend Example 1 by a tank T3 of the same size
as the others and connected to T2 by two tubes with
flow rates as between T1 and T2, what system of ODEs
will you get?
6. Find a “general solution” of the system in Prob. 5.
7–9

ELECTRICAL NETWORK

In Example 2 find the currents:
7. If the initial currents are 0 A and Ϫ3 A (minus meaning
that I2(0) flows against the direction of the arrow).
8. If the capacitance is changed to C ϭ 5>27 F. (General
solution only.)
9. If the initial currents in Example 2 are 28 A and 14 A.
10–13

CONVERSION TO SYSTEMS

Find a general solution of the given ODE (a) by first converting
it to a system, (b), as given. Show the details of your work.
10. y s ϩ 3y r ϩ 2y ϭ 0
11. 4y s Ϫ 15y r Ϫ 4y ϭ 0
12. y t ϩ 2y s Ϫ y r Ϫ 2y ϭ 0
13. y s ϩ 2y r Ϫ 24y ϭ 0

14. TEAM PROJECT. Two Masses on Springs. (a) Set
up the model for the (undamped) system in Fig. 81.
(b) Solve the system of ODEs obtained. Hint. Try
y ϭ xevt and set v2 ϭ l. Proceed as in Example 1 or
2. (c) Describe the influence of initial conditions on the
possible kind of motions.

k1 = 3
m1 = 1

(y1 = 0)

y1

y1
k2 = 2
(y2 = 0)

m2 = 1

y2

(Net change in
spring length
= y2 – y1)

y2
System in
static
equilibrium

Fig. 81.

System in
motion

Mechanical system in Team Project

15. CAS EXPERIMENT. Electrical Network. (a) In
Example 2 choose a sequence of values of C that
increases beyond bound, and compare the corresponding
sequences of eigenvalues of A. What limits of these
sequences do your numeric values (approximately)
suggest?
(b) Find these limits analytically.
(c) Explain your result physically.
(d) Below what value (approximately) must you decrease
C to get vibrations?

c04.qxd

10/27/10

9:32 PM

Page 137

SEC. 4.2 Basic Theory of Systems of ODEs. Wronskian

4.2

137

Basic Theory of Systems of ODEs.
Wronskian
In this section we discuss some basic concepts and facts about system of ODEs that are
quite similar to those for single ODEs.
The first-order systems in the last section were special cases of the more general system
y r1 ϭ f1(t, y1, Á , yn)
y r2 ϭ f2(t, y1, Á , yn)

(1)

Á
y rn ϭ fn(t, y1, Á , yn).
We can write the system (1) as a vector equation by introducing the column vectors
y ϭ [ y1 Á yn]T and f ϭ [ f1 Á fn]T (where T means transposition and saves us
the space that would be needed for writing y and f as columns). This gives
y r ϭ f(t, y).

(1)

This system (1) includes almost all cases of practical interest. For n ϭ 1 it becomes
y r1 ϭ f1(t, y1) or, simply, y r ϭ f (t, y), well known to us from Chap. 1.
A solution of (1) on some interval a Ͻ t Ͻ b is a set of n differentiable functions
y1 ϭ h 1(t),

Á , yn ϭ h n(t)

on a Ͻ t Ͻ b that satisfy (1) throughout this interval. In vector from, introducing the
“solution vector” h ϭ [h 1 Á h n]T (a column vector!) we can write
y ϭ h(t).
An initial value problem for (1) consists of (1) and n given initial conditions
(2)

y1(t 0) ϭ K 1,

y2(t 0) ϭ K 2,

Á,

yn(t 0) ϭ K n,

in vector form, y(t 0) ϭ K, where t 0 is a specified value of t in the interval considered and
the components of K ϭ [K 1 Á K n]T are given numbers. Sufficient conditions for the
existence and uniqueness of a solution of an initial value problem (1), (2) are stated in
the following theorem, which extends the theorems in Sec. 1.7 for a single equation. (For
a proof, see Ref. [A7].)
THEOREM 1

Existence and Uniqueness Theorem

Let f1, Á , fn in (1) be continuous functions having continuous partial derivatives
0f1 >0y1, Á , 0f1 >0yn, Á , 0fn >0yn in some domain R of ty1 y2 Á yn-space
containing the point (t 0, K 1, Á , K n). Then (1) has a solution on some interval
t 0 Ϫ a Ͻ t Ͻ t 0 ϩ a satisfying (2), and this solution is unique.

c04.qxd

10/27/10

9:32 PM

138

Page 138

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods

Linear Systems
Extending the notion of a linear ODE, we call (1) a linear system if it is linear in
y1, Á , yn; that is, if it can be written
y r1 ϭ a11(t)y1 ϩ Á ϩ a1n(t)yn ϩ g1(t)
(3)

o
y rn ϭ an1(t)y1 ϩ Á ϩ ann(t)yn ϩ gn(t).

As a vector equation this becomes
y r ϭ Ay ϩ g

(3)
a11
where

AϭD .
an1

Á
Á
Á

a1n

y1

. T,

y ϭ D o T,

ann

g1
g ϭ D o T.

yn

gn

This system is called homogeneous if g ϭ 0, so that it is
(4)

y r ϭ Ay.

If g 0, then (3) is called nonhomogeneous. For example, the systems in Examples 1 and
3 of Sec. 4.1 are homogeneous. The system in Example 2 of that section is nonhomogeneous.
For a linear system (3) we have 0f1 >0y1 ϭ a11(t), Á , 0fn >0yn ϭ ann(t) in Theorem 1.
Hence for a linear system we simply obtain the following.
THEOREM 2

Existence and Uniqueness in the Linear Case

Let the ajk’s and gj’s in (3) be continuous functions of t on an open interval
a Ͻ t Ͻ b containing the point t ϭ t 0. Then (3) has a solution y(t) on this interval
satisfying (2), and this solution is unique.
As for a single homogeneous linear ODE we have
THEOREM 3

Superposition Principle or Linearity Principle

If y (1) and y (2) are solutions of the homogeneous linear system (4) on some interval,
so is any linear combination y ϭ c1 y (1) ϩ c1 y (2).
PROOF

Differentiating and using (4), we obtain
y r ϭ [c1 y (1) ϩ c1 y (2)] r
ϭ c1y (1) r ϩ c2 y (2) r
ϭ c1Ay (1) ϩ c2Ay (2)
ϭ A(c1 y (1) ϩ c2 y (2)) ϭ Ay.

᭿

c04.qxd

10/27/10

9:32 PM

Page 139

SEC. 4.2 Basic Theory of Systems of ODEs. Wronskian

139

The general theory of linear systems of ODEs is quite similar to that of a single linear
ODE in Secs. 2.6 and 2.7. To see this, we explain the most basic concepts and facts. For
proofs we refer to more advanced texts, such as [A7].

Basis. General Solution. Wronskian
By a basis or a fundamental system of solutions of the homogeneous system (4) on some
interval J we mean a linearly independent set of n solutions y (1), Á , y (n) of (4) on that
interval. (We write J because we need I to denote the unit matrix.) We call a corresponding
linear combination
y ϭ c1y (1) Á ϩ cn y (n)

(5)

(c1, Á , cn arbitrary)

a general solution of (4) on J. It can be shown that if the ajk(t) in (4) are continuous on
J, then (4) has a basis of solutions on J, hence a general solution, which includes every
solution of (4) on J.
We can write n solutions y (1), Á , y (n) of (4) on some interval J as columns of an n ϫ n
matrix
Y ϭ [y (1)

(6)

y (n)].

Á

The determinant of Y is called the Wronskian of y (1), Á , y (n), written

(7)

W(y , Á , y (n)) ϭ 5
(1)

y (1)
1

y (2)
1

Á

y (n)
1

y (1)
2

y (2)
2

Á

y (n)
2

#

#

Á

#

y (1)
n

y (2)
n

Á

y (n)
n

5.

The columns are these solutions, each in terms of components. These solutions form a
basis on J if and only if W is not zero at any t 1 in this interval. W is either identically
zero or nowhere zero in J. (This is similar to Secs. 2.6 and 3.1.)
If the solutions y (1), Á , y (n) in (5) form a basis (a fundamental system), then (6) is
often called a fundamental matrix. Introducing a column vector c ϭ [c1 c2 Á cn]T,
we can now write (5) simply as
(8)

y ϭ Yc.

Furthermore, we can relate (7) to Sec. 2.6, as follows. If y and z are solutions of a
second-order homogeneous linear ODE, their Wronskian is
W( y, z) ϭ 2

y

z

yr

zr

2.

To write this ODE as a system, we have to set y ϭ y1, y r ϭ y1r ϭ y2 and similarly for z
(see Sec. 4.1). But then W( y, z) becomes (7), except for notation.

c04.qxd

10/27/10

140

4.3

9:32 PM

Page 140

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods

Constant-Coefficient Systems.
Phase Plane Method
Continuing, we now assume that our homogeneous linear system
y؅ ϭ Ay

(1)

under discussion has constant coefficients, so that the n ϫ n matrix A ϭ [ajk] has entries
not depending on t. We want to solve (1). Now a single ODE y r ϭ ky has the solution
y ϭ Cekt. So let us try
y ϭ xelt.

(2)

Substitution into (1) gives y r ϭ lxelt ϭ Ay ϭ Axelt. Dividing by elt, we obtain the
eigenvalue problem
Ax ϭ lx.

(3)

Thus the nontrivial solutions of (1) (solutions that are not zero vectors) are of the form
(2), where l is an eigenvalue of A and x is a corresponding eigenvector.
We assume that A has a linearly independent set of n eigenvectors. This holds in most
applications, in particular if A is symmetric (akj ϭ ajk) or skew-symmetric (akj ϭ Ϫajk)
or has n different eigenvalues.
Let those eigenvectors be x (1), Á , x (n) and let them correspond to eigenvalues
l1, Á , ln (which may be all different, or some––or even all––may be equal). Then the
corresponding solutions (2) are
y (4) ϭ x (1)el1t,

(4)

Á , y (n) ϭ x (n)elnt.

Their Wronskian W ϭ W(y (1), Á , y (n)) [(7) in Sec. 4.2] is given by

W ϭ (y , Á , y (n)) ϭ 5
(1)

l1t
x (1)
1 e

Á

lnt
x (n)
1 e

l1t
x (1)
2 e

Á

lnt
x (n)
2 e

#

Á

#

l1t
x (1)
n e

Á

lnt
x (n)
n e

5ϭe

l1tϩ Á ϩlnt

5

x (1)
1

Á

x (n)
1

x (1)
2

Á

x (n)
2

#

Á

x (1)
n

Á

#

5.

x (n)
n

On the right, the exponential function is never zero, and the determinant is not zero either
because its columns are the n linearly independent eigenvectors. This proves the following
theorem, whose assumption is true if the matrix A is symmetric or skew-symmetric, or if
the n eigenvalues of A are all different.

c04.qxd

10/27/10

9:32 PM

Page 141

SEC. 4.3 Constant-Coefficient Systems. Phase Plane Method

THEOREM 1

141

General Solution

If the constant matrix A in the system (1) has a linearly independent set of n
eigenvectors, then the corresponding solutions y (1), Á , y (n) in (4) form a basis of
solutions of (1), and the corresponding general solution is
y ϭ c1x (1)el1t ϩ Á ϩ cnx (n)elnt.

(5)

How to Graph Solutions in the Phase Plane
We shall now concentrate on systems (1) with constant coefficients consisting of two
ODEs

(6)

y؅ ϭ Ay;

y1r ϭ a11 y1 ϩ a12 y2

in components,

y2r ϭ a21 y1 ϩ a22 y2.

Of course, we can graph solutions of (6),
y(t) ϭ

(7)

c

y1(t)
y2(t)

d,

as two curves over the t-axis, one for each component of y(t). (Figure 80a in Sec. 4.1 shows
an example.) But we can also graph (7) as a single curve in the y1 y2-plane. This is a parametric
representation (parametric equation) with parameter t. (See Fig. 80b for an example. Many
more follow. Parametric equations also occur in calculus.) Such a curve is called a trajectory
(or sometimes an orbit or path) of (6). The y1 y2-plane is called the phase plane.1 If we fill
the phase plane with trajectories of (6), we obtain the so-called phase portrait of (6).
Studies of solutions in the phase plane have become quite important, along with
advances in computer graphics, because a phase portrait gives a good general qualitative
impression of the entire family of solutions. Consider the following example, in which
we develop such a phase portrait.
EXAMPLE 1

Trajectories in the Phase Plane (Phase Portrait)
Find and graph solutions of the system.
In order to see what is going on, let us find and graph solutions of the system
(8)

1

y r ϭ Ay ϭ

c

Ϫ3

1

1

Ϫ3

d y,

thus

y1r ϭ Ϫ3y1 ϩ y2
y2r ϭ

y1 Ϫ 3y2.

A name that comes from physics, where it is the y-(mv)-plane, used to plot a motion in terms of position y
and velocity yЈ ϭ v (m ϭ mass); but the name is now used quite generally for the y1 y2-plane.
The use of the phase plane is a qualitative method, a method of obtaining general qualitative information
on solutions without actually solving an ODE or a system. This method was created by HENRI POINCARÉ
(1854–1912), a great French mathematician, whose work was also fundamental in complex analysis, divergent
series, topology, and astronomy.

c04.qxd

10/27/10

9:32 PM

142

Page 142

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods

Solution. By substituting y ϭ xelt and y r ϭ lxelt and dropping the exponential function we get Ax ϭ lx.
The characteristic equation is
det (A Ϫ lI) ϭ 2

Ϫ3 Ϫ l

1

1

Ϫ3 Ϫ l

2 ϭ l2 ϩ 6l ϩ 8 ϭ 0.

This gives the eigenvalues l1 ϭ Ϫ2 and l2 ϭ Ϫ4. Eigenvectors are then obtained from
(Ϫ3 Ϫ l)x 1 ϩ x 2 ϭ 0.
For l1 ϭ Ϫ2 this is Ϫx 1 ϩ x 2 ϭ 0. Hence we can take x (1) ϭ [1 1]T. For l2 ϭ Ϫ4 this becomes x 1 ϩ x 2 ϭ 0,
and an eigenvector is x (2) ϭ [1 Ϫ1]T. This gives the general solution
yϭ

c d
y1
y2

ϭ c1 y (1) ϩ c2 y (2) ϭ c1

c d e؊2t ϩ c2 c
1

1

1

Ϫ1

d e؊4t.

Figure 82 shows a phase portrait of some of the trajectories (to which more trajectories could be added if so
desired). The two straight trajectories correspond to c1 ϭ 0 and c2 ϭ 0 and the others to other choices of
c1, c2.
᭿

The method of the phase plane is particularly valuable in the frequent cases when solving
an ODE or a system is inconvenient of impossible.

Critical Points of the System (6)
The point y ϭ 0 in Fig. 82 seems to be a common point of all trajectories, and we want
to explore the reason for this remarkable observation. The answer will follow by calculus.
Indeed, from (6) we obtain

(9)

dy2
dy1

ϭ

y2r dt
y1r dt

ϭ

y2r
y1r

ϭ

a21 y1 ϩ a22 y2
a11 y1 ϩ a12 y2

.

This associates with every point P: ( y1, y2) a unique tangent direction dy2>dy1 of the
trajectory passing through P, except for the point P ϭ P0 : (0, 0), where the right side of (9)
becomes 0>0. This point P0, at which dy2>dy1 becomes undetermined, is called a critical
point of (6).

Five Types of Critical Points
There are five types of critical points depending on the geometric shape of the trajectories
near them. They are called improper nodes, proper nodes, saddle points, centers, and
spiral points. We define and illustrate them in Examples 1–5.
EXAMPLE 1

(Continued ) Improper Node (Fig. 82)
An improper node is a critical point P0 at which all the trajectories, except for two of them, have the same
limiting direction of the tangent. The two exceptional trajectories also have a limiting direction of the tangent
at P0 which, however, is different.
The system (8) has an improper node at 0, as its phase portrait Fig. 82 shows. The common limiting direction
at 0 is that of the eigenvector x (1) ϭ [1 1]T because e؊4t goes to zero faster than e؊2t as t increases. The two
exceptional limiting tangent directions are those of x (2) ϭ [1 Ϫ1]T and Ϫx (2) ϭ [Ϫ1 1]T.
᭿

c04.qxd

10/27/10

9:32 PM

Page 143

SEC. 4.3 Constant-Coefficient Systems. Phase Plane Method
EXAMPLE 2

143

Proper Node (Fig. 83)
A proper node is a critical point P0 at which every trajectory has a definite limiting direction and for any given
direction d at P0 there is a trajectory having d as its limiting direction.
The system

c

yr ϭ

(10)

1

0

0

1

d y,

y1r ϭ y1

thus

y2r ϭ y2

has a proper node at the origin (see Fig. 83). Indeed, the matrix is the unit matrix. Its characteristic equation
(1 Ϫ l)2 ϭ 0 has the root l ϭ 1. Any x 0 is an eigenvector, and we can take [1 0]T and [0 1]T. Hence
a general solution is

y ϭ c1

c d et ϩ c2 c d et
1

0

0

or

1

y1 ϭ c1et

or

y2 ϭ c2et

c1 y2 ϭ c2 y1.

y2

᭿

y2
(1)

y (t)

y1

y1

(2)

y (t)

Fig. 82.

EXAMPLE 3

Fig. 83.

Trajectories of the system (8)
(Improper node)

Trajectories of the system (10)
(Proper node)

Saddle Point (Fig. 84)
A saddle point is a critical point P0 at which there are two incoming trajectories, two outgoing trajectories, and
all the other trajectories in a neighborhood of P0 bypass P0.
The system

yr ϭ

(11)

c

1
0

0
Ϫ1

d y,

thus

y1r ϭ

y1

y1r ϭ Ϫy2

has a saddle point at the origin. Its characteristic equation (1 Ϫ l)(Ϫ1 Ϫ l) ϭ 0 has the roots l1 ϭ 1 and
l2 ϭ Ϫ1. For l ϭ 1 an eigenvector [1 0]T is obtained from the second row of (A Ϫ lI)x ϭ 0, that is,
0x 1 ϩ (Ϫ1 Ϫ 1)x 2 ϭ 0. For l2 ϭ Ϫ1 the first row gives [0 1]T. Hence a general solution is
y ϭ c1

c d et ϩ c2 c d e؊t
1
0

0
1

or

y1 ϭ c1et
y2 ϭ c2e؊t

This is a family of hyperbolas (and the coordinate axes); see Fig. 84.

or

y1 y2 ϭ const.
᭿

c04.qxd

10/27/10

9:32 PM

144
EXAMPLE 4

Page 144

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods
Center (Fig. 85)
A center is a critical point that is enclosed by infinitely many closed trajectories.
The system

yr ϭ

(12)

c

0
Ϫ4

1
0

d y,

(a)
thus

y1r ϭ y2

(b) y2r ϭ Ϫ4y1

has a center at the origin. The characteristic equation l2 ϩ 4 ϭ 0 gives the eigenvalues 2i and Ϫ2i. For 2i an
eigenvector follows from the first equation Ϫ2ix 1 ϩ x 2 ϭ 0 of (A Ϫ lI)x ϭ 0, say, [1 2i]T. For l ϭ Ϫ2i that
equation is Ϫ(Ϫ2i)x 1 ϩ x 2 ϭ 0 and gives, say, [1 Ϫ2i]T. Hence a complex general solution is

(12*)

c d e2it ϩ c2 c
1

y ϭ c1

2i

1
Ϫ2i

d e؊2it,

thus

y1 ϭ

c1e2it ϩ

c2e؊2it

y2 ϭ 2ic1e2it Ϫ 2ic2e؊2it.

A real solution is obtained from (12*) by the Euler formula or directly from (12) by a trick. (Remember the
trick and call it a method when you apply it again.) Namely, the left side of (a) times the right side of (b) is
Ϫ4y1y1r . This must equal the left side of (b) times the right side of (a). Thus,
Ϫ4y1 y1r ϭ y2 y2r .

By integration,

2y 21 ϩ 12 y 22 ϭ const.
᭿

This is a family of ellipses (see Fig. 85) enclosing the center at the origin.

y2

y2

y1

y1

Fig. 85.

Fig. 84. Trajectories of the system (11)
(Saddle point)

EXAMPLE 5

Trajectories of the system (12)
(Center)

Spiral Point (Fig. 86)
A spiral point is a critical point P0 about which the trajectories spiral, approaching P0 as t : ϱ (or tracing these
spirals in the opposite sense, away from P0).
The system

(13)

yr ϭ

c

Ϫ1

1

Ϫ1

Ϫ1

d y,

thus

y1r ϭ Ϫy1 ϩ y2
y2r ϭ Ϫy1 Ϫ y2

has a spiral point at the origin, as we shall see. The characteristic equation is l2 ϩ 2l ϩ 2 ϭ 0. It gives the
eigenvalues Ϫ1 ϩ i and Ϫ1 Ϫ i. Corresponding eigenvectors are obtained from (Ϫ1 Ϫ l)x 1 ϩ x 2 ϭ 0. For

c04.qxd

10/27/10

9:32 PM

Page 145

SEC. 4.3 Constant-Coefficient Systems. Phase Plane Method

145

l ϭ Ϫ1 ϩ i this becomes Ϫix 1 ϩ x 2 ϭ 0 and we can take [1 i]T as an eigenvector. Similarly, an eigenvector
corresponding to Ϫ1 Ϫ i is [1 Ϫi]T. This gives the complex general solution
y ϭ c1

c d e(؊1؉i)t ϩ c2 c
1

1

i

Ϫi

d e(؊1؊i)t.

The next step would be the transformation of this complex solution to a real general solution by the Euler
formula. But, as in the last example, we just wanted to see what eigenvalues to expect in the case of a spiral
point. Accordingly, we start again from the beginning and instead of that rather lengthy systematic calculation
we use a shortcut. We multiply the first equation in (13) by y1, the second by y2, and add, obtaining
y1 y1r ϩ y2 y2r ϭ Ϫ(y 21 ϩ y 22).
We now introduce polar coordinates r, t, where r 2 ϭ y 21 ϩ y 22. Differentiating this with respect to t gives
2rr r ϭ 2y1 y1r ϩ 2y2 y2r . Hence the previous equation can be written
rr r ϭ Ϫr 2,

Thus,

r r ϭ Ϫr,

dr>r ϭ Ϫdt,

ln ƒ r ƒ ϭ Ϫt ϩ c*,

r ϭ ce؊t.
᭿

For each real c this is a spiral, as claimed (see Fig. 86).

y2

y1

Fig. 86.

EXAMPLE 6

Trajectories of the system (13) (Spiral point)

No Basis of Eigenvectors Available. Degenerate Node (Fig. 87)
This cannot happen if A in (1) is symmetric (akj ϭ ajk, as in Examples 1–3) or skew-symmetric (akj ϭ Ϫajk,
thus ajj ϭ 0). And it does not happen in many other cases (see Examples 4 and 5). Hence it suffices to explain
the method to be used by an example.
Find and graph a general solution of

y r ϭ Ay ϭ

(14)

c

4

1

Ϫ1

2

d y.

Solution. A is not skew-symmetric! Its characteristic equation is
det (A Ϫ lI) ϭ 2

4Ϫl

1

Ϫ1

2Ϫl

2 ϭ l2 Ϫ 6l ϩ 9 ϭ (l Ϫ 3)2 ϭ 0.

c04.qxd

10/27/10

146

9:32 PM

Page 146

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods
It has a double root l ϭ 3. Hence eigenvectors are obtained from (4 Ϫ l)x 1 ϩ x 2 ϭ 0, thus from x 1 ϩ x 2 ϭ 0,
say, x (1) ϭ [1 Ϫ1]T and nonzero multiples of it (which do not help). The method now is to substitute
y (2) ϭ xtelt ϩ uelt
with constant u ϭ [u 1 u 2]T into (14). (The xt-term alone, the analog of what we did in Sec. 2.2 in the case
of a double root, would not be enough. Try it.) This gives
y (2) r ϭ xelt ϩ lxtelt ϩ luelt ϭ Ay (2) ϭ Axtelt ϩ Auelt.
On the right, Ax ϭ lx. Hence the terms lxtelt cancel, and then division by elt gives
x ϩ lu ϭ Au,
Here l ϭ 3 and x ϭ [1

(A Ϫ lI)u ϭ x.

thus

Ϫ1]T, so that

(A Ϫ 3I)u ϭ

c

4Ϫ3

1

Ϫ1

2Ϫ3

A solution, linearly independent of x ϭ [1

d uϭ c

1
Ϫ1

d,

Ϫ1]T, is u ϭ [0

y ϭ c1y (1) ϩ c2y (2) ϭ c1

c

1
Ϫ1

u1 ϩ u2 ϭ 1

thus

Ϫu 1 Ϫ u 2 ϭ Ϫ1.

1]T. This yields the answer (Fig. 87)

d e3t ϩ c2 £ c

1
Ϫ1

d t ϩ c d ≥ e3t.
0
1

The critical point at the origin is often called a degenerate node. c1y (1) gives the heavy straight line, with
c1 Ͼ 0 the lower part and c1 Ͻ 0 the upper part of it. y (2) gives the right part of the heavy curve from 0 through
the second, first, and—finally—fourth quadrants. Ϫy (2) gives the other part of that curve.
᭿

y2

y1
y

y

Fig. 87.

(2)

(1)

Degenerate node in Example 6

We mention that for a system (1) with three or more equations and a triple eigenvalue
with only one linearly independent eigenvector, one will get two solutions, as just
discussed, and a third linearly independent one from
y (3) ϭ 12 xt 2elt ϩ utelt ϩ velt

with v from

u ϩ lv ϭ Av.

c04.qxd

10/27/10

9:32 PM

Page 147

SEC. 4.3 Constant-Coefficient Systems. Phase Plane Method

147

PROBLEM SET 4.3
1–9

GENERAL SOLUTION

Find a real general solution of the following systems. Show
the details.
1. y1r ϭ y1 ϩ y2
y2r ϭ 3y1 Ϫ y2
2. y1r ϭ 6y1 ϩ 9y2
y2r ϭ y1 ϩ 6y2
3. y1r ϭ y1 ϩ 2y2
y2r ϭ 12 y1 ϩ y2
4. y1r ϭ Ϫ8y1 Ϫ 2y2
y2r ϭ 2y1 Ϫ 4y2
5. y1r ϭ 2y1 ϩ 5y2
y2r ϭ 5y1 ϩ 12.5y2
6. y1r ϭ 2y1 Ϫ 2y2
y2r ϭ 2y1 ϩ 2y2
7. y1r ϭ y2
y2r ϭ Ϫy1 ϩ y3
y3r ϭ Ϫy2

14. y1r ϭ Ϫy1 Ϫ y2
y2r ϭ y1 Ϫ y2
y1(0) ϭ 1, y2(0) ϭ 0
15. y1r ϭ 3y1 ϩ 2y2
y2r ϭ 2y1 ϩ 3y2
y1(0) ϭ 0.5, y2(0) ϭ Ϫ0.5

CONVERSION

16–17

Find a general solution by conversion to a single ODE.
16. The system in Prob. 8.
17. The system in Example 5 of the text.
18. Mixing problem, Fig. 88. Each of the two tanks
contains 200 gal of water, in which initially 100 lb
(Tank T1) and 200 lb (Tank T2) of fertilizer are dissolved.
The inflow, circulation, and outflow are shown in
Fig. 88. The mixture is kept uniform by stirring. Find
the fertilizer contents y1(t) in T1 and y2(t) in T2.
4 gal/min

12 gal/min
(Pure water)

T1

T2

16 gal/min

12 gal/min

8. y1r ϭ 8y1 Ϫ y2
y2r ϭ y1 ϩ 10y2

Fig. 88.

9. y1r ϭ 10y1 Ϫ 10y2 Ϫ 4y3
y2r ϭ Ϫ10y1 ϩ y2 Ϫ 14y3
y3r ϭ Ϫ4y1 Ϫ 14y2 Ϫ 2y3
10–15

IVPs

Solve the following initial value problems.
10. y1r ϭ 2y1 ϩ 2y2

Tanks in Problem 18

19. Network. Show that a model for the currents I1(t) and
I2(t) in Fig. 89 is

Ύ

1
I1 dt ϩ R(I1 Ϫ I2) ϭ 0,
C

LI 2r ϩ R(I2 Ϫ I1) ϭ 0.

Find a general solution, assuming that R ϭ 3 ⍀,
L ϭ 4 H, C ϭ 1>12 F.

y2r ϭ 5y1 Ϫ y2
C

y1(0) ϭ 0, y2(0) ϭ 7

I1

11. y1r ϭ 2y1 ϩ 5y2

R

y2r ϭ Ϫ12 y1 Ϫ 32 y2
y1(0) ϭ Ϫ12, y2(0) ϭ 0
12. y1r ϭ y1 ϩ 3y2
y2r ϭ 13 y1 ϩ y2
y1(0) ϭ 12, y2(0) ϭ 2
13. y1r ϭ y2
y2r ϭ y1
y1(0) ϭ 0, y2(0) ϭ 2

L

Fig. 89.

I2

Network in Problem 19

20. CAS PROJECT. Phase Portraits. Graph some of
the figures in this section, in particular Fig. 87 on the
degenerate node, in which the vector y (2) depends on t.
In each figure highlight a trajectory that satisfies an
initial condition of your choice.

c04.qxd

10/27/10

148

4.4

9:32 PM

Page 148

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods

Criteria for Critical Points. Stability
We continue our discussion of homogeneous linear systems with constant coefficients (1).
Let us review where we are. From Sec. 4.3 we have
(1)

y r ϭ Ay ϭ

c

a11

a12

a21

a22

d y,

in components,

y r1 ϭ a11 y1 ϩ a12 y2
y r2 ϭ a21 y1 ϩ a22 y2.

From the examples in the last section, we have seen that we can obtain an overview of
families of solution curves if we represent them parametrically as y(t) ϭ [ y1(t) y2(t)]T
and graph them as curves in the y1 y2-plane, called the phase plane. Such a curve is called
a trajectory of (1), and their totality is known as the phase portrait of (1).
Now we have seen that solutions are of the form
y(t) ϭ xelt.

Substitution into (1) gives

y r (t) ϭ lxelt ϭ Ay ϭ Axelt.

Dropping the common factor elt, we have
Ax ϭ lx.

(2)

Hence y(t) is a (nonzero) solution of (1) if l is an eigenvalue of A and x a corresponding
eigenvector.
Our examples in the last section show that the general form of the phase portrait is
determined to a large extent by the type of critical point of the system (1) defined as a
point at which dy2 >dy1 becomes undetermined, 0>0; here [see (9) in Sec. 4.3]
dy2

(3)

dy1

ϭ

y r2 dt
y 1r dt

ϭ

a21 y1 ϩ a22 y2
a11 y1 ϩ a12 y2

.

We also recall from Sec. 4.3 that there are various types of critical points.
What is now new, is that we shall see how these types of critical points are related
to the eigenvalues. The latter are solutions l ϭ l1 and l2 of the characteristic equation
(4)

det (A Ϫ lI) ϭ 2

a11 Ϫ l

a12

a21

a22 Ϫ l

2 ϭ l 2 Ϫ (a11 ϩ a22)l ϩ det A ϭ 0.

This is a quadratic equation l2 Ϫ pl ϩ q ϭ 0 with coefficients p, q and discriminant ¢
given by
(5)

p ϭ a11 ϩ a22,

q ϭ det A ϭ a11a22 Ϫ a12a21,

¢ ϭ p 2 Ϫ 4q.

From algebra we know that the solutions of this equation are
(6)

l1 ϭ 12 ( p ϩ 1¢),

l2 ϭ 12 ( p Ϫ 1¢).

c04.qxd

10/27/10

9:32 PM

Page 149

SEC. 4.4 Criteria for Critical Points. Stability

149

Furthermore, the product representation of the equation gives
l2 Ϫ pl ϩ q ϭ (l Ϫ l1)(l Ϫ l2) ϭ l2 Ϫ (l1 ϩ l2)l ϩ l1l2.
Hence p is the sum and q the product of the eigenvalues. Also l1 Ϫ l2 ϭ 1¢ from (6).
Together,
p ϭ l1 ϩ l2,

(7)

q ϭ l1l2,

¢ ϭ (l1 Ϫ l2)2.

This gives the criteria in Table 4.1 for classifying critical points. A derivation will be
indicated later in this section.
Table 4.1 Eigenvalue Criteria for Critical Points
(Derivation after Table 4.2)
Name
(a) Node
(b) Saddle point
(c) Center
(d) Spiral point

p ϭ l1 ϩ l2

pϭ0
p 0

q ϭ l1l2

¢ ϭ (l1 Ϫ l2)2

Comments on l1, l2

qϾ0
qϽ0
qϾ0

¢м0

Real, same sign
Real, opposite signs
Pure imaginary
Complex, not pure
imaginary

¢Ͻ0

Stability
Critical points may also be classified in terms of their stability. Stability concepts are basic
in engineering and other applications. They are suggested by physics, where stability
means, roughly speaking, that a small change (a small disturbance) of a physical system
at some instant changes the behavior of the system only slightly at all future times t. For
critical points, the following concepts are appropriate.
DEFINITIONS

Stable, Unstable, Stable and Attractive

A critical point P0 of (1) is called stable2 if, roughly, all trajectories of (1) that at
some instant are close to P0 remain close to P0 at all future times; precisely: if for
every disk DP of radius P Ͼ 0 with center P0 there is a disk Dd of radius d Ͼ 0 with
center P0 such that every trajectory of (1) that has a point P1 (corresponding to t ϭ t 1,
say) in Dd has all its points corresponding to t м t 1 in DP. See Fig. 90.
P0 is called unstable if P0 is not stable.
P0 is called stable and attractive (or asymptotically stable) if P0 is stable and
every trajectory that has a point in Dd approaches P0 as t : ϱ . See Fig. 91.
Classification criteria for critical points in terms of stability are given in Table 4.2. Both
tables are summarized in the stability chart in Fig. 92. In this chart region of instability
is dark blue.
2

In the sense of the Russian mathematician ALEXANDER MICHAILOVICH LJAPUNOV (1857–1918),
whose work was fundamental in stability theory for ODEs. This is perhaps the most appropriate definition of
stability (and the only we shall use), but there are others, too.

10/27/10

9:32 PM

150

Page 150

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods

P1
∈

δ

∈

δ

P0

P0

Fig. 90. Stable critical point P0 of (1)
(The trajectory initiating at P1 stays
in the disk of radius ⑀.)

Fig. 91. Stable and attractive critical
point P0 of (1)

Table 4.2 Stability Criteria for Critical Points
Type of Stability

p ϭ l1 ϩ l2

q ϭ l1l2

(a) Stable and attractive
(b) Stable
(c) Unstable

pϽ0
pϹ0
pϾ0

qϾ0
qϾ0
qϽ0

OR

q

Δ>0

Δ<0

Δ<0

Δ

0

Δ>0

=

=
0

Spiral
point

Spiral
point

Node

Δ

c04.qxd

Node
p
Saddle point

Fig. 92. Stability chart of the system (1) with p, q, ⌬ defined in (5).
Stable and attractive: The second quadrant without the q-axis.
Stability also on the positive q-axis (which corresponds to centers).
Unstable: Dark blue region

We indicate how the criteria in Tables 4.1 and 4.2 are obtained. If q ϭ l1l2 Ͼ 0, both
of the eigenvalues are positive or both are negative or complex conjugates. If also
p ϭ l1 ϩ l2 Ͻ 0, both are negative or have a negative real part. Hence P0 is stable and
attractive. The reasoning for the other two lines in Table 4.2 is similar.
If ¢ Ͻ 0, the eigenvalues are complex conjugates, say, l1 ϭ a ϩ ib and l2 ϭ a Ϫ ib.
If also p ϭ l1 ϩ l2 ϭ 2a Ͻ 0, this gives a spiral point that is stable and attractive. If
p ϭ 2a Ͼ 0, this gives an unstable spiral point.
If p ϭ 0, then l2 ϭ Ϫl1 and q ϭ l1l2 ϭ Ϫl21. If also q Ͼ 0, then l21 ϭ Ϫq Ͻ 0, so
that l1, and thus l2, must be pure imaginary. This gives periodic solutions, their trajectories
being closed curves around P0, which is a center.
EXAMPLE 1

Application of the Criteria in Tables 4.1 and 4.2
In Example 1, Sec 4.3, we have y r ϭ
stable and attractive by Table 4.2(a).

c

Ϫ3

1

1

Ϫ3

d y, p ϭ Ϫ6, q ϭ 8, ¢ ϭ 4, a node by Table 4.1(a), which is

᭿

c04.qxd

10/27/10

9:32 PM

Page 151

SEC. 4.4 Criteria for Critical Points. Stability
EXAMPLE 2

151

Free Motions of a Mass on a Spring
What kind of critical point does my s ϩ cy r ϩ ky ϭ 0 in Sec. 2.4 have?

Solution. Division by m gives y s ϭ Ϫ(k>m)y Ϫ (c>m)y r . To get a system, set y1 ϭ y, y2 ϭ y r (see Sec. 4.1).
Then y2r ϭ y s ϭ Ϫ(k>m)y1 Ϫ (c>m)y2. Hence
yr ϭ

c

0

1

Ϫk>m

Ϫc>m

d y,

det (A Ϫ lI) ϭ 2

Ϫl

1

Ϫk>m

Ϫc>m Ϫ l

2 ϭ l2 ϩ

c
k
l ϩ ϭ 0.
m
m

We see that p ϭ Ϫc>m, q ϭ k>m, ¢ ϭ (c>m)2 Ϫ 4k>m. From this and Tables 4.1 and 4.2 we obtain the following
results. Note that in the last three cases the discriminant ¢ plays an essential role.
No damping. c ϭ 0, p ϭ 0, q Ͼ 0, a center.
Underdamping. c2 Ͻ 4mk, p Ͻ 0, q Ͼ 0, ¢ Ͻ 0, a stable and attractive spiral point.
Critical damping. c2 ϭ 4mk, p Ͻ 0, q Ͼ 0, ¢ ϭ 0, a stable and attractive node.
Overdamping. c2 Ͼ 4mk, p Ͻ 0, q Ͼ 0, ¢ Ͼ 0, a stable and attractive node.

᭿

PROBLEM SET 4.4
1–10

TYPE AND STABILITY OF
CRITICAL POINT

Determine the type and stability of the critical point. Then
find a real general solution and sketch or graph some of the
trajectories in the phase plane. Show the details of your work.
1. y1r ϭ y1
y2r ϭ 2y2

2. y1r ϭ Ϫ4y1
y2r ϭ Ϫ3y2

3. y1r ϭ y2
y2r ϭ Ϫ9y1

4. y1r ϭ 2y1 ϩ y2
y2r ϭ 5y1 Ϫ 2y2

5. y1r ϭ Ϫ2y1 ϩ 2y2
y2r ϭ Ϫ2y1 Ϫ 2y2

6. y1r ϭ Ϫ6y1 Ϫ y2
y2r ϭ Ϫ9y1 Ϫ 6y2

7. y1r ϭ y1 ϩ 2y2
y2r ϭ 2y1 ϩ y2

8. y1r ϭ Ϫy1 ϩ 4y2
y2r ϭ 3y1 Ϫ 2y2

9. y1r ϭ 4y1 ϩ y2
y2r ϭ 4y1 ϩ 4y2

10. y1r ϭ y2
y2r ϭ Ϫ5y1 Ϫ 2y2

11–18

TRAJECTORIES OF SYSTEMS AND
SECOND-ORDER ODEs. CRITICAL
POINTS

11. Damped oscillations. Solve y s ϩ 2y r ϩ 2y ϭ 0. What
kind of curves are the trajectories?
12. Harmonic oscillations. Solve y s ϩ 19 y ϭ 0. Find the
trajectories. Sketch or graph some of them.
13. Types of critical points. Discuss the critical points in
(10)–(13) of Sec. 4.3 by using Tables 4.1 and 4.2.
14. Transformation of parameter. What happens to the
critical point in Example 1 if you introduce t ϭ Ϫt as
a new independent variable?

15. Perturbation of center. What happens in Example 4
of Sec. 4.3 if you change A to A ϩ 0.1I, where I is the
unit matrix?
16. Perturbation of center. If a system has a center as
its critical point, what happens if you replace the
~
matrix A by A ϭ A ϩ kI with any real number k 0
(representing measurement errors in the diagonal
entries)?
17. Perturbation. The system in Example 4 in Sec. 4.3
has a center as its critical point. Replace each ajk in
Example 4, Sec. 4.3, by ajk ϩ b. Find values of b such
that you get (a) a saddle point, (b) a stable and attractive
node, (c) a stable and attractive spiral, (d) an unstable
spiral, (e) an unstable node.
18. CAS EXPERIMENT. Phase Portraits. Graph phase
portraits for the systems in Prob. 17 with the values
of b suggested in the answer. Try to illustrate how
the phase portrait changes “continuously” under a
continuous change of b.
19. WRITING PROBLEM. Stability. Stability concepts
are basic in physics and engineering. Write a two-part
report of 3 pages each (A) on general applications
in which stability plays a role (be as precise as you
can), and (B) on material related to stability in this
section. Use your own formulations and examples; do
not copy.
20. Stability chart. Locate the critical points of the
systems (10)–(14) in Sec. 4.3 and of Probs. 1, 3, 5 in
this problem set on the stability chart.

c04.qxd

10/27/10

152

4.5

9:32 PM

Page 152

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods

Qualitative Methods for Nonlinear Systems
Qualitative methods are methods of obtaining qualitative information on solutions
without actually solving a system. These methods are particularly valuable for systems
whose solution by analytic methods is difficult or impossible. This is the case for many
practically important nonlinear systems

(1)

y r ϭ f(y),

thus

y r1 ϭ f1( y1, y2)
y 2r ϭ f2( y1, y2).

In this section we extend phase plane methods, as just discussed, from linear systems
to nonlinear systems (1). We assume that (1) is autonomous, that is, the independent
variable t does not occur explicitly. (All examples in the last section are autonomous.)
We shall again exhibit entire families of solutions. This is an advantage over numeric
methods, which give only one (approximate) solution at a time.
Concepts needed from the last section are the phase plane (the y1 y2-plane), trajectories
(solution curves of (1) in the phase plane), the phase portrait of (1) (the totality of these
trajectories), and critical points of (1) (points (y1, y2) at which both f1( y1, y2) and f2( y1, y2)
are zero).
Now (1) may have several critical points. Our approach shall be to discuss one critical
point after another. If a critical point P0 is not at the origin, then, for technical
convenience, we shall move this point to the origin before analyzing the point. More
formally, if P0: (a, b) is a critical point with (a, b) not at the origin (0, 0), then we apply
the translation
~
y 1 ϭ y1 Ϫ a,

~
y 2 ϭ y2 Ϫ b

which moves P0 to (0, 0) as desired. Thus we can assume P0 to be the origin (0, 0), and
y 1, ~
y 2). We also assume that P0 is
for simplicity we continue to write y1, y2 (instead of ~
isolated, that is, it is the only critical point of (1) within a (sufficiently small) disk with
center at the origin. If (1) has only finitely many critical points, that is automatically
true. (Explain!)

Linearization of Nonlinear Systems
How can we determine the kind and stability property of a critical point P0: (0, 0) of
(1)? In most cases this can be done by linearization of (1) near P0, writing (1) as
y r ϭ f( y) ϭ Ay ϩ h( y) and dropping h(y), as follows.
Since P0 is critical, f1(0, 0) ϭ 0, f2(0, 0) ϭ 0, so that f1 and f2 have no constant terms
and we can write
(2)

y r ϭ Ay ϩ h(y),

thus

y 1r ϭ a11 y1 ϩ a12 y2 ϩ h 1( y1, y2)
y2r ϭ a21 y1 ϩ a22 y2 ϩ h 2( y1, y2).

A is constant (independent of t) since (1) is autonomous. One can prove the following
(proof in Ref. [A7], pp. 375–388, listed in App. 1).

c04.qxd

11/9/10

7:23 PM

Page 153

SEC. 4.5 Qualitative Methods for Nonlinear Systems

THEOREM 1

153

Linearization

If f1 and f2 in (1) are continuous and have continuous partial derivatives in a
neighborhood of the critical point P0: (0, 0), and if det A 0 in (2), then the kind
and stability of the critical point of (1) are the same as those of the linearized
system

(3)

y r ϭ Ay,

y r1 ϭ a11 y1 ϩ a12 y2

thus

y r2 ϭ a21 y1 ϩ a22 y2.

Exceptions occur if A has equal or pure imaginary eigenvalues; then (1) may have
the same kind of critical point as (3) or a spiral point.
EXAMPLE 1

Free Undamped Pendulum. Linearization
Figure 93a shows a pendulum consisting of a body of mass m (the bob) and a rod of length L. Determine the
locations and types of the critical points. Assume that the mass of the rod and air resistance are negligible.

Solution. Step 1. Setting up the mathematical model. Let u denote the angular displacement, measured
counterclockwise from the equilibrium position. The weight of the bob is mg (g the acceleration of gravity). It
causes a restoring force mg sin u tangent to the curve of motion (circular arc) of the bob. By Newton’s second
law, at each instant this force is balanced by the force of acceleration mLu s , where Lu s is the acceleration;
hence the resultant of these two forces is zero, and we obtain as the mathematical model
mLu s ϩ mg sin u ϭ 0.
Dividing this by mL, we have
ak ϭ

u s ϩ k sin u ϭ 0

(4)

g
L

b.

When u is very small, we can approximate sin u rather accurately by u and obtain as an approximate solution
A cos 1kt ϩ B sin 1kt, but the exact solution for any u is not an elementary function.
Step 2. Critical points (0, 0), (؎2␲, 0), (؎4␲, 0), Á , Linearization. To obtain a system of ODEs, we set
u ϭ y1, u r ϭ y2. Then from (4) we obtain a nonlinear system (1) of the form
y1r ϭ f1( y1, y2) ϭ y2

(4*)

y2r ϭ f2( y1, y2) ϭ Ϫk sin y1.

The right sides are both zero when y2 ϭ 0 and sin y1 ϭ 0. This gives infinitely many critical points (np, 0),
where n ϭ 0, Ϯ1, Ϯ2, Á . We consider (0, 0). Since the Maclaurin series is
sin y1 ϭ y1 Ϫ 16 y 31 ϩ Ϫ Á Ϸ y1,
the linearized system at (0, 0) is
y r ϭ Ay ϭ

c

0
Ϫk

1
0

d y,

thus

y1r ϭ y2
y2r ϭ Ϫky1.

To apply our criteria in Sec. 4.4 we calculate p ϭ a11 ϩ a22 ϭ 0, q ϭ det A ϭ k ϭ g>L (Ͼ0), and
¢ ϭ p 2 Ϫ 4q ϭ Ϫ4k. From this and Table 4.1(c) in Sec. 4.4 we conclude that (0, 0) is a center, which is always
stable. Since sin u ϭ sin y1 is periodic with period 2p, the critical points (np, 0), n ϭ Ϯ2, Ϯ4, Á , are all centers.
Step 3. Critical points (؎␲, 0), (؎3␲, 0), (؎5␲, 0), Á , Linearization. We now consider the critical point
(p, 0), setting u Ϫ p ϭ y1 and (u Ϫ p) r ϭ u r ϭ y2. Then in (4),
sin u ϭ sin ( y1 ϩ p) ϭ Ϫsin y1 ϭ Ϫy1 ϩ 16 y 31 Ϫ ϩ Á Ϸ Ϫy1

c04.qxd

10/27/10

9:32 PM

154

Page 154

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods
and the linearized system at (p, 0) is now
y r ϭ Ay ϭ

c

0

1

k

0

d

y,

y1r ϭ y2

thus

y2r ϭ ky1.

We see that p ϭ 0, q ϭ Ϫk (Ͻ0), and ¢ ϭ Ϫ4q ϭ 4k. Hence, by Table 4.1(b), this gives a saddle point, which
is always unstable. Because of periodicity, the critical points (np, 0), n ϭ Ϯ1, Ϯ3, Á , are all saddle points.
These results agree with the impression we get from Fig. 93b.
᭿

y2

θ

C>k

C=k

L

π

−π

m

2π

3π

y1

mg sin θ
mg
(a) Pendulum

(b) Solution curves y2( y1) of (4) in the phase plane

Fig. 93.

EXAMPLE 2

Example 1 (C will be explained in Example 4.)

Linearization of the Damped Pendulum Equation
To gain further experience in investigating critical points, as another practically important case, let us see how
Example 1 changes when we add a damping term cu r (damping proportional to the angular velocity) to equation
(4), so that it becomes
u s ϩ cu r ϩ k sin u ϭ 0

(5)

where k Ͼ 0 and c м 0 (which includes our previous case of no damping, c ϭ 0). Setting u ϭ y1, u r ϭ y2, as
before, we obtain the nonlinear system (use u s ϭ y2r )
y1r ϭ y2
y2r ϭ Ϫk sin y1 Ϫ cy2.
We see that the critical points have the same locations as before, namely, (0, 0), (Ϯp, 0), (Ϯ2p, 0), Á . We
consider (0, 0). Linearizing sin y1 Ϸ y1 as in Example 1, we get the linearized system at (0, 0)
(6)

y r ϭ Ay ϭ

c

0
Ϫk

1
Ϫc

d

y,

thus

y1r ϭ y2
y2r ϭ Ϫky1 Ϫ cy2.

This is identical with the system in Example 2 of Sec. 4.4, except for the (positive!) factor m (and except for
the physical meaning of y1). Hence for c ϭ 0 (no damping) we have a center (see Fig. 93b), for small damping
we have a spiral point (see Fig. 94), and so on.
We now consider the critical point (p, 0). We set u Ϫ p ϭ y1, (u Ϫ p) r ϭ u r ϭ y2 and linearize
sin u ϭ sin ( y1 ϩ p) ϭ Ϫsin y1 Ϸ Ϫy1.
This gives the new linearized system at (p, 0)
(6*)

y r ϭ Ay ϭ

c

0
k

1
Ϫc

d

y,

thus

y1r ϭ y2
y2r ϭ ky1 Ϫ cy2.

c04.qxd

10/27/10

9:33 PM

Page 155

SEC. 4.5 Qualitative Methods for Nonlinear Systems

155

For our criteria in Sec. 4.4 we calculate p ϭ a11 ϩ a22 ϭ Ϫc, q ϭ det A ϭ Ϫk, and ¢ ϭ p 2 Ϫ 4q ϭ c2 ϩ 4k.
This gives the following results for the critical point at (p, 0).
No damping. c ϭ 0, p ϭ 0, q Ͻ 0, ¢ Ͼ 0, a saddle point. See Fig. 93b.
Damping. c Ͼ 0, p Ͻ 0, q Ͻ 0, ¢ Ͼ 0, a saddle point. See Fig. 94.
Since sin y1 is periodic with period 2p, the critical points (Ϯ2p, 0), (Ϯ4p, 0), Á are of the same type as
(0, 0), and the critical points (Ϫp, 0), (Ϯ3p, 0), Á are of the same type as (p, 0), so that our task is finished.
Figure 94 shows the trajectories in the case of damping. What we see agrees with our physical intuition.
Indeed, damping means loss of energy. Hence instead of the closed trajectories of periodic solutions in
Fig. 93b we now have trajectories spiraling around one of the critical points (0, 0), (Ϯ2p, 0), Á . Even the
wavy trajectories corresponding to whirly motions eventually spiral around one of these points. Furthermore,
there are no more trajectories that connect critical points (as there were in the undamped case for the saddle
points).
᭿
y2

π

−π

2π

3π

y1

Fig. 94. Trajectories in the phase plane for the damped pendulum in Example 2

Lotka–Volterra Population Model
EXAMPLE 3

Predator–Prey Population Model3
This model concerns two species, say, rabbits and foxes, and the foxes prey on the rabbits.
Step 1. Setting up the model. We assume the following.
1. Rabbits have unlimited food supply. Hence, if there were no foxes, their number y1(t) would grow
exponentially, y1r ϭ ay1.
2. Actually, y1 is decreased because of the kill by foxes, say, at a rate proportional to y1 y2, where y2(t) is
the number of foxes. Hence y1r ϭ ay1 Ϫ by1 y2, where a Ͼ 0 and b Ͼ 0.
3. If there were no rabbits, then y2(t) would exponentially decrease to zero, y2r ϭ Ϫly2. However, y2 is
increased by a rate proportional to the number of encounters between predator and prey; together we
have y2r ϭ Ϫly2 ϩ ky1 y2, where k Ͼ 0 and l Ͼ 0.
This gives the (nonlinear!) Lotka–Volterra system

(7)

y1r ϭ f1( y1, y2) ϭ ay1 Ϫ by1 y2
y2r ϭ f2( y1, y2) ϭ ky1 y2 Ϫ ly2.

3
Introduced by ALFRED J. LOTKA (1880–1949), American biophysicist, and VITO VOLTERRA
(1860–1940), Italian mathematician, the initiator of functional analysis (see [GR7] in App. 1).

c04.qxd

10/27/10

156

9:33 PM

Page 156

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods
Step 2. Critical point (0, 0), Linearization. We see from (7) that the critical points are the solutions of
(7*)

f1( y1, y2) ϭ y1(a Ϫ by2) ϭ 0,

f2( y1, y2) ϭ y2(ky1 Ϫ l) ϭ 0.

l a
The solutions are ( y1, y2) ϭ (0, 0) and a , b . We consider (0, 0). Dropping Ϫby1 y2 and ky1 y2 from (7) gives
k b
the linearized system
yr ϭ

c

a

0

0

Ϫl

d y.

Its eigenvalues are l1 ϭ a Ͼ 0 and l2 ϭ Ϫl Ͻ 0. They have opposite signs, so that we get a saddle point.
Step. 3. Critical point (l>k, a>b), Linearization. We set y1 ϭ ~y1 ϩ l>k, y2 ϭ ~y 2 ϩ a>b. Then the critical point
(l>k, a>b) corresponds to (~y , ~y ) ϭ (0, 0). Since ~yr ϭ y r , ~yr ϭ y r , we obtain from (7) [factorized as in (7*)]
1

2

1

1

2

2

~ )
~ ϩ l b (Ϫby
~yr ϭ ay
~ ϩ l b Ba Ϫ b ay
~ ϩ a bR ϭ ay
2
1
2
1
1
k
b
k
~ ϩ a b ky
~ .
~yr ϭ ay
~ ϩ a b Bk ay
~ ϩ l b Ϫ lR ϭ ay
2
1
2
2
1
k
b
b
~ ~y and ky
~ ~y , we have the linearized system
Dropping the two nonlinear terms Ϫby
1 2
1 2
lb
(a) ~yr1 ϭ Ϫ ~y2
k

(7**)

(b)

~yr ϭ ak ~y .
2
1
b

The left side of (a) times the right side of (b) must equal the right side of (a) times the left side of (b),
ak ~ ~
lb
y1 y1r ϭ Ϫ ~y2~y2r .
b
k

By integration,

ak ~ 2 lb ~ 2
y 1 ϩ y2 ϭ const.
b
k

This is a family of ellipses, so that the critical point (l>k, a>b) of the linearized system (7**) is a center (Fig. 95).
It can be shown, by a complicated analysis, that the nonlinear system (7) also has a center (rather than a spiral
point) at (l>k, a>b) surrounded by closed trajectories (not ellipses).
We see that the predators and prey have a cyclic variation about the critical point. Let us move counterclockwise
around the ellipse, beginning at the right vertex, where the rabbits have a maximum number. Foxes are sharply
increasing in number until they reach a maximum at the upper vertex, and the number of rabbits is then sharply
decreasing until it reaches a minimum at the left vertex, and so on. Cyclic variations of this kind have
been observed in nature, for example, for lynx and snowshoe hare near the Hudson Bay, with a cycle of about
10 years.
For models of more complicated situations and a systematic discussion, see C. W. Clark, Mathematical
Bioeconomics: The Mathematics of Conservation, 3rd ed. Hoboken, NJ, Wiley, 2010.
᭿
y2
_a_

b

l
__

y1

k

Fig. 95. Ecological equilibrium point and trajectory
of the linearized Lotka–Volterra system (7**)

c04.qxd

10/27/10

9:33 PM

Page 157

SEC. 4.5 Qualitative Methods for Nonlinear Systems

157

Transformation to a First-Order Equation
in the Phase Plane
Another phase plane method is based on the idea of transforming a second-order
autonomous ODE (an ODE in which t does not occur explicitly)
F( y, y r , y s ) ϭ 0
to first order by taking y ϭ y1 as the independent variable, setting y r ϭ y2 and transforming
y s by the chain rule,
y s ϭ y r2 ϭ

dy2
dt

ϭ

dy2 dy1
dy1 dt

ϭ

dy2
dy1

y2.

Then the ODE becomes of first order,
F ay1, y2,

(8)

dy2
dy1

y2 b ϭ 0

and can sometimes be solved or treated by direction fields. We illustrate this for the
equation in Example 1 and shall gain much more insight into the behavior of solutions.
EXAMPLE 4

An ODE (8) for the Free Undamped Pendulum
If in (4) u s ϩ k sin u ϭ 0 we set u ϭ y1, u r ϭ y2 (the angular velocity) and use
us ϭ

dy2
dt

ϭ

dy2 dy1
dy1 dt

ϭ

dy2
dy1

y2,

we get

dy2
dy1

y2 ϭ Ϫk sin y1.

Separation of variables gives y2 dy2 ϭ Ϫk sin y1 dy1. By integration,
(9)

1 2
2 y2

ϭ k cos y1 ϩ C

(C constant).

Multiplying this by mL2, we get
1
2
2 m(Ly2)

Ϫ mL2k cos y1 ϭ mL2C.

We see that these three terms are energies. Indeed, y2 is the angular velocity, so that Ly2 is the velocity and the
first term is the kinetic energy. The second term (including the minus sign) is the potential energy of the pendulum,
and mL2C is its total energy, which is constant, as expected from the law of conservation of energy, because
there is no damping (no loss of energy). The type of motion depends on the total energy, hence on C, as follows.
Figure 93b shows trajectories for various values of C. These graphs continue periodically with period 2p to
the left and to the right. We see that some of them are ellipse-like and closed, others are wavy, and there are two
trajectories (passing through the saddle points (np, 0), n ϭ Ϯ1, Ϯ3, Á ) that separate those two types of
trajectories. From (9) we see that the smallest possible C is C ϭ Ϫk; then y2 ϭ 0, and cos y1 ϭ 1, so that the
pendulum is at rest. The pendulum will change its direction of motion if there are points at which y2 ϭ u r ϭ 0.
Then k cos y1 ϩ C ϭ 0 by (9). If y1 ϭ p, then cos y1 ϭ Ϫ1 and C ϭ k. Hence if Ϫk Ͻ C Ͻ k, then the
pendulum reverses its direction for a ƒ y1 ƒ ϭ ƒ u ƒ Ͻ p, and for these values of C with ƒ C ƒ Ͻ k the pendulum
oscillates. This corresponds to the closed trajectories in the figure. However, if C Ͼ k, then y2 ϭ 0 is impossible
and the pendulum makes a whirly motion that appears as a wavy trajectory in the y1 y2-plane. Finally, the value
C ϭ k corresponds to the two “separating trajectories” in Fig. 93b connecting the saddle points.
᭿

The phase plane method of deriving a single first-order equation (8) may be of practical
interest not only when (8) can be solved (as in Example 4) but also when a solution

c04.qxd

10/27/10

9:33 PM

158

Page 158

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods

is not possible and we have to utilize fields (Sec. 1.2). We illustrate this with a very
famous example:
EXAMPLE 5

Self-Sustained Oscillations. Van der Pol Equation
There are physical systems such that for small oscillations, energy is fed into the system, whereas for large
oscillations, energy is taken from the system. In other words, large oscillations will be damped, whereas for
small oscillations there is “negative damping” (feeding of energy into the system). For physical reasons we
expect such a system to approach a periodic behavior, which will thus appear as a closed trajectory in the phase
plane, called a limit cycle. A differential equation describing such vibrations is the famous van der Pol equation4
(␮ Ͼ 0, constant).

y s Ϫ ␮(1 Ϫ y 2)y r ϩ y ϭ 0

(10)

It first occurred in the study of electrical circuits containing vacuum tubes. For ␮ ϭ 0 this equation becomes
y s ϩ y ϭ 0 and we obtain harmonic oscillations. Let ␮ Ͼ 0. The damping term has the factor Ϫ␮(1 Ϫ y 2).
This is negative for small oscillations, when y 2 Ͻ 1, so that we have “negative damping,” is zero for y 2 ϭ 1
(no damping), and is positive if y 2 Ͼ 1 (positive damping, loss of energy). If ␮ is small, we expect a limit cycle
that is almost a circle because then our equation differs but little from y s ϩ y ϭ 0. If ␮ is large, the limit cycle
will probably look different.
Setting y ϭ y1, y r ϭ y2 and using y s ϭ (dy2>dy1)y2 as in (8), we have from (10)
dy2

(11)

dy1

y2 Ϫ ␮(1 Ϫ y 21)y2 ϩ y1 ϭ 0.

The isoclines in the y1y2-plane (the phase plane) are the curves dy2>dy1 ϭ K ϭ const, that is,
dy2
dy1

ϭ ␮(1 Ϫ y 21) Ϫ

y1
y2

ϭ K.

Solving algebraically for y2, we see that the isoclines are given by
y2 ϭ

y1

y2

K = – 12_

K=

(Figs. 96, 97).

␮(1 Ϫ y 21) Ϫ K

5

K=0

K = –1

1
_
4

K = –5

K=1

5

5

y1

K=1
K = 41_

K = –5

K = – 12_

K=0
K = –1

–5

Fig. 96. Direction field for the van der Pol equation with ␮ ϭ 0.1 in the phase plane,
showing also the limit cycle and two trajectories. See also Fig. 8 in Sec. 1.2
4

BALTHASAR VAN DER POL (1889–1959), Dutch physicist and engineer.

c04.qxd

10/27/10

9:33 PM

Page 159

SEC. 4.5 Qualitative Methods for Nonlinear Systems

159

Figure 96 shows some isoclines when ␮ is small, ␮ ϭ 0.1, the limit cycle (almost a circle), and two (blue) trajectories
approaching it, one from the outside and the other from the inside, of which only the initial portion, a small spiral, is
shown. Due to this approach by trajectories, a limit cycle differs conceptually from a closed curve (a trajectory)
surrounding a center, which is not approached by trajectories. For larger ␮ the limit cycle no longer resembles a
circle, and the trajectories approach it more rapidly than for smaller ␮. Figure 97 illustrates this for ␮ ϭ 1.
᭿

y2

K=0
K = –1

K=1

K = –1
K=0

K = –5

3

2

1

–1

y1

1

–1

–2

–3
K=0

K = –5

K=0
K=1

K = –1

K = –1

Fig. 97. Direction field for the van der Pol equation with ␮ ϭ 1 in the phase plane,
showing also the limit cycle and two trajectories approaching it

PROBLEM SET 4.5
1. Pendulum. To what state (position, speed, direction
of motion) do the four points of intersection of a
closed trajectory with the axes in Fig. 93b
correspond? The point of intersection of a wavy curve
with the y2-axis?
2. Limit cycle. What is the essential difference between
a limit cycle and a closed trajectory surrounding a
center?
3. CAS EXPERIMENT. Deformation of Limit Cycle.
Convert the van der Pol equation to a system. Graph
the limit cycle and some approaching trajectories for
␮ ϭ 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0. Try to observe how
the limit cycle changes its form continuously if you
vary ␮ continuously. Describe in words how the limit
cycle is deformed with growing ␮.

4–8

CRITICAL POINTS. LINEARIZATION

Find the location and type
linearization. Show the details
4. y1r ϭ 4y1 Ϫ y 21
y2r ϭ y2
6. y1r ϭ y2
y2r ϭ Ϫy1 Ϫ y 21

of all critical points by
of your work.
5. y1r ϭ y2
y2r ϭ Ϫy1 ϩ 12 y 21
7. y1r ϭ Ϫy1 ϩ y2 Ϫ y 22
y2r ϭ Ϫy1 Ϫ y2

8. y1r ϭ y2 Ϫ y 22
y2r ϭ y1 Ϫ y 21
9–13

CRITICAL POINTS OF ODEs

Find the location and type of all critical points by first
converting the ODE to a system and then linearizing it.
9. y s Ϫ 9y ϩ y 3 ϭ 0
10. y s ϩ y Ϫ y 3 ϭ 0
11. y s ϩ cos y ϭ 0
12. y s ϩ 9y ϩ y 2 ϭ 0

c04.qxd

10/27/10

160

9:33 PM

Page 160

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods

13. y s ϩ sin y ϭ 0
14. TEAM PROJECT. Self-sustained oscillations.
(a) Van der Pol equation. Determine the type of the
critical point at (0, 0) when ␮ Ͼ 0, ␮ ϭ 0, ␮ Ͻ 0.
(b) Rayleigh equation. Show that the Rayleigh
equation5
Y s Ϫ ␮(1 Ϫ 13Y r 2)Y r ϩ Y ϭ 0 (␮ Ͼ 0)
also describes self-sustained oscillations and that by
differentiating it and setting y ϭ Y r one obtains the van
der Pol equation.
(c) Duffing equation. The Duffing equation is
y s ϩ v20y ϩ by 3 ϭ 0
where usually ƒ b ƒ is small, thus characterizing a small
deviation of the restoring force from linearity. b Ͼ 0
and b Ͻ 0 are called the cases of a hard spring and a
soft spring, respectively. Find the equation of the
trajectories in the phase plane. (Note that for b Ͼ 0 all
these curves are closed.)

4.6

15. Trajectories. Write the ODE y s Ϫ 4y ϩ y 3 ϭ 0 as a
system, solve it for y2 as a function of y1, and sketch
or graph some of the trajectories in the phase plane.
y2

c=5

c=4

–2

c=3

2
y1

Fig. 98.

Trajectories in Problem 15

Nonhomogeneous Linear Systems of ODEs
In this section, the last one of Chap. 4, we discuss methods for solving nonhomogeneous
linear systems of ODEs
(1)

y؅ ϭ Ay ϩ g

(see Sec. 4.2)

where the vector g(t) is not identically zero. We assume g(t) and the entries of the
n ϫ n matrix A(t) to be continuous on some interval J of the t-axis. From a general
solution y (h)(t) of the homogeneous system y r ϭ Ay on J and a particular solution
y (p)(t) of (1) on J [i.e., a solution of (1) containing no arbitrary constants], we get a
solution of (1),
(2)

y ϭ y (h) ϩ y (p).

y is called a general solution of (1) on J because it includes every solution of (1) on J.
This follows from Theorem 2 in Sec. 4.2 (see Prob. 1 of this section).
Having studied homogeneous linear systems in Secs. 4.1–4.4, our present task will be
to explain methods for obtaining particular solutions of (1). We discuss the method of

5
LORD RAYLEIGH (JOHN WILLIAM STRUTT) (1842–1919), English physicist and mathematician,
professor at Cambridge and London, known by his important contributions to the theory of waves, elasticity
theory, hydrodynamics, and various other branches of applied mathematics and theoretical physics. In 1904 he
was awarded the Nobel Prize in physics.

c04.qxd

10/27/10

9:33 PM

Page 161

SEC. 4.6 Nonhomogeneous Linear Systems of ODEs

161

undetermined coefficients and the method of the variation of parameters; these have
counterparts for a single ODE, as we know from Secs. 2.7 and 2.10.

Method of Undetermined Coefficients
Just as for a single ODE, this method is suitable if the entries of A are constants and
the components of g are constants, positive integer powers of t, exponential functions,
or cosines and sines. In such a case a particular solution y (p) is assumed in a form similar
to g; for instance, y (p) ϭ u ϩ vt ϩ wt 2 if g has components quadratic in t, with u, v,
w to be determined by substitution into (1). This is similar to Sec. 2.7, except for the
Modification Rule. It suffices to show this by an example.
EXAMPLE 1

Method of Undetermined Coefficients. Modification Rule
Find a general solution of
y r ϭ Ay ϩ g ϭ

(3)

c

Ϫ3

1

1

Ϫ3

d yϩ c

Ϫ6
2

d e؊2t.

Solution. A general equation of the homogeneous system is (see Example 1 in Sec. 4.3)
y (h) ϭ c1 c

(4)

1
1

d e؊2t ϩ

c2 c

1
Ϫ1

d e؊4t.

Since l ϭ Ϫ2 is an eigenvalue of A, the function e؊2t on the right side also appears in y (h), and we must apply
the Modification Rule by setting
y (p) ϭ ute؊2t ϩ ve؊2t

(rather than ue؊2t).

Note that the first of these two terms is the analog of the modification in Sec. 2.7, but it would not be sufficient
here. (Try it.) By substitution,
y (p) r ϭ ue؊2t Ϫ 2ute؊2t Ϫ 2ve؊2t ϭ Aute؊2t ϩ Ave؊2t ϩ g.
Equating the te؊2t-terms on both sides, we have Ϫ2u ϭ Au. Hence u is an eigenvector of A corresponding to
l ϭ Ϫ2; thus [see (5)] u ϭ a[1 1]T with any a 0. Equating the other terms gives
u Ϫ 2v ϭ Av ϩ

c

Ϫ6
2

d

c d
a

thus

a

Ϫ

c

2v1
2v2

d

ϭ

c

Ϫ3v1 ϩ v2
v1 Ϫ 3v2

d

ϩ

c

Ϫ6
2

d.

Collecting terms and reshuffling gives
v1 Ϫ v2 ϭ Ϫa Ϫ 6
Ϫv1 ϩ v2 ϭ Ϫa ϩ 2.
By addition, 0 ϭ Ϫ2a Ϫ 4, a ϭ Ϫ2, and then v2 ϭ v1 ϩ 4, say, v1 ϭ k, v2 ϭ k ϩ 4, thus, v ϭ [k k ϩ 4]T.
We can simply choose k ϭ 0. This gives the answer
(5)

y ϭ y (h) ϩ y (p) ϭ c1

c d e؊2t ϩ
1
1

c2

c

1
Ϫ1

d e؊4t Ϫ 2 c d te؊2t ϩ c d e؊2t.
1

0

1

4

For other k we get other v; for instance, k ϭ Ϫ2 gives v ϭ [Ϫ2 2]T, so that the answer becomes
(5*)

y ϭ c1

c d e؊2t ϩ c2 c
1
1

1
Ϫ1

d e؊4t Ϫ

2

c d te؊2t ϩ c
1

Ϫ2

1

2

d e؊2t,

etc.

᭿

c04.qxd

10/27/10

9:33 PM

162

Page 162

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods

Method of Variation of Parameters
This method can be applied to nonhomogeneous linear systems
y r ϭ A(t)y ϩ g(t)

(6)

with variable A ϭ A(t) and general g(t). It yields a particular solution y (p) of (6) on some
open interval J on the t-axis if a general solution of the homogeneous system y r ϭ A(t)y
on J is known. We explain the method in terms of the previous example.
EXAMPLE 2

Solution by the Method of Variation of Parameters
Solve (3) in Example 1.

Solution. A basis of solutions of the homogeneous system is [e؊2t e؊2t]T and [e؊4t Ϫe؊4t]T. Hence
the general solution (4) of the homogeneous system may be written
y (h) ϭ

(7)

c

e؊2t

e؊4t

e؊2t

Ϫe

dc d
؊4t
c1

ϭ Y(t) c.

c2

Here, Y(t) ϭ [ y (1) y (2)]T is the fundamental matrix (see Sec. 4.2). As in Sec. 2.10 we replace the constant
vector c by a variable vector u(t) to obtain a particular solution
y (p) ϭ Y(t)u(t).
Substitution into (3) y r ϭ Ay ϩ g gives
Y r u ϩ Yu r ϭ AYu ϩ g.

(8)

Now since y (1) and y (2) are solutions of the homogeneous system, we have
y (1) r ϭ Ay (1),

y (2) r ϭ Ay (2),

Y r ϭ AY.

thus

Hence Y r u ϭ AYu, so that (8) reduces to
Yu r ϭ g.

u r ϭ Y؊1g;

The solution is

here we use that the inverse Y؊1 of Y (Sec. 4.0) exists because the determinant of Y is the Wronskian W, which
is not zero for a basis. Equation (9) in Sec. 4.0 gives the form of Y؊1,

c

1

Y؊1 ϭ

Ϫe؊4t

Ϫ2e؊6t Ϫe؊2t

Ϫe؊4t
e

d
؊2t

ϭ

c

2t
1 e

e2t

2 e4t

Ϫe4t

d.

We multiply this by g, obtaining
u r ϭ Y؊1g ϭ

c

2t
1 e

2 e4t

dc
4t

e2t
Ϫe

Ϫ6e؊2t
2e

d
؊2t

ϭ

1

c

d
2t

Ϫ4

2 Ϫ8e

ϭ

c

Ϫ2
Ϫ4e2t

d.

Integration is done componentwise (just as differentiation) and gives

Ύ c Ϫ4e
t

u(t) ϭ

0

Ϫ2
~
2t

d d ~t ϭ c

Ϫ2t
Ϫ2e2t ϩ 2

d

(where ϩ 2 comes from the lower limit of integration). From this and Y in (7) we obtain
Yu ϭ

c

e؊2t
e؊2t

e؊4t
Ϫe

dc
؊4t

Ϫ2t
Ϫ2e2t ϩ 2

d

ϭ

c

Ϫ2te؊2t Ϫ 2e؊2t ϩ 2e؊4t
Ϫ2te؊2t ϩ 2e؊2t Ϫ 2e

d
؊4t

ϭ

c

Ϫ2t Ϫ 2
Ϫ2t ϩ 2

d e؊2t ϩ c

2
Ϫ2

d e؊4t.

c04.qxd

10/27/10

9:33 PM

Page 163

SEC. 4.6 Nonhomogeneous Linear Systems of ODEs

163

The last term on the right is a solution of the homogeneous system. Hence we can absorb it into y (h). We thus
obtain as a general solution of the system (3), in agreement with (5*).
(9)

y ϭ c1

c d e؊2t ϩ c2 c
1

1

1

Ϫ1

d e؊4t Ϫ 2 c d te؊2t ϩ c
1

Ϫ2

1

2

d e؊2t.

᭿

PROBLEM SET 4.6
1. Prove that (2) includes every solution of (1).
2–7

GENERAL SOLUTION

Find a general solution. Show the details of your work.
2. y1r ϭ y1 ϩ y2 ϩ 10 cos t
y2r ϭ 3y1 Ϫ y2 Ϫ 10 sin t
3. y1r ϭ y2 ϩ e3t
y2r ϭ y1 Ϫ 3e3t
4. y1r ϭ 4y1 Ϫ 8y2 ϩ 2 cosh t
y2r ϭ 2y1 Ϫ 6y2 ϩ cosh t ϩ 2 sinh t
5. y1r ϭ 4y1 ϩ y2 ϩ 0.6t
y2r ϭ 2y1 ϩ 3y2 Ϫ 2.5t
6. y1r ϭ 4y2
y2r ϭ 4y1 Ϫ 16t 2 ϩ 2
7. y 1r ϭ Ϫ3y1 Ϫ 4y2 ϩ 11t ϩ 15
y2r ϭ 5y1 ϩ 6y2 ϩ 3e؊t Ϫ 15t Ϫ 20
8. CAS EXPERIMENT. Undetermined Coefficients.
Find out experimentally how general you must choose
y (p), in particular when the components of g have
a different form (e.g., as in Prob. 7). Write a short
report, covering also the situation in the case of the
modification rule.
9. Undetermined Coefficients. Explain why, in Example
1 of the text, we have some freedom in choosing the
vector v.
10–15

15. y 1r ϭ y1 ϩ 2y2 ϩ e2t Ϫ 2t
y 2r ϭ Ϫy2 ϩ 1 ϩ t
y1(0) ϭ 1, y2(0) ϭ Ϫ4
16. WRITING PROJECT. Undetermined Coefficients.
Write a short report in which you compare the
application of the method of undetermined coefficients
to a single ODE and to a system of ODEs, using ODEs
and systems of your choice.
17–20

NETWORK

Find the currents in Fig. 99 (Probs. 17–19) and Fig. 100
(Prob. 20) for the following data, showing the details of
your work.
17. R1 ϭ 2 ⍀, R2 ϭ 8 ⍀, L ϭ 1 H, C ϭ 0.5 F, E ϭ 200 V
18. Solve Prob. 17 with E ϭ 440 sin t V and the other data
as before.
19. In Prob. 17 find the particular solution when currents
and charge at t ϭ 0 are zero.
L
I1

I2
R1

E

R2

INITIAL VALUE PROBLEM

Solve, showing details:
10. y1r ϭ Ϫ3y1 Ϫ 4y2 ϩ 5et
y2r ϭ 5y1 ϩ 6y2 Ϫ 6et
y1(0) ϭ 19, y2(0) ϭ Ϫ23
11. y1r ϭ y2 ϩ 6e2t
y2r ϭ y1 Ϫ e2t
y1(0) ϭ 1, y2(0) ϭ 0
12. y1r ϭ y1 ϩ 4y2 Ϫ t 2 ϩ 6t
y2r ϭ y1 ϩ y2 Ϫ t 2 ϩ t Ϫ 1
y1(0) ϭ 2, y2(0) ϭ Ϫ1
13. y1r ϭ y2 Ϫ 5 sin t
y2r ϭ Ϫ4y1 ϩ 17 cos t
y1(0) ϭ 5, y2(0) ϭ 2
14. y 1r ϭ 4y2 ϩ 5et
y 2r ϭ Ϫy1 Ϫ 20e؊t
y1(0) ϭ 1, y2(0) ϭ 0

Switch

Fig. 99.

C

Problems 17–19

20. R1 ϭ 1 ⍀, R2 ϭ 1.4 ⍀, L 1 ϭ 0.8 H, L 2 ϭ 1 H,
E ϭ 100 V, I1(0) ϭ I2(0) ϭ 0
L1

L2
I1

I2
R1

E

R2

Fig. 100. Problem 20

c04.qxd

10/27/10

9:33 PM

Page 164

164

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods

CHAPTER 4 REVIEW QUESTIONS AND PROBLEMS
1. State some applications that can be modeled by systems
of ODEs.
2. What is population dynamics? Give examples.
3. How can you transform an ODE into a system of ODEs?
4. What are qualitative methods for systems? Why are they
important?
5. What is the phase plane? The phase plane method? A
trajectory? The phase portrait of a system of ODEs?
6. What are critical points of a system of ODEs? How did
we classify them? Why are they important?
7. What are eigenvalues? What role did they play in this
chapter?
8. What does stability mean in general? In connection with
critical points? Why is stability important in engineering?
9. What does linearization of a system mean?
10. Review the pendulum equations and their linearizations.
11–17

GENERAL SOLUTION. CRITICAL POINTS

24. Mixing problem. Tank T1 in Fig. 101 initially contains
200 gal of water in which 160 lb of salt are dissolved.
Tank T2 initially contains 100 gal of pure water. Liquid
is pumped through the system as indicated, and the
mixtures are kept uniform by stirring. Find the amounts
of salt y1(t) and y2(t) in T1 and T2, respectively.
Water,
10 gal/min

6 gal/min

T1

Mixture,
10 gal/min

T2

16 gal/min

Fig. 101. Tanks in Problem 24
25. Network. Find the currents in Fig. 102 when
R ϭ 2.5 ⍀, L ϭ 1 H, C ϭ 0.04 F, E(t) ϭ 169 sin t V,
I1(0) ϭ 0, I2(0) ϭ 0.

Find a general solution. Determine the kind and stability of
the critical point.
11. y1r ϭ 2y2
y2r ϭ 8y1

12. y1r ϭ 5y1
y2r ϭ y2

13. y1r ϭ Ϫ2y1 ϩ 5y2
y2r ϭ Ϫy1 Ϫ 6y2

14. y1r ϭ 3y1 ϩ 4y2
y2r ϭ 3y1 ϩ 2y2

15. y1r ϭ Ϫ3y1 Ϫ 2y2
y2r ϭ Ϫ2y1 Ϫ 3y2

16. y1r ϭ 4y2
y2r ϭ Ϫ4y1

17. y1r ϭ Ϫy1 ϩ 2y2
y2r ϭ Ϫ2y1 Ϫ y2
18–19

I1

C

L

Fig. 102. Network in Problem 25
26. Network. Find the currents in Fig. 103 when R ϭ 1 ⍀,
L ϭ 1.25 H, C ϭ 0.2 F, I1(0) ϭ 1 A, I2(0) ϭ 1 A.

CRITICAL POINT

What kind of critical point does y r ϭ Ay have if A has the
eigenvalues
18. Ϫ4 and 2
20–23

I2
R

E

I1
C

NONHOMOGENEOUS SYSTEMS

21. y1r ϭ 4y2
y2r ϭ 4y1 ϩ 32t 2
22. y1r ϭ y1 ϩ y2 ϩ sin t
y2r ϭ 4y1 ϩ y2
23. y1r ϭ y1 ϩ 4y2 Ϫ 2 cos t
y2r ϭ y1 ϩ y2 Ϫ cos t ϩ sin t

L

19. 2 ϩ 3i, 2 Ϫ 3i
Fig. 103. Network in Problem 26

Find a general solution. Show the details of your work.
20. y1r ϭ 2y1 ϩ 2y2 ϩ et
y2r ϭ Ϫ2y1 Ϫ 3y2 ϩ et

I2
R

27–30

LINEARIZATION

Find the location and kind of all critical points of the given
nonlinear system by linearization.
27. y1r ϭ y2
28. y1r ϭ cos y2
y2r ϭ y1 Ϫ y 31
y2r ϭ 3y1
29. y1r ϭ Ϫ4y2
30. y1r ϭ 2y2 ϩ 2y 22
y2r ϭ Ϫ8y1
y2r ϭ sin y1

c04.qxd

10/27/10

9:33 PM

Page 165

Summary of Chapter 4

165

SUMMARY OF CHAPTER

4

Systems of ODEs. Phase Plane. Qualitative Methods
Whereas single electric circuits or single mass–spring systems are modeled by
single ODEs (Chap. 2), networks of several circuits, systems of several masses
and springs, and other engineering problems lead to systems of ODEs, involving
several unknown functions y1(t), Á , yn(t). Of central interest are first-order
systems (Sec. 4.2):

y r ϭ f(t, y),

y 1r ϭ f1(t, y1, Á , yn)
.
.
.
y nr ϭ fn(t, y1, Á , yn),

in components,

to which higher order ODEs and systems of ODEs can be reduced (Sec. 4.1). In
this summary we let n ϭ 2, so that
y r ϭ f(t, y),

(1)

y r1 ϭ f1(t, y1, y2)

in components,

y 2r ϭ f2(t, y1, y2).

Then we can represent solution curves as trajectories in the phase plane (the
y1y2-plane), investigate their totality [the “phase portrait” of (1)], and study the kind
and stability of the critical points (points at which both f1 and f2 are zero), and
classify them as nodes, saddle points, centers, or spiral points (Secs. 4.3, 4.4). These
phase plane methods are qualitative; with their use we can discover various general
properties of solutions without actually solving the system. They are primarily used
for autonomous systems, that is, systems in which t does not occur explicitly.
A linear system is of the form
(2)

y r ϭ Ay ϩ g, where

Aϭ

c

a11

a12

a21

a22

d,

yϭ

c d,
y1
y2

gϭ

c d.
g1
g2

If g ϭ 0, the system is called homogeneous and is of the form
y r ϭ Ay.

(3)

If a11, Á , a22 are constants, it has solutions y ϭ xelt, where l is a solution of the
quadratic equation

2

a11 Ϫ l

a12

a21

a22 Ϫ l

2 ϭ (a11 Ϫ l)(a22 Ϫ l) Ϫ a12a21 ϭ 0

c04.qxd

10/27/10

166

9:33 PM

Page 166

CHAP. 4 Systems of ODEs. Phase Plane. Qualitative Methods

and x

0 has components x 1, x 2 determined up to a multiplicative constant by
(a11 Ϫ l)x 1 ϩ a12 x 2 ϭ 0.

(These l’s are called the eigenvalues and these vectors x eigenvectors of the
matrix A. Further explanation is given in Sec. 4.0.)
A system (2) with g 0 is called nonhomogeneous. Its general solution is of
the form y ϭ yh ϩ yp, where yh is a general solution of (3) and yp a particular
solution of (2). Methods of determining the latter are discussed in Sec. 4.6.
The discussion of critical points of linear systems based on eigenvalues is
summarized in Tables 4.1 and 4.2 in Sec. 4.4. It also applies to nonlinear systems
if the latter are first linearized. The key theorem for this is Theorem 1 in Sec. 4.5,
which also includes three famous applications, namely the pendulum and van der
Pol equations and the Lotka–Volterra predator–prey population model.

c05.qxd

11/9/10

7:27 PM

Page 167

CHAPTER

5

Series Solutions of ODEs.
Special Functions
In the previous chapters, we have seen that linear ODEs with constant coefficients can be
solved by algebraic methods, and that their solutions are elementary functions known from
calculus. For ODEs with variable coefficients the situation is more complicated, and their
solutions may be nonelementary functions. Legendre’s, Bessel’s, and the hypergeometric
equations are important ODEs of this kind. Since these ODEs and their solutions, the
Legendre polynomials, Bessel functions, and hypergeometric functions, play an important
role in engineering modeling, we shall consider the two standard methods for solving
such ODEs.
The first method is called the power series method because it gives solutions in the
form of a power series a0 ϩ a1x ϩ a2 x 2 ϩ a3 x 3 ϩ Á .
The second method is called the Frobenius method and generalizes the first; it gives
solutions in power series, multiplied by a logarithmic term ln x or a fractional power x r,
in cases such as Bessel’s equation, in which the first method is not general enough.
All those more advanced solutions and various other functions not appearing in calculus
are known as higher functions or special functions, which has become a technical term.
Each of these functions is important enough to give it a name and investigate its properties
and relations to other functions in great detail (take a look into Refs. [GenRef1],
[GenRef10], or [All] in App. 1). Your CAS knows practically all functions you will ever
need in industry or research labs, but it is up to you to find your way through this vast
terrain of formulas. The present chapter may give you some help in this task.
COMMENT. You can study this chapter directly after Chap. 2 because it needs no
material from Chaps. 3 or 4.
Prerequisite: Chap. 2.
Section that may be omitted in a shorter course: 5.5.
References and Answers to Problems: App. 1 Part A, and App. 2.

5.1

Power Series Method
The power series method is the standard method for solving linear ODEs with variable
coefficients. It gives solutions in the form of power series. These series can be used
for computing values, graphing curves, proving formulas, and exploring properties of
solutions, as we shall see. In this section we begin by explaining the idea of the power
series method.
167

c05.qxd

10/28/10

3:43 PM

168

Page 168

CHAP. 5 Series Solutions of ODEs. Special Functions

From calculus we remember that a power series (in powers of x Ϫ x 0) is an infinite
series of the form
ؕ

(1)

m
2
Á.
a am(x Ϫ x 0) ϭ a0 ϩ a1(x Ϫ x 0) ϩ a2(x Ϫ x 0) ϩ
mϭ0

Here, x is a variable. a0, a1, a2, Á are constants, called the coefficients of the series.
x 0 is a constant, called the center of the series. In particular, if x 0 ϭ 0, we obtain a power
series in powers of x
ؕ

m
2
3
Á.
a am x ϭ a0 ϩ a1x ϩ a2 x ϩ a3 x ϩ

(2)

mϭ0

We shall assume that all variables and constants are real.
We note that the term “power series” usually refers to a series of the form (1) [or (2)]
but does not include series of negative or fractional powers of x. We use m as the
summation letter, reserving n as a standard notation in the Legendre and Bessel equations
for integer values of the parameter.
EXAMPLE 1

Familiar Power Series are the Maclaurin series
1

ؕ

ϭ a xm ϭ 1 ϩ x ϩ x2 ϩ Á
1 Ϫ x mϭ0

( ƒ x ƒ Ͻ 1, geometric series)

ؕ
xm
x2
x3 Á
ex ϭ a
ϭ1ϩxϩ
ϩ
ϩ
m!
2!
3!
mϭ0
ؕ

cos x ϭ a
mϭ0
ؕ

sin x ϭ a
mϭ0

(Ϫ1)mx 2m
(2m)!

ϭ1Ϫ

(Ϫ1)mx 2mϩ1
(2m ϩ 1)!

x2
2!

ϭxϪ

ϩ
x3
3!

x4
4!
ϩ

Ϫ ϩÁ
x5
5!

Ϫ ϩÁ.

᭿

Idea and Technique of the Power Series Method
The idea of the power series method for solving linear ODEs seems natural, once we
know that the most important ODEs in applied mathematics have solutions of this form.
We explain the idea by an ODE that can readily be solved otherwise.
EXAMPLE 2

Power Series Solution. Solve y r Ϫ y ϭ 0.

Solution. In the first step we insert
ؕ

(2)

y ϭ a0 ϩ a1x ϩ a2 x 2 ϩ a3 x 3 ϩ Á ϭ a am x m
mϭ0

c05.qxd

10/28/10

3:43 PM

Page 169

SEC. 5.1 Power Series Method

169

and the series obtained by termwise differentiation
ؕ

y r ϭ a1 ϩ 2a2 x ϩ 3a3 x 2 ϩ Á ϭ a mam x m؊1

(3)

mϭ1

into the ODE:
(a1 ϩ 2a2 x ϩ 3a3 x 2 ϩ Á ) Ϫ (a0 ϩ a1x ϩ a2 x 2 ϩ Á ) ϭ 0.
Then we collect like powers of x, finding
(a1 Ϫ a0) ϩ (2a2 Ϫ a1)x ϩ (3a3 Ϫ a2)x 2 ϩ Á ϭ 0.
Equating the coefficient of each power of x to zero, we have
a1 Ϫ a0 ϭ 0,

2a2 Ϫ a1 ϭ 0,

3a3 Ϫ a2 ϭ 0, Á .

Solving these equations, we may express a1, a2, Á in terms of a0, which remains arbitrary:
a1 ϭ a0,

a2 ϭ

a1
2

ϭ

a0
2!

,

a3 ϭ

a2
3

ϭ

a0
3!

,Á.

With these values of the coefficients, the series solution becomes the familiar general solution
y ϭ a0 ϩ a0 x ϩ

a0
2!

x2 ϩ

x3
x2
ϩ b ϭ a0ex.
x 3 ϩ Á ϭ a0 a1 ϩ x ϩ
3!
2!
3!

a0

Test your comprehension by solving y s ϩ y ϭ 0 by power series. You should get the result
y ϭ a0 cos x ϩ a1 sin x.
᭿

We now describe the method in general and justify it after the next example. For a given
ODE
(4)

y s ϩ p(x)y r ϩ q(x)y ϭ 0

we first represent p(x) and q(x) by power series in powers of x (or of x Ϫ x 0 if solutions
in powers of x Ϫ x 0 are wanted). Often p(x) and q(x) are polynomials, and then nothing
needs to be done in this first step. Next we assume a solution in the form of a power series
(2) with unknown coefficients and insert it as well as (3) and
ؕ

(5)

y s ϭ 2a2 ϩ 3 # 2a3 x ϩ 4 # 3a4 x 2 ϩ Á ϭ a m(m Ϫ 1)am x m؊2
mϭ2

into the ODE. Then we collect like powers of x and equate the sum of the coefficients of
each occurring power of x to zero, starting with the constant terms, then taking the terms
containing x, then the terms in x 2, and so on. This gives equations from which we can
determine the unknown coefficients of (3) successively.
EXAMPLE 3

A Special Legendre Equation. The ODE
(1 Ϫ x 2)y s Ϫ 2xy r ϩ 2y ϭ 0
occurs in models exhibiting spherical symmetry. Solve it.

c05.qxd

10/28/10

170

1:33 PM

Page 170

CHAP. 5 Series Solutions of ODEs. Special Functions

Solution. Substitute (2), (3), and (5) into the ODE. (1 Ϫ x 2)y s gives two series, one for y s and one for
Ϫx 2y s . In the term Ϫ2xy r use (3) and in 2y use (2). Write like powers of x vertically aligned. This gives
y s ϭ 2a2 ϩ 6a3 x ϩ 12a4 x 2 ϩ 20a5 x 3 ϩ 30a6 x 4 ϩ Á
Ϫx 2y s ϭ

Ϫ 2a2 x 2 Ϫ 6a3 x 3 Ϫ 12a4 x 4 Ϫ Á

Ϫ2xy r ϭ

Ϫ 2a1x Ϫ 4a2 x 2 Ϫ 6a3 x 3 Ϫ 8a4 x 4 Ϫ Á

2y ϭ 2a0 ϩ 2a1x ϩ 2a2x 2 ϩ 2a3 x 3 ϩ 2a4 x 4 ϩ Á .
Add terms of like powers of x. For each power x 0, x, x 2, Á equate the sum obtained to zero. Denote these sums
by [0] (constant terms), [1] (first power of x), and so on:
Sum

Power

[0]

[x 0]

Equations
a2 ϭ Ϫa0

[1]

[x]

a3 ϭ 0

[2]

[x 2]

12a4 ϭ 4a2,

[3]

[x 3]

a5 ϭ 0

[4]

[x 4]

30a6 ϭ 18a4,

4
a4 ϭ 12
a2 ϭ Ϫ13 a0

since

a3 ϭ 0

18
1
1
a6 ϭ 18
30 a4 ϭ 30 (Ϫ3 )a0 ϭ Ϫ5 a0.

This gives the solution
y ϭ a1x ϩ a0(1 Ϫ x 2 Ϫ 13 x 4 Ϫ 15 x 6 Ϫ Á ).
a0 and a1 remain arbitrary. Hence, this is a general solution that consists of two solutions: x and
1 Ϫ x 2 Ϫ 13 x 4 Ϫ 15 x 6 Ϫ Á . These two solutions are members of families of functions called Legendre polynomials
Pn(x) and Legendre functions Q n(x); here we have x ϭ P1(x) and 1 Ϫ x 2 Ϫ 13 x 4 Ϫ 15 x 6 Ϫ Á ϭ ϪQ 1(x). The
minus is by convention. The index 1 is called the order of these two functions and here the order is 1. More on
Legendre polynomials in the next section.
᭿

Theory of the Power Series Method
The nth partial sum of (1) is
(6)

sn(x) ϭ a0 ϩ a1(x Ϫ x 0) ϩ a2(x Ϫ x 0)2 ϩ Á ϩ an(x Ϫ x 0)n

where n ϭ 0, 1, Á . If we omit the terms of sn from (1), the remaining expression is
(7)

Rn(x) ϭ anϩ1(x Ϫ x 0)nϩ1 ϩ anϩ2(x Ϫ x 0)nϩ2 ϩ Á .

This expression is called the remainder of (1) after the term an(x Ϫ x 0)n.
For example, in the case of the geometric series
1 ϩ x ϩ x2 ϩ Á ϩ xn ϩ Á
we have
s0 ϭ 1,

R0 ϭ x ϩ x 2 ϩ x 3 ϩ Á ,

s1 ϭ 1 ϩ x,

R1 ϭ x 2 ϩ x 3 ϩ x 4 ϩ Á ,

s2 ϭ 1 ϩ x ϩ x 2,

R2 ϭ x 3 ϩ x 4 ϩ x 5 ϩ Á ,

etc.

c05.qxd

10/28/10

1:33 PM

Page 171

SEC. 5.1 Power Series Method

171

In this way we have now associated with (1) the sequence of the partial sums
s0(x), s1(x), s2(x), Á . If for some x ϭ x 1 this sequence converges, say,
lim sn(x 1) ϭ s(x 1),

n:ϱ

then the series (1) is called convergent at x ϭ x 1, the number s(x 1) is called the value
or sum of (1) at x 1, and we write
ؕ

s(x 1) ϭ a am(x 1 Ϫ x 0)m.
mϭ0

Then we have for every n,
(8)

s(x 1) ϭ sn(x 1) ϩ Rn(x 1).

If that sequence diverges at x ϭ x 1, the series (1) is called divergent at x ϭ x 1.
In the case of convergence, for any positive P there is an N (depending on P) such that,
by (8)
(9)

ƒ Rn(x 1) ƒ ϭ ƒ s(x 1) Ϫ sn(x 1) ƒ Ͻ P

for all n Ͼ N.

Geometrically, this means that all sn(x 1) with n Ͼ N lie between s(x 1) Ϫ P and s(x 1) ϩ P
(Fig. 104). Practically, this means that in the case of convergence we can approximate the
sum s(x 1) of (1) at x 1 by sn(x 1) as accurately as we please, by taking n large enough.
∈

ε

s(x1) – ∈

∈

ε

s(x1) + ∈

s(x1)

Fig. 104. Inequality (9)

Where does a power series converge? Now if we choose x ϭ x 0 in (1), the series reduces
to the single term a0 because the other terms are zero. Hence the series converges at x 0.
In some cases this may be the only value of x for which (1) converges. If there are other
values of x for which the series converges, these values form an interval, the convergence
interval. This interval may be finite, as in Fig. 105, with midpoint x 0. Then the series (1)
converges for all x in the interior of the interval, that is, for all x for which
ƒ x Ϫ x0 ƒ Ͻ R

(10)

and diverges for ƒ x Ϫ x 0 ƒ Ͼ R. The interval may also be infinite, that is, the series may
converge for all x.
Divergence

Convergence
R

x0 – R

Divergence
R

x0

x0 + R

Fig. 105. Convergence interval (10) of a power series with center x0

c05.qxd

10/28/10

1:33 PM

172

Page 172

CHAP. 5 Series Solutions of ODEs. Special Functions

The quantity R in Fig. 105 is called the radius of convergence (because for a complex
power series it is the radius of disk of convergence). If the series converges for all x, we
set R ϭ ϱ (and 1>R ϭ 0).
The radius of convergence can be determined from the coefficients of the series by
means of each of the formulas
(a) R ϭ 1^ lim 2 ƒ am ƒ

amϩ1
(b) R ϭ 1^ lim ` a
`
m:ϱ
m

m

(11)

m:ϱ

provided these limits exist and are not zero. [If these limits are infinite, then (1) converges
only at the center x 0.]
EXAMPLE 4

Convergence Radius R ‫ؕ ؍‬, 1, 0
For all three series let m : ϱ
ؕ
xm
x2 Á
ex ϭ a
ϭ1ϩxϩ
ϩ
,
m!
2!
mϭ0

1

ؕ

ϭ a xm ϭ 1 ϩ x ϩ x2 ϩ Á ,
1 Ϫ x mϭ0
ؕ

m
2
Á,
a m!x ϭ 1 ϩ x ϩ 2x ϩ
mϭ0

amϩ1

`

amϩ1

`

amϩ1
(m ϩ 1)!
` ϭ
ϭ m ϩ 1 : ϱ,
am
m!

am

am

` ϭ

1>(m ϩ 1)!

`

` ϭ

1>m!
1
1

ϭ

1
mϩ1

: 0,

ϭ 1,

Rϭϱ

Rϭ1

R ϭ 0.

Convergence for all x (R ϭ ϱ) is the best possible case, convergence in some finite interval the usual, and
᭿
convergence only at the center (R ϭ 0) is useless.

When do power series solutions exist? Answer: if p, q, r in the ODEs
(12)

y s ϩ p(x)y r ϩ q(x)y ϭ r(x)

have power series representations (Taylor series). More precisely, a function f (x) is called
analytic at a point x ϭ x 0 if it can be represented by a power series in powers of x Ϫ x 0
with positive radius of convergence. Using this concept, we can state the following basic
theorem, in which the ODE (12) is in standard form, that is, it begins with the y s. If
your ODE begins with, say, h(x)y s , divide it first by h(x) and then apply the theorem to
the resulting new ODE.
THEOREM 1

Existence of Power Series Solutions

If p, q, and r in (12) are analytic at x ϭ x 0, then every solution of (12) is analytic
at x ϭ x 0 and can thus be represented by a power series in powers of x Ϫ x 0 with
radius of convergence R Ͼ 0.
The proof of this theorem requires advanced complex analysis and can be found in Ref.
[A11] listed in App. 1.
We mention that the radius of convergence R in Theorem 1 is at least equal to the distance
from the point x ϭ x 0 to the point (or points) closest to x 0 at which one of the functions
p, q, r, as functions of a complex variable, is not analytic. (Note that that point may not
lie on the x-axis but somewhere in the complex plane.)

c05.qxd

10/28/10

1:33 PM

Page 173

SEC. 5.1 Power Series Method

173

Further Theory: Operations on Power Series
In the power series method we differentiate, add, and multiply power series, and we obtain
coefficient recursions (as, for instance, in Example 3) by equating the sum of the
coefficients of each occurring power of x to zero. These four operations are permissible
in the sense explained in what follows. Proofs can be found in Sec. 15.3.
1. Termwise Differentiation. A power series may be differentiated term by term. More
precisely: if
ؕ

y(x) ϭ a am(x Ϫ x 0)m
mϭ0

converges for ƒ x Ϫ x 0 ƒ Ͻ R, where R Ͼ 0, then the series obtained by differentiating term
by term also converges for those x and represents the derivative y r of y for those x:
ؕ

y r (x) ϭ a mam(x Ϫ x 0)m؊1

( ƒ x Ϫ x 0 ƒ Ͻ R).

mϭ1

Similarly for the second and further derivatives.
2. Termwise Addition. Two power series may be added term by term. More precisely:
if the series
ؕ

(13)

m
a am(x Ϫ x 0)
mϭ0

ؕ

and

m
a bm(x Ϫ x 0)
mϭ0

have positive radii of convergence and their sums are f (x) and g(x), then the series
ؕ

m
a (am ϩ bm)(x Ϫ x 0)
mϭ0

converges and represents f (x) ϩ g(x) for each x that lies in the interior of the convergence
interval common to each of the two given series.
3. Termwise Multiplication. Two power series may be multiplied term by term. More
precisely: Suppose that the series (13) have positive radii of convergence and let f (x) and
g(x) be their sums. Then the series obtained by multiplying each term of the first series
by each term of the second series and collecting like powers of x Ϫ x 0, that is,
a0b0 ϩ (a0b1 ϩ a1b0)(x Ϫ x 0) ϩ (a0b2 ϩ a1b1 ϩ a2b0)(x Ϫ x 0)2 ϩ Á
ؕ

ϭ a (a0bm ϩ a1bm؊1 ϩ Á ϩ amb0)(x Ϫ x 0)m
mϭ0

converges and represents f (x)g(x) for each x in the interior of the convergence interval of
each of the two given series.

c05.qxd

10/28/10

1:33 PM

Page 174

174

CHAP. 5 Series Solutions of ODEs. Special Functions

4. Vanishing of All Coefficients (“Identity Theorem for Power Series.”) If a power
series has a positive radius of convergent convergence and a sum that is identically zero
throughout its interval of convergence, then each coefficient of the series must be zero.

PROBLEM SET 5.1
1. WRITING AND LITERATURE PROJECT. Power
Series in Calculus. (a) Write a review (2–3 pages) on
power series in calculus. Use your own formulations and
examples—do not just copy from textbooks. No proofs.
(b) Collect and arrange Maclaurin series in a systematic
list that you can use for your work.

15. Shifting summation indices is often convenient or
necessary in the power series method. Shift the index
so that the power under the summation sign is x m.
Check by writing the first few terms explicity.
ؕ

a
sϭ2

REVIEW: RADIUS OF CONVERGENCE

2–5

Determine the radius of convergence. Show the details of
your work.
ؕ

2. a (m ϩ 1)mx m
mϭ0

(Ϫ1)m

ؕ

3. a
mϭ0

k

m

x 2m

ؕ
x 2mϩ1
4. a
(2m ϩ 1)!
mϭ0
m

ؕ
2
5. a a b x 2m
3
mϭ0

6–9

SERIES SOLUTIONS BY HAND

Apply the power series method. Do this by hand, not by a
CAS, to get a feel for the method, e.g., why a series may
terminate, or has even powers only, etc. Show the details.
6. (1 ϩ x)y r ϭ y

16–19

s(s ϩ 1)
s2 ϩ 1

p2

ؕ

x sϪ1,

a

pϭ1 ( p ϩ 1)!

x pϩ4

CAS PROBLEMS. IVPs

Solve the initial value problem by a power series. Graph
the partial sums of the powers up to and including x 5. Find
the value of the sum s (5 digits) at x 1.
16. y r ϩ 4y ϭ 1, y(0) ϭ 1.25, x 1 ϭ 0.2
17. y s ϩ 3xy r ϩ 2y ϭ 0, y(0) ϭ 1,
x ϭ 0.5

y r (0) ϭ 1,

18. (1 Ϫ x 2)y s Ϫ 2xy r ϩ 30y ϭ 0, y(0) ϭ 0,
y r (0) ϭ 1.875, x 1 ϭ 0.5
19. (x Ϫ 2)y r ϭ xy, y(0) ϭ 4, x 1 ϭ 2
20. CAS Experiment. Information from Graphs of
Partial Sums. In numerics we use partial sums of
power series. To get a feel for the accuracy for various
x, experiment with sin x. Graph partial sums of the
Maclaurin series of an increasing number of terms,
describing qualitatively the “breakaway points” of
these graphs from the graph of sin x. Consider other
Maclaurin series of your choice.

7. y r ϭ Ϫ2xy
8. xy r Ϫ 3y ϭ k (ϭ const)
9. y s ϩ y ϭ 0
10–14

SERIES SOLUTIONS

Find a power series solution in powers of x. Show the details.
10. y s Ϫ y r ϩ xy ϭ 0
11. y s Ϫ y r ϩ x 2y ϭ 0
12. (1 Ϫ x 2)y s Ϫ 2xy r ϩ 2y ϭ 0
13. y s ϩ (1 ϩ x 2)y ϭ 0
14. y s Ϫ 4xy r ϩ (4x 2 Ϫ 2)y ϭ 0

1.5
1
0.5
0

1

2

3

4

5

–0.5
–1
–1.5

Fig. 106. CAS Experiment 20. sin x and partial
sums s3, s5, s7

6

x

c05.qxd

10/28/10

1:33 PM

Page 175

SEC. 5.2 Legendre’s Equation. Legendre Polynomials Pn(x)

5.2

175

Legendre’s Equation.
Legendre Polynomials Pn(x)
Legendre’s differential equation1
(1 Ϫ x 2)y s Ϫ 2xy r ϩ n(n ϩ 1)y ϭ 0

(1)

(n constant)

is one of the most important ODEs in physics. It arises in numerous problems, particularly
in boundary value problems for spheres (take a quick look at Example 1 in Sec. 12.10).
The equation involves a parameter n, whose value depends on the physical or
engineering problem. So (1) is actually a whole family of ODEs. For n ϭ 1 we solved it
in Example 3 of Sec. 5.1 (look back at it). Any solution of (1) is called a Legendre function.
The study of these and other “higher” functions not occurring in calculus is called the
theory of special functions. Further special functions will occur in the next sections.
Dividing (1) by 1 Ϫ x 2, we obtain the standard form needed in Theorem 1 of Sec. 5.1
and we see that the coefficients Ϫ2x>(1 Ϫ x 2) and n(n ϩ 1)>(1 Ϫ x 2) of the new equation
are analytic at x ϭ 0, so that we may apply the power series method. Substituting
ؕ

y ϭ a am x m

(2)

mϭ0

and its derivatives into (1), and denoting the constant n(n ϩ 1) simply by k, we obtain
ؕ

ؕ

ؕ

mϭ2

mϭ1

mϭ0

(1 Ϫ x 2) a m(m Ϫ 1)am x m؊2 Ϫ 2x a mam x m؊1 ϩ k a am x m ϭ 0.
By writing the first expression as two separate series we have the equation
ؕ

ؕ

ؕ

ؕ

mϭ2

mϭ2

mϭ1

mϭ0

m؊2
Ϫ a m(m Ϫ 1)am x m Ϫ a 2mam x m ϩ a kam x m ϭ 0.
a m(m Ϫ 1)am x

It may help you to write out the first few terms of each series explicitly, as in Example 3
of Sec. 5.1; or you may continue as follows. To obtain the same general power x s in all
four series, set m Ϫ 2 ϭ s (thus m ϭ s ϩ 2) in the first series and simply write s instead
of m in the other three series. This gives
ؕ

ؕ

ؕ

ؕ

sϭ0

sϭ2

sϭ1

sϭ0

s
s
s
s
a (s ϩ 2)(s ϩ 1)asϩ2 x Ϫ a s(s Ϫ 1)as x Ϫ a 2sas x ϩ a kas x ϭ 0.

1
ADRIEN-MARIE LEGENDRE (1752–1833), French mathematician, who became a professor in Paris in
1775 and made important contributions to special functions, elliptic integrals, number theory, and the calculus
of variations. His book Éléments de géométrie (1794) became very famous and had 12 editions in less than
30 years.
Formulas on Legendre functions may be found in Refs. [GenRef1] and [GenRef10].

c05.qxd

10/28/10

176

1:33 PM

Page 176

CHAP. 5 Series Solutions of ODEs. Special Functions

(Note that in the first series the summation begins with s ϭ 0.) Since this equation with
the right side 0 must be an identity in x if (2) is to be a solution of (1), the sum of the
coefficients of each power of x on the left must be zero. Now x 0 occurs in the first and
fourth series only, and gives [remember that k ϭ n(n ϩ 1)]
2 # 1a2 ϩ n(n ϩ 1)a0 ϭ 0.

(3a)

x 1 occurs in the first, third, and fourth series and gives
3 # 2a3 ϩ [Ϫ2 ϩ n(n ϩ 1)]a1 ϭ 0.

(3b)

The higher powers x 2, x 3, Á occur in all four series and give
(s ϩ 2)(s ϩ 1)asϩ2 ϩ [Ϫs(s Ϫ 1) Ϫ 2s ϩ n(n ϩ 1)]as ϭ 0.

(3c)

The expression in the brackets [ Á ] can be written (n Ϫ s)(n ϩ s ϩ 1), as you may
readily verify. Solving (3a) for a2 and (3b) for a3 as well as (3c) for asϩ2, we obtain the
general formula

asϩ2 ϭ Ϫ

(4)

(n Ϫ s)(n ϩ s ϩ 1)
(s ϩ 2)(s ϩ 1)

(s ϭ 0, 1, Á ).

as

This is called a recurrence relation or recursion formula. (Its derivation you may verify
with your CAS.) It gives each coefficient in terms of the second one preceding it, except
for a0 and a1, which are left as arbitrary constants. We find successively
a2 ϭ Ϫ
a4 ϭ Ϫ
ϭ

n(n ϩ 1)
2!

a3 ϭ Ϫ

a0

(n Ϫ 2)(n ϩ 3)
4 #3

a5 ϭ Ϫ

a2

(n Ϫ 2)n(n ϩ 1)(n ϩ 3)

ϭ

a0

4!

(n Ϫ 1)(n ϩ 2)
3!
(n Ϫ 3)(n ϩ 4)
5 #4

a1

a3

(n Ϫ 3)(n Ϫ 1)(n ϩ 2)(n ϩ 4)
5!

a1

and so on. By inserting these expressions for the coefficients into (2) we obtain
y(x) ϭ a0y1(x) ϩ a1y2(x)

(5)
where

(6)

(7)

y1(x) ϭ 1 Ϫ

y2(x) ϭ x Ϫ

n(n ϩ 1)
2!

x2 ϩ

(n Ϫ 1)(n ϩ 2)
3!

(n Ϫ 2)n(n ϩ 1)(n ϩ 3)

x3 ϩ

4!

x4 Ϫ ϩ Á

(n Ϫ 3)(n Ϫ 1)(n ϩ 2)(n ϩ 4)
5!

x5 Ϫ ϩ Á .

c05.qxd

10/28/10

1:33 PM

Page 177

SEC. 5.2 Legendre’s Equation. Legendre Polynomials Pn(x)

177

These series converge for ƒ x ƒ Ͻ 1 (see Prob. 4; or they may terminate, see below). Since
(6) contains even powers of x only, while (7) contains odd powers of x only, the ratio
y1>y2 is not a constant, so that y1 and y2 are not proportional and are thus linearly
independent solutions. Hence (5) is a general solution of (1) on the interval Ϫ1 Ͻ x Ͻ 1.
Note that x ϭ Ϯ1 are the points at which 1 Ϫ x 2 ϭ 0, so that the coefficients of the
standardized ODE are no longer analytic. So it should not surprise you that we do not get
a longer convergence interval of (6) and (7), unless these series terminate after finitely
many powers. In that case, the series become polynomials.

Polynomial Solutions. Legendre Polynomials Pn(x)
The reduction of power series to polynomials is a great advantage because then we have
solutions for all x, without convergence restrictions. For special functions arising as
solutions of ODEs this happens quite frequently, leading to various important families of
polynomials; see Refs. [GenRef1], [GenRef10] in App. 1. For Legendre’s equation this
happens when the parameter n is a nonnegative integer because then the right side of (4)
is zero for s ϭ n, so that anϩ2 ϭ 0, anϩ4 ϭ 0, anϩ6 ϭ 0, Á . Hence if n is even, y1(x)
reduces to a polynomial of degree n. If n is odd, the same is true for y2(x). These
polynomials, multiplied by some constants, are called Legendre polynomials and are
denoted by Pn(x). The standard choice of such constants is done as follows. We choose
the coefficient an of the highest power x n as
(8)

an ϭ

(2n)!
2n(n!)2

ϭ

1 # 3 # 5 Á (2n Ϫ 1)

(n a positive integer)

n!

(and an ϭ 1 if n ϭ 0). Then we calculate the other coefficients from (4), solved for as in
terms of asϩ2, that is,
(9)

as ϭ Ϫ

(s ϩ 2)(s ϩ 1)
(n Ϫ s)(n ϩ s ϩ 1)

(s Ϲ n Ϫ 2).

asϩ2

The choice (8) makes pn(1) ϭ 1 for every n (see Fig. 107); this motivates (8). From (9)
with s ϭ n Ϫ 2 and (8) we obtain
an؊2 ϭ Ϫ

n(n Ϫ 1)
2(2n Ϫ 1)

an ϭ Ϫ

n(n Ϫ 1)

#

2(2n Ϫ 1)

(2n)!
2n(n!)2

Using (2n)! ϭ 2n(2n Ϫ 1)(2n Ϫ 2)! in the numerator and n! ϭ n(n Ϫ 1)! and
n! ϭ n(n Ϫ 1)(n Ϫ 2)! in the denominator, we obtain
an؊2 ϭ Ϫ

n(n Ϫ 1)2n(2n Ϫ 1)(2n Ϫ 2)!
2(2n Ϫ 1)2nn(n Ϫ 1)! n(n Ϫ 1)(n Ϫ 2)!

n(n Ϫ 1)2n(2n Ϫ 1) cancels, so that we get
an؊2 ϭ Ϫ

(2n Ϫ 2)!
2 (n Ϫ 1)! (n Ϫ 2)!
n

.

.

c05.qxd

10/28/10

178

1:33 PM

Page 178

CHAP. 5 Series Solutions of ODEs. Special Functions

Similarly,
an؊4 ϭ Ϫ
ϭ

(n Ϫ 2)(n Ϫ 3)
4(2n Ϫ 3)

an؊2

(2n Ϫ 4)!
2 2! (n Ϫ 2)! (n Ϫ 4)!
n

and so on, and in general, when n Ϫ 2m м 0,
(2n Ϫ 2m)!

an؊2m ϭ (Ϫ1)m

(10)

2nm! (n Ϫ m)! (n Ϫ 2m)!

.

The resulting solution of Legendre’s differential equation (1) is called the Legendre
polynomial of degree n and is denoted by Pn(x).
From (10) we obtain
(2n Ϫ 2m)!

M

Pn(x) ϭ a (Ϫ1)m
mϭ0

(11)
ϭ

(2n)!
n

2

2 (n!)

2 m! (n Ϫ m)! (n Ϫ 2m)!

xn Ϫ

n

(2n Ϫ 2)!
2 1! (n Ϫ 1)! (n Ϫ 2)!
n

x n؊2m

x n؊2 ϩ Ϫ Á

where M ϭ n>2 or (n Ϫ 1)>2, whichever is an integer. The first few of these functions
are (Fig. 107)

(11؅)

P0(x) ϭ 1,

P1(x) ϭ x

P2(x) ϭ 12 (3x 2 Ϫ 1),

P3(x) ϭ 12 (5x 3 Ϫ 3x)

P4(x) ϭ 18 (35x 4 Ϫ 30x 2 ϩ 3),

P5(x) ϭ 18 (63x 5 Ϫ 70x 3 ϩ 15x)

and so on. You may now program (11) on your CAS and calculate Pn(x) as needed.
Pn(x)

P0

1

P1

P4
–1

P3

P2

–1

Fig. 107. Legendre polynomials

1

x

c05.qxd

10/28/10

1:33 PM

Page 179

SEC. 5.2 Legendre’s Equation. Legendre Polynomials Pn(x)

179

The Legendre polynomials Pn(x) are orthogonal on the interval Ϫ1 Ϲ x Ϲ 1, a basic
property to be defined and used in making up “Fourier–Legendre series” in the chapter
on Fourier series (see Secs. 11.5–11.6).

PROBLEM SET 5.2
1–5

LEGENDRE POLYNOMIALS AND
FUNCTIONS

1. Legendre functions for n ‫ ؍‬0. Show that (6) with
n ϭ 0 gives P0(x) ϭ 1 and (7) gives (use ln (1 ϩ x) ϭ
x Ϫ 12 x 2 ϩ 13 x 3 ϩ Á )
1
1
1 1ϩx
y2(x) ϭ x ϩ x 3 ϩ x 5 ϩ Á ϭ ln
.
3
5
2 1Ϫx
Verify this by solving (1) with n ϭ 0 , setting z ϭ y r
and separating variables.
2. Legendre functions for n ‫ ؍‬1. Show that (7) with
n ϭ 1 gives y2(x) ϭ P1(x) ϭ x and (6) gives

(a) Legendre polynomials. Show that
(12) G(u, x) ϭ

1
r ϭ

1 4 1 6
x Ϫ x Ϫ Á
3
5
1
1ϩx
ϭ 1 Ϫ x ln
.
2
1Ϫx

6–9

CAS PROBLEMS

6. Graph P2(x), Á , P10(x) on common axes. For what x
(approximately) and n ϭ 2, Á , 10 is ƒ Pn(x) ƒ Ͻ 12?
7. From what n on will your CAS no longer produce
faithful graphs of Pn(x)? Why?
8. Graph Q 0(x), Q 1(x), and some further Legendre
functions.
9. Substitute asx s ϩ asϩ1x sϩ1 ϩ asϩ2x sϩ2 into Legendre’s equation and obtain the coefficient recursion (4).
10. TEAM PROJECT. Generating Functions. Generating
functions play a significant role in modern applied
mathematics (see [GenRef5]). The idea is simple. If we
want to study a certain sequence ( fn(x)) and can find a
function
ؕ

G(u, x) ϭ a fn(x)u n,
nϭ0

we may obtain properties of ( fn(x)) from those of G,
which “generates” this sequence and is called a
generating function of the sequence.
2

21 Ϫ 2xu ϩ u 2

ϭ a Pn(x)u n
nϭ0

is a generating function of the Legendre polynomials.
Hint: Start from the binomial expansion of 1> 11 Ϫ v,
then set v ϭ 2xu Ϫ u 2, multiply the powers of 2xu Ϫ u 2
out, collect all the terms involving u n, and verify that
the sum of these terms is Pn(x)u n.
(b) Potential theory. Let A1 and A2 be two points in
space (Fig. 108, r2 Ͼ 0 ). Using (12), show that

y1 ϭ 1 Ϫ x 2 Ϫ

3. Special n. Derive (11 r ) from (11).
4. Legendre’s ODE. Verify that the polynomials in (11 r )
satisfy (1).
5. Obtain P6 and P7.

ؕ

1

1
2r 21 ϩ r 22 Ϫ 2r1r2 cos u
m

r1
1 ؕ
ϭ r a Pm(cos u) a r b .
2
2
mϭ0

This formula has applications in potential theory. (Q>r
is the electrostatic potential at A2 due to a charge Q
located at A1. And the series expresses 1>r in terms of
the distances of A1 and A2 from any origin O and the
angle u between the segments OA1 and OA2.)
A2

r2
r

θ

0

r1

A1

Fig. 108. Team Project 10
(c) Further applications of (12). Show that
Pn(1) ϭ 1, Pn(Ϫ1) ϭ (Ϫ1) n, P2nϩ1(0) ϭ 0, and
P2n(0) ϭ (Ϫ1) n # 1 # 3 Á (2n Ϫ 1)>[2 # 4 Á (2n)].
11–15

FURTHER FORMULAS

11. ODE. Find a solution of (a 2 Ϫ x 2)y s Ϫ 2xy r ϩ
n(n ϩ 1)y ϭ 0, a 0 , by reduction to the Legendre
equation.
12. Rodrigues’s formula (13)2 Applying the binomial
theorem to (x 2 Ϫ 1) n , differentiating it n times term
by term, and comparing the result with (11), show that
(13)

Pn(x) ϭ

1 dn
[(x 2 Ϫ 1)n].
2 n! dx n
n

OLINDE RODRIGUES (1794–1851), French mathematician and economist.

c05.qxd

10/28/10

3:43 PM

180

Page 180

CHAP. 5 Series Solutions of ODEs. Special Functions

13. Rodrigues’s formula. Obtain (11 r ) from (13).
14. Bonnet’s recursion.3 Differentiating (13) with
respect to u, using (13) in the resulting formula, and
comparing coefficients of u n, obtain the Bonnet
recursion.

15. Associated Legendre functions P kn (x) are needed, e.g.,
in quantum physics. They are defined by
P kn(x) ϭ (1 Ϫ x 2)k>2

(15)

(14) (n ϩ 1)Pnϩ1(x) ϭ (2n ϩ 1)xPn(x) Ϫ npnϪ1(x),

and are solutions of the ODE

where n ϭ 1, 2, Á . This formula is useful for computations, the loss of significant digits being small
(except near zeros). Try (14) out for a few computations
of your own choice.

(16)

5.3

d kpn(x)
dx k

(1 Ϫ x 2)y s Ϫ 2xy r ϩ q(x)y ϭ 0

where q(x) ϭ n(n ϩ 1) Ϫ k 2>(1 Ϫ x 2) . Find P 11(x),
P 12(x), P 22(x), and P 24(x) and verify that they satisfy (16).

Extended Power Series Method:
Frobenius Method
Several second-order ODEs of considerable practical importance—the famous Bessel
equation among them—have coefficients that are not analytic (definition in Sec. 5.1), but
are “not too bad,” so that these ODEs can still be solved by series (power series times a
logarithm or times a fractional power of x, etc.). Indeed, the following theorem permits
an extension of the power series method. The new method is called the Frobenius
method.4 Both methods, that is, the power series method and the Frobenius method, have
gained in significance due to the use of software in actual calculations.

THEOREM 1

Frobenius Method

Let b(x) and c(x) be any functions that are analytic at x ϭ 0. Then the ODE
(1)

ys ϩ

b(x)
x

yr ϩ

c(x)
x2

yϭ0

has at least one solution that can be represented in the form
ؕ

(2)

y(x) ϭ x r a am x m ϭ x r(a0 ϩ a1x ϩ a2 x 2 ϩ Á )

(a0

0)

mϭ0

where the exponent r may be any (real or complex) number (and r is chosen so that
a0 0).
The ODE (1) also has a second solution (such that these two solutions are linearly
independent) that may be similar to (2) (with a different r and different coefficients)
or may contain a logarithmic term. (Details in Theorem 2 below.)
3

OSSIAN BONNET (1819–1892), French mathematician, whose main work was in differential geometry.
GEORG FROBENIUS (1849–1917), German mathematician, professor at ETH Zurich and University of Berlin,
student of Karl Weierstrass (see footnote, Sect. 15.5). He is also known for his work on matrices and in group theory.
In this theorem we may replace x by x Ϫ x0 with any number x0. The condition a0 0 is no restriction; it
simply means that we factor out the highest possible power of x.
The singular point of (1) at x ϭ 0 is often called a regular singular point, a term confusing to the student,
which we shall not use.
4

c05.qxd

10/28/10

1:33 PM

Page 181

SEC. 5.3 Extended Power Series Method: Frobenius Method

181

For example, Bessel’s equation (to be discussed in the next section)
ys ϩ

1
x 2 Ϫ v2
yr ϩ a
byϭ0
x
x2

(v a parameter)

is of the form (1) with b(x) ϭ 1 and c(x) ϭ x 2 Ϫ v 2 analytic at x ϭ 0, so that the theorem
applies. This ODE could not be handled in full generality by the power series method.
Similarly, the so-called hypergeometric differential equation (see Problem Set 5.3) also
requires the Frobenius method.
The point is that in (2) we have a power series times a single power of x whose exponent
r is not restricted to be a nonnegative integer. (The latter restriction would make the whole
expression a power series, by definition; see Sec. 5.1.)
The proof of the theorem requires advanced methods of complex analysis and can be
found in Ref. [A11] listed in App. 1.
Regular and Singular Points.
A regular point of the ODE

The following terms are practical and commonly used.
y s ϩ p(x)y r ϩ q(x)y ϭ 0

is a point x 0 at which the coefficients p and q are analytic. Similarly, a regular point of
the ODE
~
h (x)y s ϩ ~
p (x)y r (x) ϩ ~
q (x)y ϭ 0
~
~
~
p, ~
q are analytic and h (x 0) 0 (so what we can divide by h and get
is an x 0 at which h , ~
the previous standard form). Then the power series method can be applied. If x 0 is not a
regular point, it is called a singular point.

Indicial Equation, Indicating the Form of Solutions
We shall now explain the Frobenius method for solving (1). Multiplication of (1) by x 2
gives the more convenient form
x 2y s ϩ xb(x)y r ϩ c(x)y ϭ 0.

(1 r )

We first expand b(x) and c(x) in power series,
b(x) ϭ b0 ϩ b1x ϩ b2 x 2 ϩ Á ,

c(x) ϭ c0 ϩ c1x ϩ c2 x 2 ϩ Á

or we do nothing if b(x) and c(x) are polynomials. Then we differentiate (2) term by term,
finding
ؕ

y r (x) ϭ a (m ϩ r)am x mϩr؊1 ϭ x r؊13ra0 ϩ (r ϩ 1)a1x ϩ Á 4
mϭ0
ؕ

(2*)

y s (x) ϭ a (m ϩ r)(m ϩ r Ϫ 1)am x mϩr؊2
mϭ0

ϭ x r؊23r(r Ϫ 1)a0 ϩ (r ϩ 1)ra1x ϩ Á 4.

c05.qxd

10/28/10

1:33 PM

182

Page 182

CHAP. 5 Series Solutions of ODEs. Special Functions

By inserting all these series into (1 r ) we obtain
x r[r(r Ϫ 1)a0 ϩ Á ] ϩ (b0 ϩ b1x ϩ Á ) x r(ra0 ϩ Á )

(3)

ϩ (c0 ϩ c1x ϩ Á ) x r(a0 ϩ a1x ϩ Á ) ϭ 0.

We now equate the sum of the coefficients of each power x r, x rϩ1, x rϩ2, Á to zero. This
yields a system of equations involving the unknown coefficients am. The smallest power
is x r and the corresponding equation is
[r (r Ϫ 1) ϩ b0 r ϩ c0 ]a0 ϭ 0.
Since by assumption a0
gives

0, the expression in the brackets [ Á ] must be zero. This

r (r Ϫ 1) ϩ b0 r ϩ c0 ϭ 0.

(4)

This important quadratic equation is called the indicial equation of the ODE (1). Its role
is as follows.
The Frobenius method yields a basis of solutions. One of the two solutions will always
be of the form (2), where r is a root of (4). The other solution will be of a form indicated
by the indicial equation. There are three cases:
Case 1. Distinct roots not differing by an integer 1, 2, 3, Á .
Case 2. A double root.
Case 3. Roots differing by an integer 1, 2, 3, Á .
Cases 1 and 2 are not unexpected because of the Euler–Cauchy equation (Sec. 2.5), the
simplest ODE of the form (1). Case 1 includes complex conjugate roots r1 and r2 ϭ r1
because r1 Ϫ r2 ϭ r1 Ϫ r1 ϭ 2i Im r1 is imaginary, so it cannot be a real integer. The
form of a basis will be given in Theorem 2 (which is proved in App. 4), without a general
theory of convergence, but convergence of the occurring series can be tested in each
individual case as usual. Note that in Case 2 we must have a logarithm, whereas in Case 3
we may or may not.

THEOREM 2

Frobenius Method. Basis of Solutions. Three Cases

Suppose that the ODE (1) satisfies the assumptions in Theorem 1. Let r1 and r2 be
the roots of the indicial equation (4). Then we have the following three cases.
Case 1. Distinct Roots Not Differing by an Integer. A basis is
(5)

y1(x) ϭ x r1(a0 ϩ a1x ϩ a2 x 2 ϩ Á )

and
(6)

y2(x) ϭ x r2(A0 ϩ A1x ϩ A2 x 2 ϩ Á )

with coefficients obtained successively from (3) with r ϭ r1 and r ϭ r2, respectively.

c05.qxd

10/28/10

1:33 PM

Page 183

SEC. 5.3 Extended Power Series Method: Frobenius Method

183

Case 2. Double Root r1 ‫ ؍‬r2 ‫ ؍‬r. A basis is
y1(x) ϭ x r(a0 ϩ a1x ϩ a2 x 2 ϩ Á )

(7)

[r ϭ 12 (1 Ϫ b0)]

(of the same general form as before) and
y2(x) ϭ y1(x) ln x ϩ x r(A1x ϩ A2 x 2 ϩ Á )

(8)

(x Ͼ 0).

Case 3. Roots Differing by an Integer. A basis is
y1(x) ϭ x r1(a0 ϩ a1x ϩ a2 x 2 ϩ Á )

(9)

(of the same general form as before) and
y2(x) ϭ ky1(x) ln x ϩ x r2(A0 ϩ A1x ϩ A2 x 2 ϩ Á ),

(10)

where the roots are so denoted that r1 Ϫ r2 Ͼ 0 and k may turn out to be zero.

Typical Applications
Technically, the Frobenius method is similar to the power series method, once the roots
of the indicial equation have been determined. However, (5)–(10) merely indicate the
general form of a basis, and a second solution can often be obtained more rapidly by
reduction of order (Sec. 2.1).
EXAMPLE 1

Euler–Cauchy Equation, Illustrating Cases 1 and 2 and Case 3 without a Logarithm
For the Euler–Cauchy equation (Sec. 2.5)
x 2y s ϩ b0 xy r ϩ c0y ϭ 0

(b0, c0 constant)

substitution of y ϭ x r gives the auxiliary equation
r(r Ϫ 1) ϩ b0r ϩ c0 ϭ 0,
which is the indicial equation [and y ϭ x r is a very special form of (2)!]. For different roots r1, r2 we get a basis
y1 ϭ x r1, y2 ϭ x r2, and for a double root r we get a basis x r, x r ln x. Accordingly, for this simple ODE, Case 3
plays no extra role.
᭿

EXAMPLE 2

Illustration of Case 2 (Double Root)
Solve the ODE
x(x Ϫ 1)y s ϩ (3x Ϫ 1)y r ϩ y ϭ 0.

(11)

(This is a special hypergeometric equation, as we shall see in the problem set.)

Solution. Writing (11) in the standard form (1), we see that it satisfies the assumptions in Theorem 1. [What
are b(x) and c(x) in (11)?] By inserting (2) and its derivatives (2*) into (11) we obtain
ؕ

ؕ

mϩr
Ϫ a (m ϩ r)(m ϩ r Ϫ 1)am x mϩr؊1
a (m ϩ r)(m ϩ r Ϫ 1)am x

(12)

mϭ0

mϭ0
ؕ

ؕ

ؕ

mϭ0

mϭ0

mϭ0

ϩ 3 a (m ϩ r)am x mϩr Ϫ a (m ϩ r)am x mϩr؊1 ϩ a am x mϩr ϭ 0.

c05.qxd

10/28/10

1:33 PM

184

Page 184

CHAP. 5 Series Solutions of ODEs. Special Functions
The smallest power is x r؊1, occurring in the second and the fourth series; by equating the sum of its coefficients
to zero we have
[Ϫr (r Ϫ 1) Ϫ r]a0 ϭ 0,

r 2 ϭ 0.

thus

Hence this indicial equation has the double root r ϭ 0.

First Solution. We insert this value r ϭ 0 into (12) and equate the sum of the coefficients of the power
x s to zero, obtaining

s(s Ϫ 1)as Ϫ (s ϩ 1)sasϩ1 ϩ 3sas Ϫ (s ϩ 1)asϩ1 ϩ as ϭ 0
thus asϩ1 ϭ as. Hence a0 ϭ a1 ϭ a2 ϭ Á , and by choosing a0 ϭ 1 we obtain the solution
ؕ
1
y1(x) ϭ a x m ϭ
1
Ϫ
x
mϭ0

( ƒ x ƒ Ͻ 1).

Second Solution. We get a second independent solution y2 by the method of reduction of order (Sec. 2.1),

substituting y2 ϭ uy1 and its derivatives into the equation. This leads to (9), Sec. 2.1, which we shall use in
this example, instead of starting reduction of order from scratch (as we shall do in the next example). In (9) of
Sec. 2.1 we have p ϭ (3x Ϫ 1)>(x 2 Ϫ x), the coefficient of y r in (11) in standard form. By partial fractions,

Ύ

Ϫ p dx ϭ Ϫ

3x Ϫ 1

Ύ x(x Ϫ 1) dx ϭ Ϫ Ύ a x Ϫ 1 ϩ x b dx ϭ Ϫ2 ln (x Ϫ 1) Ϫ ln x.
1

2

Hence (9), Sec. 2.1, becomes
؊ ͐p dx
u r ϭ U ϭ y ؊2
ϭ
1 e

(x Ϫ 1)2
(x Ϫ 1)2x

ϭ

1
x

u ϭ ln x,

,

y2 ϭ uy1 ϭ

ln x
1Ϫx

.

y1 and y2 are shown in Fig. 109. These functions are linearly independent and thus form a basis on the interval
0 Ͻ x Ͻ 1 (as well as on 1 Ͻ x Ͻ ϱ ).
᭿
y
4
3
2
1
–2

0
–1
–2

y2

2

4

6

x

y1

–3
–4

Fig. 109. Solutions in Example 2

EXAMPLE 3

Case 3, Second Solution with Logarithmic Term
Solve the ODE
(x 2 Ϫ x)y s Ϫ xy r ϩ y ϭ 0.

(13)

Solution. Substituting (2) and (2*) into (13), we have
ؕ

ؕ

ؕ

mϭ0

mϭ0

mϭ0

(x 2 Ϫ x) a (m ϩ r)(m ϩ r Ϫ 1)am x mϩr؊2 Ϫ x a (m ϩ r)am x mϩr؊1 ϩ a am x mϩr ϭ 0.

c05.qxd

10/28/10

1:33 PM

Page 185

SEC. 5.3 Extended Power Series Method: Frobenius Method

185

We now take x 2, x, and x inside the summations and collect all terms with power x mϩr and simplify algebraically,
ؕ

ؕ

mϭ0

mϭ0

2
mϩr
Ϫ a (m ϩ r)(m ϩ r Ϫ 1)am x mϩr؊1 ϭ 0.
a (m ϩ r Ϫ 1) am x

In the first series we set m ϭ s and in the second m ϭ s ϩ 1, thus s ϭ m Ϫ 1. Then

(14)

ؕ

ؕ

sϭ0

sϭ؊1

2
sϩr
Ϫ a (s ϩ r ϩ 1)(s ϩ r)asϩ1x sϩr ϭ 0.
a (s ϩ r Ϫ 1) as x

The lowest power is x r؊1 (take s ϭ Ϫ1 in the second series) and gives the indicial equation
r(r Ϫ 1) ϭ 0.
The roots are r1 ϭ 1 and r2 ϭ 0. They differ by an integer. This is Case 3.

First Solution. From (14) with r ϭ r1 ϭ 1 we have
ؕ

2
sϩ1
ϭ 0.
a 3s as Ϫ (s ϩ 2)(s ϩ 1)asϩ14x

sϭ0

This gives the recurrence relation
asϩ1 ϭ

s2
(s ϩ 2)(s ϩ 1)

(s ϭ 0, 1, Á ).

as

Hence a1 ϭ 0, a2 ϭ 0, Á successively. Taking a0 ϭ 1, we get as a first solution y1 ϭ x r1a0 ϭ x.

Second Solution. Applying reduction of order (Sec. 2.1), we substitute y2 ϭ y1u ϭ xu, y2r ϭ xu r ϩ u and
y s2 ϭ xu s ϩ 2u r into the ODE, obtaining
(x 2 Ϫ x)(xu s ϩ 2u r ) Ϫ x(xu r ϩ u) ϩ xu ϭ 0.
xu drops out. Division by x and simplification give
(x 2 Ϫ x)u s ϩ (x Ϫ 2)u r ϭ 0.
From this, using partial fractions and integrating (taking the integration constant zero), we get
us
xϪ2
2
1
ϭϪ 2
,
ϭϪ ϩ
x Ϫx
x
1Ϫx
ur

ln u r ϭ ln 2

xϪ1
2.
x2

Taking exponents and integrating (again taking the integration constant zero), we obtain
ur ϭ

xϪ1
1
1
ϭ Ϫ 2,
x
x2
x

u ϭ ln x ϩ

1
,
x

y2 ϭ xu ϭ x ln x ϩ 1.

y1 and y2 are linearly independent, and y2 has a logarithmic term. Hence y1 and y2 constitute a basis of solutions
᭿
for positive x.

The Frobenius method solves the hypergeometric equation, whose solutions include
many known functions as special cases (see the problem set). In the next section we use
the method for solving Bessel’s equation.

c05.qxd

10/28/10

1:33 PM

186

Page 186

CHAP. 5 Series Solutions of ODEs. Special Functions

PROBLEM SET 5.3
1. WRITING PROJECT. Power Series Method and
Frobenius Method. Write a report of 2–3 pages
explaining the difference between the two methods. No
proofs. Give simple examples of your own.
2–13

FROBENIUS METHOD

Find a basis of solutions by the Frobenius method. Try to
identify the series as expansions of known functions. Show
the details of your work.
2. (x ϩ 2)2y s ϩ (x ϩ 2)y r Ϫ y ϭ 0
3. xy s ϩ 2y r ϩ xy ϭ 0
4. xy s ϩ y ϭ 0
5. xy s ϩ (2x ϩ 1)y r ϩ (x ϩ 1)y ϭ 0
6. xy s ϩ 2x 3y r ϩ (x 2 Ϫ 2)y ϭ 0
7. y s ϩ (x Ϫ 1)y ϭ 0
8. xy s ϩ y r Ϫ xy ϭ 0
9. 2x(x Ϫ 1)y s Ϫ (x ϩ 1)y r ϩ y ϭ 0
10. xy s ϩ 2y r ϩ 4xy ϭ 0
11. xy s ϩ (2 Ϫ 2x)y r ϩ (x Ϫ 2)y ϭ 0
12. x 2y s ϩ 6xy r ϩ (4 x 2 ϩ 6)y ϭ 0
13. xy s ϩ (1 Ϫ 2x)y r ϩ (x Ϫ 1)y ϭ 0
14. TEAM PROJECT. Hypergeometric Equation, Series,
and Function. Gauss’s hypergeometric ODE5 is
(15)

x(1 Ϫ x)y s ϩ [c Ϫ (a ϩ b ϩ 1)x]y r Ϫ aby ϭ 0.

Here, a, b, c are constants. This ODE is of the form
p2 y s ϩ p1y r ϩ p0y ϭ 0 , where p2, p1, p0 are polynomials of degree 2, 1, 0, respectively. These polynomials
are written so that the series solution takes a most practical form, namely,
y1(x) ϭ 1 ϩ
(16)
ϩ

ab
a(a ϩ 1)b(b ϩ 1) 2
xϩ
x
1! c
2! c(c ϩ 1)

a(a ϩ 1)(a ϩ 2)b(b ϩ 1)(b ϩ 2)
3! c(c ϩ 1)(c ϩ 2)

x3 ϩ Á .

This series is called the hypergeometric series. Its sum
y1(x) is called the hypergeometric function and is
denoted by F(a, b, c; x). Here, c 0, Ϫ1, Ϫ2, Á . By
choosing specific values of a, b, c we can obtain an
incredibly large number of special functions as solutions

of (15) [see the small sample of elementary functions
in part (c)]. This accounts for the importance of (15).
(a) Hypergeometric series and function. Show that
the indicial equation of (15) has the roots r1 ϭ 0 and
r2 ϭ 1 Ϫ c . Show that for r1 ϭ 0 the Frobenius
method gives (16). Motivate the name for (16) by
showing that
F (1, 1, 1; x) ϭ F(1, b, b; x) ϭ F (a, 1, a; x) ϭ

1
.
1Ϫx

(b) Convergence. For what a or b will (16) reduce to
a polynomial? Show that for any other a, b, c
(c 0, Ϫ1, Ϫ2, Á ) the series (16) converges when
ƒ x ƒ Ͻ 1.
(c) Special cases. Show that
(1 ϩ x)n ϭ F (Ϫn, b, b; Ϫx),
(1 Ϫ x)n ϭ 1 Ϫ nxF (1 Ϫ n, 1, 2; x),
arctan x ϭ xF(12 , 1, 32 ; Ϫx 2)
arcsin x ϭ xF(12 , 12 , 32 ; x 2),
ln (1 ϩ x) ϭ xF(1, 1, 2; Ϫx),
1ϩx
ϭ 2xF(12 , 1, 32 ; x 2).
ln
1Ϫx
Find more such relations from the literature on special
functions, for instance, from [GenRef1] in App. 1.
(d) Second solution. Show that for r2 ϭ 1 Ϫ c the
Frobenius method yields the following solution (where
c 2, 3, 4, Á):
y2(x) ϭ x 1؊c a1 ϩ

(a Ϫ c ϩ 1)(b Ϫ c ϩ 1)

x
1! (Ϫc ϩ 2)
(17)
(a Ϫ c ϩ 1)(a Ϫ c ϩ 2)(b Ϫ c ϩ 1)(b Ϫ c ϩ 2) 2
ϩ
x
2! (Ϫc ϩ 2)(Ϫc ϩ 3)
ϩ Á b.

Show that
y2(x) ϭ x 1؊cF(a Ϫ c ϩ 1, b Ϫ c ϩ 1, 2 Ϫ c; x).
(e) On the generality of the hypergeometric equation.
Show that
(18)

##

#

(t 2 ϩ At ϩ B)y ϩ (Ct ϩ D)y ϩ Ky ϭ 0

5
CARL FRIEDRICH GAUSS (1777–1855), great German mathematician. He already made the first of his great
discoveries as a student at Helmstedt and Göttingen. In 1807 he became a professor and director of the Observatory
at Göttingen. His work was of basic importance in algebra, number theory, differential equations, differential
geometry, non-Euclidean geometry, complex analysis, numeric analysis, astronomy, geodesy, electromagnetism,
and theoretical mechanics. He also paved the way for a general and systematic use of complex numbers.

c05.qxd

10/28/10

1:33 PM

Page 187

SEC. 5.4 Bessel’s Equation. Bessel Functions J␯ (x)

187

#

with y ϭ dy>dt, etc., constant A, B, C, D, K, and t 2 ϩ
At ϩ B ϭ (t Ϫ t 1)(t Ϫ t 2), t 1 t 2, can be reduced to
the hypergeometric equation with independent variable
xϭ

15. 2x(1 Ϫ x)y s Ϫ (1 ϩ 6x)y r Ϫ 2y ϭ 0

t Ϫ t1
t2 Ϫ t1

16. x(1 Ϫ x)y s ϩ (12 ϩ 2x)y r Ϫ 2y ϭ 0

and parameters related by Ct 1 ϩ D ϭ Ϫc(t 2 Ϫ t 1),
C ϭ a ϩ b ϩ 1, K ϭ ab. From this you see that (15)
is a “normalized form” of the more general (18) and
that various cases of (18) can thus be solved in terms
of hypergeometric functions.

5.4

HYPERGEOMETRIC ODE

15–20

Find a general solution in terms of hypergeometric
functions.

17. 4x(1 Ϫ x)y s ϩ y r ϩ 8y ϭ 0

## #
##
#
2(t 2 Ϫ 5t ϩ 6)y ϩ (2t Ϫ 3)y Ϫ 8y ϭ 0
## #
3t(1 ϩ t)y ϩ ty Ϫ y ϭ 0

18. 4(t 2 Ϫ 3t ϩ 2)y Ϫ 2y ϩ y ϭ 0
19.
20.

Bessel’s Equation. Bessel Functions J␯(x)
One of the most important ODEs in applied mathematics in Bessel’s equation,6
x 2y s ϩ xy r ϩ (x 2 Ϫ ␯2)y ϭ 0

(1)

where the parameter ␯ (nu) is a given real number which is positive or zero. Bessel’s
equation often appears if a problem shows cylindrical symmetry, for example, as the
membranes in Sec.12.9. The equation satisfies the assumptions of Theorem 1. To see this,
divide (1) by x 2 to get the standard form y s ϩ y r >x ϩ (1 Ϫ ␯2>x 2)y ϭ 0. Hence, according
to the Frobenius theory, it has a solution of the form
ؕ

y(x) ϭ a am x mϩr

(2)

(a0

0).

mϭ0

Substituting (2) and its first and second derivatives into Bessel’s equation, we obtain
ؕ

ؕ

mϭ0

mϭ0

mϩr
ϩ a (m ϩ r)am x mϩr
a (m ϩ r)(m ϩ r Ϫ 1)am x

ؕ

ؕ

mϭ0

mϭ0

ϩ a am x mϩrϩ2 Ϫ ␯2 a am x mϩr ϭ 0.
sϩr

We equate the sum of the coefficients of x
to zero. Note that this power x sϩr
corresponds to m ϭ s in the first, second, and fourth series, and to m ϭ s Ϫ 2 in the third
series. Hence for s ϭ 0 and s ϭ 1, the third series does not contribute since m м 0.

6

FRIEDRICH WILHELM BESSEL (1784–1846), German astronomer and mathematician, studied astronomy
on his own in his spare time as an apprentice of a trade company and finally became director of the new Königsberg
Observatory.
Formulas on Bessel functions are contained in Ref. [GenRef10] and the standard treatise [A13].

c05.qxd

10/28/10

188

1:33 PM

Page 188

CHAP. 5 Series Solutions of ODEs. Special Functions

For s ϭ 2, 3, Á all four series contribute, so that we get a general formula for all these s.
We find
(a)
(3) (b)
(c)

r(r Ϫ 1)a0 ϩ ra0 Ϫ ␯2a0 ϭ 0

(s ϭ 0)

(r ϩ 1)ra1 ϩ (r ϩ 1)a1 Ϫ ␯ a1 ϭ 0

(s ϭ 1)

2

(s ϩ r)(s ϩ r Ϫ 1)as ϩ (s ϩ r)as ϩ as؊2 Ϫ ␯ as ϭ 0
2

(s ϭ 2, 3, Á ).

From (3a) we obtain the indicial equation by dropping a0,
(r ϩ ␯)(r Ϫ ␯) ϭ 0.

(4)

The roots are r1 ϭ ␯ (м 0) and r2 ϭ Ϫ␯.
Coefficient Recursion for r ‫ ؍‬r1 ‫ ؍‬v. For r ϭ ␯, Eq. (3b) reduces to (2␯ ϩ 1)a1 ϭ 0.
Hence a1 ϭ 0 since ␯ м 0. Substituting r ϭ ␯ in (3c) and combining the three terms
containing as gives simply
(s ϩ 2␯)sas ϩ as؊2 ϭ 0.

(5)

Since a1 ϭ 0 and ␯ м 0, it follows from (5) that a3 ϭ 0, a5 ϭ 0, Á . Hence we have to
deal only with even-numbered coefficients as with s ϭ 2m. For s ϭ 2m, Eq. (5) becomes
(2m ϩ 2␯)2ma2m ϩ a2m؊2 ϭ 0.
Solving for a2m gives the recursion formula
a2m ϭ Ϫ

(6)

1
2 m(␯ ϩ m)
2

m ϭ 1, 2, Á .

a2m؊2,

From (6) we can now determine a2, a4, Á successively. This gives
a2 ϭ Ϫ
a4 ϭ Ϫ

a2
2 2(v ϩ 2)
2

a0
2 (␯ ϩ 1)
2

ϭ

a0
2 2! (␯ ϩ 1)(␯ ϩ 2)
4

and so on, and in general

(7)

a2m ϭ

(Ϫ1)ma0
22mm! (␯ ϩ 1)(␯ ϩ 2) Á (␯ ϩ m)

,

m ϭ 1, 2, Á .

Bessel Functions Jn(x) for Integer ␯ ϭ n
Integer values of v are denoted by n. This is standard. For ␯ ϭ n the relation (7) becomes
(8)

a2m ϭ

(Ϫ1)ma0
22mm! (n ϩ 1)(n ϩ 2) Á (n ϩ m)

,

m ϭ 1, 2, Á .

c05.qxd

10/28/10

1:33 PM

Page 189

SEC. 5.4 Bessel’s Equation. Bessel Functions J␯ (x)

189

a0 is still arbitrary, so that the series (2) with these coefficients would contain this arbitrary
factor a0. This would be a highly impractical situation for developing formulas or
computing values of this new function. Accordingly, we have to make a choice. The choice
a0 ϭ 1 would be possible. A simpler series (2) could be obtained if we could absorb the
growing product (n ϩ 1)(n ϩ 2) Á (n ϩ m) into a factorial function (n ϩ m)! What
should be our choice? Our choice should be
a0 ϭ

(9)

1
2 n!
n

because then n! (n ϩ 1) Á (n ϩ m) ϭ (n ϩ m)! in (8), so that (8) simply becomes
(Ϫ1)m

a2m ϭ

(10)

22mϩnm! (n ϩ m)!

m ϭ 1, 2, Á .

,

By inserting these coefficients into (2) and remembering that c1 ϭ 0, c3 ϭ 0, Á we obtain
a particular solution of Bessel’s equation that is denoted by Jn(x):
Jn(x) ϭ x

(11)

ؕ

(Ϫ1) mx 2m

mϭ0

22mϩnm! (n ϩ m)!

n

a

(n м 0).

Jn(x) is called the Bessel function of the first kind of order n. The series (11) converges
for all x, as the ratio test shows. Hence Jn(x) is defined for all x. The series converges
very rapidly because of the factorials in the denominator.
EXAMPLE 1

Bessel Functions J0(x) and J1(x)
For n ϭ 0 we obtain from (11) the Bessel function of order 0
ؕ

(12)

(Ϫ1)mx 2m

J0(x) ϭ a
mϭ0

22m(m!)2

ϭ1Ϫ

x2
22(1!)2

ϩ

x4
24(2!)2

Ϫ

x6
26(3!)2

ϩϪÁ

which looks similar to a cosine (Fig. 110). For n ϭ 1 we obtain the Bessel function of order 1
ؕ

(13)

J1(x) ϭ a
mϭ0

(Ϫ1)mx 2mϩ1
2

m! (m ϩ 1)!

2mϩ1

ϭ

x
2

Ϫ

x3
3

2 1! 2!

ϩ

x5
5

2 2! 3!

Ϫ

x7
7

2 3! 4!

ϩϪÁ,

which looks similar to a sine (Fig. 110). But the zeros of these functions are not completely regularly spaced
(see also Table A1 in App. 5) and the height of the “waves” decreases with increasing x. Heuristically, n 2>x 2
in (1) in standard form [(1) divided by x 2] is zero (if n ϭ 0) or small in absolute value for large x, and so is
y r >x, so that then Bessel’s equation comes close to y s ϩ y ϭ 0, the equation of cos x and sin x; also y r >x acts
as a “damping term,” in part responsible for the decrease in height. One can show that for large x,

(14)

Jn(x) ϳ

np
p
2
cos ax Ϫ
Ϫ b
B px
2
4

where ϳ is read “asymptotically equal” and means that for fixed n the quotient of the two sides approaches 1
as x : ϱ .

c05.qxd

10/28/10

1:33 PM

Page 190

190

CHAP. 5 Series Solutions of ODEs. Special Functions
1
J0
0.5

J1

0

5

10

x

Fig. 110. Bessel functions of the first kind J0 and J1
Formula (14) is surprisingly accurate even for smaller x (Ͼ0). For instance, it will give you good starting
values in a computer program for the basic task of computing zeros. For example, for the first three zeros of J0
you obtain the values 2.356 (2.405 exact to 3 decimals, error 0.049), 5.498 (5.520, error 0.022), 8.639 (8.654,
error 0.015), etc.
᭿

Bessel Functions J␯(x) for any ␯ м 0. Gamma Function
We now proceed from integer ␯ ϭ n to any ␯ м 0. We had a0 ϭ 1>(2nn!) in (9). So we
have to extend the factorial function n! to any ␯ м 0. For this we choose
a0 ϭ

(15)

1
2 ⌫(␯ ϩ 1)
␯

with the gamma function ⌫(␯ ϩ 1) defined by
⌫(␯ ϩ 1) ϭ

(16)

ϱ

Ύe

؊t ␯

t dt

(␯ Ͼ Ϫ1).

0

(CAUTION! Note the convention ␯ ϩ 1 on the left but ␯ in the integral.) Integration
by parts gives
ϱ

⌫(␯ ϩ 1) ϭ Ϫe؊tt ␯ ` ϩ ␯
0

ϱ

Ύe

؊t ␯؊1

t

dt ϭ 0 ϩ ␯⌫(␯).

0

This is the basic functional relation of the gamma function
⌫(␯ ϩ 1) ϭ ␯⌫(␯).

(17)

Now from (16) with ␯ ϭ 0 and then by (17) we obtain
⌫(1) ϭ

Ύ

ϱ

0

ϱ

e؊t dt ϭ Ϫe؊t ` ϭ 0 Ϫ (Ϫ1) ϭ 1
0

and then ⌫(2) ϭ 1 # ⌫(1) ϭ 1!, ⌫(3) ϭ 2⌫(1) ϭ 2! and in general
(18)

⌫(n ϩ 1) ϭ n!

(n ϭ 0, 1, Á ).

c05.qxd

10/28/10

1:33 PM

Page 191

SEC. 5.4 Bessel’s Equation. Bessel Functions J␯ (x)

191

Hence the gamma function generalizes the factorial function to arbitrary positive ␯.
Thus (15) with ␯ ϭ n agrees with (9).
Furthermore, from (7) with a0 given by (15) we first have
a2m ϭ

(Ϫ1)m
22mm! (␯ ϩ 1)(␯ ϩ 2) Á (␯ ϩ m)2␯⌫(␯ ϩ 1)

.

Now (17) gives (␯ ϩ 1)⌫(␯ ϩ 1) ϭ ⌫(␯ ϩ 2), (␯ ϩ 2)⌫(␯ ϩ 2) ϭ ⌫(␯ ϩ 3) and so on,
so that
(␯ ϩ 1)(␯ ϩ 2) Á (␯ ϩ m)⌫(␯ ϩ 1) ϭ ⌫(␯ ϩ m ϩ 1).
Hence because of our (standard!) choice (15) of a0 the coefficients (7) are simply
a2m ϭ

(19)

(Ϫ1)m
22mϩ␯m! ⌫(␯ ϩ m ϩ 1)

.

With these coefficients and r ϭ r1 ϭ ␯ we get from (2) a particular solution of (1), denoted
by J␯(x) and given by

(20)

ؕ

(Ϫ1)mx 2m

mϭ0

22mϩ␯m! ⌫(␯ ϩ m ϩ 1)

J␯(x) ϭ x ␯ a

.

J␯(x) is called the Bessel function of the first kind of order ␯. The series (20) converges
for all x, as one can verify by the ratio test.

Discovery of Properties from Series
Bessel functions are a model case for showing how to discover properties and relations of
functions from series by which they are defined. Bessel functions satisfy an incredibly large
number of relationships—look at Ref. [A13] in App. 1; also, find out what your CAS knows.
In Theorem 3 we shall discuss four formulas that are backbones in applications and theory.
THEOREM 1

Derivatives, Recursions

The derivative of J␯(x) with respect to x can be expressed by J␯؊1(x) or J␯؉1(x) by
the formulas
(21)

(a)

[x ␯J␯(x)] r ϭ x ␯J␯؊1(x)

(b) [x ؊␯J␯(x)] r ϭ Ϫx ؊␯J␯ϩ1(x).

Furthermore, J␯(x) and its derivative satisfy the recurrence relations

(21)

2␯
(c) J␯؊1(x) ϩ J␯ϩ1(x) ϭ x J␯(x)
(d) J␯؊1(x) Ϫ J␯ϩ1(x) ϭ 2J␯r(x).

c05.qxd

10/28/10

1:33 PM

192

Page 192

CHAP. 5 Series Solutions of ODEs. Special Functions

PROOF

(a) We multiply (20) by x ␯ and take x 2␯ under the summation sign. Then we have
␯

ؕ

(Ϫ1)mx 2mϩ2␯

mϭ0

22mϩ␯m! ⌫(␯ ϩ m ϩ 1)

x J␯(x) ϭ a

.

We now differentiate this, cancel a factor 2, pull x 2␯؊1 out, and use the functional
relationship ⌫(␯ ϩ m ϩ 1) ϭ (␯ ϩ m)⌫(␯ ϩ m) [see (17)]. Then (20) with ␯ Ϫ 1 instead
of ␯ shows that we obtain the right side of (21a). Indeed,
ؕ

(x ␯J␯) r ϭ a
mϭ0

(Ϫ1)m2(m ϩ ␯)x 2mϩ2␯؊1
22mϩ␯m! ⌫(␯ ϩ m ϩ 1)

ؕ

(Ϫ1)mx 2m

mϭ0

22mϩ␯؊1m! ⌫(␯ ϩ m)

ϭ x ␯x ␯؊1 a

.

(b) Similarly, we multiply (20) by x ؊␯, so that x ␯ in (20) cancels. Then we differentiate,
cancel 2m, and use m! ϭ m(m Ϫ 1)!. This gives, with m ϭ s ϩ 1,
(x

؊␯

ؕ

(Ϫ1)mx 2m؊1

mϭ1

22mϩ␯؊1(m Ϫ 1)! ⌫(␯ ϩ m ϩ 1)

J␯) r ϭ a

ؕ

(Ϫ1)sϩ1x 2sϩ1

sϭ0

22sϩ␯ϩ1s! ⌫(␯ ϩ s ϩ 2)

ϭ a

.

Equation (20) with ␯ ϩ 1 instead of ␯ and s instead of m shows that the expression on
the right is Ϫx ؊␯J␯ϩ1(x). This proves (21b).
(c), (d) We perform the differentiation in (21a). Then we do the same in (21b) and
multiply the result on both sides by x 2␯. This gives
(a*)

␯x ␯؊1J␯ ϩ x ␯J␯r ϭ x ␯J␯؊1

(b*)

Ϫ␯x ␯؊1J␯ ϩ x ␯J␯r ϭ Ϫx ␯J␯ϩ1.

Substracting (b*) from (a*) and dividing the result by x ␯ gives (21c). Adding (a*) and
᭿
(b*) and dividing the result by x ␯ gives (21d).
EXAMPLE 2

Application of Theorem 1 in Evaluation and Integration
Formula (21c) can be used recursively in the form
J␯ϩ1(x) ϭ

2␯
J (x) Ϫ J␯؊1(x)
x ␯

for calculating Bessel functions of higher order from those of lower order. For instance, J2(x) ϭ 2J1(x)>x Ϫ J0(x),
so that J2 can be obtained from tables of J0 and J1 (in App. 5 or, more accurately, in Ref. [GenRef1] in App. 1).
To illustrate how Theorem 1 helps in integration, we use (21b) with ␯ ϭ 3 integrated on both sides. This
evaluates, for instance, the integral
Iϭ

Ύ

2

1

2

1
x ؊3J4(x) dx ϭ Ϫx ؊3J3(x) 2 ϭ Ϫ J3(2) ϩ J3(1).
8
1

A table of J3 (on p. 398 of Ref. [GenRef1]) or your CAS will give you
Ϫ18 # 0.128943 ϩ 0.019563 ϭ 0.003445.
Your CAS (or a human computer in precomputer times) obtains J3 from (21), first using (21c) with ␯ ϭ 2,
that is, J3 ϭ 4x ؊1J2 Ϫ J1, then (21c) with ␯ ϭ 1, that is, J2 ϭ 2x ؊1J1 Ϫ J0. Together,

c05.qxd

10/29/10

10:56 PM

Page 193

SEC. 5.4 Bessel’s Equation. Bessel Functions J␯ (x)

193

I ϭ x ؊3(4x ؊1(2x ؊1J1 Ϫ J0) Ϫ J1) 2

2

1

ϭ Ϫ18 32J1(2) Ϫ 2J0(2) Ϫ J1(2)4 ϩ 38J1(1) Ϫ 4J0(1) Ϫ J1(1)4
ϭ Ϫ18 J1(2) ϩ 14 J0(2) ϩ 7J1(1) Ϫ 4J0(1).
This is what you get, for instance, with Maple if you type int( Á ). And if you type evalf(int( Á )), you obtain
0.003445448, in agreement with the result near the beginning of the example.
᭿

Bessel Functions J␯ with Half-Integer ␯ Are Elementary
We discover this remarkable fact as another property obtained from the series (20) and
confirm it in the problem set by using Bessel’s ODE.
EXAMPLE 3

Elementary Bessel Functions J␯ with ␯ ‫ ؎ ؍‬21 , ؎ 23 , ؎ 25 , Á . The Value ⌫( 21 )
We first prove (Fig. 111)
2
sin x,
B px

(a) J1>2(x) ϭ

(22)

The series (20) with ␯ ϭ

1
2

(b) J؊1>2(x) ϭ

2
cos x.
B px

is

ؕ
(Ϫ1) mx 2m
(Ϫ1) mx 2mϩ1
2 ؕ
J1>2(x) ϭ 1x a 2mϩ1>2
.
3 ϭ
2mϩ1
a
2
m! ⌫(m ϩ 2 ) B x mϭ0 2
m! ⌫(m ϩ 32 )
mϭ0

The denominator can be written as a product AB, where (use (16) in B)
A ϭ 2mm! ϭ 2m(2m Ϫ 2)(2m Ϫ 4) Á 4 # 2,
B ϭ 2mϩ1⌫(m ϩ 32 ) ϭ 2mϩ1(m ϩ 12 )(m Ϫ 12 ) Á

3
2

# 12⌫(12)

ϭ (2m ϩ 1)(2m Ϫ 1) Á 3 # 1 # 1p ;
here we used (proof below)
⌫(12 ) ϭ 1p.

(23)

The product of the right sides of A and B can be written
AB ϭ (2m ϩ 1)2m (2m Ϫ 1) Á 3 # 2 # 1 1p ϭ (2m ϩ 1)!1p.
Hence
J1>2(x) Ϫ

2 ؕ (Ϫ1)mx 2mϩ1
2
a (2m ϩ 1)! ϭ B px sin x.
B px mϭ0

1

0

2π

4π

Fig. 111. Bessel functions J1>2 and J؊1>2

6π

x

c05.qxd

10/28/10

194

1:33 PM

Page 194

CHAP. 5 Series Solutions of ODEs. Special Functions
This proves (22a). Differentiation and the use of (21a) with ␯ ϭ 12 now gives
2
cos x ϭ x 1>2 J ؊1>2(x).
Bp

[ 1x J1> 2(x)] r ϭ

This proves (22b). From (22) follow further formulas successively by (21c), used as in Example 2.
We finally prove ⌫( 12 ) ϭ 1p by a standard trick worth remembering. In (15) we set t ϭ u 2. Then
dt ϭ 2u du and
1
⌫a b ϭ
2

ؕ

Ύe

ؕ

؊t ؊1>2

t

Ύe

dt ϭ 2

0

؊u2

du.

0

We square on both sides, write v instead of u in the second integral, and then write the product of the integrals
as a double integral:
2

Ύ

1
⌫a b ϭ 4
2

ؕ

0

e؊u du
2

Ύ

ؕ

e ؊v dv ϭ 4
2

0

ؕ

ΎΎ
0

ؕ

e؊(u

2

ϩv2)

du dv.

0

We now use polar coordinates r, u by setting u ϭ r cos u, v ϭ r sin u. Then the element of area is du dv ϭ r dr du
and we have to integrate over r from 0 to ϱ and over u from 0 to p>2 (that is, over the first quadrant of the
uv-plane):
2

1
⌫a b ϭ 4
2

p>2

Ύ Ύ
0

ؕ

0

2
p
e؊r r dr du ϭ 4 #
2

Ύ

ؕ

0

ؕ

2
2
1
e؊r r dr ϭ 2 aϪ b e؊r ` ϭ p.
2
0

᭿

By taking the square root on both sides we obtain (23).

General Solution. Linear Dependence
For a general solution of Bessel’s equation (1) in addition to J␯ we need a second linearly
independent solution. For ␯ not an integer this is easy. Replacing ␯ by Ϫ␯ in (20), we
have
(24)

ؕ

(Ϫ1)mx 2m

mϭ0

22m؊␯m! ⌫(m Ϫ ␯ ϩ 1)

J؊␯(x) ϭ x ؊␯ a

.

Since Bessel’s equation involves ␯2, the functions J␯ and J؊␯ are solutions of the equation
for the same ␯. If ␯ is not an integer, they are linearly independent, because the first terms
in (20) and in (24) are finite nonzero multiples of x ␯ and x ؊␯. Thus, if ␯ is not an integer,
a general solution of Bessel’s equation for all x 0 is
y(x) ϭ c1J␯(x) ϩ c2J؊␯(x)
This cannot be the general solution for an integer ␯ ϭ n because, in that case, we have
linear dependence. It can be seen that the first terms in (20) and (24) are finite nonzero
multiples of x ␯ and x ؊␯, respectively. This means that, for any integer ␯ ϭ n, we have
linear dependence because
(25)

J؊n(x) ϭ (Ϫ1)n Jn(x)

(n ϭ 1, 2, Á ).

c05.qxd

10/28/10

1:33 PM

Page 195

SEC. 5.4 Bessel’s Equation. Bessel Functions J␯ (x)

PROOF

195

To prove (25), we use (24) and let ␯ approach a positive integer n. Then the gamma
function in the coefficients of the first n terms becomes infinite (see Fig. 553 in App.
A3.1), the coefficients become zero, and the summation starts with m ϭ n. Since in
this case ⌫(m Ϫ n ϩ 1) ϭ (m Ϫ n)! by (18), we obtain
(26)

ؕ

(Ϫ1) mx 2m؊ n

mϭn

22m؊nm! (m Ϫ n)!

J؊n(x) ϭ a

ؕ

(Ϫ1)nϩsx 2sϩn

sϭ0

22sϩn (n ϩ s)! s!

ϭ a

(m ϭ n ϩ s).

The last series represents (Ϫ1)nJn(x), as you can see from (11) with m replaced by s. This
᭿
completes the proof.
The difficulty caused by (25) will be overcome in the next section by introducing further
Bessel functions, called of the second kind and denoted by Y␯.

PROBLEM SET 5.4
1. Convergence. Show that the series (11) converges for
all x. Why is the convergence very rapid?
2–10

ODES REDUCIBLE TO BESSEL’S ODE

This is just a sample of such ODEs; some more follow in
the next problem set. Find a general solution in terms of J␯
and J؊␯ or indicate when this is not possible. Use the
indicated substitutions. Show the details of your work.
4
2. x 2 y s ϩ xy r ϩ (x 2 Ϫ 49
)y ϭ 0
3. xy s ϩ y r ϩ 14 y ϭ 0 (1x ϭ z)
4. y s ϩ (e؊2x Ϫ 19)y ϭ 0 (e ؊x ϭ z)
5. Two-parameter ODE
x 2 y s ϩ xy r ϩ (l2x 2 Ϫ ␯2)y ϭ 0 (lx ϭ z)
6. x 2y s ϩ 14 (x ϩ 34) y ϭ 0 ( y ϭ u 1x, 1x ϭ z)
7. x 2 y s ϩ xy r ϩ 14 (x 2 Ϫ 1)y ϭ 0 (x ϭ 2z)
8. (2x ϩ 1) 2 y s ϩ 2(2x ϩ 1)y r ϩ 16x(x ϩ 1)y ϭ 0
(2x ϩ 1 ϭ z)
9. xy s ϩ (2␯ ϩ 1)y r ϩ xy ϭ 0 (y ϭ x ؊␯u)
10. x 2 y s ϩ (1 Ϫ 2␯)xy r ϩ ␯2(x 2␯ ϩ 1 Ϫ ␯2)y ϭ 0
( y ϭ x ␯u, x ␯ ϭ z)
11. CAS EXPERIMENT. Change of Coefficient. Find
and graph (on common axes) the solutions of
y s ϩ kx ؊1 y r ϩ y ϭ 0, y(0) ϭ 1, y r (0) ϭ 0,
for k ϭ 0, 1, 2, Á , 10 (or as far as you get useful
graphs). For what k do you get elementary functions?
Why? Try for noninteger k, particularly between 0 and 2,
to see the continuous change of the curve. Describe the
change of the location of the zeros and of the extrema as
k increases from 0. Can you interpret the ODE as a model
in mechanics, thereby explaining your observations?
12. CAS EXPERIMENT. Bessel Functions for Large x.
(a) Graph Jn(x) for n ϭ 0, Á , 5 on common axes.

(b) Experiment with (14) for integer n. Using graphs,
find out from which x ϭ x n on the curves of (11)
and (14) practically coincide. How does x n change
with n?
(c) What happens in (b) if n ϭ Ϯ12? (Our usual notation
in this case would be ␯.)
(d) How does the error of (14) behave as a function of x for fixed n? [Error ϭ exact value minus
approximation (14).]
(e) Show from the graphs that J0(x) has extrema where
J1(x) ϭ 0. Which formula proves this? Find further
relations between zeros and extrema.
13–15
ZEROS of Bessel functions play a key role in
modeling (e.g. of vibrations; see Sec. 12.9).
13. Interlacing of zeros. Using (21) and Rolle’s theorem,
show that between any two consecutive positive zeros
of Jn(x) there is precisely one zero of Jnϩ1(x).
14. Zeros. Compute the first four positive zeros of J0(x)
and J1(x) from (14). Determine the error and comment.
15. Interlacing of zeros. Using (21) and Rolle’s theorem,
show that between any two consecutive zeros of J0(x)
there is precisely one zero of J1(x).
16–18

HALF-INTEGER PARAMETER: APPROACH
BY THE ODE

16. Elimination of first derivative. Show that y ϭ uv
with v(x) ϭ exp (Ϫ12 ͐ p(x) dx) gives from the ODE
y s ϩ p(x)y r ϩ q(x)y ϭ 0 the ODE
u s ϩ 3q(x) Ϫ 14 p(x)2 Ϫ 12 p r (x)4 u ϭ 0,

not containing the first derivative of u.

c05.qxd

10/28/10

196

1:33 PM

Page 196

CHAP. 5 Series Solutions of ODEs. Special Functions

17. Bessel’s equation. Show that for (1) the substitution
in Prob. 16 is y ϭ ux ؊1>2 and gives
(27)

21. Basic integral formula. Show that

Ύx J
␯

x 2uЉ ϩ (x 2 ϩ _14 Ϫ ␯ 2)u ϭ 0.

dx ϭ x ␯J␯(x) ϩ c.

22. Basic integral formulas. Show that

Ύx

18. Elementary Bessel functions. Derive (22) in Example 3
from (27).
19–25

␯؊1(x)

ΎJ

APPLICATION OF (21): DERIVATIVES,
INTEGRALS

؊␯

J␯ϩ1(x) dx ϭ Ϫx ؊␯J␯(x) ϩ c,

␯ϩ1(x)

dx ϭ

ΎJ

␯؊1(x)

dx Ϫ 2J␯(x).

23. Integration. Show that ͐ x 2J0(x) dx ϭ x 2J1(x) ϩ
xJ0(x) Ϫ ͐ J0(x) dx. (The last integral is nonelementary; tables exist, e.g., in Ref. [A13] in App. 1.)

Use the powerful formulas (21) to do Probs. 19–25. Show
the details of your work.

24. Integration. Evaluate ͐ x ؊1J4(x) dx.

19. Derivatives. Show that J 0r (x) ϭ ϪJ1(x), J1r (x) ϭ
J0 (x) Ϫ J1(x)>x, J2r (x) ϭ 12 [J1(x) Ϫ J3(x)].

25. Integration. Evaluate ͐ J5(x) dx.

20. Bessel’s equation. Derive (1) from (21).

5.5

Bessel Functions Yn (x). General Solution
To obtain a general solution of Bessel’s equation (1), Sec. 5.4, for any ␯, we now introduce
Bessel functions of the second kind Y␯(x), beginning with the case ␯ ϭ n ϭ 0.
When n ϭ 0, Bessel’s equation can be written (divide by x)
xy s ϩ y r ϩ xy ϭ 0.

(1)

Then the indicial equation (4) in Sec. 5.4 has a double root r ϭ 0. This is Case 2 in Sec.
5.3. In this case we first have only one solution, J0(x). From (8) in Sec. 5.3 we see that
the desired second solution must be of the form
ؕ

y2(x) ϭ J0(x) ln x ϩ a Am x m.

(2)

mϭ1

We substitute y2 and its derivatives
ؕ
J0
y r2 ϭ J0r ln x ϩ x ϩ a mAm x m؊1
mϭ1

y s2 ϭ J s0 ln x ϩ

2J0r
x

Ϫ

ؕ

J0

ϩ a m (m Ϫ 1) Am x m؊2
x
mϭ1
2

into (1). Then the sum of the three logarithmic terms x J0s ln x, J 0r ln x, and x J0 ln x is zero
because J0 is a solution of (1). The terms ϪJ0>x and J0>x (from xy s and y r ) cancel. Hence
we are left with
ؕ

ؕ

ؕ

mϭ1

mϭ1

mϭ1

2 J0r ϩ a m(m Ϫ 1) Am x m؊1 ϩ a m Am x m؊1 ϩ a Am x mϩ1 ϭ 0.

c05.qxd

10/28/10

1:33 PM

Page 197

SEC. 5.5 Bessel Functions Y␯ (x). General Solution

197

Addition of the first and second series gives ͚m2 Am x m؊1. The power series of J 0r (x) is
obtained from (12) in Sec. 5.4 and the use of m!>m ϭ (m Ϫ 1)! in the form
ؕ

(Ϫ1)m2mx 2m؊1

mϭ1

22m (m!)2

J 0r (x) ϭ a

ؕ

(Ϫ1)mx 2m؊1

mϭ1

22m؊1m! (m Ϫ 1)!

ϭ a

.

Together with ͚m 2Am x m ؊1 and ͚Am x mϩ1 this gives
(Ϫ1)mx 2m؊1

ؕ

(3*)

a

m! (m Ϫ 1)!

2m؊2

mϭ1

2

ؕ

ؕ

mϭ1

mϭ1

ϩ a m 2Am x m؊1 ϩ a Am x mϩ1 ϭ 0.

First, we show that the Am with odd subscripts are all zero. The power x 0 occurs only in
the second series, with coefficient A1. Hence A1 ϭ 0. Next, we consider the even powers
x 2s. The first series contains none. In the second series, m Ϫ 1 ϭ 2s gives the term
(2s ϩ 1)2A2sϩ1x 2s. In the third series, m ϩ 1 ϭ 2s. Hence by equating the sum of the
coefficients of x 2s to zero we have
(2s ϩ 1)2A2sϩ1 ϩ A2s؊1 ϭ 0,

s ϭ 1, 2, Á .

Since A1 ϭ 0, we thus obtain A3 ϭ 0, A5 ϭ 0, Á , successively.
We now equate the sum of the coefficients of x 2sϩ1 to zero. For s ϭ 0 this gives
Ϫ1 ϩ 4A2 ϭ 0,

thus

A2 ϭ 14.

For the other values of s we have in the first series in (3*) 2m Ϫ 1 ϭ 2s ϩ 1, hence
m ϭ s ϩ 1, in the second m Ϫ 1 ϭ 2s ϩ 1, and in the third m ϩ 1 ϭ 2s ϩ 1. We thus obtain
(Ϫ1)sϩ1
2 (s ϩ 1)! s!
2s

ϩ (2s ϩ 2)2A2sϩ2 ϩ A2s ϭ 0.

For s ϭ 1 this yields
1
8

ϩ 16A4 ϩ A2 ϭ 0,

thus

3
A4 ϭ Ϫ 128

and in general
(3)

A2m ϭ

(Ϫ1)m؊1
1
1
1
a1 ϩ ϩ ϩ Á ϩ b ,
2
3
m
22m(m!)2

m ϭ 1, 2, Á .

Using the short notations
(4)

h1 ϭ 1

hm ϭ 1 ϩ

1
1
ϩ Á ϩ
2
m

and inserting (4) and A1 ϭ A3 ϭ Á ϭ 0 into (2), we obtain the result
ؕ

(Ϫ1)m؊1h m

mϭ1

22m(m!)2

y2(x) ϭ J0(x) ln x ϩ a

(5)

ϭ J0(x) ln x ϩ

x 2m

1 2
3 4
11
x Ϫ
x ϩ
x6 Ϫ ϩ Á .
4
128
13,824

m ϭ 2, 3, Á

c05.qxd

11/4/10

198

12:19 PM

Page 198

CHAP. 5 Series Solutions of ODEs. Special Functions

Since J0 and y2 are linearly independent functions, they form a basis of (1) for x Ͼ 0.
Of course, another basis is obtained if we replace y2 by an independent particular solution
of the form a( y2 ϩ bJ0), where a ( 0) and b are constants. It is customary to choose
a ϭ 2> p and b ϭ g Ϫ ln 2, where the number g ϭ 0.57721566490 Á is the so-called
Euler constant, which is defined as the limit of
1
1
1 ϩ 2 ϩ Á ϩ s Ϫ ln s
as s approaches infinity. The standard particular solution thus obtained is called the Bessel
function of the second kind of order zero (Fig. 112) or Neumann’s function of order
zero and is denoted by Y0(x). Thus [see (4)]

(6)

ؕ (Ϫ1)m؊1h
m 2m
x
Y0(x) ϭ
J (x) aln ϩ gb ϩ a
x d.
22m(m!)2
p c 0
2
mϭ1

2

For small x Ͼ 0 the function Y0(x) behaves about like ln x (see Fig. 112, why?), and
Y0(x) : Ϫϱ as x : 0.

Bessel Functions of the Second Kind Yn(x)
For ␯ ϭ n ϭ 1, 2, Á a second solution can be obtained by manipulations similar to those
for n ϭ 0, starting from (10), Sec. 5.4. It turns out that in these cases the solution also
contains a logarithmic term.
The situation is not yet completely satisfactory, because the second solution is defined
differently, depending on whether the order ␯ is an integer or not. To provide uniformity
of formalism, it is desirable to adopt a form of the second solution that is valid for all
values of the order. For this reason we introduce a standard second solution Y␯(x) defined
for all ␯ by the formula

(7)

(a)
(b)

1
[J␯(x) cos ␯p Ϫ J؊␯(x)]
sin ␯p
Yn(x) ϭ lim Y␯(x).

Y␯(x) ϭ

␯:n

This function is called the Bessel function of the second kind of order ␯ or Neumann’s
function7 of order ␯. Figure 112 shows Y0(x) and Y1(x).
Let us show that J␯ and Y␯ are indeed linearly independent for all ␯ (and x Ͼ 0).
For noninteger order ␯, the function Y␯(x) is evidently a solution of Bessel’s equation
because J␯(x) and J؊␯ (x) are solutions of that equation. Since for those ␯ the solutions
J␯ and J؊␯ are linearly independent and Y␯ involves J؊␯, the functions J␯ and Y␯ are
7
CARL NEUMANN (1832–1925), German mathematician and physicist. His work on potential theory using
integer equation methods inspired VITO VOLTERRA (1800–1940) of Rome, ERIK IVAR FREDHOLM (1866–1927)
of Stockholm, and DAVID HILBERT (1962–1943) of Göttingen (see the footnote in Sec. 7.9) to develop the field
of integral equations. For details see Birkhoff, G. and E. Kreyszig, The Establishment of Functional Analysis, Historia
Mathematica 11 (1984), pp. 258–321.
The solutions Y␯(x) are sometimes denoted by N␯(x); in Ref. [A13] they are called Weber’s functions; Euler’s
constant in (6) is often denoted by C or ln g.

c05.qxd

10/28/10

1:33 PM

Page 199

SEC. 5.5 Bessel Functions Y␯ (x). General Solution

199

Y0

0.5

Y1
0

10

5

x

–0.5

Fig. 112. Bessel functions of the second kind Y0 and Y1.
(For a small table, see App. 5.)

linearly independent. Furthermore, it can be shown that the limit in (7b) exists and Yn
is a solution of Bessel’s equation for integer order; see Ref. [A13] in App. 1. We shall
see that the series development of Yn(x) contains a logarithmic term. Hence Jn(x) and
Yn(x) are linearly independent solutions of Bessel’s equation. The series development
of Yn(x) can be obtained if we insert the series (20) in Sec. 5.4 and (2) in this section
for J␯(x) and J؊␯ (x) into (7a) and then let ␯ approach n; for details see Ref. [A13]. The
result is

Yn(x) ϭ

2

p

Jn(x) aln

(8)
Ϫ

xn ؕ (Ϫ1)m؊1(h m ϩ h mϩn) 2m
x
x
ϩ gb ϩ
a
22mϩnm! (m ϩ n)!
p mϭ0
2

x ؊n

n؊1

(n Ϫ m Ϫ 1)! 2m
x
p mϭ0 22m؊nm!
a

where x Ͼ 0, n ϭ 0, 1, Á , and [as in (4)]
hm ϭ 1 ϩ

1
1
ϩÁϩ ,
2
m

h 0 ϭ 0, h 1 ϭ 1,
h mϩn ϭ 1 ϩ

1
1
ϩÁϩ
.
2
mϩn

For n ϭ 0 the last sum in (8) is to be replaced by 0 [giving agreement with (6)].
Furthermore, it can be shown that
Y؊n(x) ϭ (Ϫ1)nYn(x).
Our main result may now be formulated as follows.
THEOREM 1

General Solution of Bessel’s Equation

A general solution of Bessel’s equation for all values of ␯ (and x Ͼ 0) is
(9)

y(x) ϭ C1J␯(x) ϩ C2Y␯(x).

We finally mention that there is a practical need for solutions of Bessel’s equation that
are complex for real values of x. For this purpose the solutions
(10)

H (1)
␯ (x) ϭ J␯(x) ϩ iY␯(x)
H (2)
␯ (x) ϭ J␯(x) Ϫ iY␯(x)

c05.qxd

10/28/10

200

1:33 PM

Page 200

CHAP. 5 Series Solutions of ODEs. Special Functions

are frequently used. These linearly independent functions are called Bessel functions of
the third kind of order ␯ or first and second Hankel functions8 of order ␯.
This finishes our discussion on Bessel functions, except for their “orthogonality,” which
we explain in Sec. 11.6. Applications to vibrations follow in Sec. 12.10.

PROBLEM SET 5.5
1–9

FURTHER ODE’s REDUCIBLE
TO BESSEL’S ODE

Find a general solution in terms of J␯ and Y␯. Indicate
whether you could also use J؊␯ instead of Y␯. Use the
indicated substitution. Show the details of your work.
1. x 2 y s ϩ xy r ϩ (x 2 Ϫ 16) y ϭ 0

(c) Calculate the first ten zeros x m, m ϭ 1, Á , 10, of
Y0(x) from your CAS and from (11). How does the error
behave as m increases?
(d) Do (c) for Y1(x) and Y2(x). How do the errors
compare to those in (c)?
11–15

2. xy s ϩ 5y r ϩ xy ϭ 0 ( y ϭ u>x )
2

3. 9x 2 y s ϩ 9xy r ϩ (36x 4 Ϫ 16)y ϭ 0 (x 2 ϭ z)
4. y s ϩ xy ϭ 0 ( y ϭ u 1x,

2 3>2
3x

ϭ z)

5. 4xy s ϩ 4y r ϩ y ϭ 0 (1x ϭ z)
6. xy s ϩ y r ϩ 36y ϭ 0 (12 1x ϭ z)
7. y s ϩ k 2x 2y ϭ 0 ( y ϭ u 1x, 12 kx 2 ϭ z)
8. y s ϩ k 2x 4y ϭ 0 ( y ϭ u 1x, 13 kx 3 ϭ z)
9. xy s Ϫ 5y r ϩ xy ϭ 0 ( y ϭ x 3u)
10. CAS EXPERIMENT. Bessel Functions for Large x.
It can be shown that for large x,

HANKEL AND MODIFIED
BESSEL FUNCTIONS

11. Hankel functions. Show that the Hankel functions (10)
form a basis of solutions of Bessel’s equation for any ␯.
12. Modified Bessel functions of the first kind of order
␯ are defined by I␯ (x) ϭ i ؊␯J␯ (ix), i ϭ 1Ϫ1. Show
that I␯ satisfies the ODE
(12)

x 2 y s ϩ xy r Ϫ (x 2 ϩ ␯2) y ϭ 0.

13. Modified Bessel functions. Show that I␯(x) has the
representation
ؕ

(13)

x 2mϩ␯

I␯(x) ϭ a
mϭ0

(11)

Yn(x) ϳ 22>(px) sin (x Ϫ 12 np Ϫ 14 p)

with ϳ defined as in (14) of Sec. 5.4.
(a) Graph Yn(x) for n ϭ 0, Á , 5 on common axes. Are
there relations between zeros of one function and
extrema of another? For what functions?
(b) Find out from graphs from which x ϭ x n on the
curves of (8) and (11) (both obtained from your CAS)
practically coincide. How does x n change with n?

m! ⌫(m ϩ ␯ ϩ 1)

2mϩ␯

2

.

14. Reality of I␯. Show that I␯(x) is real for all real x (and
real ␯), I␯(x) 0 for all real x 0, and I؊n(x) ϭ In(x),
where n is any integer.
15. Modified Bessel functions of the third kind (sometimes
called of the second kind) are defined by the formula (14)
below. Show that they satisfy the ODE (12).
(14)

K ␯(x) ϭ

p
2 sin ␯p

3I؊␯(x) Ϫ I␯(x)4.

CHAPTER 5 REVIEW QUESTIONS AND PROBLEMS
1. Why are we looking for power series solutions of ODEs?
2. What is the difference between the two methods in this
chapter? Why do we need two methods?
3. What is the indicial equation? Why is it needed?
4. List the three cases of the Frobenius method, and give
examples of your own.
5. Write down the most important ODEs in this chapter
from memory.
8

6. Can a power series solution reduce to a polynomial?
When? Why is this important?
7. What is the hypergeometric equation? Where does the
name come from?
8. List some properties of the Legendre polynomials.
9. Why did we introduce two kinds of Bessel functions?
10. Can a Bessel function reduce to an elementary function? When?

HERMANN HANKEL (1839–1873), German mathematician.

c05.qxd

10/28/10

1:33 PM

Page 201

Summary of Chapter 5
11–20

201

POWER SERIES METHOD
OR FROBENIUS METHOD

Find a basis of solutions. Try to identify the series as
expansions of known functions. Show the details of your
work.
11. y s ϩ 4y ϭ 0
12. xy s ϩ (1 Ϫ 2x) y r ϩ (x Ϫ 1) y ϭ 0
13. (x Ϫ 1)2 y s Ϫ (x Ϫ 1) y r Ϫ 35y ϭ 0

SUMMARY OF CHAPTER

16(x ϩ 1)2 y s ϩ 3y ϭ 0
x 2 y s ϩ xy r ϩ (x 2 Ϫ 5) y ϭ 0
x 2 y s ϩ 2x 3 y r ϩ (x 2 Ϫ 2) y ϭ 0
xy s Ϫ (x ϩ 1) y r ϩ y ϭ 0
xy s ϩ 3y r ϩ 4x 3 y ϭ 0
1
19. y s ϩ
yϭ0
4x
20. xy s ϩ y r Ϫ xy ϭ 0
14.
15.
16.
17.
18.

5

Series Solution of ODEs. Special Functions
The power series method gives solutions of linear ODEs
y s ϩ p(x) y r ϩ q(x)y ϭ 0

(1)

with variable coefficients p and q in the form of a power series (with any center x 0,
e.g., x 0 ϭ 0)
ؕ

(2)

y(x) ϭ a am(x Ϫ x 0)m ϭ a0 ϩ a1(x Ϫ x 0) ϩ a2(x Ϫ x 0)2 ϩ Á .
mϭ0

Such a solution is obtained by substituting (2) and its derivatives into (1). This gives
a recurrence formula for the coefficients. You may program this formula (or even
obtain and graph the whole solution) on your CAS.
If p and q are analytic at x 0 (that is, representable by a power series in powers
of x – x 0 with positive radius of convergence; Sec. 5.1), then (1) has solutions of
ෂ
this form (2). The same holds if h, ෂ
p, ෂ
q in
ෂ
h (x)y s ϩ ෂ
p(x)y r ϩ ෂ
q (x)y ϭ 0
are analytic at x 0 and ෂ
h (x 0) 0, so that we can divide by ෂ
h and obtain the standard
form (1). Legendre’s equation is solved by the power series method in Sec. 5.2.
The Frobenius method (Sec. 5.3) extends the power series method to ODEs
(3)

ys ϩ

a(x)
x Ϫ x0

yr ϩ

b(x)
(x Ϫ x0)2

yϭ0

whose coefficients are singular (i.e., not analytic) at x 0, but are “not too bad,”
namely, such that a and b are analytic at x 0. Then (3) has at least one solution of
the form
ؕ

(4) y(x) ϭ (x Ϫ x0)r a am(x Ϫ x0)m ϭ a0(x Ϫ x0)r ϩ a1(x Ϫ x0)rϩ1 ϩ Á
mϭ0

c05.qxd

10/28/10

202

1:33 PM

Page 202

CHAP. 5 Series Solutions of ODEs. Special Functions

where r can be any real (or even complex) number and is determined by substituting
(4) into (3) from the indicial equation (Sec. 5.3), along with the coefficients of (4).
A second linearly independent solution of (3) may be of a similar form (with different
r and am’s) or may involve a logarithmic term. Bessel’s equation is solved by the
Frobenius method in Secs. 5.4 and 5.5.
“Special functions” is a common name for higher functions, as opposed to the
usual functions of calculus. Most of them arise either as nonelementary integrals [see
(24)–(44) in App. 3.1] or as solutions of (1) or (3). They get a name and notation
and are included in the usual CASs if they are important in application or in theory.
Of this kind, and particularly useful to the engineer and physicist, are Legendre’s
equation and polynomials P0 , P1 , Á (Sec. 5.2), Gauss’s hypergeometric equation
and functions F(a, b, c; x) (Sec. 5.3), and Bessel’s equation and functions J␯ and
Y␯ (Secs. 5.4, 5.5).

c06.qxd

10/28/10

6:33 PM

Page 203

CHAPTER

6

Laplace Transforms
Laplace transforms are invaluable for any engineer’s mathematical toolbox as they make
solving linear ODEs and related initial value problems, as well as systems of linear ODEs,
much easier. Applications abound: electrical networks, springs, mixing problems, signal
processing, and other areas of engineering and physics.
The process of solving an ODE using the Laplace transform method consists of three
steps, shown schematically in Fig. 113:
Step 1. The given ODE is transformed into an algebraic equation, called the subsidiary
equation.
Step 2. The subsidiary equation is solved by purely algebraic manipulations.
Step 3. The solution in Step 2 is transformed back, resulting in the solution of the given
problem.

IVP
Initial Value
Problem

1

AP
Algebraic
Problem

2

Solving
AP
by Algebra

3

Solution
of the
IVP

Fig. 113. Solving an IVP by Laplace transforms

The key motivation for learning about Laplace transforms is that the process of solving
an ODE is simplified to an algebraic problem (and transformations). This type of
mathematics that converts problems of calculus to algebraic problems is known as
operational calculus. The Laplace transform method has two main advantages over the
methods discussed in Chaps. 1–4:
I. Problems are solved more directly: Initial value problems are solved without first
determining a general solution. Nonhomogenous ODEs are solved without first solving
the corresponding homogeneous ODE.
II. More importantly, the use of the unit step function (Heaviside function in Sec. 6.3)
and Dirac’s delta (in Sec. 6.4) make the method particularly powerful for problems with
inputs (driving forces) that have discontinuities or represent short impulses or complicated
periodic functions.

203

c06.qxd

10/28/10

204

6:33 PM

Page 204

CHAP. 6 Laplace Transforms

The following chart shows where to find information on the Laplace transform in this
book.
Topic

Where to find it

ODEs, engineering applications and Laplace transforms
PDEs, engineering applications and Laplace transforms
List of general formulas of Laplace transforms
List of Laplace transforms and inverses

Chapter 6
Section 12.11
Section 6.8
Section 6.9

Note: Your CAS can handle most Laplace transforms.

Prerequisite: Chap. 2
Sections that may be omitted in a shorter course: 6.5, 6.7
References and Answers to Problems: App. 1 Part A, App. 2.

6.1

Laplace Transform. Linearity.
First Shifting Theorem (s-Shifting)
In this section, we learn about Laplace transforms and some of their properties. Because
Laplace transforms are of basic importance to the engineer, the student should pay close
attention to the material. Applications to ODEs follow in the next section.
Roughly speaking, the Laplace transform, when applied to a function, changes that
function into a new function by using a process that involves integration. Details are as
follows.
If f (t) is a function defined for all t м 0, its Laplace transform1 is the integral of f (t)
times e؊st from t ϭ 0 to ϱ . It is a function of s, say, F(s), and is denoted by l( f ); thus
(1)

F(s) ϭ l( f ) ϭ
˛

Ύ

ؕ

e؊stf (t) dt.

0

Here we must assume that f (t) is such that the integral exists (that is, has some finite
value). This assumption is usually satisfied in applications—we shall discuss this near the
end of the section.

1

PIERRE SIMON MARQUIS DE LAPLACE (1749–1827), great French mathematician, was a professor in
Paris. He developed the foundation of potential theory and made important contributions to celestial mechanics,
astronomy in general, special functions, and probability theory. Napoléon Bonaparte was his student for a year.
For Laplace’s interesting political involvements, see Ref. [GenRef2], listed in App. 1.
The powerful practical Laplace transform techniques were developed over a century later by the English
electrical engineer OLIVER HEAVISIDE (1850–1925) and were often called “Heaviside calculus.”
We shall drop variables when this simplifies formulas without causing confusion. For instance, in (1) we
wrote l( f ) instead of l( f )(s) and in (1*) l؊1(F) instead of l؊1 (F)(t).

c06.qxd

10/28/10

6:33 PM

Page 205

SEC. 6.1 Laplace Transform. Linearity. First Shifting Theorem (s-Shifting)

205

Not only is the result F(s) called the Laplace transform, but the operation just described,
which yields F(s) from a given f (t), is also called the Laplace transform. It is an “integral
transform”

Ύ

F(s) ϭ

ؕ

k(s, t) f (t) dt

0

with “kernel” k(s, t) ϭ e؊st.
Note that the Laplace transform is called an integral transform because it transforms
(changes) a function in one space to a function in another space by a process of integration
that involves a kernel. The kernel or kernel function is a function of the variables in the
two spaces and defines the integral transform.
Furthermore, the given function f (t) in (1) is called the inverse transform of F(s) and
is denoted by l؊1(F ); that is, we shall write
˛

f (t) ϭ l؊1(F ).

(1*)

Note that (1) and (1*) together imply lϪ1(l( f )) ϭ f and l(lϪ1(F )) ϭ F.

Notation
Original functions depend on t and their transforms on s—keep this in mind! Original
functions are denoted by lowercase letters and their transforms by the same letters in capital,
so that F(s) denotes the transform of f (t), and Y(s) denotes the transform of y(t), and so on.
EXAMPLE 1

Laplace Transform
Let f (t) ϭ 1 when t м 0. Find F(s).

Solution. From (1) we obtain by integration
l( f ) ϭ l(1) ϭ

Ύ

ؕ

0

1
eϪst dt ϭ Ϫ eϪst `
s

ؕ

ϭ
0

1
s

(s Ͼ 0).

Such an integral is called an improper integral and, by definition, is evaluated according to the rule

Ύ

T

ؕ

e؊stf (t) dt ϭ lim

T:ϱ

0

Ύe

؊st

f (t) dt.

0

Hence our convenient notation means

Ύ

e؊st dt ϭ lim

0

1
1
1
1
lim c Ϫ e؊sT ϩ e0 d ϭ
c Ϫ s e؊st d ϭ T:ϱ
s
s
s
T

ؕ

T:ϱ

᭿

We shall use this notation throughout this chapter.

EXAMPLE 2

(s Ͼ 0).

0

Laplace Transform l (eat) of the Exponential Function eat
Let f (t) ϭ eat when t м 0, where a is a constant. Find l( f ).

Solution. Again by (1),
l(eat) ϭ

Ύ

ؕ

ؕ

e؊steat dt ϭ

0

1
e؊(s؊a)t 2 ;
aϪs
0

hence, when s Ϫ a Ͼ 0,
l(eat) ϭ

1
.
sϪa

᭿

c06.qxd

10/28/10

6:33 PM

206

Page 206

CHAP. 6 Laplace Transforms

Must we go on in this fashion and obtain the transform of one function after another
directly from the definition? No! We can obtain new transforms from known ones by the
use of the many general properties of the Laplace transform. Above all, the Laplace
transform is a “linear operation,” just as are differentiation and integration. By this we
mean the following.

THEOREM 1

Linearity of the Laplace Transform

The Laplace transform is a linear operation; that is, for any functions f (t) and g(t)
whose transforms exist and any constants a and b the transform of af (t) ϩ bg(t)
exists, and
l{af (t) ϩ bg(t)} ϭ al{f (t)} ϩ bl{g(t)}.

PROOF

This is true because integration is a linear operation so that (1) gives
l{af (t) ϩ bg(t)} ϭ

Ύ

ؕ

e؊st3af (t) ϩ bg(t)4 dt

0

ϭa

Ύ

ؕ

e؊stf (t) dt ϩ b

0

EXAMPLE 3

Ύ

ؕ

e؊stg(t) dt ϭ al{f (t)} ϩ bl{g(t)}. ᭿

0

Application of Theorem 1: Hyperbolic Functions
Find the transforms of cosh at and sinh at.

Solution. Since cosh at ϭ 12(eat ϩ e؊at) and sinh at ϭ 12(eat Ϫ e؊at), we obtain from Example 2 and
Theorem 1
l(cosh at) ϭ
l(sinh at) ϭ

EXAMPLE 4

1
2
1
2

(l(eat) ϩ l(e؊at)) ϭ
(l(eat) Ϫ l(e؊at)) ϭ

1
2

a

1
sϪa

ϩ

1
sϩa

bϭ

s
s2 Ϫ a2

1
1
a
1
Ϫ
.
a
bϭ 2
2 sϪa
sϩa
s Ϫ a2

᭿

Cosine and Sine
Derive the formulas
l(cos vt) ϭ

s
s ϩv
2

2

l(sin vt) ϭ

,

v
s ϩ v2
2

.

Solution. We write L c ϭ l(cos vt) and L s ϭ l(sin vt). Integrating by parts and noting that the integralfree parts give no contribution from the upper limit ϱ , we obtain
Lc ϭ

Ύ

ؕ

Ύ

ؕ

e؊st cos vt dt ϭ

0

Ls ϭ

0

ؕ

e؊st
v
cos vt 2 Ϫ
Ϫs
s
0

e؊st sin vt dt ϭ

ؕ

e؊st
v
sin vt 2 ϩ
Ϫs
s
0

Ύ

ؕ

e؊st sin vt dt ϭ

1
v
Ϫ L s,
s
s

e؊st cos vt dt ϭ

v
L .
s c

0

Ύ

ؕ

0

c06.qxd

10/28/10

7:44 PM

Page 207

SEC. 6.1 Laplace Transform. Linearity. First Shifting Theorem (s-Shifting)

207

By substituting L s into the formula for L c on the right and then by substituting L c into the formula for L s on
the right, we obtain
Lc ϭ

1
v v
Ϫ a Lcb ,
s
s s

L c a1 ϩ

v2
1
bϭ ,
s
s2

Lc ϭ

s
,
s 2 ϩ v2

Ls ϭ

v 1
v
a Ϫ Lsb ,
s s
s

L s a1 ϩ

v2
v
b ϭ 2,
s2
s

Ls ϭ

v
.
s 2 ϩ v2

᭿

Basic transforms are listed in Table 6.1. We shall see that from these almost all the others
can be obtained by the use of the general properties of the Laplace transform. Formulas
1–3 are special cases of formula 4, which is proved by induction. Indeed, it is true for
n ϭ 0 because of Example 1 and 0! ϭ 1. We make the induction hypothesis that it holds
for any integer n м 0 and then get it for n ϩ 1 directly from (1). Indeed, integration by
parts first gives
l(t nϩ1) ϭ

Ύ

ϱ

ؕ

1
nϩ1
e؊stt nϩ1 dt ϭ Ϫ s e؊stt nϩ1 2 ϩ s
0

0

Ύ

ؕ

e؊stt n dt.

0

Now the integral-free part is zero and the last part is (n ϩ 1)>s times l(t n). From this
and the induction hypothesis,
l(t nϩ1) ϭ

nϩ1
n ϩ 1 # n!
(n ϩ 1)!
l(t n) ϭ
ϭ nϩ2 .
s nϩ1
s
s
s

This proves formula 4.

Table 6.1 Some Functions ƒ(t) and Their Laplace Transforms ᏸ( ƒ)
ƒ(t)

ᏸ(ƒ)

1

1

1>s

7

cos ␻ t

2

t

1>s 2

8

sin ␻ t

3

t2

2!>s 3

9

cosh at

4

tn
(n ϭ 0, 1, • • •)

10

sinh at

11

eat cos ␻ t

12

eat sin ␻ t

5

6

n!
s

nϩ1

ta
(a positive)

⌫(a ϩ 1)

eat

1
sϪa

s

aϩ1

ƒ(t)

ᏸ(ƒ)
s
s 2 ϩ v2
v
s ϩ v2
2

s
s Ϫ a2
2

a
s Ϫ a2
2

sϪa
(s Ϫ a) 2 ϩ v2
v
(s Ϫ a) 2 ϩ v2

c06.qxd

10/28/10

6:33 PM

208

Page 208

CHAP. 6 Laplace Transforms

⌫(a ϩ 1) in formula 5 is the so-called gamma function [(15) in Sec. 5.5 or (24) in
App. A3.1]. We get formula 5 from (1), setting st ϭ x:
l(t a) ϭ

ؕ

Ύ

e؊stta dt ϭ

0

Ύ

a

ؕ

x dx
1
e؊x a b
ϭ aϩ1
s
s
s

0

Ύ

ؕ

e؊xx a dx

0

where s Ͼ 0. The last integral is precisely that defining ⌫(a ϩ 1), so we have
⌫(a ϩ 1)>s aϩ1, as claimed. (CAUTION! ⌫(a ϩ 1) has x a in the integral, not x aϩ1.)
Note the formula 4 also follows from 5 because ⌫(n ϩ 1) ϭ n! for integer n м 0.
Formulas 6–10 were proved in Examples 2–4. Formulas 11 and 12 will follow from 7
and 8 by “shifting,” to which we turn next.

s-Shifting: Replacing s by s Ϫ a in the Transform
The Laplace transform has the very useful property that, if we know the transform of f (t),
we can immediately get that of eatf (t), as follows.
THEOREM 2

First Shifting Theorem, s-Shifting

If f (t) has the transform F(s) (where s Ͼ k for some k), then eatf (t) has the transform
F(s Ϫ a) (where s Ϫ a Ͼ k). In formulas,
l{eatf (t)} ϭ F(s Ϫ a)
or, if we take the inverse on both sides,
eatf (t) ϭ l؊1{F(s Ϫ a)}.

PROOF

We obtain F(s Ϫ a) by replacing s with s Ϫ a in the integral in (1), so that
F(s Ϫ a) ϭ

Ύ

ؕ

e؊(s؊a)tf (t) dt ϭ

0

Ύ

ؕ

0

e؊st3eatf (t)4 dt ϭ l{eatf (t)}.

If F(s) exists (i.e., is finite) for s greater than some k, then our first integral exists for
s Ϫ a Ͼ k. Now take the inverse on both sides of this formula to obtain the second formula
in the theorem. (CAUTION! Ϫa in F(s Ϫ a) but ϩa in eatf (t).)
᭿
EXAMPLE 5

s-Shifting: Damped Vibrations. Completing the Square
From Example 4 and the first shifting theorem we immediately obtain formulas 11 and 12 in Table 6.1,
l{eat cos vt} ϭ

sϪa
(s Ϫ a) ϩ v
2

2

,

l{eat sin vt} ϭ

For instance, use these formulas to find the inverse of the transform
l( f ) ϭ

3s Ϫ 137
s ϩ 2s ϩ 401
2

.

v
(s Ϫ a)2 ϩ v2

.

c06.qxd

10/28/10

6:33 PM

Page 209

SEC. 6.1 Laplace Transform. Linearity. First Shifting Theorem (s-Shifting)

209

Solution. Applying the inverse transform, using its linearity (Prob. 24), and completing the square, we obtain
f ϭ l؊1b

3(s ϩ 1) Ϫ 140
(s ϩ 1) ϩ 400
2

r ϭ 3l؊1b

sϩ1
(s ϩ 1) ϩ 20
2

2

r Ϫ 7l؊1b

20
(s ϩ 1)2 ϩ 202

r.

We now see that the inverse of the right side is the damped vibration (Fig. 114)

᭿

f (t) ϭ e؊t(3 cos 20t Ϫ 7 sin 20t).

6
4
2

0

0.5

1.0

1.5

2.0

2.5

t

3.0

–2
–4
–6

Fig. 114. Vibrations in Example 5

Existence and Uniqueness of Laplace Transforms
This is not a big practical problem because in most cases we can check the solution of
an ODE without too much trouble. Nevertheless we should be aware of some basic facts.
A function f (t) has a Laplace transform if it does not grow too fast, say, if for all t м 0
and some constants M and k it satisfies the “growth restriction”
ƒ f (t) ƒ Ϲ Mekt.

(2)

(The growth restriction (2) is sometimes called “growth of exponential order,” which may
be misleading since it hides that the exponent must be kt, not kt 2 or similar.)
f (t) need not be continuous, but it should not be too bad. The technical term (generally
used in mathematics) is piecewise continuity. f (t) is piecewise continuous on a finite
interval a Ϲ t Ϲ b where f is defined, if this interval can be divided into finitely many
subintervals in each of which f is continuous and has finite limits as t approaches either
endpoint of such a subinterval from the interior. This then gives finite jumps as in
Fig. 115 as the only possible discontinuities, but this suffices in most applications, and
so does the following theorem.

a

b

t

Fig. 115. Example of a piecewise continuous function f (t).
(The dots mark the function values at the jumps.)

c06.qxd

10/28/10

6:33 PM

210

Page 210

CHAP. 6 Laplace Transforms

THEOREM 3

Existence Theorem for Laplace Transforms

If f (t) is defined and piecewise continuous on every finite interval on the semi-axis
t м 0 and satisfies (2) for all t м 0 and some constants M and k, then the Laplace
transform l( f ) exists for all s Ͼ k.
PROOF

Since f (t) is piecewise continuous, e؊stf (t) is integrable over any finite interval on the
t-axis. From (2), assuming that s Ͼ k (to be needed for the existence of the last of the
following integrals), we obtain the proof of the existence of l( f ) from
ƒ l( f ) ƒ ϭ `

Ύ

ؕ

0

e؊stf (t) dt ` Ϲ

Ύ

ؕ

ƒ f (t) ƒ e؊st dt Ϲ

0

Ύ

ؕ

Mekte؊st dt ϭ

0

M
.
sϪk

᭿

Note that (2) can be readily checked. For instance, cosh t Ͻ et, t n Ͻ n!et (because t n>n!
is a single term of the 2Maclaurin series), and so on. A function that does not satisfy (2)
for any M and k is et (take logarithms to see it). We mention that the conditions in
Theorem 3 are sufficient rather than necessary (see Prob. 22).
Uniqueness. If the Laplace transform of a given function exists, it is uniquely
determined. Conversely, it can be shown that if two functions (both defined on the positive
real axis) have the same transform, these functions cannot differ over an interval of positive
length, although they may differ at isolated points (see Ref. [A14] in App. 1). Hence we
may say that the inverse of a given transform is essentially unique. In particular, if two
continuous functions have the same transform, they are completely identical.

PROBLEM SET 6.1
1–16

LAPLACE TRANSFORMS

15.

Find the transform. Show the details of your work. Assume
that a, b, v, u are constants.
1. 3t ϩ 12
2. (a Ϫ bt)2
3. cos pt
4. cos2 vt
2t
5. e sinh t
6. e؊t sinh 4t
7. sin (vt ϩ u)
8. 1.5 sin (3t Ϫ p>2)
9.
10.
k

1

c
1

11.

12.

b

1
1

b

13.

14.
1

2

k

2
a
–1

16.
1

b

1

0.5
1

17–24

1

2

SOME THEORY

17. Table 6.1. Convert this table to a table for finding
inverse transforms (with obvious changes, e.g.,
l؊1(1>s n) ϭ t n؊1>(n Ϫ 1), etc).
18. Using l( f ) in Prob. 10, find l( f1), where f1(t) ϭ 0
if t Ϲ 2 and f1(t) ϭ 1 if t Ͼ 2.
19. Table 6.1. Derive formula 6 from formulas 9 and 10.
2
20. Nonexistence. Show that et does not satisfy a
condition of the form (2).
21. Nonexistence. Give simple examples of functions
(defined for all t м 0) that have no Laplace
transform.
22. Existence. Show that l(1> 1t) ϭ 1p>s. [Use (30)
⌫(12) ϭ 1p in App. 3.1.] Conclude from this that the
conditions in Theorem 3 are sufficient but not
necessary for the existence of a Laplace transform.

c06.qxd

10/28/10

6:33 PM

Page 211

SEC. 6.2 Transforms of Derivatives and Integrals. ODEs
23. Change of scale. If l( f (t)) ϭ F(s) and c is any
positive constant, show that l( f (ct)) ϭ F(s>c)>c (Hint:
Use (1).) Use this to obtain l(cos vt) from l(cos t).
24. Inverse transform. Prove that l؊1 is linear. Hint:
Use the fact that l is linear.

INVERSE LAPLACE TRANSFORMS

25–32

Given F(s) ϭ l( f ), find f (t). a, b, L, n are constants. Show
the details of your work.
25.
27.
29.
31.

0.2s ϩ 1.8

26.

s 2 ϩ 3.24
s
L s ϩn p
2 2

12
s

4

Ϫ

2

2

228
s

6

s ϩ 10
s2 Ϫ s Ϫ 2

6.2

28.
30.
32.

5s ϩ 1

211
33–45

41.

s 2 Ϫ 25
1
(s ϩ 12)(s Ϫ 13)
4s ϩ 32
s Ϫ 16
2

1
(s ϩ a)(s ϩ b)

APPLICATION OF s-SHIFTING

In Probs. 33–36 find the transform. In Probs. 37–45 find
the inverse transform. Show the details of your work.
33. t 2e؊3t
34. ke؊at cos vt
؊4.5t
35. 0.5e
36. sinh t cos t
sin 2pt
p
6
37.
38.
2
(s ϩ p)
(s ϩ 1)3
4
21
39.
40. 2
4
s Ϫ 2s Ϫ 3
(s ϩ 22)

p

s ϩ 10ps ϩ 24p2
a0
a2
a1
42.
ϩ
2 ϩ
(s ϩ 1)
(s ϩ 1)3
sϩ1
43.
45.

2

2s Ϫ 1
s Ϫ 6s ϩ 18
k 0 (s ϩ a) ϩ k 1
2

44.

a (s ϩ k) ϩ bp
(s ϩ k)2 ϩ p2

(s ϩ a)2

Transforms of Derivatives and Integrals.
ODEs
The Laplace transform is a method of solving ODEs and initial value problems. The crucial
idea is that operations of calculus on functions are replaced by operations of algebra
on transforms. Roughly, differentiation of f (t) will correspond to multiplication of l( f )
by s (see Theorems 1 and 2) and integration of f (t) to division of l( f ) by s. To solve
ODEs, we must first consider the Laplace transform of derivatives. You have encountered
such an idea in your study of logarithms. Under the application of the natural logarithm,
a product of numbers becomes a sum of their logarithms, a division of numbers becomes
their difference of logarithms (see Appendix 3, formulas (2), (3)). To simplify calculations
was one of the main reasons that logarithms were invented in pre-computer times.

THEOREM 1

Laplace Transform of Derivatives

The transforms of the first and second derivatives of f (t) satisfy
(1)

l( f r ) ϭ sl( f ) Ϫ f (0)

(2)

l( f s ) ϭ s 2l( f ) Ϫ sf (0) Ϫ f r (0).

Formula (1) holds if f (t) is continuous for all t м 0 and satisfies the growth
restriction (2) in Sec. 6.1 and f r (t) is piecewise continuous on every finite interval
on the semi-axis t м 0. Similarly, (2) holds if f and f r are continuous for all t м 0
and satisfy the growth restriction and f s is piecewise continuous on every finite
interval on the semi-axis t м 0.

c06.qxd

10/28/10

6:33 PM

212

Page 212

CHAP. 6 Laplace Transforms

PROOF

We prove (1) first under the additional assumption that f r is continuous. Then, by the
definition and integration by parts,
l( f r ) ϭ

Ύ

ؕ

0

e

f r (t) dt ϭ 3e

؊st

f (t)4 `

ؕ

؊st

0

ϩs

Ύ

ؕ

e؊stf (t) dt.

0

Since f satisfies (2) in Sec. 6.1, the integrated part on the right is zero at the upper limit
when s Ͼ k, and at the lower limit it contributes Ϫf (0). The last integral is l( f ). It exists
for s Ͼ k because of Theorem 3 in Sec. 6.1. Hence l( f r ) exists when s Ͼ k and (1) holds.
If f r is merely piecewise continuous, the proof is similar. In this case the interval of
integration of f r must be broken up into parts such that f r is continuous in each such part.
The proof of (2) now follows by applying (1) to f s and then substituting (1), that is
l( f s ) ϭ sl( f r ) Ϫ f r (0) ϭ s3sl( f ) Ϫ f (0)4 ϭ s 2l( f ) Ϫ sf (0) Ϫ f r (0).

᭿

Continuing by substitution as in the proof of (2) and using induction, we obtain the
following extension of Theorem 1.
THEOREM 2

Laplace Transform of the Derivative f (n) of Any Order

Let f, f r , Á , f (n؊1) be continuous for all t м 0 and satisfy the growth restriction
(2) in Sec. 6.1. Furthermore, let f (n) be piecewise continuous on every finite interval
on the semi-axis t м 0. Then the transform of f (n) satisfies
(3)

EXAMPLE 1

l( f (n)) ϭ s nl( f ) Ϫ s n؊1f (0) Ϫ s n؊2f r (0) Ϫ Á Ϫ f (n؊1)(0).

Transform of a Resonance Term (Sec. 2.8)
Let f (t) ϭ t sin vt. Then f (0) ϭ 0, f r (t) ϭ sin vt ϩ vt cos vt, f r (0) ϭ 0, f s ϭ 2v cos vt Ϫ v2t sin vt. Hence
by (2),
l( f s ) ϭ 2v

EXAMPLE 2

s
s ϩv
2

2

Ϫ v2l( f ) ϭ s 2l( f ),

thus

l( f ) ϭ l(t sin vt) ϭ

2vs
(s ϩ v2)2
2

.

᭿

Formulas 7 and 8 in Table 6.1, Sec. 6.1
This is a third derivation of l(cos vt) and l(sin vt); cf. Example 4 in Sec. 6.1. Let f (t) ϭ cos vt. Then
f (0) ϭ 1, f r (0) ϭ 0, f s (t) ϭ Ϫv2 cos vt. From this and (2) we obtain
l( f s ) ϭ s 2l( f ) Ϫ s ϭ Ϫv2l( f ).

By algebra,

l(cos vt) ϭ

s
s 2 ϩ v2

.

Similarly, let g ϭ sin vt. Then g(0) ϭ 0, g r ϭ v cos vt. From this and (1) we obtain
l(g r ) ϭ sl(g) ϭ vl(cos vt).

Hence,

l(sin vt) ϭ

v
v
.
l(cos vt) ϭ 2
s
s ϩ v2

᭿

Laplace Transform of the Integral of a Function
Differentiation and integration are inverse operations, and so are multiplication and division.
Since differentiation of a function f (t) (roughly) corresponds to multiplication of its transform
l( f ) by s, we expect integration of f (t) to correspond to division of l( f ) by s:

c06.qxd

10/28/10

6:33 PM

Page 213

SEC. 6.2 Transforms of Derivatives and Integrals. ODEs

THEOREM 3

213

Laplace Transform of Integral

Let F(s) denote the transform of a function f (t) which is piecewise continuous for t м 0
and satisfies a growth restriction (2), Sec. 6.1. Then, for s Ͼ 0, s Ͼ k, and t Ͼ 0,
le

(4)

PROOF

Ύ

t

0

1
f (t) dt f ϭ s F(s),

t

Ύ f (t) dt ϭ l

thus

؊1

0

1
e s F(s) f .

Denote the integral in (4) by g(t). Since f (t) is piecewise continuous, g(t) is continuous,
and (2), Sec. 6.1, gives
ƒ g(t) ƒ ϭ `

Ύ

t

0

f (t) dt ` Ϲ

Ύ

t

t

ƒ f (t) ƒ dt Ϲ M

0

Ύe

kt

M kt
M
(e Ϫ 1) Ϲ ekt
k
k

dt ϭ

0

(k Ͼ 0).

This shows that g(t) also satisfies a growth restriction. Also, g r (t) ϭ f (t), except at points
at which f (t) is discontinuous. Hence g r (t) is piecewise continuous on each finite interval
and, by Theorem 1, since g(0) ϭ 0 (the integral from 0 to 0 is zero)
l{f (t)} ϭ l{g r (t)} ϭ sl{g(t)} Ϫ g(0) ϭ sl{g(t)}.
Division by s and interchange of the left and right sides gives the first formula in (4),
from which the second follows by taking the inverse transform on both sides.
᭿
EXAMPLE 3

Application of Theorem 3: Formulas 19 and 20 in the Table of Sec. 6.9
Using Theorem 3, find the inverse of

1
s(s 2 ϩ v2)

and

1
s 2(s 2 ϩ v2)

.

Solution. From Table 6.1 in Sec. 6.1 and the integration in (4) (second formula with the sides interchanged)
we obtain
l؊1 b

1
sin vt
,
rϭ
s 2 ϩ v2
v

l؊1 b

1
rϭ
s(s 2 ϩ v2)

Ύ

t

0

sin vt
1
dt ϭ 2 (1 Ϫ cos vt).
v
v

This is formula 19 in Sec. 6.9. Integrating this result again and using (4) as before, we obtain formula 20
in Sec. 6.9:
l؊1 b

1
s 2(s 2 ϩ v2)

rϭ

1
v2

Ύ (1 Ϫ cos vt) dt ϭ c v
t

0

t
2

Ϫ

sin vt
v3

d

t

ϭ
0

t
v2

Ϫ

sin vt
v3

.

It is typical that results such as these can be found in several ways. In this example, try partial fraction
reduction.
᭿

Differential Equations, Initial Value Problems
Let us now discuss how the Laplace transform method solves ODEs and initial value
problems. We consider an initial value problem
(5)

y s ϩ ay r ϩ by ϭ r(t),

y(0) ϭ K 0,

y r (0) ϭ K 1

c06.qxd

10/28/10

6:33 PM

214

Page 214

CHAP. 6 Laplace Transforms

where a and b are constant. Here r(t) is the given input (driving force) applied to the
mechanical or electrical system and y(t) is the output (response to the input) to be obtained.
In Laplace’s method we do three steps:
Step 1. Setting up the subsidiary equation. This is an algebraic equation for the transform
Y ϭ l(y) obtained by transforming (5) by means of (1) and (2), namely,
3s 2Y Ϫ sy(0) Ϫ y r (0)4 ϩ a3sY Ϫ y(0)4 ϩ bY ϭ R(s)
where R(s) ϭ l(r). Collecting the Y-terms, we have the subsidiary equation
(s 2 ϩ as ϩ b)Y ϭ (s ϩ a)y(0) ϩ y r (0) ϩ R(s).
Step 2. Solution of the subsidiary equation by algebra. We divide by s 2 ϩ as ϩ b and
use the so-called transfer function
(6)

Q(s) ϭ

1
s 2 ϩ as ϩ b

ϭ

1
(s ϩ 12 a)2 ϩ b Ϫ 14 a 2

.

(Q is often denoted by H, but we need H much more frequently for other purposes.) This
gives the solution
(7)

Y(s) ϭ 3(s ϩ a)y(0) ϩ y r (0)4Q(s) ϩ R(s)Q(s).

If y(0) ϭ y r (0) ϭ 0, this is simply Y ϭ RQ; hence
Qϭ

l(output)
Y
ϭ
R
l(input)

and this explains the name of Q. Note that Q depends neither on r(t) nor on the initial
conditions (but only on a and b).
Step 3. Inversion of Y to obtain y ‫ ؍‬l؊1(Y ). We reduce (7) (usually by partial fractions
as in calculus) to a sum of terms whose inverses can be found from the tables (e.g., in
Sec. 6.1 or Sec. 6.9) or by a CAS, so that we obtain the solution y(t) ϭ l؊1(Y ) of (5).
EXAMPLE 4

Initial Value Problem: The Basic Laplace Steps
Solve
y s Ϫ y ϭ t,

y(0) ϭ 1,

y r (0) ϭ 1.

Solution. Step 1. From (2) and Table 6.1 we get the subsidiary equation 3with Y ϭ l(y)4
s 2Y Ϫ sy(0) Ϫ y r (0) Ϫ Y ϭ 1>s 2,

thus

(s 2 Ϫ 1)Y ϭ s ϩ 1 ϩ 1>s 2.

Step 2. The transfer function is Q ϭ 1>(s 2 Ϫ 1), and (7) becomes
Y ϭ (s ϩ 1)Q ϩ

1
s

2

Qϭ

sϩ1
s Ϫ1
2

ϩ

1
s (s Ϫ 1)
2

2

Simplification of the first fraction and an expansion of the last fraction gives
Yϭ

1
1
1
ϩ
Ϫ 2b.
s Ϫ 1 a s2 Ϫ 1
s

.

c06.qxd

10/28/10

6:33 PM

Page 215

SEC. 6.2 Transforms of Derivatives and Integrals. ODEs

215

Step 3. From this expression for Y and Table 6.1 we obtain the solution
y(t) ϭ l؊1(Y ) ϭ l؊1 e

1
1
1
ϩ l؊1 e 2
Ϫ l؊1 e 2 f ϭ et ϩ sinh t Ϫ t.
s Ϫ 1f
s Ϫ 1f
s

᭿

The diagram in Fig. 116 summarizes our approach.

t-space

s-space

Given problem
y" – y = t
y(0) = 1
y'(0) =1

(s2 – 1)Y = s + 1 + 1/s2

Solution of given problem

Solution of subsidiary equation

Subsidiary equation

y(t) = et + sinh t – t

Y=

1
1 – 1
+
s – 1 s2 – 1 s2

Fig. 116. Steps of the Laplace transform method

EXAMPLE 5

Comparison with the Usual Method
Solve the initial value problem
y s ϩ y r ϩ 9y ϭ 0.

y(0) ϭ 0.16,

y r (0) ϭ 0.

Solution. From (1) and (2) we see that the subsidiary equation is
s 2Y Ϫ 0.16s ϩ sY Ϫ 0.16 ϩ 9Y ϭ 0,

thus

(s 2 ϩ s ϩ 9)Y ϭ 0.16(s ϩ 1).

The solution is
Yϭ

0.16(s ϩ 1)
s2 ϩ s ϩ 9

ϭ

0.16(s ϩ 12 ) ϩ 0.08
(s ϩ 12 )2 ϩ 35
4

.

Hence by the first shifting theorem and the formulas for cos and sin in Table 6.1 we obtain
y(t) ϭ l؊1(Y ) ϭ e؊t>2 a0.16 cos

35
0.08
35
tϩ1
sin
tb
B4
B4
35
22

ϭ e؊0.5t(0.16 cos 2.96t ϩ 0.027 sin 2.96t).
This agrees with Example 2, Case (III) in Sec. 2.4. The work was less.

Advantages of the Laplace Method

1. Solving a nonhomogeneous ODE does not require first solving the
homogeneous ODE. See Example 4.
2. Initial values are automatically taken care of. See Examples 4 and 5.
3. Complicated inputs r(t) (right sides of linear ODEs) can be handled very
efficiently, as we show in the next sections.

᭿

c06.qxd

10/28/10

6:33 PM

216

Page 216

CHAP. 6 Laplace Transforms

EXAMPLE 6

Shifted Data Problems
This means initial value problems with initial conditions given at some t ϭ t 0 Ͼ 0 instead of t ϭ 0. For such a
~
~
problem set t ϭ t ϩ t 0, so that t ϭ t 0 gives t ϭ 0 and the Laplace transform can be applied. For instance, solve
y(14 p) ϭ 12 p,

y s ϩ y ϭ 2t,

y r (14 p) ϭ 2 Ϫ 12.

Solution. We have t 0 ϭ 14 p and we set t ϭ ~t ϩ 14 p. Then the problem is
~y s ϩ ~y ϭ 2(~t ϩ 1 p),
4

~y (0) ϭ 1 p,
2

~y r (0) ϭ 2 Ϫ 12

~
~
where ~y ( t ) ϭ y(t). Using (2) and Table 6.1 and denoting the transform of ~y by Y , we see that the subsidiary
equation of the “shifted” initial value problem is
1
2
2p
~
~
s 2Y Ϫ s # 12 p Ϫ (2 Ϫ 12) ϩ Y ϭ 2 ϩ
,
s
s

1
2
2p
1
~
(s 2 ϩ 1)Y ϭ 2 ϩ
ϩ ps ϩ 2 Ϫ 12.
s
s
2

thus

~
Solving this algebraically for Y , we obtain
~
Yϭ

2
(s ϩ 1)s
2

2

ϩ

1
2

p

ϩ

(s ϩ 1)s
2

1
2

ps

s ϩ1
2

ϩ

2 Ϫ 12
s2 ϩ 1

.

The inverse of the first two terms can be seen from Example 3 (with v ϭ 1), and the last two terms give cos
and sin,
~
~
~
~
~
~
~
y ϭ l؊1( Y ) ϭ 2( t Ϫ sin t ) ϩ 12 p(1 Ϫ cos t ) ϩ 12 p cos t ϩ (2 Ϫ 12) sin t
~
~
ϭ 2t ϩ 12 p Ϫ 12 sin t .
1
~
~
Now t ϭ t Ϫ 14 p, sin t ϭ
(sin t Ϫ cos t), so that the answer (the solution) is
12

᭿

y ϭ 2t Ϫ sin t ϩ cos t.

PROBLEM SET 6.2
1–11

INITIAL VALUE PROBLEMS (IVPS)

Solve the IVPs by the Laplace transform. If necessary, use
partial fraction expansion as in Example 4 of the text. Show
all details.
1. y r ϩ 5.2y ϭ 19.4 sin 2t, y(0) ϭ 0
2. y r ϩ 2y ϭ 0, y(0) ϭ 1.5
3. y s Ϫ y r Ϫ 6y ϭ 0, y(0) ϭ 11, y r (0) ϭ 28
4. y s ϩ 9y ϭ 10e؊t, y(0) ϭ 0, y r (0) ϭ 0
5. y s Ϫ 14 y ϭ 0, y(0) ϭ 12, y r (0) ϭ 0
6. y s Ϫ 6y r ϩ 5y ϭ 29 cos 2t, y(0) ϭ 3.2,
y r (0) ϭ 6.2
7. y s ϩ 7y r ϩ 12y ϭ 21e3t, y(0) ϭ 3.5,
y r (0) ϭ Ϫ10
8. y s Ϫ 4y r ϩ 4y ϭ 0, y(0) ϭ 8.1, y r (0) ϭ 3.9
9. y s Ϫ 4y r ϩ 3y ϭ 6t Ϫ 8, y(0) ϭ 0, y r (0) ϭ 0
10. y s ϩ 0.04y ϭ 0.02t 2, y(0) ϭ Ϫ25, y r (0) ϭ 0
11. y s ϩ 3y r ϩ 2.25y ϭ 9t 3 ϩ 64, y(0) ϭ 1,
y r (0) ϭ 31.5

12–15

SHIFTED DATA PROBLEMS

Solve the shifted data IVPs by the Laplace transform. Show
the details.
12. y s Ϫ 2y r Ϫ 3y ϭ 0, y(4) ϭ Ϫ3,
y r (4) ϭ Ϫ17
13. y r Ϫ 6y ϭ 0, y(Ϫ1) ϭ 4
14. y s ϩ 2y r ϩ 5y ϭ 50t Ϫ 100, y(2) ϭ Ϫ4,
y r (2) ϭ 14
15. y s ϩ 3y r Ϫ 4y ϭ 6e2t؊3,
y r (1.5) ϭ 5
16–21

y(1.5) ϭ 4,

OBTAINING TRANSFORMS
BY DIFFERENTIATION

Using (1) or (2), find l( f ) if f (t) equals:
16. t cos 4t

17. te؊at

18. cos2 2t

19. sin2 vt

20. sin4 t. Use Prob. 19.

21. cosh2 t

c06.qxd

10/28/10

6:33 PM

Page 217

SEC. 6.3 Unit Step Function (Heaviside Function). Second Shifting Theorem (t-Shifting)
22. PROJECT. Further Results by Differentiation.
Proceeding as in Example 1, obtain
(a)

l(t cos vt) ϭ

s 2 Ϫ v2
(s 2 ϩ v2)2

and from this and Example 1: (b) formula 21, (c) 22,
(d) 23 in Sec. 6.9,
(e) l(t cosh at) ϭ
(f ) l(t sinh at) ϭ
23–29

s2 ϩ a2
(s 2 Ϫ a 2)2

,

2as

.
(s 2 Ϫ a 2)2

INVERSE TRANSFORMS
BY INTEGRATION

Using Theorem 3, find f (t) if l(F ) equals:
20
3
23. 2
24. 3
s ϩ s>4
s Ϫ 2ps 2
1
1
25.
26. 4
s(s 2 ϩ v2)
s Ϫ s2
sϩ1
3s ϩ 4
27. 4
28. 4
s ϩ 9s 2
s ϩ k 2s 2
1
29. 3
s ϩ as 2

6.3

217

30. PROJECT. Comments on Sec. 6.2. (a) Give reasons
why Theorems 1 and 2 are more important than
Theorem 3.
(b) Extend Theorem 1 by showing that if f (t) is
continuous, except for an ordinary discontinuity (finite
jump) at some t ϭ a (Ͼ0), the other conditions remaining
as in Theorem 1, then (see Fig. 117)
(1*) l( f r ) ϭ sl( f ) Ϫ f (0) Ϫ 3 f (a ϩ 0) Ϫ f (a Ϫ 0)4e؊as.
(c) Verify (1*) for f (t) ϭ e؊t if 0 Ͻ t Ͻ 1 and 0 if
t Ͼ 1.
(d) Compare the Laplace transform of solving ODEs
with the method in Chap. 2. Give examples of your
own to illustrate the advantages of the present method
(to the extent we have seen them so far).
f (t)
f (a – 0)
f (a + 0)

0

a

t

Fig. 117. Formula (1*)

Unit Step Function (Heaviside Function).
Second Shifting Theorem (t-Shifting)
This section and the next one are extremely important because we shall now reach the
point where the Laplace transform method shows its real power in applications and its
superiority over the classical approach of Chap. 2. The reason is that we shall introduce
two auxiliary functions, the unit step function or Heaviside function u(t Ϫ a) (below) and
Dirac’s delta d(t Ϫ a) (in Sec. 6.4). These functions are suitable for solving ODEs with
complicated right sides of considerable engineering interest, such as single waves, inputs
(driving forces) that are discontinuous or act for some time only, periodic inputs more
general than just cosine and sine, or impulsive forces acting for an instant (hammerblows,
for example).

Unit Step Function (Heaviside Function) u(t Ϫ a)
The unit step function or Heaviside function u(t Ϫ a) is 0 for t Ͻ a, has a jump of size
1 at t ϭ a (where we can leave it undefined), and is 1 for t Ͼ a, in a formula:
(1)

u(t Ϫ a) ϭ b

0

if t Ͻ a

1

if t Ͼ a

(a м 0).

c06.qxd

10/28/10

218

6:33 PM

Page 218

CHAP. 6 Laplace Transforms
u(t – a)

u(t)

1

1

0

t

0

a

t

Fig. 119. Unit step function u(t Ϫ a)

Fig. 118. Unit step function u(t)

Figure 118 shows the special case u(t), which has its jump at zero, and Fig. 119 the general
case u(t Ϫ a) for an arbitrary positive a. (For Heaviside, see Sec. 6.1.)
The transform of u(t Ϫ a) follows directly from the defining integral in Sec. 6.1,
l{u(t Ϫ a)} ϭ

Ύ

ؕ

e

؊st

u(t Ϫ a) dt ϭ

0

Ύ

ؕ

e

؊st

0

؊st ؕ

# 1 dt ϭ Ϫ e `
s

;
tϭa

here the integration begins at t ϭ a (м 0) because u(t Ϫ a) is 0 for t Ͻ a. Hence
l{u(t Ϫ a)} ϭ

(2)

e؊as
s

(s Ͼ 0).

The unit step function is a typical “engineering function” made to measure for engineering
applications, which often involve functions (mechanical or electrical driving forces) that
are either “off ” or “on.” Multiplying functions f (t) with u(t Ϫ a), we can produce all sorts
of effects. The simple basic idea is illustrated in Figs. 120 and 121. In Fig. 120 the given
function is shown in (A). In (B) it is switched off between t ϭ 0 and t ϭ 2 (because
u(t Ϫ 2) ϭ 0 when t Ͻ 2) and is switched on beginning at t ϭ 2. In (C) it is shifted to the
right by 2 units, say, for instance, by 2 sec, so that it begins 2 sec later in the same fashion
as before. More generally we have the following.
Let f (t) ϭ 0 for all negative t. Then f (t Ϫ a)u(t Ϫ a) with a Ͼ 0 is f (t) shifted
(translated) to the right by the amount a.
Figure 121 shows the effect of many unit step functions, three of them in (A) and
infinitely many in (B) when continued periodically to the right; this is the effect of a
rectifier that clips off the negative half-waves of a sinuosidal voltage. CAUTION! Make
sure that you fully understand these figures, in particular the difference between parts (B)
and (C) of Fig. 120. Figure 120(C) will be applied next.
f (t)
5
0

5

π 2π

t

0

5

2 π 2π

–5

–5

(A) f (t) = 5 sin t

(B) f (t)u(t – 2)

t

0

2 π +2 2π +2

t

–5
(C) f (t – 2)u(t – 2)

Fig. 120. Effects of the unit step function: (A) Given function.
(B) Switching off and on. (C) Shift.

c06.qxd

10/28/10

6:33 PM

Page 219

SEC. 6.3 Unit Step Function (Heaviside Function). Second Shifting Theorem (t-Shifting)

219

4

k
1

4

t

6

–k

0

2

4

6

8

10

t

(B) 4 sin (12_ π t)[u(t) – u(t – 2) + u(t – 4) – + ⋅⋅⋅]

(A) k[u(t – 1) – 2u(t – 4) + u(t – 6)]

Fig. 121. Use of many unit step functions.

Time Shifting (t-Shifting): Replacing t by t Ϫ a in f (t)
The first shifting theorem (“s-shifting”) in Sec. 6.1 concerned transforms F(s) ϭ l{f (t)}
and F(s Ϫ a) ϭ l{eatf (t)}. The second shifting theorem will concern functions f (t) and
f (t Ϫ a). Unit step functions are just tools, and the theorem will be needed to apply them
in connection with any other functions.
THEOREM 1

Second Shifting Theorem; Time Shifting

If f (t) has the transform F(s), then the “shifted function”
(3)

~
f (t) ϭ f (t Ϫ a)u(t Ϫ a) ϭ b

0

if t Ͻ a

f (t Ϫ a)

if t Ͼ a

has the transform e؊asF(s). That is, if l{f (t)} ϭ F(s), then
(4)

l{f (t Ϫ a)u(t Ϫ a)} ϭ e؊asF(s).

Or, if we take the inverse on both sides, we can write
(4*)

f (t Ϫ a)u(t Ϫ a) ϭ l؊1{e؊asF(s)}.

Practically speaking, if we know F(s), we can obtain the transform of (3) by multiplying
F(s) by e؊as. In Fig. 120, the transform of 5 sin t is F(s) ϭ 5>(s 2 ϩ 1), hence the shifted
function 5 sin (t Ϫ 2)u(t Ϫ 2) shown in Fig. 120(C) has the transform
e؊2sF(s) ϭ 5e؊2s>(s 2 ϩ 1).
PROOF

We prove Theorem 1. In (4), on the right, we use the definition of the Laplace transform,
writing t for t (to have t available later). Then, taking e؊as inside the integral, we have
e؊asF(s) ϭ e؊as

Ύ

ؕ

e؊stf (t) dt ϭ

0

Ύ

ؕ

e؊s(tϩa)f (t) dt.

0

Substituting t ϩ a ϭ t, thus t ϭ t Ϫ a, dt ϭ dt in the integral (CAUTION, the lower
limit changes!), we obtain
e؊asF(s) ϭ

Ύ

ؕ

a

e؊stf (t Ϫ a) dt.

c06.qxd

10/28/10

6:33 PM

220

Page 220

CHAP. 6 Laplace Transforms

To make the right side into a Laplace transform, we must have an integral from 0 to ϱ ,
not from a to ϱ . But this is easy. We multiply the integrand by u(t Ϫ a). Then for t from
~
0 to a the integrand is 0, and we can write, with f as in (3),
e؊asF(s) ϭ

Ύ

ؕ

e؊stf (t Ϫ a)u(t Ϫ a) dt ϭ

0

Ύ

ؕ

~
e؊stf (t) dt.

0

(Do you now see why u(t Ϫ a) appears?) This integral is the left side of (4), the Laplace
~
᭿
transform of f (t) in (3). This completes the proof.
EXAMPLE 1

Application of Theorem 1. Use of Unit Step Functions
Write the following function using unit step functions and find its transform.
if 0 Ͻ t Ͻ 1

2
f (t) ϭ

d 12 t 2

if 1 Ͻ t Ͻ 12 p

cos t

Solution.

(Fig. 122)

1
2

t Ͼ p.

if

Step 1. In terms of unit step functions,
f (t) ϭ 2(1 Ϫ u(t Ϫ 1)) ϩ 12 t 2(u(t Ϫ 1) Ϫ u(t Ϫ 12 p)) ϩ (cos t)u(t Ϫ 12 p).

Indeed, 2(1 Ϫ u(t Ϫ 1)) gives f (t) for 0 Ͻ t Ͻ 1, and so on.
Step 2. To apply Theorem 1, we must write each term in f (t) in the form f (t Ϫ a)u(t Ϫ a). Thus, 2(1 Ϫ u(t Ϫ 1))
remains as it is and gives the transform 2(1 Ϫ e؊s)>s. Then
1
1
1
1
1
1
l e t 2u(t Ϫ 1) f ϭ l a (t Ϫ 1)2 ϩ (t Ϫ 1) ϩ b u(t Ϫ 1) f ϭ a 3 ϩ 2 ϩ b e؊s
2
2
2
2s
s
s
2

1
1
1
1
p
1
p2
1
l e t 2u at Ϫ p b f ϭ l e at Ϫ p b ϩ at Ϫ p b ϩ
b u at Ϫ p b f
2
2
2
2
2
2
8
2
ϭa
l e (cos t) u at Ϫ

1
2

1
p
p2 ؊ps>2
be
3 ϩ
2 ϩ
8s
s
2s

p b f ϭ l e Ϫasin at Ϫ

1
2

p bb u at Ϫ

1
2

pb f ϭ Ϫ

1
e؊ps>2.
s2 ϩ 1

Together,
l( f ) ϭ

2
2
1
1
1
1
p
p2 ؊ps>2
1
Ϫ e؊s ϩ a 3 ϩ 2 ϩ b e؊s Ϫ a 3 ϩ 2 ϩ
Ϫ 2
e؊ps>2.
be
s
s
2s
8s
s
s
s
2s
s ϩ1

If the conversion of f (t) to f (t Ϫ a) is inconvenient, replace it by
l{ f (t)u(t Ϫ a)} ϭ e؊asl{ f (t ϩ a)}.

(4**)

(4**) follows from (4) by writing f (t Ϫ a) ϭ g(t), hence f (t) ϭ g(t ϩ a) and then again writing f for g. Thus,
1
1
1
1
1
1
1
l e t 2u(t Ϫ 1) f ϭ e؊sl e (t ϩ 1)2 f ϭ e؊sl e t 2 ϩ t ϩ f ϭ e؊s a 3 ϩ 2 ϩ b
2
2
2
2
2s
s
s
as before. Similarly for l{ 12 t 2u(t Ϫ 12 p)}. Finally, by (4**),
l e cos t u at Ϫ

1
1
1
p b f ϭ e؊ps>2l e cos at ϩ p b f ϭ e؊ps>2l{Ϫsin t} ϭ Ϫe؊ps>2 2
.
2
2
s ϩ1

᭿

c06.qxd

10/28/10

6:33 PM

Page 221

SEC. 6.3 Unit Step Function (Heaviside Function). Second Shifting Theorem (t-Shifting)

221

f (t)
2
1
0

␲

1

2␲

t

4␲

–1

Fig. 122. ƒ(t) in Example 1

EXAMPLE 2

Application of Both Shifting Theorems. Inverse Transform
Find the inverse transform f (t) of
F(s) ϭ

e؊s
s 2 ϩ p2

ϩ

e؊2s
s 2 ϩ p2

ϩ

e؊3s
(s ϩ 2)2

.

Solution.

Without the exponential functions in the numerator the three terms of F(s) would have the inverses
(sin pt)> p, (sin pt)> p, and te؊2t because 1>s 2 has the inverse t, so that 1>(s ϩ 2)2 has the inverse te؊2t by the
first shifting theorem in Sec. 6.1. Hence by the second shifting theorem (t-shifting),
f (t) ϭ

1

1

p sin (p(t Ϫ 1)) u(t Ϫ 1) ϩ p sin (p(t Ϫ 2)) u(t Ϫ 2) ϩ (t Ϫ 3)e

؊2(tϪ3)

u(t Ϫ 3).

Now sin (pt Ϫ p) ϭ Ϫsin pt and sin (pt Ϫ 2p) ϭ sin pt, so that the first and second terms cancel each other
when t Ͼ 2. Hence we obtain f (t) ϭ 0 if 0 Ͻ t Ͻ 1, Ϫ(sin pt)> p if 1 Ͻ t Ͻ 2, 0 if 2 Ͻ t Ͻ 3, and
(t Ϫ 3)e؊2(t؊3) if t Ͼ 3. See Fig. 123.
᭿

0.3
0.2
0.1
0

0

1

2

3

4

5

t

6

Fig. 123. ƒ(t) in Example 2

EXAMPLE 3

Response of an RC-Circuit to a Single Rectangular Wave
Find the current i(t) in the RC-circuit in Fig. 124 if a single rectangular wave with voltage V0 is applied. The
circuit is assumed to be quiescent before the wave is applied.

Solution. The input is V03u(t Ϫ a) Ϫ u(t Ϫ b)4. Hence the circuit is modeled by the integro-differential
equation (see Sec. 2.9 and Fig. 124)
Ri(t) ϩ
C

v(t)

R

q(t)
C

ϭ Ri(t) ϩ

1
C

t

Ύ i(t) dt ϭ v(t) ϭ V 3u(t Ϫ a) Ϫ u(t Ϫ b)4.
0

0

v(t)

i(t)

V0

V0/R

0

a

b

t

0

a

b

Fig. 124. RC-circuit, electromotive force v(t), and current in Example 3

t

c06.qxd

10/28/10

6:33 PM

222

Page 222

CHAP. 6 Laplace Transforms
Using Theorem 3 in Sec. 6.2 and formula (1) in this section, we obtain the subsidiary equation
RI(s) ϩ

I(s)
sC

ϭ

V0
s

3e؊as Ϫ e؊bs4.

Solving this equation algebraically for I(s), we get
I(s) ϭ F(s)(e؊as Ϫ e؊bs)

where

F(s) ϭ

V0IR
s ϩ 1>(RC)

l؊1(F) ϭ

and

V0
R

e؊t>(RC),

the last expression being obtained from Table 6.1 in Sec. 6.1. Hence Theorem 1 yields the solution (Fig. 124)
i(t) ϭ l؊1(I) ϭ l؊1{e؊asF(s)} Ϫ l؊1{e؊bsF(s)} ϭ

V0
R

3e؊(t؊a)>(RC)u(t Ϫ a) Ϫ e؊(t؊b)>(RC)u(t Ϫ b)4;

that is, i(t) ϭ 0 if t Ͻ a, and
i(t) ϭ c

K 1e؊t>(RC)

if a Ͻ t Ͻ b

(K 1 Ϫ K 2)e

؊t>(RC)

if a Ͼ b

where K 1 ϭ V0ea>(RC)>R and K 2 ϭ V0eb>(RC)>R.

EXAMPLE 4

᭿

Response of an RLC-Circuit to a Sinusoidal Input Acting Over a Time Interval
Find the response (the current) of the RLC-circuit in Fig. 125, where E(t) is sinusoidal, acting for a short time
interval only, say,
E(t) ϭ 100 sin 400t if 0 Ͻ t Ͻ 2p

and

E(t) ϭ 0 if t Ͼ 2p

and current and charge are initially zero.

Solution. The electromotive force E(t) can be represented by (100 sin 400t)(1 Ϫ u(t Ϫ 2p)). Hence the
model for the current i(t) in the circuit is the integro-differential equation (see Sec. 2.9)
t

0.1i r ϩ 11i ϩ 100

Ύ i(t) dt ϭ (100 sin 400t)(1 Ϫ u(t Ϫ 2p)).

i(0) ϭ 0,

i r (0) ϭ 0.

0

From Theorems 2 and 3 in Sec. 6.2 we obtain the subsidiary equation for I(s) ϭ l(i)
0.1sI ϩ 11I ϩ 100

100 # 400s 1
e؊2ps
I
ϭ 2
a Ϫ
b.
s
s
s ϩ 4002 s

Solving it algebraically and noting that s 2 ϩ 110s ϩ 1000 ϭ (s ϩ 10)(s ϩ 100), we obtain
l(s) ϭ

s
se؊2ps
1000 # 400
Ϫ 2
a
b.
(s ϩ 10)(s ϩ 100) s 2 ϩ 4002
s ϩ 4002

For the first term in the parentheses ( Á ) times the factor in front of them we use the partial fraction
expansion
400,000s
(s ϩ 10)(s ϩ 100)(s 2 ϩ 4002)

ϭ

B
Ds ϩ K
A
ϩ
ϩ 2
.
s ϩ 10
s ϩ 100
s ϩ 4002

Now determine A, B, D, K by your favorite method or by a CAS or as follows. Multiplication by the common
denominator gives
400,000s ϭ A(s ϩ 100)(s 2 ϩ 4002) ϩ B(s ϩ 10)(s 2 ϩ 4002) ϩ (Ds ϩ K)(s ϩ 10)(s ϩ 100).

c06.qxd

10/28/10

6:33 PM

Page 223

SEC. 6.3 Unit Step Function (Heaviside Function). Second Shifting Theorem (t-Shifting)

223

We set s ϭ Ϫ10 and Ϫ100 and then equate the sums of the s 3 and s 2 terms to zero, obtaining (all values rounded)
(s ϭ Ϫ10)

Ϫ4,000,000 ϭ 90(102 ϩ 4002)A,

(s ϭ Ϫ100)

A ϭ Ϫ0.27760

Ϫ40,000,000 ϭ Ϫ90(1002 ϩ 4002)B,

B ϭ 2.6144

(s 3-terms)

0 ϭ A ϩ B ϩ D,

D ϭ Ϫ2.3368

(s 2-terms)

0 ϭ 100A ϩ 10B ϩ 110D ϩ K,

K ϭ 258.66.

Since K ϭ 258.66 ϭ 0.6467 # 400, we thus obtain for the first term I1 in I ϭ I1 Ϫ I2
I1 ϭ Ϫ

0.2776
2.6144
2.3368s
0.6467 # 400
.
ϩ
Ϫ 2
2 ϩ
s ϩ 10
s ϩ 100
s ϩ 400
s 2 ϩ 4002

From Table 6.1 in Sec. 6.1 we see that its inverse is
i 1(t) ϭ Ϫ0.2776e؊10t ϩ 2.6144e؊100t Ϫ 2.3368 cos 400t ϩ 0.6467 sin 400t.
This is the current i(t) when 0 Ͻ t Ͻ 2p. It agrees for 0 Ͻ t Ͻ 2p with that in Example 1 of Sec. 2.9 (except
for notation), which concerned the same RLC-circuit. Its graph in Fig. 63 in Sec. 2.9 shows that the exponential
terms decrease very rapidly. Note that the present amount of work was substantially less.
The second term I1 of I differs from the first term by the factor e؊2ps. Since cos 400(t Ϫ 2p) ϭ cos 400t
and sin 400(t Ϫ 2p) ϭ sin 400t, the second shifting theorem (Theorem 1) gives the inverse i 2(t) ϭ 0 if
0 Ͻ t Ͻ 2p, and for Ͼ 2p it gives
i 2(t) ϭ Ϫ0.2776e؊10(t؊2p) ϩ 2.6144e؊100(t؊2p) Ϫ 2.3368 cos 400t ϩ 0.6467 sin 400t.
Hence in i(t) the cosine and sine terms cancel, and the current for t Ͼ 2p is
i(t) ϭ Ϫ0.2776(e؊10t Ϫ e؊10(t؊2p)) ϩ 2.6144(e؊100t Ϫ e؊100(t؊2p)).

᭿

It goes to zero very rapidly, practically within 0.5 sec.
C = 10 –2 F

R = 11 Ω

L = 0.1 H

E(t)

Fig. 125. RLC-circuit in Example 4

PROBLEM SET 6.3
1. Report on Shifting Theorems. Explain and compare
the different roles of the two shifting theorems, using your
own formulations and simple examples. Give no proofs.
2–11

SECOND SHIFTING THEOREM,
UNIT STEP FUNCTION

Sketch or graph the given function, which is assumed to be
zero outside the given interval. Represent it, using unit step
functions. Find its transform. Show the details of your work.
2. t (0 Ͻ t Ͻ 2)
4. cos 4t (0 Ͻ t Ͻ p)

3. t Ϫ 2 (t Ͼ 2)

5. et (0 Ͻ t Ͻ p>2)

6. sin pt (2 Ͻ t Ͻ 4)
8. t 2 (1 Ͻ t Ͻ 2)
10. sinh t (0 Ͻ t Ͻ 2)
12–17

7. e؊pt (2 Ͻ t Ͻ 4)
9. t 2 (t Ͼ 32)
11. sin t (p>2 Ͻ t Ͻ p)

INVERSE TRANSFORMS BY THE
2ND SHIFTING THEOREM

Find and sketch or graph f (t) if l( f ) equals
12. e؊3s>(s Ϫ 1) 3
13. 6(1 Ϫ e؊ps)>(s 2 ϩ 9)
؊2s
؊5s
14. 4(e
15. e؊3s>s 4
Ϫ 2e )>s
؊s
؊3s
2
16. 2(e Ϫ e )>(s Ϫ 4)
17. (1 ϩ e؊2p(sϩ1))(s ϩ 1)>((s ϩ 1) 2 ϩ 1)

c06.qxd

10/28/10

6:33 PM

224
18–27

Page 224

CHAP. 6 Laplace Transforms

IVPs, SOME WITH DISCONTINUOUS
INPUT

Using the Laplace transform and showing the details, solve
18. 9y s Ϫ 6y r ϩ y ϭ 0, y(0) ϭ 3, y r (0) ϭ 1
19. y s ϩ 6y r ϩ 8y ϭ e؊3t Ϫ e؊5t, y(0) ϭ 0, y r (0) ϭ 0
20. y s ϩ 10y r ϩ 24y ϭ 144t 2, y(0) ϭ 19>12,
y r (0) ϭ Ϫ5
21. y s ϩ 9y ϭ 8 sin t if 0 Ͻ t Ͻ p and 0 if t Ͼ p;
y(0) ϭ 0, y r (0) ϭ 4
22. y s ϩ 3y r ϩ 2y ϭ 4t if 0 Ͻ t Ͻ 1 and 8 if t Ͼ 1;
y(0) ϭ 0, y r (0) ϭ 0
23. y s ϩ y r Ϫ 2y ϭ 3 sin t Ϫ cos t if 0 Ͻ t Ͻ 2p and
3 sin 2t Ϫ cos 2t if t Ͼ 2p; y(0) ϭ 1, y r (0) ϭ 0
24. y s ϩ 3y r ϩ 2y ϭ 1 if 0 Ͻ t Ͻ 1 and 0 if t Ͼ 1;
y(0) ϭ 0, y r (0) ϭ 0
25. y s ϩ y ϭ t if 0 Ͻ t Ͻ 1 and 0 if t Ͼ 1; y(0) ϭ 0,
y r (0) ϭ 0
26. Shifted data. y s ϩ 2y r ϩ 5y ϭ 10 sin t if 0 Ͻ t Ͻ 2p
and 0 if t Ͼ 2p; y(p) ϭ 1, y r (p) ϭ 2e؊p Ϫ 2
27. Shifted data. y s ϩ 4y ϭ 8t 2 if 0 Ͻ t Ͻ 5 and 0 if
t Ͼ 5; y(1) ϭ 1 ϩ cos 2, y r (1) ϭ 4 Ϫ 2 sin 2
28–40

MODELS OF ELECTRIC CIRCUITS

28–30

RL-CIRCUIT

31. Discharge in RC-circuit. Using the Laplace transform,
find the charge q(t) on the capacitor of capacitance C
in Fig. 127 if the capacitor is charged so that its potential
is V0 and the switch is closed at t ϭ 0.
32–34

Using the Laplace transform and showing the details, find
the current i(t) in the circuit in Fig. 128 with R ϭ 10 ⍀ and
C ϭ 10؊2 F, where the current at t ϭ 0 is assumed to be
zero, and:
32. v ϭ 0 if t Ͻ 4 and 14 # 106e؊3t V if t Ͼ 4
33. v ϭ 0 if t Ͻ 2 and 100(t Ϫ 2) V if t Ͼ 2
34. v(t) ϭ 100 V if 0.5 Ͻ t Ͻ 0.6 and 0 otherwise. Why
does i(t) have jumps?

C

R

R

v(t)

Fig. 128. Problems 32–34
35–37

Using the Laplace transform and showing the details, find
the current i(t) in the circuit in Fig. 126, assuming i(0) ϭ 0
and:
28. R ϭ 1 k⍀ (ϭ1000 ⍀), L ϭ 1 H, v ϭ 0 if 0 Ͻ t Ͻ p,
and 40 sin t V if t Ͼ p
29. R ϭ 25 ⍀, L ϭ 0.1 H, v ϭ 490 e؊5t V if 0 Ͻ t Ͻ 1
and 0 if t Ͼ 1
30. R ϭ 10 ⍀, L ϭ 0.5 H, v ϭ 200t V if 0 Ͻ t Ͻ 2 and 0
if t Ͼ 2

RC-CIRCUIT

LC-CIRCUIT

Using the Laplace transform and showing the details, find
the current i(t) in the circuit in Fig. 129, assuming zero
initial current and charge on the capacitor and:
35. L ϭ 1 H, C ϭ 10؊2 F, v ϭ Ϫ9900 cos t V if
p Ͻ t Ͻ 3p and 0 otherwise
36. L ϭ 1 H, C ϭ 0.25 F, v ϭ 200 (t Ϫ 13 t 3) V if
0 Ͻ t Ͻ 1 and 0 if t Ͼ 1
37. L ϭ 0.5 H, C ϭ 0.05 F, v ϭ 78 sin t V if 0 Ͻ t Ͻ p
and 0 if t Ͼ p

L

C

L

v(t)

v(t)

Fig. 126. Problems 28–30

Fig. 129. Problems 35–37
38–40

C

R

Fig. 127. Problem 31

RLC-CIRCUIT

Using the Laplace transform and showing the details, find
the current i(t) in the circuit in Fig. 130, assuming zero
initial current and charge and:
38. R ϭ 4 ⍀, L ϭ 1 H, C ϭ 0.05 F, v ϭ 34e؊t V if
0 Ͻ t Ͻ 4 and 0 if t Ͼ 4

c06.qxd

10/28/10

6:33 PM

Page 225

SEC. 6.4 Short Impulses. Dirac’s Delta Function. Partial Fractions
39. R ϭ 2 ⍀, L ϭ 1 H, C ϭ 0.5 F, v(t) ϭ 1 kV if
0 Ͻ t Ͻ 2 and 0 if t Ͼ 2

225

40. R ϭ 2 ⍀, L ϭ 1 H, C ϭ 0.1 F, v ϭ 255 sin t V
if 0 Ͻ t Ͻ 2p and 0 if t Ͼ 2p
30

C

20
10
R

0

L

2

4

6

8

10

12

t

–10
–20

v(t)

Fig. 131. Current in Problem 40

Fig. 130. Problems 38–40

6.4

Short Impulses. Dirac’s Delta Function.
Partial Fractions
An airplane making a “hard” landing, a mechanical system being hit by a hammerblow,
a ship being hit by a single high wave, a tennis ball being hit by a racket, and many other
similar examples appear in everyday life. They are phenomena of an impulsive nature
where actions of forces—mechanical, electrical, etc.—are applied over short intervals
of time.
We can model such phenomena and problems by “Dirac’s delta function,” and solve
them very effecively by the Laplace transform.
To model situations of that type, we consider the function
(1)

fk(t Ϫ a) ϭ b

1>k

if a Ϲ t Ϲ a ϩ k

0

otherwise

(Fig. 132)

(and later its limit as k : 0). This function represents, for instance, a force of magnitude
1>k acting from t ϭ a to t ϭ a ϩ k, where k is positive and small. In mechanics, the
integral of a force acting over a time interval a Ϲ t Ϲ a ϩ k is called the impulse of
the force; similarly for electromotive forces E(t) acting on circuits. Since the blue rectangle
in Fig. 132 has area 1, the impulse of fk in (1) is

(2)

Ik ϭ

Ύ

ؕ

fk(t Ϫ a) dt ϭ

Ύ

aϩk

a

0

1
dt ϭ 1.
k

Area = 1
1/k

a a+k

t

Fig. 132. The function ƒk(t Ϫ a) in (1)

c06.qxd

10/28/10

226

6:33 PM

Page 226

CHAP. 6 Laplace Transforms

To find out what will happen if k becomes smaller and smaller, we take the limit of fk
as k : 0 (k Ͼ 0). This limit is denoted by d(t Ϫ a), that is,
d(t Ϫ a) ϭ lim fk(t Ϫ a).
k:0

d(t Ϫ a) is called the Dirac delta function2 or the unit impulse function.
d(t Ϫ a) is not a function in the ordinary sense as used in calculus, but a so-called
generalized function.2 To see this, we note that the impulse Ik of fk is 1, so that from (1)
and (2) by taking the limit as k : 0 we obtain
(3)

d(t Ϫ a) ϭ b

ϱ

if t ϭ a

0

otherwise

and

Ύ

ؕ

d(t Ϫ a) dt ϭ 1,

0

but from calculus we know that a function which is everywhere 0 except at a single point
must have the integral equal to 0. Nevertheless, in impulse problems, it is convenient to
operate on d(t Ϫ a) as though it were an ordinary function. In particular, for a continuous
function g(t) one uses the property [often called the sifting property of d(t Ϫ a), not to
be confused with shifting]

Ύ

(4)

ؕ

g(t)d(t Ϫ a) dt ϭ g(a)

0

which is plausible by (2).
To obtain the Laplace transform of d(t Ϫ a), we write
fk(t Ϫ a) ϭ

1
3u(t Ϫ a) Ϫ u(t Ϫ (a ϩ k))4
k

and take the transform [see (2)]
l{fk(t Ϫ a)} ϭ

1 ؊as
1 Ϫ e؊ks
3e
Ϫ e؊(aϩk)s4 ϭ e؊as
.
ks
ks

We now take the limit as k : 0. By l’Hôpital’s rule the quotient on the right has the limit
1 (differentiate the numerator and the denominator separately with respect to k, obtaining
se؊ks and s, respectively, and use se؊ks>s : 1 as k : 0). Hence the right side has the
limit e؊as. This suggests defining the transform of d(t Ϫ a) by this limit, that is,
(5)

l{d(t Ϫ a)} ϭ e؊as.

The unit step and unit impulse functions can now be used on the right side of ODEs
modeling mechanical or electrical systems, as we illustrate next.
2
PAUL DIRAC (1902–1984), English physicist, was awarded the Nobel Prize [jointly with the Austrian
ERWIN SCHRÖDINGER (1887–1961)] in 1933 for his work in quantum mechanics.
Generalized functions are also called distributions. Their theory was created in 1936 by the Russian
mathematician SERGEI L’VOVICH SOBOLEV (1908–1989), and in 1945, under wider aspects, by the French
mathematician LAURENT SCHWARTZ (1915–2002).

c06.qxd

10/28/10

6:33 PM

Page 227

SEC. 6.4 Short Impulses. Dirac’s Delta Function. Partial Fractions
EXAMPLE 1

227

Mass–Spring System Under a Square Wave
Determine the response of the damped mass–spring system (see Sec. 2.8) under a square wave, modeled by
(see Fig. 133)
y s ϩ 3y r ϩ 2y ϭ r(t) ϭ u(t Ϫ 1) Ϫ u(t Ϫ 2),

y(0) ϭ 0,

y r (0) ϭ 0.

Solution. From (1) and (2) in Sec. 6.2 and (2) and (4) in this section we obtain the subsidiary equation
s 2Y ϩ 3sY ϩ 2Y ϭ

1 ؊s
(e Ϫ e؊2s).
s

Y(s) ϭ

Solution

1
(e؊s Ϫ e؊2s).
s(s 2 ϩ 3s ϩ 2)

Using the notation F(s) and partial fractions, we obtain
F(s) ϭ

1
s(s ϩ 3s ϩ 2)
2

ϭ

1
s(s ϩ 1)(s ϩ 2)

ϭ

1
2

s

Ϫ

1
sϩ1

ϩ

1
2

sϩ2

.

From Table 6.1 in Sec. 6.1, we see that the inverse is
f (t) ϭ l؊1(F) ϭ 12 Ϫ e؊t ϩ 12 e؊2t.
Therefore, by Theorem 1 in Sec. 6.3 (t-shifting) we obtain the square-wave response shown in Fig. 133,
y ϭ l؊1(F(s)e؊s Ϫ F(s)e؊2s)
ϭ f (t Ϫ 1)u(t Ϫ 1) Ϫ f (t Ϫ 2)u(t Ϫ 2)
(0 Ͻ t Ͻ 1)

0
1
2

ϭd Ϫe
Ϫe

؊(t؊1)

؊(t؊1)

ϩ

ϩe

1 ؊2(t؊1)
2e

؊(t؊2)

ϩ

(1 Ͻ t Ͻ 2)

1 ؊2(t؊1)
2e

Ϫ

1 ؊2(t؊2)
2e

(t Ͼ 2).

᭿

y(t)
1

0.5

0
0

1

2

3

4

t

Fig. 133. Square wave and response in Example 1

EXAMPLE 2

Hammerblow Response of a Mass–Spring System
Find the response of the system in Example 1 with the square wave replaced by a unit impulse at time t ϭ 1.

Solution. We now have the ODE and the subsidiary equation
y s ϩ 3y r ϩ 2y ϭ d(t Ϫ 1),

(s 2 ϩ 3s ϩ 2)Y ϭ e؊s.

and

Solving algebraically gives
Y(s) ϭ

e؊s
(s ϩ 1)(s ϩ 2)

ϭa

1
sϩ1

Ϫ

1
sϩ2

b e؊s.

By Theorem 1 the inverse is
y(t) ϭ l؊1(Y) ϭ c

0
e؊(t؊1) Ϫ e؊2(t؊1)

if 0 Ͻ t Ͻ 1
if

t Ͼ 1.

c06.qxd

10/28/10

6:33 PM

228

Page 228

CHAP. 6 Laplace Transforms
y(t) is shown in Fig. 134. Can you imagine how Fig. 133 approaches Fig. 134 as the wave becomes shorter and
shorter, the area of the rectangle remaining 1?
᭿
y(t)

0.2

0.1

0
0

1

3

t

5

Fig. 134. Response to a hammerblow in Example 2

EXAMPLE 3

Four-Terminal RLC-Network
Find the output voltage response in Fig. 135 if R ϭ 20 ⍀, L ϭ 1 H, C ϭ 10؊4 F, the input is d(t) (a unit impulse
at time t ϭ 0), and current and charge are zero at time t ϭ 0.

Solution. To understand what is going on, note that the network is an RLC-circuit to which two wires at A
and B are attached for recording the voltage v(t) on the capacitor. Recalling from Sec. 2.9 that current i(t) and
charge q(t) are related by i ϭ q r ϭ dq>dt, we obtain the model
Li r ϩ Ri ϩ

q
C

ϭ Lq s ϩ Rq r ϩ

q
C

ϭ q s ϩ 20q r ϩ 10,000q ϭ d(t).

From (1) and (2) in Sec. 6.2 and (5) in this section we obtain the subsidiary equation for Q(s) ϭ l(q)
(s 2 ϩ 20s ϩ 10,000)Q ϭ 1.

Solution

Qϭ

1
(s ϩ 10)2 ϩ 9900

.

By the first shifting theorem in Sec. 6.1 we obtain from Q damped oscillations for q and v; rounding 9900 Ϸ 99.502,
we get (Fig. 135)
q ϭ l؊1(Q) ϭ

1
99.50

␦(t)

e؊10t sin 99.50t

and

vϭ

q
C

᭿

ϭ 100.5e؊10t sin 99.50t.

v
80

R

L
C

40
0

A

B

0.05

0.1

0.15

0.2

0.25

0.3

t

–40
v(t) = ?

–80

Network

Voltage on the capacitor

Fig. 135. Network and output voltage in Example 3

More on Partial Fractions
We have seen that the solution Y of a subsidiary equation usually appears as a quotient
of polynomials Y(s) ϭ F(s)>G(s), so that a partial fraction representation leads to a sum
of expressions whose inverses we can obtain from a table, aided by the first shifting
theorem (Sec. 6.1). These representations are sometimes called Heaviside expansions.

c06.qxd

10/28/10

6:33 PM

Page 229

SEC. 6.4 Short Impulses. Dirac’s Delta Function. Partial Fractions

229

An unrepeated factor s Ϫ a in G(s) requires a single partial fraction A>(s Ϫ a).
See Examples 1 and 2. Repeated real factors (s Ϫ a)2, (s Ϫ a)3, etc., require partial
fractions
A2
(s Ϫ a)

2

ϩ

A1
sϪa

A3

,

(s Ϫ a)

3

ϩ

A2
(s Ϫ a)

2

ϩ

A1
sϪa

,

etc.,

The inverses are (A2t ϩ A1)eat, (12A3t 2 ϩ A2t ϩ A1)eat, etc.
Unrepeated complex factors (s Ϫ a)(s Ϫ a), a ϭ a ϩ ib, a ϭ a Ϫ ib, require a partial
fraction (As ϩ B)>3(s Ϫ a)2 ϩ b24. For an application, see Example 4 in Sec. 6.3.
A further one is the following.

EXAMPLE 4

Unrepeated Complex Factors. Damped Forced Vibrations
Solve the initial value problem for a damped mass–spring system acted upon by a sinusoidal force for some
time interval (Fig. 136),
y s ϩ 2y r ϩ 2y ϭ r(t), r(t) ϭ 10 sin 2t if 0 Ͻ t Ͻ p and 0 if t Ͼ p;

y(0) ϭ 1,

y r (0) ϭ Ϫ5.

Solution. From Table 6.1, (1), (2) in Sec. 6.2, and the second shifting theorem in Sec. 6.3, we obtain the
subsidiary equation
(s 2Y Ϫ s ϩ 5) ϩ 2(sY Ϫ 1) ϩ 2Y ϭ 10

2
s ϩ4
2

(1 Ϫ e؊ps).

We collect the Y-terms, (s 2 ϩ 2s ϩ 2)Y, take Ϫs ϩ 5 Ϫ 2 ϭ Ϫs ϩ 3 to the right, and solve,
Yϭ

(6)

20
(s ϩ 4)(s ϩ 2s ϩ 2)
2

2

Ϫ

20e؊ps
(s ϩ 4)(s ϩ 2s ϩ 2)
2

2

ϩ

sϪ3
s ϩ 2s ϩ 2
2

.

For the last fraction we get from Table 6.1 and the first shifting theorem
l؊1 b

(7)

sϩ1Ϫ4
(s ϩ 1)2 ϩ 1

؊t
r ϭ e (cos t Ϫ 4 sin t).

In the first fraction in (6) we have unrepeated complex roots, hence a partial fraction representation
20
(s 2 ϩ 4)(s 2 ϩ 2s ϩ 2)

ϭ

As ϩ B
s2 ϩ 4

ϩ

Ms ϩ N
s 2 ϩ 2s ϩ 2

.

Multiplication by the common denominator gives
20 ϭ (As ϩ B)(s 2 ϩ 2s ϩ 2) ϩ (Ms ϩ N)(s 2 ϩ 4).
We determine A, B, M, N. Equating the coefficients of each power of s on both sides gives the four equations
(a) 3s 34 :

0ϭAϩM

(b)

(c)

0 ϭ 2A ϩ 2B ϩ 4M

(d)

3s4 :

3s 24 :

0 ϭ 2A ϩ B ϩ N

3s 04 : 20 ϭ 2B ϩ 4N.

We can solve this, for instance, obtaining M ϭ ϪA from (a), then A ϭ B from (c), then N ϭ Ϫ3A from (b),
and finally A ϭ Ϫ2 from (d). Hence A ϭ Ϫ2, B ϭ Ϫ2, M ϭ 2, N ϭ 6, and the first fraction in (6) has the
representation
(8)

Ϫ2s Ϫ 2
s2 ϩ 4

ϩ

2(s ϩ 1) ϩ 6 Ϫ 2
(s ϩ 1)2 ϩ 1

.

Inverse transform:

Ϫ2 cos 2t Ϫ sin 2t ϩ e؊t(2 cos t ϩ 4 sin t).

c06.qxd

10/28/10

230

6:33 PM

Page 230

CHAP. 6 Laplace Transforms
The sum of this inverse and (7) is the solution of the problem for 0 Ͻ t Ͻ p, namely (the sines cancel),
y(t) ϭ 3e؊t cos t Ϫ 2 cos 2t Ϫ sin 2t

(9)

if 0 Ͻ t Ͻ p.

In the second fraction in (6), taken with the minus sign, we have the factor e؊ps, so that from (8) and the second
shifting theorem (Sec. 6.3) we get the inverse transform of this fraction for t Ͼ 0 in the form
ϩ2 cos (2t Ϫ 2p) ϩ sin (2t Ϫ 2p) Ϫ e؊(t؊p) 32 cos (t Ϫ p) ϩ 4 sin (t Ϫ p)4
ϭ 2 cos 2t ϩ sin 2t ϩ e؊(t؊p) (2 cos t ϩ 4 sin t).
The sum of this and (9) is the solution for t Ͼ p,
y(t) ϭ e؊t3(3 ϩ 2ep) cos t ϩ 4ep sin t4

(10)

if t Ͼ p.

Figure 136 shows (9) (for 0 Ͻ t Ͻ p) and (10) (for t Ͼ p), a beginning vibration, which goes to zero rapidly
because of the damping and the absence of a driving force after t ϭ p.
᭿
y(t)
2
1
y = 0 (Equilibrium
position)
y

0

π

2π

3π

4π

t

–1

Driving force
Dashpot (damping)

–2

Mechanical system

Output (solution)

Fig. 136. Example 4

The case of repeated complex factors 3(s Ϫ a)(s Ϫ a )42, which is important in connection
with resonance, will be handled by “convolution” in the next section.

PROBLEM SET 6.4
1. CAS PROJECT. Effect of Damping. Consider a
vibrating system of your choice modeled by
y s ϩ cy r ϩ ky ϭ d(t).
(a) Using graphs of the solution, describe the effect of
continuously decreasing the damping to 0, keeping k
constant.
(b) What happens if c is kept constant and k is
continuously increased, starting from 0?
(c) Extend your results to a system with two
d-functions on the right, acting at different times.
2. CAS EXPERIMENT. Limit of a Rectangular Wave.
Effects of Impulse.
(a) In Example 1 in the text, take a rectangular wave
of area 1 from 1 to 1 ϩ k. Graph the responses for a
sequence of values of k approaching zero, illustrating
that for smaller and smaller k those curves approach

the curve shown in Fig. 134. Hint: If your CAS gives
no solution for the differential equation, involving k,
take specific k’s from the beginning.
(b) Experiment on the response of the ODE in Example
1 (or of another ODE of your choice) to an impulse
d(t Ϫ a) for various systematically chosen a (Ͼ 0);
choose initial conditions y(0) 0, y r (0) ϭ 0. Also consider the solution if no impulse is applied. Is there a
dependence of the response on a? On b if you choose
bd(t Ϫ a)? Would Ϫd(t Ϫ aෂ) with aෂ Ͼ a annihilate the
effect of d(t Ϫ a)? Can you think of other questions that
one could consider experimentally by inspecting graphs?
3–12

EFFECT OF DELTA (IMPULSE)
ON VIBRATING SYSTEMS

Find and graph or sketch the solution of the IVP. Show the
details.
3. y s ϩ 4y ϭ d(t Ϫ p), y(0) ϭ 8, y r (0) ϭ 0

c06.qxd

10/28/10

6:33 PM

Page 231

SEC. 6.4 Short Impulses. Dirac’s Delta Function. Partial Fractions
4. y s ϩ 16y ϭ 4d(t Ϫ 3p), y(0) ϭ 2, y r (0) ϭ 0
5. y s ϩ y ϭ d(t Ϫ p) Ϫ d(t Ϫ 2p),
y(0) ϭ 0, y r (0) ϭ 1
6. y s ϩ 4y r ϩ 5y ϭ d(t Ϫ 1), y(0) ϭ 0, y r (0) ϭ 3
7. 4y s ϩ 24y r ϩ 37y ϭ 17eϪt ϩ d(t Ϫ 12),
y(0) ϭ 1, y r (0) ϭ 1
8. y s ϩ 3y r ϩ 2y ϭ 10(sin t ϩ d(t Ϫ 1)), y(0) ϭ 1,
y r (0) ϭ Ϫ1
9. y s ϩ 4y r ϩ 5y ϭ 31 Ϫ u(t Ϫ 10)4et Ϫ e10d(t Ϫ 10),
y(0) ϭ 0, y r (0) ϭ 1
10. y s ϩ 5y r ϩ 6y ϭ d(t Ϫ 12p) ϩ u(t Ϫ p) cos t,
y(0) ϭ 0, y r (0) ϭ 0
11. y s ϩ 5y r ϩ 6y ϭ u(t Ϫ 1) ϩ d(t Ϫ 2),
y(0) ϭ 0, y r (0) ϭ 1
12. y s ϩ 2y r ϩ 5y ϭ 25t Ϫ 100d(t Ϫ p), y(0) ϭ Ϫ2,
y r (0) ϭ 5
13. PROJECT. Heaviside Formulas. (a) Show that for
a simple root a and fraction A>(s Ϫ a) in F(s)>G(s) we
have the Heaviside formula
A ϭ lim

(s Ϫ a)F(s)
G(s)

s:a

231

Set t ϭ (n Ϫ 1)p in the nth integral. Take out e؊(n؊1)p
from under the integral sign. Use the sum formula for
the geometric series.
(b) Half-wave rectifier. Using (11), show that the
half-wave rectification of sin vt in Fig. 137 has the
Laplace transform
(s 2 ϩ v2)(1 Ϫ e؊2ps>v)
v
ϭ
.
2
2
(s ϩ v )(1 Ϫ e؊ps>v)

(A half-wave rectifier clips the negative portions of the
curve. A full-wave rectifier converts them to positive;
see Fig. 138.)
(c) Full-wave rectifier. Show that the Laplace transform of the full-wave rectification of sin vt is
v

F(s)

ϭ

Am

ps
2v

.

f (t)
1

Am؊1

ϩ

2

.

(s Ϫ a)
(s Ϫ a)m؊1
A1
ϩ s Ϫ a ϩ further fractions
m

coth

s ϩv
2

0

(b) Similarly, show that for a root a of order m and
fractions in

G(s)

v(1 ϩ e؊ps>v)

l( f ) ϭ

π /ω

2π /ω

3π /ω

t

Fig. 137. Half-wave rectification
f (t)

ϩ Á

1
0

π /ω

2π /ω

3π /ω

t

Fig. 138. Full-wave rectification
we have the Heaviside formulas for the first coefficient
Am ϭ lim

(d) Saw-tooth wave. Find the Laplace transform of the
saw-tooth wave in Fig. 139.

(s Ϫ a)mF(s)

s:a

G(s)

f (t)

and for the other coefficients

k

m
d m؊k (s Ϫ a) F(s)
1
lim
Ak ϭ
d,
c
(m Ϫ k)! s:a ds m؊k
G(s)
k ϭ 1, Á , m Ϫ 1.

0

p

2p

t

3p

Fig. 139. Saw-tooth wave

14. TEAM PROJECT. Laplace Transform of Periodic
Functions
(a) Theorem. The Laplace transform of a piecewise
continuous function f (t) with period p is

15. Staircase function. Find the Laplace transform of the
staircase function in Fig. 140 by noting that it is the
difference of kt>p and the function in 14(d).
f (t)

(11)

l( f ) ϭ

1
1 Ϫ e؊ps

p

Ύe

؊st

f (t) dt

(s Ͼ 0).

k

0

0

Prove this theorem. Hint: Write ͐0ϱ ϭ ͐0p ϩ ͐p2p ϩ Á .

p

2p

3p

Fig. 140. Staircase function

t

c06.qxd

10/28/10

6:33 PM

232

6.5

Page 232

CHAP. 6 Laplace Transforms

Convolution. Integral Equations
Convolution has to do with the multiplication of transforms. The situation is as follows.
Addition of transforms provides no problem; we know that l( f ϩ g) ϭ l( f ) ϩ l(g).
Now multiplication of transforms occurs frequently in connection with ODEs, integral
equations, and elsewhere. Then we usually know l( f ) and l(g) and would like to know
the function whose transform is the product l( f )l(g). We might perhaps guess that it
is fg, but this is false. The transform of a product is generally different from the product
of the transforms of the factors,
l( fg)

l( f )l(g)

in general.

To see this take f ϭ et and g ϭ 1. Then fg ϭ et, l( fg) ϭ 1>(s Ϫ 1), but l( f ) ϭ 1>(s Ϫ 1)
and l(1) ϭ 1>s give l( f )l(g) ϭ 1>(s 2 Ϫ s).
According to the next theorem, the correct answer is that l( f )l(g) is the transform of
the convolution of f and g, denoted by the standard notation f * g and defined by the integral
t

h(t) ϭ ( f * g)(t) ϭ

(1)

˛

Ύ f (t)g(t Ϫ t) dt.
˛

0

THEOREM 1

Convolution Theorem

If two functions f and g satisfy the assumption in the existence theorem in Sec. 6.1,
so that their transforms F and G exist, the product H ϭ FG is the transform of h
given by (1). (Proof after Example 2.)

EXAMPLE 1

Convolution
Let H(s) ϭ 1>[(s Ϫ a)s]. Find h(t).

Solution. 1>(s Ϫ a) has the inverse f (t) ϭ eat, and 1>s has the inverse g(t) ϭ 1. With f (t) ϭ eat and
g(t Ϫ t) ϵ 1 we thus obtain from (1) the answer

t

h(t) ϭ eat * 1 ϭ

Ύe

at

0

# 1 dt ϭ 1 (eat Ϫ 1).
a

To check, calculate
H(s) ϭ l(h)(s) ϭ

EXAMPLE 2

1
1
1
a
1
1 # 1
a
Ϫ bϭ # 2
ϭ
ϭ l(eat)l(1).
s
sϪa s
a sϪa
a s Ϫ as

᭿

Convolution
Let H(s) ϭ 1>(s 2 ϩ v2)2. Find h(t).

Solution. The inverse of 1>(s 2 ϩ v2) is (sin vt)>v. Hence from (1) and the first formula in (11) in App. 3.1
we obtain
h(t) ϭ

t

Ύ sin vt sin v(t Ϫ t) dt

1
sin vt sin vt
*
ϭ 2
v
v
v
ϭ

0

t

1
2

2v

Ύ [Ϫcos vt ϩ cos (2vt Ϫ vt)] dt
0

c06.qxd

10/28/10

6:33 PM

Page 233

SEC. 6.5 Convolution. Integral Equations

233
ϭ
ϭ

1
2

2v
1

2v2

c Ϫt cos vt ϩ

sin vt t
v d tϭ0

c Ϫt cos vt ϩ

sin vt
v d

᭿

in agreement with formula 21 in the table in Sec. 6.9.

PROOF

We prove the Convolution Theorem 1. CAUTION! Note which ones are the variables
of integration! We can denote them as we want, for instance, by t and p, and write

Ύ

F(s) ϭ

ؕ

e؊stf (t) dt

and

Ύ

G(s) ϭ

0

ؕ

e؊spg( p) dp.

0

We now set t ϭ p ϩ t, where t is at first constant. Then p ϭ t Ϫ t, and t varies from
t to ϱ . Thus
G(s) ϭ

Ύ

ؕ

e؊s(t؊t)g(t Ϫ t) dt ϭ est

t

Ύ

ؕ

e؊stg(t Ϫ t) dt.

t

t in F and t in G vary independently. Hence we can insert the G-integral into the
F-integral. Cancellation of e؊st and est then gives
F(s)G(s) ϭ

Ύ

ؕ

e؊stf (t)est

Ύ

ؕ

Ύ

e؊stg(t Ϫ t) dt dt ϭ

t

0

ؕ

f (t)

0

Ύ

ؕ

e؊stg(t Ϫ t) dt dt.

t

Here we integrate for fixed t over t from t to ϱ and then over t from 0 to ϱ . This is the
blue region in Fig. 141. Under the assumption on f and g the order of integration can be
reversed (see Ref. [A5] for a proof using uniform convergence). We then integrate first
over t from 0 to t and then over t from 0 to ϱ , that is,
F(s)G(s) ϭ

Ύ

ؕ

e؊st

0

Ύ

t

f (t)g(t Ϫ t) dt dt ϭ

0

Ύ

ؕ

e؊sth(t) dt ϭ l(h) ϭ H(s).

0

᭿

This completes the proof.
τ

t

Fig. 141. Region of integration in the
t␶-plane in the proof of Theorem 1

c06.qxd

10/28/10

6:33 PM

234

Page 234

CHAP. 6 Laplace Transforms

From the definition it follows almost immediately that convolution has the properties
f *g ϭ g* f

(commutative law)

f * (g1 ϩ g2) ϭ f * g1 ϩ f * g2

(distributive law)

( f * g) * v ϭ f * (g * v)

(associative law)

f *0ϭ0*fϭ0
similar to those of the multiplication of numbers. However, there are differences of which
you should be aware.
EXAMPLE 3

Unusual Properties of Convolution
f *1

f in general. For instance,
t*1ϭ

Ύ

t

0

1
t # 1 dt ϭ t 2
2

t.

( f * f )(t) м 0 may not hold. For instance, Example 2 with v ϭ 1 gives
sin t * sin t ϭ Ϫ12 t cos t ϩ 12 sin t

(Fig. 142).

᭿

4
2
0

2 4 6 8 10

t

–2
–4

Fig. 142. Example 3

We shall now take up the case of a complex double root (left aside in the last section in
connection with partial fractions) and find the solution (the inverse transform) directly by
convolution.
EXAMPLE 4

Repeated Complex Factors. Resonance
In an undamped mass–spring system, resonance occurs if the frequency of the driving force equals the natural
frequency of the system. Then the model is (see Sec. 2.8)
y s ϩ v 02 y ϭ K sin v 0 t
where v20 ϭ k>m, k is the spring constant, and m is the mass of the body attached to the spring. We assume
y(0) ϭ 0 and y r (0) ϭ 0, for simplicity. Then the subsidiary equation is
s 2Y ϩ v 02Y ϭ

Kv 0
s 2 ϩ v 02

.

Its solution is

Yϭ

Kv 0
(s 2 ϩ v 02) 2

.

c06.qxd

10/28/10

6:33 PM

Page 235

SEC. 6.5 Convolution. Integral Equations

235

This is a transform as in Example 2 with v ϭ v0 and multiplied by Kv0. Hence from Example 2 we can see
directly that the solution of our problem is
y(t) ϭ

K
Kv 0
sin v 0 t
aϪt cos v 0 t ϩ
bϭ
(Ϫv 0 t cos v 0 t ϩ sin v 0 t).
2v 02
2v 02
v0

We see that the first term grows without bound. Clearly, in the case of resonance such a term must occur. (See
᭿
also a similar kind of solution in Fig. 55 in Sec. 2.8.)

Application to Nonhomogeneous Linear ODEs
Nonhomogeneous linear ODEs can now be solved by a general method based on
convolution by which the solution is obtained in the form of an integral. To see this, recall
from Sec. 6.2 that the subsidiary equation of the ODE
y s ϩ ay r ϩ by ϭ r(t)

(2)

(a, b constant)

has the solution [(7) in Sec. 6.2]
Y(s) ϭ [(s ϩ a)y(0) ϩ y r (0)]Q(s) ϩ R(s)Q(s)
with R(s) ϭ l(r) and Q(s) ϭ 1>(s 2 ϩ as ϩ b) the transfer function. Inversion of the first
term 3 Á 4 provides no difficulty; depending on whether 14a 2 Ϫ b is positive, zero, or
negative, its inverse will be a linear combination of two exponential functions, or of the
form (c1 ϩ c2t)e؊at>2, or a damped oscillation, respectively. The interesting term is
R(s)Q(s) because r(t) can have various forms of practical importance, as we shall see. If
y(0) ϭ 0 and y r (0) ϭ 0, then Y ϭ RQ, and the convolution theorem gives the solution
t

Ύ q(t Ϫ t)r(t) dt.

y(t) ϭ

(3)

0

EXAMPLE 5

Response of a Damped Vibrating System to a Single Square Wave
Using convolution, determine the response of the damped mass–spring system modeled by
y s ϩ 3y r ϩ 2y ϭ r(t),

r(t) ϭ 1 if 1 Ͻ t Ͻ 2 and 0 otherwise,

y(0) ϭ y r (0) ϭ 0.

This system with an input (a driving force) that acts for some time only (Fig. 143) has been solved by partial
fraction reduction in Sec. 6.4 (Example 1).

Solution by Convolution. The transfer function and its inverse are
Q(s) ϭ

1
s 2 ϩ 3s ϩ 2

ϭ

1
(s ϩ 1)(s ϩ 2)

ϭ

1
sϩ1

Ϫ

1
sϩ2

,

q(t) ϭ e؊t Ϫ e؊2t.

hence

Hence the convolution integral (3) is (except for the limits of integration)
y(t) ϭ

Ύ q(t Ϫ t) # 1 dt ϭ Ύ 3e

؊(t؊t)

Ϫ e؊2(t؊t)4 dt ϭ e؊(t؊t) Ϫ

1
2

e؊2(t؊t).

Now comes an important point in handling convolution. r(t) ϭ 1 if 1 Ͻ t Ͻ 2 only. Hence if t Ͻ 1, the integral
is zero. If 1 Ͻ t Ͻ 2, we have to integrate from t ϭ 1 (not 0) to t. This gives (with the first two terms from the
upper limit)
y(t) ϭ e؊0 Ϫ 12 e؊0 Ϫ (e؊(t؊1) Ϫ 12 e؊2(t؊1)) ϭ 12 Ϫ e؊(t؊1) ϩ 12 e؊2(t؊1).

c06.qxd

11/4/10

12:22 PM

236

Page 236

CHAP. 6 Laplace Transforms
If t Ͼ 2, we have to integrate from t ϭ 1 to 2 (not to t). This gives
y(t) ϭ e؊(t؊2) Ϫ 12 e؊2(t؊2) Ϫ (e؊(t؊1) Ϫ 12 e؊2(t؊1)).
Figure 143 shows the input (the square wave) and the interesting output, which is zero from 0 to 1, then increases,
reaches a maximum (near 2.6) after the input has become zero (why?), and finally decreases to zero in a monotone
fashion.
᭿
y(t)
1
Output (response)

0.5

0
0

1

2

3

4

t

Fig. 143. Square wave and response in Example 5

Integral Equations
Convolution also helps in solving certain integral equations, that is, equations in which the
unknown function y(t) appears in an integral (and perhaps also outside of it). This concerns
equations with an integral of the form of a convolution. Hence these are special and it suffices
to explain the idea in terms of two examples and add a few problems in the problem set.
EXAMPLE 6

A Volterra Integral Equation of the Second Kind
Solve the Volterra integral equation of the second kind3
y(t) Ϫ

Ύ

t

y(t) sin (t Ϫ t) dt ϭ t.

0

Solution. From (1) we see that the given equation can be written as a convolution, y Ϫ y * sin t ϭ t. Writing
Y ϭ l(y) and applying the convolution theorem, we obtain
Y(s) Ϫ Y(s)

1
s2 ϩ 1

ϭ Y(s)

s2
s2 ϩ 1

ϭ

1
s2

.

The solution is
Y(s) ϭ

s2 ϩ 1
s

4

ϭ

1
s

2

ϩ

1
s

4

and gives the answer

y(t) ϭ t ϩ

t3
6

.

Check the result by a CAS or by substitution and repeated integration by parts (which will need patience).

EXAMPLE 7

᭿

Another Volterra Integral Equation of the Second Kind
Solve the Volterra integral equation
y(t) Ϫ

Ύ

t

(1 ϩ t) y(t Ϫ t) dt ϭ 1 Ϫ sinh t.

0

3

If the upper limit of integration is variable, the equation is named after the Italian mathematician VITO
VOLTERRA (1860–1940), and if that limit is constant, the equation is named after the Swedish mathematician
ERIK IVAR FREDHOLM (1866–1927). “Of the second kind (first kind)” indicates that y occurs (does not
occur) outside of the integral.

c06.qxd

10/28/10

6:33 PM

Page 237

SEC. 6.5 Convolution. Integral Equations

237

Solution. By (1) we can write y Ϫ (1 ϩ t) * y ϭ 1 Ϫ sinh t. Writing Y ϭ l(y), we obtain by using the
convolution theorem and then taking common denominators
1
1
1
1
,
Y(s) c 1 Ϫ a ϩ 2 b d ϭ Ϫ 2
s
s
s
s Ϫ1

s2 Ϫ s Ϫ 1
s2 Ϫ 1 Ϫ s
Y(s) #
ϭ
.
2
s
s(s 2 Ϫ 1)

hence

(s 2 Ϫ s Ϫ 1)>s cancels on both sides, so that solving for Y simply gives
Y(s) ϭ

s
s2 Ϫ 1

and the solution is

᭿

y(t) ϭ cosh t.

PROBLEM SET 6.5
1–7
CONVOLUTIONS BY INTEGRATION
Find:
1. 1 * 1
2. 1 * sin vt
t
؊t
3. e * e
4. (cos vt) * (cos vt)
5. (sin vt) * (cos vt)
6. eat * ebt (a b)
t
7. t * e
8–14
INTEGRAL EQUATIONS
Solve by the Laplace transform, showing the details:
8. y(t) ϩ 4

Ύ

16. TEAM PROJECT. Properties of Convolution. Prove:
(a) Commutativity, f * g ϭ g * f
(b) Associativity, ( f * g) * v ϭ f * (g * v)
(c) Distributivity, f * (g1 ϩ g2) ϭ f * g1 ϩ f * g2
(d) Dirac’s delta. Derive the sifting formula (4) in Sec.
6.4 by using fk with a ϭ 0 [(1), Sec. 6.4] and applying
the mean value theorem for integrals.
(e) Unspecified driving force. Show that forced
vibrations governed by

t

y s ϩ v2y ϭ r(t), y(0) ϭ K 1,

y(t)(t Ϫ t) dt ϭ 2t

0

9. y(t) Ϫ

Ύ

t

Ύ

t

Ύ

t

Ύ

t

with v 0 and an unspecified driving force r(t)
can be written in convolution form,

y(t) dt ϭ 1

0

10. y(t) Ϫ

y r (0) ϭ K 2

yϭ
y(t) sin 2(t Ϫ t) dt ϭ sin 2t

K2
1
sin vt * r(t) ϩ K 1 cos vt ϩ
sin vt.
v
v

0

11. y(t) ϩ

17–26
(t Ϫ t)y(t) dt ϭ 1

0

12. y(t) ϩ

y(t) cosh (t Ϫ t) dt ϭ t ϩ e

t

0

13. y(t) ϩ 2et

Ύ

t

y(t)e؊t dt ϭ tet

0

14. y(t) Ϫ

Ύ

t

0

1
y(t)(t Ϫ t) dt ϭ 2 Ϫ t 2
2

15. CAS EXPERIMENT. Variation of a Parameter.
(a) Replace 2 in Prob. 13 by a parameter k and
investigate graphically how the solution curve changes
if you vary k, in particular near k ϭ Ϫ2.
(b) Make similar experiments with an integral equation
of your choice whose solution is oscillating.

INVERSE TRANSFORMS
BY CONVOLUTION

Showing details, find f (t) if l( f )
5.5
17.
18.
(s ϩ 1.5)(s Ϫ 4)
2ps
19. 2
20.
(s ϩ p2)2
v
21. 2 2
22.
s (s ϩ v2)
40.5
23.
24.
s(s 2 Ϫ 9)
25.

equals:
1
(s Ϫ a)2
9
s(s ϩ 3)
e؊as
s(s Ϫ 2)
240
(s 2 ϩ 1)(s 2 ϩ 25)

18s
(s 2 ϩ 36)2

26. Partial Fractions. Solve Probs. 17, 21, and 23 by
partial fraction reduction.

c06.qxd

10/28/10

6:33 PM

238

6.6

Page 238

CHAP. 6 Laplace Transforms

Differentiation and Integration of Transforms.
ODEs with Variable Coefficients
The variety of methods for obtaining transforms and inverse transforms and their
application in solving ODEs is surprisingly large. We have seen that they include direct
integration, the use of linearity (Sec. 6.1), shifting (Secs. 6.1, 6.3), convolution (Sec. 6.5),
and differentiation and integration of functions f (t) (Sec. 6.2). In this section, we shall
consider operations of somewhat lesser importance. They are the differentiation and
integration of transforms F(s) and corresponding operations for functions f (t). We show
how they are applied to ODEs with variable coefficients.

Differentiation of Transforms
It can be shown that, if a function f(t) satisfies the conditions of the existence theorem in
Sec. 6.1, then the derivative F r (s) ϭ dF>ds of the transform F(s) ϭ l( f ) can be obtained
by differentiating F(s) under the integral sign with respect to s (proof in Ref. [GenRef4]
listed in App. 1). Thus, if
F(s) ϭ

Ύ

ؕ

e؊stf (t) dt,

F r(s) ϭ Ϫ

then

0

Ύ

ؕ

e؊stt f (t) dt.

0

Consequently, if l( f ) ϭ F(s), then
(1)

l{tf (t)} ϭ ϪF r (s),

l؊1{F r (s)} ϭ Ϫtf (t)

hence

where the second formula is obtained by applying l؊1 on both sides of the first formula.
In this way, differentiation of the transform of a function corresponds to the multiplication
of the function by Ϫt.
EXAMPLE 1

Differentiation of Transforms. Formulas 21–23 in Sec. 6.9
We shall derive the following three formulas.

l( f )
(2)
(3)
(4)

1

1

(s ϩ b )
s
2

2 2

(s 2 ϩ b2)2
s2
(s ϩ b )
2

f (t)

2 2

(sin bt Ϫ bt cos bt)
2b3
1
sin bt
2b
1
(sin bt ϩ bt cos bt)
2b

Solution. From (1) and formula 8 (with v ϭ b) in Table 6.1 of Sec. 6.1 we obtain by differentiation
(CAUTION! Chain rule!)
l(t sin bt) ϭ

2bs
(s ϩ b2)2
2

.

c06.qxd

10/30/10

12:06 AM

Page 239

SEC. 6.6 Differentiation and Integration of Transforms. ODEs with Variable Coefficients

239

Dividing by 2b and using the linearity of l, we obtain (3).
Formulas (2) and (4) are obtained as follows. From (1) and formula 7 (with v ϭ b) in Table 6.1 we find
l(t cos bt) ϭ Ϫ

(5)

(s 2 ϩ b2) Ϫ 2s 2
(s ϩ b )
2

2 2

s 2 Ϫ b2

ϭ

(s 2 ϩ b2)2

.

From this and formula 8 (with v ϭ b) in Table 6.1 we have
l at cos bt Ϯ

1
b

sin btb ϭ

s 2 Ϫ b2

Ϯ

(s 2 ϩ b2)2

1
˛

s 2 ϩ b2

.

On the right we now take the common denominator. Then we see that for the plus sign the numerator becomes
s 2 Ϫ b2 ϩ s 2 ϩ b2 ϭ 2s 2, so that (4) follows by division by 2. Similarly, for the minus sign the numerator
takes the form s 2 Ϫ b2 Ϫ s 2 Ϫ b2 ϭ Ϫ2b2, and we obtain (2). This agrees with Example 2 in Sec. 6.5. ᭿

Integration of Transforms
Similarly, if f (t) satisfies the conditions of the existence theorem in Sec. 6.1 and the limit
of f (t)>t, as t approaches 0 from the right, exists, then for s Ͼ k,
(6)

le

f (t)
f ϭ
t

Ύ

ؕ

F(sෂ) dsෂ

l؊1 e

hence

s

Ύ

ؕ

F(sෂ ) dsෂ f ϭ

s

f (t)
.
t

In this way, integration of the transform of a function f (t) corresponds to the division of
f (t) by t.
We indicate how (6) is obtained. From the definition it follows that

Ύ

ؕ

ؕ

ෂ

Ύ cΎ

ෂ

F(s ) ds ϭ

s

s

ؕ

0

e؊s tf (t) dt d dsෂ,
~

and it can be shown (see Ref. [GenRef4] in App. 1) that under the above assumptions we
may reverse the order of integration, that is,

Ύ

ؕ

ؕ

F(sෂ) dsෂ ϭ

s

ؕ

Ύ cΎ
0

s

~

ෂt
؊s

Integration of e
with respect to ෂs gives e
؊st
equals e >t. Therefore,

Ύ

ؕ
ෂ

ෂ

F(s ) ds ϭ

s

EXAMPLE 2

Ύ

e؊stf (t) dsෂ d dt ϭ

Ύ

ෂt
؊s

ؕ

e؊st

0

ؕ

0

f (t) c

Ύ

s

ؕ

e؊st dsෂ d dt.
~

>(Ϫt). Here the integral over ෂs on the right

f (t)
f (t)
dt ϭ l e
f
t
t

(s Ͼ k). ᭿

Differentiation and Integration of Transforms
Find the inverse transform of ln a1 ϩ

v2
s2

b ϭ ln

s 2 ϩ v2
s2

.

Solution. Denote the given transform by F(s). Its derivative is
F r (s) ϭ

d
ds

(ln (s 2 ϩ v2) Ϫ ln s 2) ϭ

2s
s 2 ϩ v2

Ϫ

2s
s2

.

c06.qxd

10/28/10

6:33 PM

240

Page 240

CHAP. 6 Laplace Transforms
Taking the inverse transform and using (1), we obtain
l؊{F r (s)} ϭ l؊1 e

2s
2
Ϫ f ϭ 2 cos vt Ϫ 2 ϭ Ϫtf (t2.
s 2 ϩ v2
s

Hence the inverse f (t) of F(s) is f (t) ϭ 2(1 Ϫ cos vt)>t. This agrees with formula 42 in Sec. 6.9.
Alternatively, if we let
G(s) ϭ

2s
2
Ϫ ,
s
s 2 ϩ v2

g(t) ϭ l؊1(G) Ϫ 2(cos vt Ϫ 1).

then

From this and (6) we get, in agreement with the answer just obtained,
l؊1 e ln

s 2 ϩ v2
f ϭ l؊1 e
s2

Ύ

ؕ

s

G(s) ds f ϭ Ϫ

g(t)
t

ϭ

2
(1 Ϫ cos vt2,
t

the minus occurring since s is the lower limit of integration.
In a similar way we obtain formula 43 in Sec. 6.9,
l؊1 e ln a1 Ϫ

a2
2
b f ϭ (1 Ϫ cosh at2.
t
s2

᭿

Special Linear ODEs with Variable Coefficients
Formula (1) can be used to solve certain ODEs with variable coefficients. The idea is this.
Let l(y) ϭ Y. Then l(y r ) ϭ sY Ϫ y(0) (see Sec. 6.2). Hence by (1),
l(ty r ) ϭ Ϫ

(7)

d
dY
[sY Ϫ y(0)] ϭ ϪY Ϫ s .
ds
ds

Similarly, l(y s ) ϭ s 2Y Ϫ sy(0) Ϫ y r (0) and by (1)
(8)

l(ty s ) ϭ Ϫ

d 2
dY
[s Y Ϫ sy(0) Ϫ y r (0)] ϭ Ϫ2sY Ϫ s 2
ϩ y(0).
ds
ds

Hence if an ODE has coefficients such as at ϩ b, the subsidiary equation is a first-order
ODE for Y, which is sometimes simpler than the given second-order ODE. But if the latter
has coefficients at 2 ϩ bt ϩ c, then two applications of (1) would give a second-order
ODE for Y, and this shows that the present method works well only for rather special
ODEs with variable coefficients. An important ODE for which the method is advantageous
is the following.
EXAMPLE 3

Laguerre’s Equation. Laguerre Polynomials
Laguerre’s ODE is
ty s ϩ (1 Ϫ t)y r ϩ ny ϭ 0.

(9)

We determine a solution of (9) with n ϭ 0, 1, 2, Á . From (7)–(9) we get the subsidiary equation
2
c Ϫ2sY Ϫ s

dY
ds

ϩ y(0) d ϩ sY Ϫ y(0) Ϫ aϪY Ϫ s

dY
ds

b ϩ nY ϭ 0.

c06.qxd

10/28/10

6:33 PM

Page 241

SEC. 6.6 Differentiation and Integration of Transforms. ODEs with Variable Coefficients

241

Simplification gives
(s Ϫ s 2)

dY
ds

ϩ (n ϩ 1 Ϫ s)Y ϭ 0.

Separating variables, using partial fractions, integrating (with the constant of integration taken to be zero), and
taking exponentials, we get
(10*)

n
dY
nϩ1Ϫs
nϩ1
ds ϭ a
b ds
ϭϪ
Ϫ
s
Y
sϪ1
s Ϫ s2

Yϭ

and

(s Ϫ 1)n
s nϩ1

.

We write l n ϭ l؊1(Y) and prove Rodrigues’s formula
l 0 ϭ 1,

(10)

l n(t) ϭ

et d n
n! dt n

(t ne؊t),

n ϭ 1, 2, Á .

These are polynomials because the exponential terms cancel if we perform the indicated differentiations. They
are called Laguerre polynomials and are usually denoted by L n (see Problem Set 5.7, but we continue to reserve
capital letters for transforms). We prove (10). By Table 6.1 and the first shifting theorem (s-shifting),
l(t ne؊t) ϭ

n!
(s ϩ 1)

nϩ1

,

le

hence by (3) in Sec. 6.2

dn
dt

n

(t ne؊t) f ϭ

n!s n
(s ϩ 1)nϩ1

because the derivatives up to the order n Ϫ 1 are zero at 0. Now make another shift and divide by n! to get
[see (10) and then (10*)]
l(l n) ϭ

(s Ϫ 1)n
s nϩ1

᭿

ϭ Y.

PROBLEM SET 6.6
1. REVIEW REPORT. Differentiation and Integration
of Functions and Transforms. Make a draft of these
four operations from memory. Then compare your draft
with the text and write a 2- to 3-page report on these
operations and their significance in applications.
2–11

TRANSFORMS BY DIFFERENTIATION

Showing the details of your work, find l( f ) if f (t) equals:
2. 3t sinh 4t
3. 12 te؊3t
4. te؊t cos t
5. t cos vt
6. t 2 sin 3t
7. t 2 cosh 2t
8. te؊kt sin t
9. 12t 2 sin pt
10. t nekt
11. 4t cos 12 pt
12. CAS PROJECT. Laguerre Polynomials. (a) Write a
CAS program for finding l n(t) in explicit form from (10).
Apply it to calculate l 0, Á , l 10. Verify that l 0, Á , l 10
satisfy Laguerre’s differential equation (9).

(b) Show that
(Ϫ1)m n m
a bt
m
mϭ0 m!
n

l n(t) ϭ a

and calculate l 0, Á , l 10 from this formula.
(c) Calculate l 0, Á , l 10 recursively from l 0 ϭ 1, l 1 ϭ
1 Ϫ t by
(n ϩ 1)l nϩ1 ϭ (2n ϩ 1 Ϫ t)l n Ϫ nl n؊1.
(d) A generating function (definition in Problem Set
5.2) for the Laguerre polynomials is
ؕ

n
؊1 tx>(x؊1)
.
a l n(t)x ϭ (1 Ϫ x) e
nϭ0

Obtain l 0, Á , l 10 from the corresponding partial sum
of this power series in x and compare the l n with those
in (a), (b), or (c).
13. CAS EXPERIMENT. Laguerre Polynomials. Experiment with the graphs of l 0, Á , l 10, finding out
empirically how the first maximum, first minimum, Á
is moving with respect to its location as a function of
n. Write a short report on this.

c06.qxd

10/28/10

6:33 PM

242

Page 242

CHAP. 6 Laplace Transforms

14–20
INVERSE TRANSFORMS
Using differentiation, integration, s-shifting, or convolution,
and showing the details, find f (t) if l( f ) equals:
s
14. 2
(s ϩ 16)2
s
15. 2
(s Ϫ 9)2

6.7

2s ϩ 6

16.

(s ϩ 6s ϩ 10)2
s
17. ln
sϪ1
2

19. ln

s2 ϩ 1
(s Ϫ 1)

2

s
18. arccot p
20. ln

sϩa
sϩb

Systems of ODEs
The Laplace transform method may also be used for solving systems of ODEs, as we shall
explain in terms of typical applications. We consider a first-order linear system with
constant coefficients (as discussed in Sec. 4.1)
y1r ϭ a11y1 ϩ a12y2 ϩ g1(t)

(1)

y2r ϭ a21y1 ϩ a22y2 ϩ g2(t).

Writing Y1 ϭ l( y1), Y2 ϭ l( y2), G1 ϭ l(g1), G2 ϭ l(g2), we obtain from (1) in Sec. 6.2
the subsidiary system
˛

˛˛

sY1 Ϫ y1(0) ϭ a11Y1 ϩ a12Y2 ϩ G1(s)
sY2 Ϫ y2(0) ϭ a21Y1 ϩ a22Y2 ϩ G2(s).
By collecting the Y1- and Y2-terms we have
(2)

(a11 Ϫ s)Y1 ϩ
a21Y1

a12Y2

ϭ Ϫy1(0) Ϫ G1(s)

ϩ (a22 Ϫ s)Y2 ϭ Ϫy2(0) Ϫ G2(s).

By solving this system algebraically for Y1(s),Y2(s) and taking the inverse transform we
obtain the solution y1 ϭ l؊1(Y1), y2 ϭ l؊1(Y2) of the given system (1).
Note that (1) and (2) may be written in vector form (and similarly for the systems in
the examples); thus, setting y ϭ 3y1 y24T, A ϭ 3ajk4, g ϭ 3g1 g24T, Y ϭ 3Y1 Y24T,
G ϭ 3G1 G24T we have
y r ϭ Ay ϩ g
EXAMPLE 1

and

(A Ϫ sI)Y ϭ Ϫy(0) Ϫ G.

Mixing Problem Involving Two Tanks
Tank T1 in Fig. 144 initially contains 100 gal of pure water. Tank T2 initially contains 100 gal of water in which
150 lb of salt are dissolved. The inflow into T1 is 2 gal>min from T2 and 6 gal>min containing 6 lb of salt from
the outside. The inflow into T2 is 8 gal/min from T1. The outflow from T2 is 2 ϩ 6 ϭ 8 gal>min, as shown in
the figure. The mixtures are kept uniform by stirring. Find and plot the salt contents y1(t) and y2(t) in T1 and
T2, respectively.

c06.qxd

10/30/10

1:52 AM

Page 243

SEC. 6.7 Systems of ODEs

243

Solution. The model is obtained in the form of two equations
Time rate of change ϭ Inflow>min Ϫ Outflow>min
for the two tanks (see Sec. 4.1). Thus,
8
2
y1r ϭ Ϫ 100
y1 ϩ 100
y2 ϩ 6.

8
8
y2r ϭ 100
y1 Ϫ 100
y2.

The initial conditions are y1(0) ϭ 0, y2(0) ϭ 150. From this we see that the subsidiary system (2) is
(Ϫ0.08 Ϫ s)Y1 ϩ
0.08Y1

ϭϪ

0.02Y2

6
s

ϩ (Ϫ0.08 Ϫ s)Y2 ϭ Ϫ150.

We solve this algebraically for Y1 and Y2 by elimination (or by Cramer’s rule in Sec. 7.7), and we write the
solutions in terms of partial fractions,
Y1 ϭ
Y2 ϭ

9s ϩ 0.48
s(s ϩ 0.12)(s ϩ 0.04)
150s 2 ϩ 12s ϩ 0.48
s(s ϩ 0.12)(s ϩ 0.04)

ϭ

100

ϭ

100

s
s

62.5

Ϫ
ϩ

Ϫ

s ϩ 0.12
125

Ϫ

s ϩ 0.12

37.5
s ϩ 0.04
75
s ϩ 0.04

.

By taking the inverse transform we arrive at the solution
y1 ϭ 100 Ϫ 62.5e؊0.12t Ϫ 37.5e؊0.04t
y2 ϭ 100 ϩ 125e؊0.12t Ϫ 75e؊0.04t.
Figure 144 shows the interesting plot of these functions. Can you give physical explanations for their main
features? Why do they have the limit 100? Why is y2 not monotone, whereas y1 is? Why is y1 from some time
on suddenly larger than y2? Etc.
᭿
6 gal/min

y(t)
150
2 gal/min

Salt content in T2
100

T1

8 gal/min

T2
50

6 gal/min

Salt content in T1
50

100

150

200

t

Fig. 144. Mixing problem in Example 1

Other systems of ODEs of practical importance can be solved by the Laplace transform
method in a similar way, and eigenvalues and eigenvectors, as we had to determine them
in Chap. 4, will come out automatically, as we have seen in Example 1.
EXAMPLE 2

Electrical Network
Find the currents i 1(t) and i 2(t) in the network in Fig. 145 with L and R measured in terms of the usual units
(see Sec. 2.9), v(t) ϭ 100 volts if 0 Ϲ t Ϲ 0.5 sec and 0 thereafter, and i(0) ϭ 0, i r (0) ϭ 0.

Solution. The model of the network is obtained from Kirchhoff’s Voltage Law as in Sec. 2.9. For the lower
circuit we obtain
0.8i 1r ϩ 1(i 1 Ϫ i 2) ϩ 1.4i 1 ϭ 100[1 Ϫ u(t Ϫ 12 )]

c06.qxd

10/28/10

244

6:33 PM

Page 244

CHAP. 6 Laplace Transforms
L2 = 1 H

i2

i(t)
30

R1 = 1 Ω
i1
L1 = 0.8 H

i1(t)

20

i2(t)

10

R2 = 1.4 Ω

0
0

v(t)

0.5

1

1.5 2
Currents

2.5

3

t

Network

Fig. 145. Electrical network in Example 2

and for the upper
1 # i 2r ϩ 1(i 2 Ϫ i 1)

ϭ 0.

Division by 0.8 and ordering gives for the lower circuit
i 1r ϩ 3i 1 Ϫ 1.25i 2 ϭ 125[1 Ϫ u(t Ϫ 12 )]
and for the upper
i 2r Ϫ i 1 ϩ

i 2 ϭ 0.

With i 1(0) ϭ 0, i 2(0) ϭ 0 we obtain from (1) in Sec. 6.2 and the second shifting theorem the subsidiary
system
1
e؊s>2
(s ϩ 3)I1 Ϫ 1.25I2 ϭ 125 a Ϫ
b
s
s
ϪI1 ϩ (s ϩ 1)I2 ϭ 0.
Solving algebraically for I1 and I2 gives
I1 ϭ
I2 ϭ

125(s ϩ 1)
s(s ϩ 12 )(s ϩ 72 )
125
s(s ϩ 12 )(s ϩ 72 )

(1 Ϫ e؊s>2),
(1 Ϫ e؊s>2).

The right sides, without the factor 1 Ϫ e؊s>2, have the partial fraction expansions
500
7s

Ϫ

125
3(s ϩ

1
2)

Ϫ

625
21(s ϩ 72 )

and
500
7s

Ϫ

250
3(s ϩ

1
2)

ϩ

250
21(s ϩ 72 )

,

respectively. The inverse transform of this gives the solution for 0 Ϲ t Ϲ 12 ,
؊t>2
؊7t>2
i 1(t) ϭ Ϫ 125
Ϫ 625
ϩ 500
3 e
21 e
7
؊t>2
؊7t>2
i 2(t) ϭ Ϫ 250
ϩ 250
ϩ 500
3 e
21 e
7

(0 Ϲ t Ϲ 12 ).

c06.qxd

10/28/10

6:33 PM

Page 245

SEC. 6.7 Systems of ODEs

245

According to the second shifting theorem the solution for t Ͼ 12 is i 1(t) Ϫ i 1(t Ϫ 12 ) and i 2(t) Ϫ i 2(t Ϫ 12 ), that is,
1>4 ؊t>2
7>4 ؊7t>2
i 1(t) ϭ Ϫ 125
)e
Ϫ 625
)e
3 (1 Ϫ e
21 (1 Ϫ e
1>4 ؊t>2
7>4 ؊7t>2
i 2(t) ϭ Ϫ 250
)e
ϩ 250
)e
3 (1 Ϫ e
21 (1 Ϫ e

(t Ͼ 12 ).

Can you explain physically why both currents eventually go to zero, and why i 1(t) has a sharp cusp whereas
i 2(t) has a continuous tangent direction at t ϭ 12?
᭿

Systems of ODEs of higher order can be solved by the Laplace transform method in a
similar fashion. As an important application, typical of many similar mechanical systems,
we consider coupled vibrating masses on springs.

k
m1 = 1

0
y1

k
m2 = 1

0
y2

k

Fig. 146. Example 3

EXAMPLE 3

Model of Two Masses on Springs (Fig. 146)
The mechanical system in Fig. 146 consists of two bodies of mass 1 on three springs of the same spring constant
k and of negligibly small masses of the springs. Also damping is assumed to be practically zero. Then the model
of the physical system is the system of ODEs
y s1 ϭ Ϫky1 ϩ k(y2 Ϫ y1)
(3)

y s2 ϭ Ϫk(y2 Ϫ y1) Ϫ ky2.

Here y1 and y2 are the displacements of the bodies from their positions of static equilibrium. These ODEs
follow from Newton’s second law, Mass ϫ Acceleration ϭ Force, as in Sec. 2.4 for a single body. We again
regard downward forces as positive and upward as negative. On the upper body, Ϫky1 is the force of the
upper spring and k(y2 Ϫ y1) that of the middle spring, y2 Ϫ y1 being the net change in spring length—think
this over before going on. On the lower body, Ϫk(y2 Ϫ y1) is the force of the middle spring and Ϫky2 that
of the lower spring.
We shall determine the solution corresponding to the initial conditions y1(0) ϭ 1, y2(0) ϭ 1, y1r (0) ϭ 23k,
y r2(0) ϭ Ϫ 23k. Let Y1 ϭ l(y1) and Y2 ϭ l(y2). Then from (2) in Sec. 6.2 and the initial conditions we obtain
the subsidiary system
s 2Y1 Ϫ s Ϫ 23k ϭ ϪkY1 ϩ k(Y2 Ϫ Y1)
s 2Y2 Ϫ s ϩ 23k ϭ Ϫk(Y2 Ϫ Y1) Ϫ kY2.
This system of linear algebraic equations in the unknowns Y1 and Y2 may be written
(s 2 ϩ 2k)Y1 Ϫ
Ϫky1

kY2

ϭ s ϩ 23k

ϩ (s ϩ 2k)Y2 ϭ s Ϫ 23k.
2

c06.qxd

10/28/10

246

6:33 PM

Page 246

CHAP. 6 Laplace Transforms
Elimination (or Cramer’s rule in Sec. 7.7) yields the solution, which we can expand in terms of partial fractions,
Y1 ϭ

(s ϩ 23k)(s 2 ϩ 2k) ϩ k(s Ϫ 23k)
(s ϩ 2k) Ϫ k
2

2

2

(s ϩ 2k)(s Ϫ 23k) ϩ k(s ϩ 23k)

ϭ

s
s ϩk
2

2

Y2 ϭ

(s 2 ϩ 2k) 2 Ϫ k 2

ϭ

s
s2 ϩ k

ϩ

Ϫ

23k
s ϩ 3k
2

23k
s 2 ϩ 3k

.

Hence the solution of our initial value problem is (Fig. 147)
y1(t) ϭ l؊1(Y1) ϭ cos 2kt ϩ sin 23kt
y2(t) ϭ l؊1(Y2) ϭ cos 2kt Ϫ sin 23kt.
We see that the motion of each mass is harmonic (the system is undamped!), being the superposition of a “slow”
oscillation and a “rapid” oscillation.
᭿

2

y1(t)

y2(t)

1
2π

0

4π

t

–1
–2

Fig. 147. Solutions in Example 3

PROBLEM SET 6.7
1. TEAM PROJECT. Comparison of Methods for
Linear Systems of ODEs
(a) Models. Solve the models in Examples 1 and 2 of
Sec. 4.1 by Laplace transforms and compare the amount
of work with that in Sec. 4.1. Show the details of your
work.
(b) Homogeneous Systems. Solve the systems (8),
(11)–(13) in Sec. 4.3 by Laplace transforms. Show the
details.
(c) Nonhomogeneous System. Solve the system (3) in
Sec. 4.6 by Laplace transforms. Show the details.
2–15
SYSTEMS OF ODES
Using the Laplace transform and showing the details of
your work, solve the IVP:
2. y1r ϩ y2 ϭ 0, y1 ϩ y2r ϭ 2 cos t,
y1(0) ϭ 1, y2(0) ϭ 0
3. y1r ϭ Ϫy1 ϩ 4y2, y2r ϭ 3y1 Ϫ 2y2,
y1(0) ϭ 3, y2(0) ϭ 4
4. y1r ϭ 4y2 Ϫ 8 cos 4t, y2r ϭ Ϫ3y1 Ϫ 9 sin 4t,
y1(0) ϭ 0, y2(0) ϭ 3

5. y1r ϭ y2 ϩ 1 Ϫ u(t Ϫ 1), y2r ϭ Ϫy1 ϩ 1 Ϫ u(t Ϫ 1),
y1(0) ϭ 0, y2(0) ϭ 0
6. y1r ϭ 5y1 ϩ y2, y2r ϭ y1 ϩ 5y2,
y1(0) ϭ 1, y2(0) ϭ Ϫ3
7. y1r ϭ 2y1 Ϫ 4y2 ϩ u(t Ϫ 1)et,
y2r ϭ y1 Ϫ 3y2 ϩ u(t Ϫ 1)et, y1(0) ϭ 3, y2(0) ϭ 0
8. y1r ϭ Ϫ2y1 ϩ 3y2, y2r ϭ 4y1 Ϫ y2,
y1(0) ϭ 4, y2(0) ϭ 3
9. y1r ϭ 4y1 ϩ y2,
y2(0) ϭ 1

y2r ϭ Ϫy1 ϩ 2y2, y1(0) ϭ 3,

10. y1r ϭ Ϫy2, y2r ϭ Ϫy1 ϩ 2[1 Ϫ u(t Ϫ 2p)] cos t,
y1(0) ϭ 1, y2(0) ϭ 0
11. y1s ϭ y1 ϩ 3y2, y2s ϭ 4y1 Ϫ 4et,
y1(0) ϭ 2, y1r (0) ϭ 3, y2(0) ϭ 1,

y2r (0) ϭ 2

12. y1s ϭ Ϫ2y1 ϩ 2y2, y2s ϭ 2y1 Ϫ 5y2,
y1(0) ϭ 1, y1r (0) ϭ 0, y2(0) ϭ 3, y2r (0) ϭ 0
13. y1s ϩ y2 ϭ Ϫ101 sin 10t, y2s ϩ y1 ϭ 101 sin 10t,
y1(0) ϭ 0, y1r (0) ϭ 6, y2(0) ϭ 8, y2r (0) ϭ Ϫ6

c06.qxd

10/28/10

6:33 PM

Page 247

SEC. 6.7 Systems of ODEs
14. 4y1r ϩ y2r Ϫ 2y3r ϭ 0, Ϫ2y1r ϩ y3r ϭ 1,
2y2r Ϫ 4y3r ϭ Ϫ16t
y1(0) ϭ 2, y2(0) ϭ 0, y3(0) ϭ 0
15. y1r ϩ y2r ϭ 2 sinh t, y2r ϩ y3r ϭ et,
y3r ϩ y1r ϭ 2et ϩ e؊t, y1(0) ϭ 1, y2(0) ϭ 1,
y3(0) ϭ 0

247
will the currents practically reach their steady state?
4Ω

8Ω

i1

i2
8Ω

v(t)

FURTHER APPLICATIONS
16. Forced vibrations of two masses. Solve the model in
Example 3 with k ϭ 4 and initial conditions y1(0) ϭ 1,
y1r (0) ϭ 1, y2(0) ϭ 1, y2r ϭ Ϫ1 under the assumption
that the force 11 sin t is acting on the first body and the
force Ϫ11 sin t on the second. Graph the two curves
on common axes and explain the motion physically.
17. CAS Experiment. Effect of Initial Conditions. In
Prob. 16, vary the initial conditions systematically,
describe and explain the graphs physically. The great
variety of curves will surprise you. Are they always
periodic? Can you find empirical laws for the changes
in terms of continuous changes of those conditions?
18. Mixing problem. What will happen in Example 1 if
you double all flows (in particular, an increase to
12 gal>min containing 12 lb of salt from the outside),
leaving the size of the tanks and the initial conditions
as before? First guess, then calculate. Can you relate
the new solution to the old one?
19. Electrical network. Using Laplace transforms,
find the currents i 1(t) and i 2(t) in Fig. 148, where
v(t) ϭ 390 cos t and i 1(0) ϭ 0, i 2(0) ϭ 0. How soon

2H

4H
Network

i(t)
40

i1(t)

20

i2(t)

0

2

4

6

8

10

t

–20
–40
Currents

Fig. 148. Electrical network and
currents in Problem 19
20. Single cosine wave. Solve Prob. 19 when the EMF
(electromotive force) is acting from 0 to 2p only. Can
you do this just by looking at Prob. 19, practically
without calculation?

c06.qxd

10/28/10

248

6.8

6:33 PM

Page 248

CHAP. 6 Laplace Transforms

Laplace Transform: General Formulas
Formula

Ύ

F(s) ϭ l{ f (t)} ϭ

Name, Comments

Sec.

ؕ

e؊stf (t) dt

Definition of Transform

0

6.1

f (t) ϭ l؊1{F(s)}

Inverse Transform

l{af (t) ϩ bg(t)} ϭ al{ f (t)} ϩ bl{g(t)}

Linearity

6.1

s-Shifting
(First Shifting Theorem)

6.1

l{eatf (t)} ϭ F(s Ϫ a)
l؊1{F(s Ϫ a)} ϭ eatf (t)
l( f r ) ϭ sl( f ) Ϫ f (0)
l( f s ) ϭ s 2l( f ) Ϫ sf (0) Ϫ f r (0)

Differentiation of Function

l( f (n)) ϭ s nl( f ) Ϫ s (n؊1)f (0) Ϫ Á
Á Ϫf
le

6.2

(n؊1)

(0)

t

Ύ f (t) dtf ϭ 1s l( f )

Integration of Function

0

t

( f * g)(t) ϭ

Ύ f (t)g(t Ϫ t) dt
0
t

ϭ

Ύ f (t Ϫ t)g(t) dt

Convolution

6.5

t-Shifting
(Second Shifting Theorem)

6.3

0

l( f * g) ϭ l( f )l(g)
l{ f (t Ϫ a) u(t Ϫ a)} ϭ e؊asF(s)
˛

؊1

l

{e؊asF (s)} ϭ f (t Ϫ a) u(t Ϫ a)
l{tf (t)} ϭ ϪF r (s)
le

f (t)

l( f ) ϭ

t

f ϭ

Ύ

Differentiation of Transform

ؕ

F( ෂ
s ) dෂ
s

6.6
Integration of Transform

s

1
1 Ϫ e؊ps

Ύ

0

p

e؊stf (t) dt

f Periodic with Period p

6.4
Project
16

c06.qxd

10/28/10

6:33 PM

Page 249

SEC. 6.9 Table of Laplace Transforms

6.9

249

Table of Laplace Transforms
For more extensive tables, see Ref. [A9] in Appendix 1.
F (s) ϭ l{ f (t)}

f (t)

˛

1
2
3
4
5
6
7
8
9
10

11
12

13
14
15
16
17
18

19
20

1>s
1>s 2
1>s n
1> 1s
1>s 3>2
1>s a

1
t
t n؊1>(n Ϫ 1)!
1> 1pt
21t> p
t a؊1>⌫(a)

(n ϭ 1, 2, Á )

(a Ͼ 0)

1
sϪa
1

teat

(s Ϫ a)

n

1
(s Ϫ a)

k

(n ϭ 1, 2, Á )

1
t n؊1eat
(n Ϫ 1)!

(k Ͼ 0)

1 k؊1 at
t
e
⌫(k)

1
(s Ϫ a)(s Ϫ b)
s
(s Ϫ a)(s Ϫ b)
1
s ϩv
s

b)

(a

b)

1
(eat Ϫ ebt)
aϪb
1
(aeat Ϫ bebt)
aϪb

cos vt

s 2 ϩ v2
1
s Ϫa
s

(a

1
sinh at
a

2

cosh at

s2 Ϫ a2
1
(s Ϫ a)2 ϩ v2
sϪa
(s Ϫ a) ϩ v
2

2

eat cos vt

s(s ϩ v )

v2

1

1

s 2(s 2 ϩ v2)

v3

2

t 6.1

1 at
e sinh vt
v

1

1
2

t 6.1

1
sin vt
v

2

2

t 6.1

eat

(s Ϫ a)2
1

2

Sec.

(1 Ϫ cos vt)

x 6.2
(vt Ϫ sin vt)

(continued )

c06.qxd

10/28/10

250

6:33 PM

Page 250

CHAP. 6 Laplace Transforms
Table of Laplace Transforms (continued )

F (s) ϭ l{ f (t)}
21
22
23
24

25
26
27
28

29
30
31

32
33
34
35
36
37

1

(sin vt Ϫ vt cos vt)
2v3
t
sin vt
2v

(s ϩ v )
s

2 2

(s 2 ϩ v2) 2
s2
2 2

(s 2 ϩ a 2)(s 2 ϩ b 2)

(a 2

1

b 2)

1
b 2 Ϫ a2
1

s ϩ 4k
s
4

4

4k 3
1

s 4 ϩ 4k 4
1

2k 2
1

s4 Ϫ k 4
s

2k 3
1

s4 Ϫ k 4

2k 2

1s Ϫ a Ϫ 1s Ϫ b
1
1s ϩ a 1s ϩ b
1

s

(k Ͼ 0)

1 ؊k>s
e
s
1 ؊k>s
e
1s
1

(sinh kt Ϫ sin kt)
(cosh kt Ϫ cos kt)

1
22pt 3

(ebt Ϫ eat)

e؊(aϩb)t>2I0 a

aϪb
tb
2

eat(1 ϩ 2at)
k؊1>2

Ik؊1>2(at)

I 5.5

u(t Ϫ a)
d(t Ϫ a)

6.3
6.4

J0(2 1kt)

J 5.4

1pt
1
1pk
(k Ͼ 0)

I 5.5
J 5.4

1p t
a b
⌫(k) 2a

1

ek>s

e؊k1s

sin kt sinh kt

1pt

e؊as>s
e؊as

3>2

(sin kt cos kt Ϫ cos kt sinh kt)

1
3>2

(s 2 Ϫ a 2)k

(cos at Ϫ cos bt)

J0(at)

2s ϩ a 2
2

(s Ϫ a)
1

t 6.6

1
(sin vt ϩ vt cos vt)
2v

(s ϩ v )
s
2

s
39

1

2

38

Sec.

f (t)

˛

cos 2 1kt
sinh 2 1kt

k
22pt

e؊k

>4t

2

3

(continued )

c06.qxd

10/28/10

6:33 PM

Page 251

Chapter 6 Review Questions and Problems

251
Table of Laplace Transforms (continued )

F (s) ϭ l{ f (t)}

f (t)

˛

40

1
ln s
s

41

ln

42

ln

43

ln

Sec.

Ϫln t Ϫ g (g Ϸ 0.5772)

sϪa
sϪb

1 bt
(e Ϫ eat)
t

s 2 ϩ v2

2
(1 Ϫ cos vt)
t

s2
s2 Ϫ a2
s

2

2
(1 Ϫ cosh at)
t

v
s

1
sin vt
t

44

arctan

45

1
arccot s
s

g 5.5

6.6

App.
A3.1

Si(t)

CHAPTER 6 REVIEW QUESTIONS AND PROBLEMS
1. State the Laplace transforms of a few simple functions
from memory.
2. What are the steps of solving an ODE by the Laplace
transform?
3. In what cases of solving ODEs is the present method
preferable to that in Chap. 2?
4. What property of the Laplace transform is crucial in
solving ODEs?
5. Is l{ f (t) ϩ g(t)} ϭ l{ f (t)} ϩ l{g(t)}?
l{ f (t)g(t)} ϭ l{ f (t)}l{g(t)}? Explain.
6. When and how do you use the unit step function and
Dirac’s delta?
7. If you know f (t) ϭ l؊1{F(s)}, how would you find
l؊1{F(s)>s 2 } ?
8. Explain the use of the two shifting theorems from memory.
9. Can a discontinuous function have a Laplace transform?
Give reason.
10. If two different continuous functions have transforms,
the latter are different. Why is this practically important?
11–19
LAPLACE TRANSFORMS
Find the transform, indicating the method used and showing
the details.
11. 5 cosh 2t Ϫ 3 sinh t
12. e؊t(cos 4t Ϫ 2 sin 4t)
1
13. sin2 (2pt)
14. 16t 2u(t Ϫ 14)

15. et>2u(t Ϫ 3)
17. t cos t ϩ sin t
19. 12t * e؊3t

16. u(t Ϫ 2p) sin t
18. (sin vt) * (cos vt)

20–28
INVERSE LAPLACE TRANSFORM
Find the inverse transform, indicating the method used and
showing the details:
7.5
s ϩ 1 ؊s
20. 2
21.
e
s Ϫ 2s Ϫ 8
s2
22.
24.

1
16
1
2

s ϩsϩ
s 2 Ϫ 6.25
2

(s 2 ϩ 6.25)2
2s Ϫ 10 ؊5s
26.
e
s3
3s
28. 2
s Ϫ 2s ϩ 2

23.
25.
27.

v cos u ϩ s sin u
s 2 ϩ v2
6(s ϩ 1)
s4
3s ϩ 4
s 2 ϩ 4s ϩ 5

29–37
ODEs AND SYSTEMS
Solve by the Laplace transform, showing the details and
graphing the solution:
29. y s ϩ 4y r ϩ 5y ϭ 50t, y(0) ϭ 5, y r (0) ϭ Ϫ5
30. y s ϩ 16y ϭ 4d(t Ϫ p), y(0) ϭ Ϫ1, y r (0) ϭ 0

c06.qxd

10/28/10

6:33 PM

Page 252

252

CHAP. 6 Laplace Transforms

31. y s Ϫ y r Ϫ 2y ϭ 12u(t Ϫ p) sin t, y(0) ϭ 1,
y r (0) ϭ Ϫ1
32. y s ϩ 4y ϭ d(t Ϫ p) Ϫ d(t Ϫ 2p), y(0) ϭ 1,
y r (0) ϭ 0
33. y s ϩ 3y r ϩ 2y ϭ 2u(t Ϫ 2), y(0) ϭ 0, y r (0) ϭ 0
34. y1r ϭ y2, y2r ϭ Ϫ4y1 ϩ d(t Ϫ p), y1(0) ϭ 0,
y2(0) ϭ 0
35. y1r ϭ 2y1 Ϫ 4y2, y2r ϭ y1 Ϫ 3y2, y1(0) ϭ 3,
y2(0) ϭ 0
36. y1r ϭ 2y1 ϩ 4y2, y2r ϭ y1 ϩ 2y2, y1(0) ϭ Ϫ4,
y2(0) ϭ Ϫ4
37. y1r ϭ y2 ϩ u(t Ϫ p), y2r ϭ Ϫy1 ϩ u(t Ϫ 2p),
y1(0) ϭ 1, y2(0) ϭ 0
38–45

MASS–SPRING SYSTEMS, CIRCUITS,
NETWORKS

Model and solve by the Laplace transform:
38. Show that the model of the mechanical system in
Fig. 149 (no friction, no damping) is

42. Find and graph the charge q(t) and the current i(t) in
the LC-circuit in Fig. 151, assuming L ϭ 1 H, C ϭ 1 F,
v(t) ϭ 1 Ϫ e؊t if 0 Ͻ t Ͻ p, v(t) ϭ 0 if t Ͼ p, and
zero initial current and charge.
43. Find the current i(t) in the RLC-circuit in Fig. 152, where
R ϭ 160 ⍀, L ϭ 20 H, C ϭ 0.002 F, v(t) ϭ 37 sin 10t V,
and current and charge at t ϭ 0 are zero.
C

C

L

v(t)

Fig. 152. RLC-circuit

44. Show that, by Kirchhoff’s Voltage Law (Sec. 2.9), the
currents in the network in Fig. 153 are obtained from
the system
Li 1r ϩ R(i 1 Ϫ i 2) ϭ v(t)
R(i 2r Ϫ i 1r ) ϩ

˛˛

˛˛˛

m 2 y2s ϭ Ϫk 2( y2 Ϫ y1) Ϫ k 3y2).

L

v(t)

Fig. 151. LC-circuit

m 1 y1s ϭ Ϫk 1 y1 ϩ k 2( y2 Ϫ y1)
˛˛

R

1
i 2 ϭ 0.
C

˛˛

˛˛

˛

0
k1

0

y1
k2

Solve this system, assuming that R ϭ 10 ⍀, L ϭ 20 H,
C ϭ 0.05 F, v ϭ 20 V, i 1(0) ϭ 0, i 2(0) ϭ 2 A.

y2

L

k3

i1

i2

v(t)

R

Fig. 149. System in Problems 38 and 39
39. In Prob. 38, let m 1 ϭ m 2 ϭ 10 kg, k 1 ϭ k 3 ϭ 20 kg>sec2,
k 2 ϭ 40 kg>sec2. Find the solution satisfying the initial conditions y1(0) ϭ y2(0) ϭ 0, y1r (0) ϭ 1 meter>sec,
y2r (0) ϭ Ϫ1 meter>sec.
40. Find the model (the system of ODEs) in Prob. 38
extended by adding another mass m 3 and another spring
of modulus k 4 in series.
41. Find the current i(t) in the RC-circuit in Fig. 150,
where R ϭ 10 ⍀, C ϭ 0.1 F, v(t) ϭ 10t V if 0 Ͻ t Ͻ 4,
v(t) ϭ 40 V if t Ͼ 4, and the initial charge on the
capacitor is 0.

C

Fig. 153. Network in Problem 44
45. Set up the model of the network in Fig. 154 and find
the solution, assuming that all charges and currents are
0 when the switch is closed at t ϭ 0. Find the limits of
i 1(t) and i 2(t) as t : ϱ , (i) from the solution, (ii) directly
from the given network.
L=5H

i1

i2

V

C = 0.05 F

Switch
R

C

v(t)

Fig. 150. RC-circuit

Fig. 154. Network in Problem 45

c06.qxd

10/28/10

6:33 PM

Page 253

Summary of Chapter 6

253

SUMMARY OF CHAPTER

6

Laplace Transforms
The main purpose of Laplace transforms is the solution of differential equations and
systems of such equations, as well as corresponding initial value problems. The
Laplace transform F(s) ϭ l( f ) of a function f (t) is defined by
(1)

F(s) ϭ l( f ) ϭ

Ύ

ؕ

e؊stf (t) dt

(Sec. 6.1).

0

This definition is motivated by the property that the differentiation of f with respect
to t corresponds to the multiplication of the transform F by s; more precisely,
(2)

l( f r ) ϭ sl( f ) Ϫ f (0)

(Sec. 6.2)

l( f s ) ϭ s 2l( f ) Ϫ sf (0) Ϫ f r (0)

etc. Hence by taking the transform of a given differential equation
(3)

y s ϩ ay r ϩ by ϭ r(t)

(a, b constant)

and writing l(y) ϭ Y(s), we obtain the subsidiary equation
(4)

(s 2 ϩ as ϩ b)Y ϭ l(r) ϩ sf (0) ϩ f r (0) ϩ af (0).

Here, in obtaining the transform l(r) we can get help from the small table in Sec. 6.1
or the larger table in Sec. 6.9. This is the first step. In the second step we solve the
subsidiary equation algebraically for Y(s). In the third step we determine the inverse
transform y(t) ϭ l؊1(Y), that is, the solution of the problem. This is generally
the hardest step, and in it we may again use one of those two tables. Y(s) will often
be a rational function, so that we can obtain the inverse l؊1(Y) by partial fraction
reduction (Sec. 6.4) if we see no simpler way.
The Laplace method avoids the determination of a general solution of the
homogeneous ODE, and we also need not determine values of arbitrary constants
in a general solution from initial conditions; instead, we can insert the latter directly
into (4). Two further facts account for the practical importance of the Laplace
transform. First, it has some basic properties and resulting techniques that simplify
the determination of transforms and inverses. The most important of these properties
are listed in Sec. 6.8, together with references to the corresponding sections. More
on the use of unit step functions and Dirac’s delta can be found in Secs. 6.3 and
6.4, and more on convolution in Sec. 6.5. Second, due to these properties, the present
method is particularly suitable for handling right sides r(t) given by different
expressions over different intervals of time, for instance, when r(t) is a square wave
or an impulse or of a form such as r(t) ϭ cos t if 0 Ϲ t Ϲ 4p and 0 elsewhere.
The application of the Laplace transform to systems of ODEs is shown in Sec. 6.7.
(The application to PDEs follows in Sec. 12.12.)

c06.qxd

10/28/10

6:33 PM

Page 254

c07.qxd

10/28/10

7:30 PM

Page 255

PART

B

Linear Algebra.
Vector Calculus
CHAPTER 7
CHAPTER 8
CHAPTER 9
CHAPTER 10

Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
Linear Algebra: Matrix Eigenvalue Problems
Vector Differential Calculus. Grad, Div, Curl
Vector Integral Calculus. Integral Theorems
Matrices and vectors, which underlie linear algebra (Chaps. 7 and 8), allow us to represent
numbers or functions in an ordered and compact form. Matrices can hold enormous amounts
of data—think of a network of millions of computer connections or cell phone connections—
in a form that can be rapidly processed by computers. The main topic of Chap. 7 is how
to solve systems of linear equations using matrices. Concepts of rank, basis, linear
transformations, and vector spaces are closely related. Chapter 8 deals with eigenvalue
problems. Linear algebra is an active field that has many applications in engineering
physics, numerics (see Chaps. 20–22), economics, and others.
Chapters 9 and 10 extend calculus to vector calculus. We start with vectors from linear
algebra and develop vector differential calculus. We differentiate functions of several
variables and discuss vector differential operations such as grad, div, and curl. Chapter 10
extends regular integration to integration over curves, surfaces, and solids, thereby
obtaining new types of integrals. Ingenious theorems by Gauss, Green, and Stokes allow
us to transform these integrals into one another.
Software suitable for linear algebra (Lapack, Maple, Mathematica, Matlab) can be found
in the list at the opening of Part E of the book if needed.
Numeric linear algebra (Chap. 20) can be studied directly after Chap. 7 or 8 because
Chap. 20 is independent of the other chapters in Part E on numerics.

255

c07.qxd

10/28/10

7:30 PM

Page 256

CHAPTER

7

Linear Algebra: Matrices,
Vectors, Determinants.
Linear Systems
Linear algebra is a fairly extensive subject that covers vectors and matrices, determinants,
systems of linear equations, vector spaces and linear transformations, eigenvalue problems,
and other topics. As an area of study it has a broad appeal in that it has many applications
in engineering, physics, geometry, computer science, economics, and other areas. It also
contributes to a deeper understanding of mathematics itself.
Matrices, which are rectangular arrays of numbers or functions, and vectors are the
main tools of linear algebra. Matrices are important because they let us express large
amounts of data and functions in an organized and concise form. Furthermore, since
matrices are single objects, we denote them by single letters and calculate with them
directly. All these features have made matrices and vectors very popular for expressing
scientific and mathematical ideas.
The chapter keeps a good mix between applications (electric networks, Markov
processes, traffic flow, etc.) and theory. Chapter 7 is structured as follows: Sections 7.1
and 7.2 provide an intuitive introduction to matrices and vectors and their operations,
including matrix multiplication. The next block of sections, that is, Secs. 7.3–7.5 provide
the most important method for solving systems of linear equations by the Gauss
elimination method. This method is a cornerstone of linear algebra, and the method
itself and variants of it appear in different areas of mathematics and in many applications.
It leads to a consideration of the behavior of solutions and concepts such as rank of a
matrix, linear independence, and bases. We shift to determinants, a topic that has
declined in importance, in Secs. 7.6 and 7.7. Section 7.8 covers inverses of matrices.
The chapter ends with vector spaces, inner product spaces, linear transformations, and
composition of linear transformations. Eigenvalue problems follow in Chap. 8.
COMMENT. Numeric linear algebra (Secs. 20.1–20.5) can be studied immediately
after this chapter.
Prerequisite: None.
Sections that may be omitted in a short course: 7.5, 7.9.
References and Answers to Problems: App. 1 Part B, and App. 2.

256

c07.qxd

10/28/10

7:30 PM

Page 257

SEC. 7.1 Matrices, Vectors: Addition and Scalar Multiplication

7.1

257

Matrices, Vectors:
Addition and Scalar Multiplication
The basic concepts and rules of matrix and vector algebra are introduced in Secs. 7.1 and
7.2 and are followed by linear systems (systems of linear equations), a main application,
in Sec. 7.3.
Let us first take a leisurely look at matrices before we formalize our discussion. A matrix
is a rectangular array of numbers or functions which we will enclose in brackets. For example,

(1)

c

0.3

1

Ϫ5

0

Ϫ0.2

16

c

e

؊x

e6x

2x
4x

2

d,

d,

a11

a12

a13

Da21

a22

a23T ,

a31

a32

a33

c1d
4

[a1

a2 a3],

2

are matrices. The numbers (or functions) are called entries or, less commonly, elements
of the matrix. The first matrix in (1) has two rows, which are the horizontal lines of entries.
Furthermore, it has three columns, which are the vertical lines of entries. The second and
third matrices are square matrices, which means that each has as many rows as columns—
3 and 2, respectively. The entries of the second matrix have two indices, signifying their
location within the matrix. The first index is the number of the row and the second is the
number of the column, so that together the entry’s position is uniquely identified. For
example, a23 (read a two three) is in Row 2 and Column 3, etc. The notation is standard
and applies to all matrices, including those that are not square.
Matrices having just a single row or column are called vectors. Thus, the fourth matrix
in (1) has just one row and is called a row vector. The last matrix in (1) has just one
column and is called a column vector. Because the goal of the indexing of entries was
to uniquely identify the position of an element within a matrix, one index suffices for
vectors, whether they are row or column vectors. Thus, the third entry of the row vector
in (1) is denoted by a3.
Matrices are handy for storing and processing data in applications. Consider the
following two common examples.
EXAMPLE 1

Linear Systems, a Major Application of Matrices
We are given a system of linear equations, briefly a linear system, such as
4x 1 ϩ 6x 2 ϩ 9x 3 ϭ 6
6x 1

Ϫ 2x 3 ϭ 20

5x 1 Ϫ 8x 2 ϩ x 3 ϭ 10
where x 1, x 2, x 3 are the unknowns. We form the coefficient matrix, call it A, by listing the coefficients of the
unknowns in the position in which they appear in the linear equations. In the second equation, there is no
unknown x 2, which means that the coefficient of x 2 is 0 and hence in matrix A, a22 ϭ 0, Thus,

c07.qxd

10/28/10

7:30 PM

258

Page 258

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
4

6

9

A ϭ D6

0

Ϫ2T .

5

Ϫ8

We form another matrix

4
~
A ϭ D6

6

9

0

Ϫ2

5

Ϫ8

1

1

6
20T
10

by augmenting A with the right sides of the linear system and call it the augmented matrix of the system.
~ ~
Since we can go back and recapture the system of linear equations directly from the augmented matrix A, A
contains all the information of the system and can thus be used to solve the linear system. This means that we
can just use the augmented matrix to do the calculations needed to solve the system. We shall explain this in
detail in Sec. 7.3. Meanwhile you may verify by substitution that the solution is x 1 ϭ 3, x 2 ϭ 12 , x 3 ϭ Ϫ1.
The notation x 1, x 2, x 3 for the unknowns is practical but not essential; we could choose x, y, z or some other
letters.
᭿

EXAMPLE 2

Sales Figures in Matrix Form
Sales figures for three products I, II, III in a store on Monday (Mon), Tuesday (Tues), Á may for each week
be arranged in a matrix
Mon

Tues

Wed

Thur

Fri

Sat

Sun

40

33

81

0

21

47

33

I

Aϭ D 0

12

78

50

50

96

90 T # II

10

0

0

27

43

78

56

III

If the company has 10 stores, we can set up 10 such matrices, one for each store. Then, by adding corresponding
entries of these matrices, we can get a matrix showing the total sales of each product on each day. Can you think
of other data which can be stored in matrix form? For instance, in transportation or storage problems? Or in
listing distances in a network of roads?
᭿

General Concepts and Notations
Let us formalize what we just have discussed. We shall denote matrices by capital boldface
letters A, B, C, Á , or by writing the general entry in brackets; thus A ϭ [ajk], and so
on. By an m ؋ n matrix (read m by n matrix) we mean a matrix with m rows and n
columns—rows always come first! m ϫ n is called the size of the matrix. Thus an m ϫ n
matrix is of the form

(2)

a12

Á

a1n

a21

a22

Á

a2n

#

#

Á

#

am1

am2

Á

amn

A ϭ 3ajk4 ϭ E

a11

U.

The matrices in (1) are of sizes 2 ϫ 3, 3 ϫ 3, 2 ϫ 2, 1 ϫ 3, and 2 ϫ 1, respectively.
Each entry in (2) has two subscripts. The first is the row number and the second is the
column number. Thus a21 is the entry in Row 2 and Column 1.
If m ϭ n, we call A an n ϫ n square matrix. Then its diagonal containing the entries
a11, a22, Á , ann is called the main diagonal of A. Thus the main diagonals of the two
square matrices in (1) are a11, a22, a33 and e؊x, 4x, respectively.
Square matrices are particularly important, as we shall see. A matrix of any size m ϫ n
is called a rectangular matrix; this includes square matrices as a special case.

c07.qxd

10/28/10

7:30 PM

Page 259

SEC. 7.1 Matrices, Vectors: Addition and Scalar Multiplication

259

Vectors
A vector is a matrix with only one row or column. Its entries are called the components
of the vector. We shall denote vectors by lowercase boldface letters a, b, Á or by its
general component in brackets, a ϭ 3aj4, and so on. Our special vectors in (1) suggest
that a (general) row vector is of the form
a ϭ 3a1 a2

an4.

Á

a ϭ 3Ϫ2 5

For instance,

0.8

0

14.

A column vector is of the form
b1

4

b2
b ϭ E . U.
.
.
bm

b ϭ D 0T .

For instance,

Ϫ7

Addition and Scalar Multiplication
of Matrices and Vectors
What makes matrices and vectors really useful and particularly suitable for computers is
the fact that we can calculate with them almost as easily as with numbers. Indeed, we
now introduce rules for addition and for scalar multiplication (multiplication by numbers)
that were suggested by practical applications. (Multiplication of matrices by matrices
follows in the next section.) We first need the concept of equality.
DEFINITION

EXAMPLE 3

Equality of Matrices

Two matrices A ϭ 3ajk4 and B ϭ 3bjk4 are equal, written A ϭ B, if and only if
they have the same size and the corresponding entries are equal, that is, a11 ϭ b11,
a12 ϭ b12, and so on. Matrices that are not equal are called different. Thus, matrices
of different sizes are always different.

Equality of Matrices
Let
Aϭ

c

a11

a12

a21

a22

d

and

Bϭ

c

4

0

3

Ϫ1

d.

Then
AϭB

if and only if

a11 ϭ 4,

a12 ϭ

a21 ϭ 3,

a22 ϭ Ϫ1.

0,

The following matrices are all different. Explain!

c

1

3

4

2

d

c

4

2

1

3

d

c

4

1

2

3

d

c

1

3

0

4

2

0

d

c

0

1

3

0

4

2

d

᭿

c07.qxd

10/28/10

7:30 PM

260

Page 260

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

DEFINITION

Addition of Matrices

The sum of two matrices A ϭ 3ajk4 and B ϭ 3bjk4 of the same size is written
A ϩ B and has the entries ajk ϩ bjk obtained by adding the corresponding entries
of A and B. Matrices of different sizes cannot be added.

As a special case, the sum a ϩ b of two row vectors or two column vectors, which
must have the same number of components, is obtained by adding the corresponding
components.
EXAMPLE 4

Addition of Matrices and Vectors
If

Aϭ

c

Ϫ4

6

0

1

3
2

d

and

Bϭ

c

5

Ϫ1

0

1

0

3

d,

AϩBϭ

then

c

1

5

3

3

2

2

d.

A in Example 3 and our present A cannot be added. If a ϭ 35 7 24 and b ϭ 3Ϫ6 2
a ϩ b ϭ 3Ϫ1 9 24.
An application of matrix addition was suggested in Example 2. Many others will follow.

DEFINITION

04, then

᭿

Scalar Multiplication (Multiplication by a Number)

The product of any m ϫ n matrix A ϭ 3ajk4 and any scalar c (number c) is written
cA and is the m ϫ n matrix cA ϭ 3cajk4 obtained by multiplying each entry of A
by c.

Here (Ϫ1)A is simply written ϪA and is called the negative of A. Similarly, (Ϫk)A is
written ϪkA. Also, A ϩ (ϪB) is written A Ϫ B and is called the difference of A and B
(which must have the same size!).
EXAMPLE 5

Scalar Multiplication
2.7
If A ϭ D0
9.0

Ϫ1.8
0.9T , then

Ϫ2.7
ϪA ϭ D 0

Ϫ4.5

Ϫ9.0

1.8
Ϫ0.9T ,

3
10
9

AϭD 0

4.5

10

Ϫ2
1T ,
Ϫ5

0
0A ϭ D0
0

0
0T .
0

If a matrix B shows the distances between some cities in miles, 1.609B gives these distances in kilometers.

᭿

Rules for Matrix Addition and Scalar Multiplication. From the familiar laws for the
addition of numbers we obtain similar laws for the addition of matrices of the same size
m ϫ n, namely,
(a)

AϩBϭBϩA

(b)

(A ϩ B) ϩ C ϭ A ϩ (B ϩ C)

(3)
(c)

Aϩ0ϭA

(d)

A ϩ (ϪA) ϭ 0.

(written A ϩ B ϩ C)

Here 0 denotes the zero matrix (of size m ϫ n), that is, the m ϫ n matrix with all entries
zero. If m ϭ 1 or n ϭ 1, this is a vector, called a zero vector.

c07.qxd

10/28/10

7:30 PM

Page 261

SEC. 7.1 Matrices, Vectors: Addition and Scalar Multiplication

261

Hence matrix addition is commutative and associative [by (3a) and (3b)].
Similarly, for scalar multiplication we obtain the rules

(4)

(a)

c(A ϩ B) ϭ cA ϩ cB

(b)

(c ϩ k)A ϭ cA ϩ kA

(c)

c(kA) ϭ (ck)A

(d)

1A ϭ A.

(written ckA)

PROBLEM SET 7.1
1–7

GENERAL QUESTIONS

0

1. Equality. Give reasons why the five matrices in
Example 3 are all different.
2. Double subscript notation. If you write the matrix in
Example 2 in the form A ϭ 3ajk4, what is a31? a13?
a26? a33?
3. Sizes. What sizes do the matrices in Examples 1, 2, 3,
and 5 have?
4. Main diagonal. What is the main diagonal of A in
Example 1? Of A and B in Example 3?
5. Scalar multiplication. If A in Example 2 shows the
number of items sold, what is the matrix B of units sold
if a unit consists of (a) 5 items and (b) 10 items?
6. If a 12 ϫ 12 matrix A shows the distances between
12 cities in kilometers, how can you obtain from A the
matrix B showing these distances in miles?
7. Addition of vectors. Can you add: A row and
a column vector with different numbers of components? With the same number of components? Two
row vectors with the same number of components
but different numbers of zeros? A vector and a
scalar? A vector with four components and a 2 ϫ 2
matrix?
8–16

ADDITION AND SCALAR
MULTIPLICATION OF MATRICES
AND VECTORS
2

4

A ϭ D6

5

5T ,

1

0
5

C ϭ DϪ2
1

2

5

2

BϭD 5

3

4T

Ϫ2

4

4T ,
0

Ϫ4
DϭD 5
2

1
0T ,
Ϫ1

3

4T
Ϫ1
1.5

u ϭ D 0 T,
Ϫ3.0

Ϫ1
v ϭ D 3T ,

Ϫ5
w ϭ DϪ30T .

2

10

Find the following expressions, indicating which of the
rules in (3) or (4) they illustrate, or give reasons why they
are not defined.
8. 2A ϩ 4B, 4B ϩ 2A, 0A ϩ B, 0.4B Ϫ 4.2A
9. 3A,

0.5B,

3A ϩ 0.5B, 3A ϩ 0.5B ϩ C

10. (4 # 3)A, 4(3A), 14B Ϫ 3B, 11B
11. 8C ϩ 10D, 2(5D ϩ 4C), 0.6C Ϫ 0.6D,
0.6(C Ϫ D)
12. (C ϩ D) ϩ E, (D ϩ E) ϩ C, 0(C Ϫ E) ϩ 4D,
A Ϫ 0C
13. (2 # 7)C, 2(7C), ϪD ϩ 0E, E Ϫ D ϩ C ϩ u
14. (5u ϩ 5v) Ϫ 12 w, Ϫ20(u ϩ v) ϩ 2w,
E Ϫ (u ϩ v), 10(u ϩ v) ϩ w

16. 15v Ϫ 3w Ϫ 0u, Ϫ3w ϩ 15v, D Ϫ u ϩ 3C,
8.5w Ϫ 11.1u ϩ 0.4v

0

Ϫ3

E ϭ D3

15. (u ϩ v) Ϫ w, u ϩ (v Ϫ w), C ϩ 0w,
0E ϩ u Ϫ v

Let
0

2

Ϫ2

17. Resultant of forces. If the above vectors u, v, w
represent forces in space, their sum is called their
resultant. Calculate it.
18. Equilibrium. By definition, forces are in equilibrium
if their resultant is the zero vector. Find a force p such
that the above u, v, w, and p are in equilibrium.
19. General rules. Prove (3) and (4) for general 2 ϫ 3
matrices and scalars c and k.

c07.qxd

10/28/10

7:30 PM

Page 262

262

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

20. TEAM PROJECT. Matrices for Networks. Matrices
have various engineering applications, as we shall see.
For instance, they can be used to characterize connections
in electrical networks, in nets of roads, in production
processes, etc., as follows.
(a) Nodal Incidence Matrix. The network in Fig. 155
consists of six branches (connections) and four nodes
(points where two or more branches come together).
One node is the reference node (grounded node, whose
voltage is zero). We number the other nodes and
number and direct the branches. This we do arbitrarily.
The network can now be described by a matrix
A ϭ 3ajk4, where

(c) Sketch the three networks corresponding to the
nodal incidence matrices
1 Ϫ1

1

0

0

1

DϪ1

1

0

0T ,

0 Ϫ1

1

0

1

0

1

0

0

DϪ1

1

0

1

0T .

0 Ϫ1 Ϫ1

0

1

DϪ1
0

0

0

1

1 Ϫ1

1

0T ,

1 Ϫ1

0

(d) Mesh Incidence Matrix. A network can also be
characterized by the mesh incidence matrix M ϭ 3m jk4,
where

ϩ1 if branch k leaves node j
ajk ϭ d Ϫ1 if branch k enters node j
0 if branch k does not touch node j .

ϩ1 if branch k is in mesh

A is called the nodal incidence matrix of the network.
Show that for the network in Fig. 155 the matrix A has
the given form.

j

and has the same orientation
m jk ϭ f Ϫ1 if branch k is in mesh

j

and has the opposite orientation

3

0 if branch k is not in mesh
1
2

5
4

and a mesh is a loop with no branch in its interior (or
in its exterior). Here, the meshes are numbered and
directed (oriented) in an arbitrary fashion. Show that
for the network in Fig. 157, the matrix M has the given
form, where Row 1 corresponds to mesh 1, etc.

6

(Reference node)

1

Branch

2

3

4

5

3

4

6

3
Node 1

1

–1

–1

0

0

0

Node 2

0

1

0

1

1

0

Node 3

0

0

1

0

–1

–1

2

5
1

2
4

1

6

Fig. 155. Network and nodal incidence
matrix in Team Project 20(a)
(b) Find the nodal incidence matrices of the networks
in Fig. 156.
1
1
2

M=

2

3
7

1

2

1

5

4

j

3

2

1

0

3

5

1

1

0

–1

0

0

0

0

0

1

–1

1

0

–1

1

0

1

0

1

0

1

0

0

1

2
6
4

3
4

3

Fig. 156. Electrical networks in Team Project 20(b)

Fig. 157. Network and matrix M in
Team Project 20(d)

c07.qxd

10/28/10

7:30 PM

Page 263

SEC. 7.2 Matrix Multiplication

7.2

263

Matrix Multiplication
Matrix multiplication means that one multiplies matrices by matrices. Its definition is
standard but it looks artificial. Thus you have to study matrix multiplication carefully,
multiply a few matrices together for practice until you can understand how to do it. Here
then is the definition. (Motivation follows later.)

DEFINITION

Multiplication of a Matrix by a Matrix

The product C ϭ AB (in this order) of an m ϫ n matrix A ϭ 3ajk4 times an r ϫ p
matrix B ϭ 3bjk4 is defined if and only if r ϭ n and is then the m ϫ p matrix
C ϭ 3cjk4 with entries
j ϭ 1, Á , m

n

(1)

cjk ϭ a ajlblk ϭ aj1b1k ϩ aj2b2k ϩ Á ϩ ajnbnk

k ϭ 1, Á , p.

lϭ1

The condition r ϭ n means that the second factor, B, must have as many rows as the first
factor has columns, namely n. A diagram of sizes that shows when matrix multiplication
is possible is as follows:
A
B
ϭ
C
3m ϫ n4 3n ϫ p4 ϭ 3m ϫ p4.
The entry cjk in (1) is obtained by multiplying each entry in the jth row of A by the
corresponding entry in the kth column of B and then adding these n products. For instance,
c21 ϭ a21b11 ϩ a22b21 ϩ Á ϩ a2nbn1, and so on. One calls this briefly a multiplication
of rows into columns. For n ϭ 3, this is illustrated by
n=3

m=4

p=2

p=2

a11

a12

a13

b11

b12

a21

a22

a23

b21

b22

a31

a32

a33

b31

b32

a41

a42

a43

=

c11

c12

c21

c22

c31

c32

c41

c42

m=4

Notations in a product AB ϭ C

where we shaded the entries that contribute to the calculation of entry c21 just discussed.
Matrix multiplication will be motivated by its use in linear transformations in this
section and more fully in Sec. 7.9.
Let us illustrate the main points of matrix multiplication by some examples. Note that
matrix multiplication also includes multiplying a matrix by a vector, since, after all,
a vector is a special matrix.
EXAMPLE 1

Matrix Multiplication
3

5

AB ϭ D 4

0

Ϫ6

Ϫ3

Ϫ1

2

Ϫ2

3

1

22

Ϫ2

43

2T D5

0

7

8T ϭ D 26

Ϫ16

14

Ϫ4

1

1

Ϫ9

4

Ϫ37

2

9

42
6T
Ϫ28

Here c11 ϭ 3 # 2 ϩ 5 # 5 ϩ (Ϫ1) # 9 ϭ 22, and so on. The entry in the box is c23 ϭ 4 # 3 ϩ 0 # 7 ϩ 2 # 1 ϭ 14.
The product BA is not defined.

᭿

c07.qxd

10/28/10

7:30 PM

264
EXAMPLE 2

Page 264

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
Multiplication of a Matrix and a Vector

c
EXAMPLE 3

4
1

2
8

dc d
3
5

ϭ

c #
d
1 3ϩ8 # 5
4 # 3ϩ2 # 5

ϭ

c d

whereas

43

3

4

2

5

1

8

d

᭿

is undefined.

Products of Row and Column Vectors
1
33

1

14 D2T ϭ 3194,

6

D2T 33

4

EXAMPLE 4

c dc

22

6

3

6

14 ϭ D 6

12

2T .

12

24

4

4

CAUTION! Matrix Multiplication Is Not Commutative, AB

1

᭿

BA in General

This is illustrated by Examples 1 and 2, where one of the two products is not even defined, and by Example 3,
where the two products have different sizes. But it also holds for square matrices. For instance,

c

1
100

1
100

dc

Ϫ1

1

1

Ϫ1

d

ϭ

c

0
0

0
0

d

but

c

Ϫ1

1

1

Ϫ1

dc

1

1

100

100

d

ϭ

c

99

99

Ϫ99

Ϫ99

d.

It is interesting that this also shows that AB ϭ 0 does not necessarily imply BA ϭ 0 or A ϭ 0 or B ϭ 0. We
shall discuss this further in Sec. 7.8, along with reasons when this happens.
᭿

Our examples show that in matrix products the order of factors must always be observed
very carefully. Otherwise matrix multiplication satisfies rules similar to those for numbers,
namely.
(a)

(kA)B ϭ k(AB) ϭ A(kB) written kAB or AkB

(b)

A(BC) ϭ (AB)C

(c)

(A ϩ B)C ϭ AC ϩ BC

(d)

C(A ϩ B) ϭ CA ϩ CB

(2)

written ABC

provided A, B, and C are such that the expressions on the left are defined; here, k is any
scalar. (2b) is called the associative law. (2c) and (2d) are called the distributive laws.
Since matrix multiplication is a multiplication of rows into columns, we can write the
defining formula (1) more compactly as
cjk ϭ ajbk,

(3)

j ϭ 1, Á , m; k ϭ 1, Á , p,

where aj is the jth row vector of A and bk is the kth column vector of B, so that in
agreement with (1),

ajbk ϭ 3aj1 aj2

Á

b1k
.
ajn4 D . T ϭ aj1b1k ϩ aj2b2k ϩ Á ϩ ajnbnk.
.
bnk

c07.qxd

10/28/10

7:30 PM

Page 265

SEC. 7.2 Matrix Multiplication
EXAMPLE 5

265

Product in Terms of Row and Column Vectors
If A ϭ 3ajk4 is of size 3 ϫ 3 and B ϭ 3bjk4 is of size 3 ϫ 4, then

(4)

Taking a1 ϭ 33 5

a1b1

a1b2

a1b3

a1b4

AB ϭ Da2b1

a2b2

a2b3

a2b4T .

a3b1

a3b2

a3b3

a3b4

Ϫ14, a2 ϭ 34 0

᭿

24, etc., verify (4) for the product in Example 1.

Parallel processing of products on the computer is facilitated by a variant of (3) for
computing C ϭ AB, which is used by standard algorithms (such as in Lapack). In this
method, A is used as given, B is taken in terms of its column vectors, and the product is
computed columnwise; thus,
Á bp4 ϭ 3Ab1

AB ϭ A3b1 b2

(5)

Á Abp4.

Ab2

Columns of B are then assigned to different processors (individually or several to
each processor), which simultaneously compute the columns of the product matrix
Ab1, Ab2, etc.
EXAMPLE 6

Computing Products Columnwise by (5)
To obtain
AB ϭ

c

4

1

Ϫ5

2

dc

4

6

d

ϭ

c

4

1

Ϫ5

2

dc d

ϭ

c d, c

3

0

7

Ϫ1

11

4

34

Ϫ17

8

Ϫ23

d

from (5), calculate the columns

c

4

1

Ϫ5

2

dc

3
Ϫ1

d

ϭ

c

11
Ϫ17

d, c

0
4

4

4

1

8

Ϫ5

2

dc d
7
6

ϭ

c

34
Ϫ23

d
᭿

of AB and then write them as a single matrix, as shown in the first formula on the right.

Motivation of Multiplication
by Linear Transformations
Let us now motivate the “unnatural” matrix multiplication by its use in linear
transformations. For n ϭ 2 variables these transformations are of the form
y1 ϭ a11x 1 ϩ a12x 2

(6*)

y2 ϭ a21x 1 ϩ a22x 2

and suffice to explain the idea. (For general n they will be discussed in Sec. 7.9.) For
instance, (6*) may relate an x 1x 2-coordinate system to a y1y2-coordinate system in the
plane. In vectorial form we can write (6*) as
(6)

yϭ

c d
y1
y2

ϭ Ax ϭ

c

a11

a12

a21

a22

dc d
x1
x2

ϭ

c

a11x 1 ϩ a12x 2
a21x 1 ϩ a22x 2

d.

c07.qxd

10/28/10

7:30 PM

266

Page 266

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

Now suppose further that the x 1x 2-system is related to a w1w2-system by another linear
transformation, say,
(7)

xϭ

c d
x1
x2

ϭ Bw ϭ

c

b11

b12

b21

b22

dc d
w1
w2

ϭ

c

b11w1 ϩ b12w2
b21w1 ϩ b22w2

d.

Then the y1y2-system is related to the w1w2-system indirectly via the x 1x 2-system, and
we wish to express this relation directly. Substitution will show that this direct relation is
a linear transformation, too, say,
(8)

y ϭ Cw ϭ

c

c11

c12

c21

c22

dc d
w1
w2

ϭ

c

c11w1 ϩ c12w2
c21w1 ϩ c22w2

d.

Indeed, substituting (7) into (6), we obtain
y1 ϭ a11(b11w1 ϩ b12w2) ϩ a12(b21w1 ϩ b22w2)
ϭ (a11b11 ϩ a12b21)w1 ϩ (a11b12 ϩ a12b22)w2
y2 ϭ a21(b11w1 ϩ b12w2) ϩ a22(b21w1 ϩ b22w2)
ϭ (a21b11 ϩ a22b21)w1 ϩ (a21b12 ϩ a22b22)w2.
Comparing this with (8), we see that
c11 ϭ a11b11 ϩ a12b21

c12 ϭ a11b12 ϩ a12b22

c21 ϭ a21b11 ϩ a22b21

c22 ϭ a21b12 ϩ a22b22.

This proves that C ϭ AB with the product defined as in (1). For larger matrix sizes the
idea and result are exactly the same. Only the number of variables changes. We then have
m variables y and n variables x and p variables w. The matrices A, B, and C ϭ AB then
have sizes m ϫ n, n ϫ p, and m ϫ p, respectively. And the requirement that C be the
product AB leads to formula (1) in its general form. This motivates matrix multiplication.

Transposition
We obtain the transpose of a matrix by writing its rows as columns (or equivalently its
columns as rows). This also applies to the transpose of vectors. Thus, a row vector becomes
a column vector and vice versa. In addition, for square matrices, we can also “reflect”
the elements along the main diagonal, that is, interchange entries that are symmetrically
positioned with respect to the main diagonal to obtain the transpose. Hence a12 becomes
a21, a31 becomes a13, and so forth. Example 7 illustrates these ideas. Also note that, if A
is the given matrix, then we denote its transpose by AT.
EXAMPLE 7

Transposition of Matrices and Vectors

If

Aϭ

c

5

Ϫ8

1

4

0

0

d,

5
then

AT ϭ DϪ8
1

4
0T .
0

c07.qxd

10/28/10

7:30 PM

Page 267

SEC. 7.2 Matrix Multiplication

267

A little more compactly, we can write

c

5

Ϫ8

1

4

0

0

Furthermore, the transpose 36

d

2

5

T

ϭ DϪ8
1

4
0T ,

3

0

D8
1

0

8

Ϫ1

5T ϭ D0

Ϫ1

Ϫ9

4

6
36 2

34T ϭ D2T #

Conversely,

7

1
Ϫ9T ,

5

4

34 is the column vector
6

3

DEFINITION

T

3

34T of the row vector 36 2

7

T

D2T ϭ 36 2

34.

᭿

3

Transposition of Matrices and Vectors

The transpose of an m ϫ n matrix A ϭ 3ajk4 is the n ϫ m matrix AT (read A
transpose) that has the first row of A as its first column, the second row of A as its
second column, and so on. Thus the transpose of A in (2) is AT ϭ 3akj4, written out

(9)

a21

Á

am1

a12

a22

Á

am2

#

#

Á

a1n

a2n

Á

AT ϭ 3akj4 ϭ E

a11

#

U.

amn

As a special case, transposition converts row vectors to column vectors and conversely.

Transposition gives us a choice in that we can work either with the matrix or its
transpose, whichever is more convenient.
Rules for transposition are
(a)
(10)

(AT)T ϭ A

(b)

(A ϩ B)T ϭ AT ϩ BT

(c)

(cA)T ϭ cAT

(d)

(AB)T ϭ BTAT.

CAUTION! Note that in (10d) the transposed matrices are in reversed order. We leave
the proofs as an exercise in Probs. 9 and 10.

Special Matrices
Certain kinds of matrices will occur quite frequently in our work, and we now list the
most important ones of them.
Symmetric and Skew-Symmetric Matrices. Transposition gives rise to two useful
classes of matrices. Symmetric matrices are square matrices whose transpose equals the

c07.qxd

10/28/10

7:30 PM

268

Page 268

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

matrix itself. Skew-symmetric matrices are square matrices whose transpose equals
minus the matrix. Both cases are defined in (11) and illustrated by Example 8.
(11)

AT ϭ A

(thus akj ϭ ajk),

AT ϭ ϪA

(thus akj ϭ Ϫajk, hence ajj ϭ 0).

Symmetric Matrix

EXAMPLE 8

Skew-Symmetric Matrix

Symmetric and Skew-Symmetric Matrices
20

120

200

A ϭ D120

10

150T

200

150

30

is symmetric, and

0

1

Ϫ3

B ϭ DϪ1

0

Ϫ2T

3

2

0

is skew-symmetric.

For instance, if a company has three building supply centers C1, C2, C3, then A could show costs, say, ajj for
handling 1000 bags of cement at center Cj, and ajk ( j k) the cost of shipping 1000 bags from Cj to Ck. Clearly,
ajk ϭ akj if we assume shipping in the opposite direction will cost the same.
Symmetric matrices have several general properties which make them important. This will be seen as we
proceed.
᭿

Triangular Matrices. Upper triangular matrices are square matrices that can have nonzero
entries only on and above the main diagonal, whereas any entry below the diagonal must be
zero. Similarly, lower triangular matrices can have nonzero entries only on and below the
main diagonal. Any entry on the main diagonal of a triangular matrix may be zero or not.
EXAMPLE 9

Upper and Lower Triangular Matrices

c

1
0

3
2

d,

1

4

2

D0

3

2T ,

0

0

6

2

0

D8

Ϫ1

7

6

Upper triangular

3

0

0

0

9

Ϫ3

0

0

1

0

2

0

1
9
Lower triangular

3

6

0
0T ,

E

U.

᭿

8

Diagonal Matrices. These are square matrices that can have nonzero entries only on
the main diagonal. Any entry above or below the main diagonal must be zero.
If all the diagonal entries of a diagonal matrix S are equal, say, c, we call S a scalar
matrix because multiplication of any square matrix A of the same size by S has the same
effect as the multiplication by a scalar, that is,
AS ϭ SA ϭ cA.

(12)

In particular, a scalar matrix, whose entries on the main diagonal are all 1, is called a unit
matrix (or identity matrix) and is denoted by I n or simply by I. For I, formula (12) becomes
AI ϭ IA ϭ A.

(13)
EXAMPLE 10

Diagonal Matrix D. Scalar Matrix S. Unit Matrix I
2

0

D ϭ D0

Ϫ3

0

0

0
0T ,
0

c

0

0

S ϭ D0

c

0T ,

0

0

c

1

0

0

I ϭ D0

1

0T

0

0

1

᭿

c07.qxd

10/28/10

7:30 PM

Page 269

SEC. 7.2 Matrix Multiplication

269

Some Applications of Matrix Multiplication
EXAMPLE 11

Computer Production. Matrix Times Matrix
Supercomp Ltd produces two computer models PC1086 and PC1186. The matrix A shows the cost per computer
(in thousands of dollars) and B the production figures for the year 2010 (in multiples of 10,000 units.) Find a
matrix C that shows the shareholders the cost per quarter (in millions of dollars) for raw material, labor, and
miscellaneous.

PC1086
1.2
A ϭ D0.3
0.5

PC1186

Quarter
2 3 4

1

1.6

Raw Components

0.4T

Labor

0.6

Miscellaneous

c

Bϭ

3

8

6

9

6

2

4

3

d

PC1086
PC1186

Solution.
Quarter
2
3

1

4

13.2

12.8

13.6

15.6

Raw Components

C ϭ AB ϭ D 3.3

3.2

3.4

3.9T Labor

5.1

5.2

5.4

6.3

Miscellaneous

Since cost is given in multiples of $1000 and production in multiples of 10,000 units, the entries of C are
᭿
multiples of $10 millions; thus c11 ϭ 13.2 means $132 million, etc.

EXAMPLE 12

Weight Watching. Matrix Times Vector
Suppose that in a weight-watching program, a person of 185 lb burns 350 cal/hr in walking (3 mph), 500 in
bicycling (13 mph), and 950 in jogging (5.5 mph). Bill, weighing 185 lb, plans to exercise according to the
matrix shown. Verify the calculations 1W ϭ Walking, B ϭ Bicycling, J ϭ Jogging2.
B

J

1.0

0

0.5

MON

W

825

MON

1325

WED

1000

FRI

2400

SAT

350
WED
FRI

1.0

1.0

0.5

1.5

0

0.5

E

U D500T ϭ E

U

950
SAT

EXAMPLE 13

2.0

1.5

1.0

᭿

Markov Process. Powers of a Matrix. Stochastic Matrix
Suppose that the 2004 state of land use in a city of 60 mi2 of built-up area is
C: Commercially Used 25%

I: Industrially Used 20%

R: Residentially Used 55%.

Find the states in 2009, 2014, and 2019, assuming that the transition probabilities for 5-year intervals are given
by the matrix A and remain practically the same over the time considered.
From C From I From R
0.7

0.1

0

To C

A ϭ D0.2

0.9

0.2T

To I

0

0.8

To R

0.1

c07.qxd

10/28/10

270

7:30 PM

Page 270

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
A is a stochastic matrix, that is, a square matrix with all entries nonnegative and all column sums equal to 1.
Our example concerns a Markov process,1 that is, a process for which the probability of entering a certain state
depends only on the last state occupied (and the matrix A), not on any earlier state.

Solution. From the matrix A and the 2004 state we can compute the 2009 state,
0.7 # 25 ϩ 0.1 # 20 ϩ 0

C
I

# 55

0.7

0.1

0

D0.2 # 25 ϩ 0.9 # 20 ϩ 0.2 # 55T ϭ D0.2

0.9

0.2T D20T ϭ D34.0T .

R

0.1 # 25 ϩ 0 # 20 ϩ 0.8 # 55

0.1

0

25

0.8

55

19.5

46.5

To explain: The 2009 figure for C equals 25% times the probability 0.7 that C goes into C, plus 20% times the
probability 0.1 that I goes into C, plus 55% times the probability 0 that R goes into C. Together,
25 # 0.7 ϩ 20 # 0.1 ϩ 55 # 0 ϭ 19.5 3%4.

25 # 0.2 ϩ 20 # 0.9 ϩ 55 # 0.2 ϭ 34 3%4.

Also

Similarly, the new R is 46.5% . We see that the 2009 state vector is the column vector
y ϭ 319.5 34.0

46.54T ϭ Ax ϭ A 325 20

554T

where the column vector x ϭ 325 20 554T is the given 2004 state vector. Note that the sum of the entries of
y is 100 3%4. Similarly, you may verify that for 2014 and 2019 we get the state vectors
z ϭ Ay ϭ A(Ax) ϭ A2x ϭ 317.05 43.80

39.154T

u ϭ Az ϭ A2y ϭ A3x ϭ 316.315 50.660

33.0254T.

Answer. In 2009 the commercial area will be 19.5% (11.7 mi2), the industrial 34% (20.4 mi2), and the
residential 46.5% (27.9 mi2). For 2014 the corresponding figures are 17.05%, 43.80%, and 39.15% . For 2019
they are 16.315%, 50.660%, and 33.025% . (In Sec. 8.2 we shall see what happens in the limit, assuming that
those probabilities remain the same. In the meantime, can you experiment or guess?)
᭿

PROBLEM SET 7.2
1–10

GENERAL QUESTIONS

1. Multiplication. Why is multiplication of matrices
restricted by conditions on the factors?
2. Square matrix. What form does a 3 ϫ 3 matrix have
if it is symmetric as well as skew-symmetric?
3. Product of vectors. Can every 3 ϫ 3 matrix be
represented by two vectors as in Example 3?
4. Skew-symmetric matrix. How many different entries
can a 4 ϫ 4 skew-symmetric matrix have? An n ϫ n
skew-symmetric matrix?
5. Same questions as in Prob. 4 for symmetric matrices.
6. Triangular matrix. If U1, U2 are upper triangular and
L 1, L 2 are lower triangular, which of the following are
triangular?
U1 ϩ U2, U1U2,
L1 ϩ L2

U 21,

U1 ϩ L 1,

U1L 1,

7. Idempotent matrix, defined by A2 ϭ A. Can you find
four 2 ϫ 2 idempotent matrices?
1

8. Nilpotent matrix, defined by Bm ϭ 0 for some m.
Can you find three 2 ϫ 2 nilpotent matrices?
9. Transposition. Can you prove (10a)–(10c) for 3 ϫ 3
matrices? For m ϫ n matrices?
10. Transposition. (a) Illustrate (10d) by simple examples.
(b) Prove (10d).
11–20

MULTIPLICATION, ADDITION, AND
TRANSPOSITION OF MATRICES AND
VECTORS

Let
4 Ϫ2

1 Ϫ3

3

A ϭ DϪ2

1

6T ,

1

2

2

0

1

CϭD 3
Ϫ2

2T ,
0

B ϭ DϪ3
0

1

0
0T

0 Ϫ2
3

a ϭ 31 Ϫ2 04, b ϭ D 1T .
Ϫ1

ANDREI ANDREJEVITCH MARKOV (1856–1922), Russian mathematician, known for his work in
probability theory.

c07.qxd

10/28/10

7:30 PM

Page 271

SEC. 7.2 Matrix Multiplication
Showing all intermediate results, calculate the following
expressions or give reasons why they are undefined:
11. AB, ABT, BA, BTA
12. AAT, A2, BBT, B2
13. CC T, BC, CB, C TB
14. 3A Ϫ 2B, (3A Ϫ 2B)T, 3AT Ϫ 2BT,
(3A Ϫ 2B)TaT
15. Aa, AaT, (Ab)T, bTAT
16. BC, BC T, Bb, bTB
17. ABC, ABa, ABb, CaT
18. ab, ba, aA, Bb
19. 1.5a ϩ 3.0b, 1.5aT ϩ 3.0b, (A Ϫ B)b, Ab Ϫ Bb
20. bTAb, aBaT, aCC T, C Tba
21. General rules. Prove (2) for 2 ϫ 2 matrices A ϭ 3ajk4,
B ϭ 3bjk4, C ϭ 3cjk4, and a general scalar.
22. Product. Write AB in Prob. 11 in terms of row and
column vectors.
23. Product. Calculate AB in Prob. 11 columnwise. See
Example 1.
24. Commutativity. Find all 2 ϫ 2 matrices A ϭ 3ajk4
that commute with B ϭ 3bjk4, where bjk ϭ j ϩ k.
25. TEAM PROJECT. Symmetric and Skew-Symmetric
Matrices. These matrices occur quite frequently in
applications, so it is worthwhile to study some of their
most important properties.
(a) Verify the claims in (11) that akj ϭ ajk for a
symmetric matrix, and akj ϭ Ϫajk for a skewsymmetric matrix. Give examples.
(b) Show that for every square matrix C the matrix
C ϩ C T is symmetric and C Ϫ C T is skew-symmetric.
Write C in the form C ϭ S ϩ T, where S is symmetric
and T is skew-symmetric and find S and T in terms
of C. Represent A and B in Probs. 11–20 in this form.
(c) A linear combination of matrices A, B, C, Á , M
of the same size is an expression of the form
(14)

aA ϩ bB ϩ cC ϩ Á ϩ mM,

where a, Á , m are any scalars. Show that if these
matrices are square and symmetric, so is (14); similarly,
if they are skew-symmetric, so is (14).
(d) Show that AB with symmetric A and B is symmetric
if and only if A and B commute, that is, AB ϭ BA.
(e) Under what condition is the product of skewsymmetric matrices skew-symmetric?
26–30

FURTHER APPLICATIONS

26. Production. In a production process, let N mean “no
trouble” and T “trouble.” Let the transition probabilities
from one day to the next be 0.8 for N : N, hence 0.2
for N : T, and 0.5 for T : N, hence 0.5 for T : T.

271
If today there is no trouble, what is the probability of
N two days after today? Three days after today?
27. CAS Experiment. Markov Process. Write a program
for a Markov process. Use it to calculate further steps
in Example 13 of the text. Experiment with other
stochastic 3 ϫ 3 matrices, also using different starting
values.
28. Concert subscription. In a community of 100,000
adults, subscribers to a concert series tend to renew their
subscription with probability 90% and persons presently
not subscribing will subscribe for the next season with
probability 0.2% . If the present number of subscribers
is 1200, can one predict an increase, decrease, or no
change over each of the next three seasons?
29. Profit vector. Two factory outlets F1 and F2 in New
York and Los Angeles sell sofas (S), chairs (C), and
tables (T) with a profit of $35, $62, and $30, respectively.
Let the sales in a certain week be given by the matrix
S
Aϭ

c

C

T

400

60

240

100

120

500

d

F1
F2

Introduce a “profit vector” p such that the components
of v ϭ Ap give the total profits of F1 and F2.
30. TEAM PROJECT. Special Linear Transformations.
Rotations have various applications. We show in this
project how they can be handled by matrices.
(a) Rotation in the plane. Show that the linear
transformation y ϭ Ax with
Aϭ

c

cos u

Ϫsin u

sin u

cos u

d,

xϭ

c d,
x1
x2

yϭ

c d
y1
y2

is a counterclockwise rotation of the Cartesian x 1x 2coordinate system in the plane about the origin, where
u is the angle of rotation.
(b) Rotation through n␪. Show that in (a)
An ϭ

c

cos nu

Ϫsin nu

sin nu

cos nu

d.

Is this plausible? Explain this in words.
(c) Addition formulas for cosine and sine. By
geometry we should have

c

cos a

Ϫsin a

sin a

cos a

ϭ

c

dc

cos b

Ϫsin b

sin b

cos b

d

cos (a ϩ b)

Ϫsin (a ϩ b)

sin (a ϩ b)

cos (a ϩ b)

d.

Derive from this the addition formulas (6) in App. A3.1.

c07.qxd

10/28/10

272

7:30 PM

Page 272

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
(d) Computer graphics. To visualize a threedimensional object with plane faces (e.g., a cube), we
may store the position vectors of the vertices with
respect to a suitable x 1x 2x 3-coordinate system (and a
list of the connecting edges) and then obtain a twodimensional image on a video screen by projecting
the object onto a coordinate plane, for instance, onto
the x 1x 2-plane by setting x 3 ϭ 0. To change the
appearance of the image, we can impose a linear
transformation on the position vectors stored. Show
that a diagonal matrix D with main diagonal entries 3,
1, 12 gives from an x ϭ 3x j4 the new position vector
y ϭ Dx, where y1 ϭ 3x 1 (stretch in the x 1-direction
by a factor 3), y2 ϭ x 2 (unchanged), y3 ϭ 12 x 3 (contraction in the x 3-direction). What effect would a scalar
matrix have?

7.3

(e) Rotations in space. Explain y ϭ Ax geometrically
when A is one of the three matrices

cos ␸
D 0
sin ␸

1

0

0

D0

cos u

Ϫsin u T ,

0

sin u

cos u

0

Ϫsin ␸

1

0

0

cos ␸

T,

cos c

Ϫsin c

D sin c

cos c

0

0

0
0T .
1

What effect would these transformations have in situations
such as that described in (d)?

Linear Systems of Equations.
Gauss Elimination
We now come to one of the most important use of matrices, that is, using matrices to
solve systems of linear equations. We showed informally, in Example 1 of Sec. 7.1, how
to represent the information contained in a system of linear equations by a matrix, called
the augmented matrix. This matrix will then be used in solving the linear system of
equations. Our approach to solving linear systems is called the Gauss elimination method.
Since this method is so fundamental to linear algebra, the student should be alert.
A shorter term for systems of linear equations is just linear systems. Linear systems
model many applications in engineering, economics, statistics, and many other areas.
Electrical networks, traffic flow, and commodity markets may serve as specific examples
of applications.

Linear System, Coefficient Matrix, Augmented Matrix
A linear system of m equations in n unknowns x 1, Á , x n is a set of equations of
the form
a11x1 ϩ Á ϩ a1nxn ϭ b1
a21x1 ϩ Á ϩ a2nxn ϭ b2
(1)
.......................
am1x1 ϩ Á ϩ amnxn ϭ bm.
The system is called linear because each variable x j appears in the first power only, just
as in the equation of a straight line. a11, Á , amn are given numbers, called the coefficients
of the system. b1, Á , bm on the right are also given numbers. If all the bj are zero, then
(1) is called a homogeneous system. If at least one bj is not zero, then (1) is called a
nonhomogeneous system.

c07.qxd

10/28/10

7:30 PM

Page 273

SEC. 7.3 Linear Systems of Equations. Gauss Elimination

273

A solution of (1) is a set of numbers x 1, Á , x n that satisfies all the m equations.
A solution vector of (1) is a vector x whose components form a solution of (1). If the
system (1) is homogeneous, it always has at least the trivial solution x 1 ϭ 0, Á , x n ϭ 0.
Matrix Form of the Linear System (1). From the definition of matrix multiplication
we see that the m equations of (1) may be written as a single vector equation
Ax ϭ b

(2)

where the coefficient matrix A ϭ 3ajk4 is the m ϫ n matrix
a12

Á

a1n

a21

a22

Á

a2n

#

#

Á

#

am1

am2

Á

amn

AϭE

a11

x1

#
U , and x ϭ G # W and

#

b1
.
bϭD . T
.
bm

xn

are column vectors. We assume that the coefficients ajk are not all zero, so that A is
not a zero matrix. Note that x has n components, whereas b has m components. The
matrix
a11

Á

a1n

|

b1

|

#

~
Aϭ E

Á

#

#
|

#

Á

#

am1

Á

amn

|

#

U

|
|

bm

is called the augmented matrix of the system (1). The dashed vertical line could be
~
omitted, as we shall do later. It is merely a reminder that the last column of A did not
come from matrix A but came from vector b. Thus, we augmented the matrix A.
~
Note that the augmented matrix A determines the system (1) completely because it
contains all the given numbers appearing in (1).

EXAMPLE 1

Geometric Interpretation. Existence and Uniqueness of Solutions
If m ϭ n ϭ 2, we have two equations in two unknowns x 1, x 2
a11x 1 ϩ a12x 2 ϭ b1
a 21x 1 ϩ a 22x 2 ϭ b2.
If we interpret x 1, x 2 as coordinates in the x 1x 2-plane, then each of the two equations represents a straight line,
and (x 1, x 2) is a solution if and only if the point P with coordinates x 1, x 2 lies on both lines. Hence there are
three possible cases (see Fig. 158 on next page):
(a) Precisely one solution if the lines intersect
(b) Infinitely many solutions if the lines coincide
(c) No solution if the lines are parallel

c07.qxd

10/28/10

7:30 PM

274

Page 274

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
For instance,

Unique solution

x1 + x2 = 1

x1 + x2 = 1

x1 + x2 = 1

2x1 – x2 = 0

2x1 + 2x2 = 2

x1 + x2 = 0

Case (a)

Case (b)

Case (c)

x2

x2

2
3

P
1
3

Infinitely
many solutions

x2

1

x1

1

x1

x1

1

If the system is homogenous, Case (c) cannot happen, because then those two straight lines pass through the
origin, whose coordinates (0, 0) constitute the trivial solution. Similarly, our present discussion can be extended
from two equations in two unknowns to three equations in three unknowns. We give the geometric interpretation
of three possible cases concerning solutions in Fig. 158. Instead of straight lines we have planes and the solution
depends on the positioning of these planes in space relative to each other. The student may wish to come up
with some specific examples.
᭿

Our simple example illustrated that a system (1) may have no solution. This leads to such
questions as: Does a given system (1) have a solution? Under what conditions does it have
precisely one solution? If it has more than one solution, how can we characterize the set
of all solutions? We shall consider such questions in Sec. 7.5.
First, however, let us discuss an important systematic method for solving linear systems.

Gauss Elimination and Back Substitution
The Gauss elimination method can be motivated as follows. Consider a linear system that
is in triangular form (in full, upper triangular form) such as
2x 1 ϩ 5x 2 ϭ

2

13x 2 ϭ Ϫ26

No solution

Fig. 158. Three
equations in
three unknowns
interpreted as
planes in space

(Triangular means that all the nonzero entries of the corresponding coefficient matrix lie
above the diagonal and form an upside-down 90° triangle.) Then we can solve the system
by back substitution, that is, we solve the last equation for the variable, x 2 ϭ Ϫ26>13 ϭ Ϫ2,
and then work backward, substituting x 2 ϭ Ϫ2 into the first equation and solving it for x 1,
obtaining x 1 ϭ 12 (2 Ϫ 5x 2) ϭ 12 (2 Ϫ 5 # (Ϫ2)) ϭ 6. This gives us the idea of first reducing
a general system to triangular form. For instance, let the given system be
2x 1 ϩ 5x 2 ϭ

2

Ϫ4x 1 ϩ 3x 2 ϭ Ϫ30.

Its augmented matrix is

c

2

5

2

Ϫ4

3

Ϫ30

d.

We leave the first equation as it is. We eliminate x 1 from the second equation, to get a
triangular system. For this we add twice the first equation to the second, and we do the same

c07.qxd

10/28/10

7:30 PM

Page 275

SEC. 7.3 Linear Systems of Equations. Gauss Elimination

275

operation on the rows of the augmented matrix. This gives Ϫ4x 1 ϩ 4x 1 ϩ 3x 2 ϩ 10x 2 ϭ
Ϫ30 ϩ 2 # 2, that is,
2x 1 ϩ 5x 2 ϭ

2

13x 2 ϭ Ϫ26

Row 2 ϩ 2 Row 1

c

2
0

5

2

13 Ϫ26

d

where Row 2 ϩ 2 Row 1 means “Add twice Row 1 to Row 2” in the original matrix. This
is the Gauss elimination (for 2 equations in 2 unknowns) giving the triangular form, from
which back substitution now yields x 2 ϭ Ϫ2 and x 1 ϭ 6, as before.
Since a linear system is completely determined by its augmented matrix, Gauss
elimination can be done by merely considering the matrices, as we have just indicated.
We do this again in the next example, emphasizing the matrices by writing them first and
the equations behind them, just as a help in order not to lose track.

EXAMPLE 2

Gauss Elimination. Electrical Network
Solve the linear system
x1 Ϫ

x2 ϩ

x3 ϭ 0

Ϫx 1 ϩ

x2 Ϫ

x3 ϭ 0

10x 2 ϩ 25x 3 ϭ 90
20x 1 ϩ 10x 2

ϭ 80.

Derivation from the circuit in Fig. 159 (Optional). This is the system for the unknown currents
x 1 ϭ i 1, x 2 ϭ i 2, x 3 ϭ i 3 in the electrical network in Fig. 159. To obtain it, we label the currents as shown,
choosing directions arbitrarily; if a current will come out negative, this will simply mean that the current flows
against the direction of our arrow. The current entering each battery will be the same as the current leaving it.
The equations for the currents result from Kirchhoff’s laws:
Kirchhoff’s Current Law (KCL). At any point of a circuit, the sum of the inflowing currents equals the sum
of the outflowing currents.
Kirchhoff’s Voltage Law (KVL). In any closed loop, the sum of all voltage drops equals the impressed
electromotive force.
Node P gives the first equation, node Q the second, the right loop the third, and the left loop the fourth, as
indicated in the figure.

20 Ω

10 Ω

Q

i1

i3
10 Ω

80 V

i1 –

i2 +

i3 = 0

Node Q:

–i1 +

i2 –

i3 = 0

90 V
Right loop:

i2
P

Node P:

15 Ω

Left loop:

10i2 + 25i3 = 90
20i1 + 10i2

= 80

Fig. 159. Network in Example 2 and equations relating the currents

Solution by Gauss Elimination. This system could be solved rather quickly by noticing its particular
form. But this is not the point. The point is that the Gauss elimination is systematic and will work in general,

c07.qxd

10/29/10

276

11:01 PM

Page 276

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
also for large systems. We apply it to our system and then do back substitution. As indicated, let us write the
augmented matrix of the system first and then the system itself:
~
Augmented Matrix A
1

Ϫ1

1

Ϫ1

1

Ϫ1

0

10

25

20

10

0

Pivot 1 wwö

wö
Eliminate w

E

Equations
Pivot 1 wwö

0

|
|
|
|
|
|
|

0

U

x1 Ϫ

x2 ϩ

x3 ϭ 0

Ϫx 1 ϩ

x2 Ϫ

x3 ϭ 0

10x 2 ϩ 25x 3 ϭ 90

wö
Eliminate w

90

20x 1 ϩ 10x 2

80

ϭ 80.

Step 1. Elimination of x1
Call the first row of A the pivot row and the first equation the pivot equation. Call the coefficient 1 of its
x 1-term the pivot in this step. Use this equation to eliminate x 1 (get rid of x 1) in the other equations. For this, do:
Add 1 times the pivot equation to the second equation.
Add Ϫ20 times the pivot equation to the fourth equation.
This corresponds to row operations on the augmented matrix as indicated in BLUE behind the new matrix in
(3). So the operations are performed on the preceding matrix. The result is
1

Ϫ1

1

0

0

0

0

10

25

0

30

Ϫ20

E

(3)

|
|
|
|
|
|
|
|
|

x1 Ϫ

0
0

x2 ϩ

x3 ϭ 0

Row 2 ϩ Row 1

U

0ϭ 0
10x 2 ϩ 25x 3 ϭ 90

90

Row 4 Ϫ 20 Row 1

80

30x 2 Ϫ 20x 3 ϭ 80.

Step 2. Elimination of x2
The first equation remains as it is. We want the new second equation to serve as the next pivot equation. But
since it has no x2-term (in fact, it is 0 ϭ 0), we must first change the order of the equations and the corresponding
rows of the new matrix. We put 0 ϭ 0 at the end and move the third equation and the fourth equation one place
up. This is called partial pivoting (as opposed to the rarely used total pivoting, in which the order of the
unknowns is also changed). It gives
1

Ϫ1

1

Pivot 10 wwö 0
E
Eliminate 30 wwö 0

10

25

30

Ϫ20

0

0

0

|
|
|
|
|
|
|

x1 Ϫ

0

x2 ϩ

x3 ϭ 0

90

Pivot 10 wwwö 10x2 ϩ 25x3 ϭ 90

80

wö 30x2 Ϫ 20x3 ϭ 80
Eliminate 30x2 w

0

0 ϭ 0.

U

To eliminate x 2, do:
Add Ϫ3 times the pivot equation to the third equation.
The result is

(4)

1

Ϫ1

1

0

10

25

0

0

Ϫ95

0

0

0

E

|
|
|
|
|
|
|

x1 Ϫ

0
90

U

Ϫ190

Row 3 Ϫ 3 Row 2

x2 ϩ

x3 ϭ

0

10x2 ϩ 25x3 ϭ

90

Ϫ 95x3 ϭ Ϫ190
0ϭ

0

0.

Back Substitution. Determination of x3, x2, x1 (in this order)
Working backward from the last to the first equation of this “triangular” system (4), we can now readily find
x 3, then x 2, and then x 1:
x 3 ϭ i 3 ϭ 2 3A4

Ϫ 95x 3 ϭ Ϫ190
10x 2 ϩ 25x 3 ϭ
x1 Ϫ

x2 ϩ

x3 ϭ

90
0

x2 ϭ

1
10 (90

Ϫ 25x 3) ϭ i 2 ϭ 4 3A4

x 1 ϭ x 2 Ϫ x 3 ϭ i 1 ϭ 2 3A4

where A stands for “amperes.” This is the answer to our problem. The solution is unique.

᭿

c07.qxd

10/28/10

7:30 PM

Page 277

SEC. 7.3 Linear Systems of Equations. Gauss Elimination

277

Elementary Row Operations. Row-Equivalent Systems
Example 2 illustrates the operations of the Gauss elimination. These are the first two of
three operations, which are called

Elementary Row Operations for Matrices:
Interchange of two rows
Addition of a constant multiple of one row to another row
Multiplication of a row by a nonzero constant c
CAUTION! These operations are for rows, not for columns! They correspond to the
following

Elementary Operations for Equations:
Interchange of two equations
Addition of a constant multiple of one equation to another equation
Multiplication of an equation by a nonzero constant c
Clearly, the interchange of two equations does not alter the solution set. Neither does their
addition because we can undo it by a corresponding subtraction. Similarly for their
multiplication, which we can undo by multiplying the new equation by 1>c (since c 0),
producing the original equation.
We now call a linear system S1 row-equivalent to a linear system S2 if S1 can be
obtained from S2 by (finitely many!) row operations. This justifies Gauss elimination and
establishes the following result.
THEOREM 1

Row-Equivalent Systems

Row-equivalent linear systems have the same set of solutions.
Because of this theorem, systems having the same solution sets are often called
equivalent systems. But note well that we are dealing with row operations. No column
operations on the augmented matrix are permitted in this context because they would
generally alter the solution set.
A linear system (1) is called overdetermined if it has more equations than unknowns,
as in Example 2, determined if m ϭ n, as in Example 1, and underdetermined if it has
fewer equations than unknowns.
Furthermore, a system (1) is called consistent if it has at least one solution (thus, one
solution or infinitely many solutions), but inconsistent if it has no solutions at all, as
x 1 ϩ x 2 ϭ 1, x 1 ϩ x 2 ϭ 0 in Example 1, Case (c).

Gauss Elimination: The Three Possible
Cases of Systems
We have seen, in Example 2, that Gauss elimination can solve linear systems that have a
unique solution. This leaves us to apply Gauss elimination to a system with infinitely
many solutions (in Example 3) and one with no solution (in Example 4).

c07.qxd

10/28/10

7:30 PM

278
EXAMPLE 3

Page 278

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
Gauss Elimination if Infinitely Many Solutions Exist
Solve the following linear system of three equations in four unknowns whose augmented matrix is

(5)

3.0

2.0

2.0

Ϫ5.0

D0.6

1.5

1.5

Ϫ5.4

1.2

Ϫ0.3

Ϫ0.3

2.4

3.0x 1 ϩ 2.0x 2 ϩ 2.0x 3 Ϫ 5.0x 4 ϭ 8.0

8.0

|
|
|
|
|

2.7T .

Thus,

0.6x 1 ϩ 1.5x 2 ϩ 1.5x 3 Ϫ 5.4x 4 ϭ 2.7
1.2x 1 Ϫ 0.3x 2 Ϫ 0.3x 3 ϩ 2.4x 4 ϭ 2.1.

2.1

Solution.

As in the previous example, we circle pivots and box terms of equations and corresponding
entries to be eliminated. We indicate the operations in terms of equations and operate on both equations and
matrices.
Step 1. Elimination of x1 from the second and third equations by adding
Ϫ0.6>3.0 ϭ Ϫ0.2 times the first equation to the second equation,
Ϫ1.2>3.0 ϭ Ϫ0.4 times the first equation to the third equation.

This gives the following, in which the pivot of the next step is circled.
2.0

2.0

Ϫ5.0

D0

1.1

1.1

Ϫ4.4

0

Ϫ1.1

Ϫ1.1

4.4

3.0
(6)

|
|
|
|
|

8.0
1.1T

3.0x1 ϩ 2.0x2 ϩ 2.0x3 Ϫ 5.0x4 ϭ

8.0

1.1x2 ϩ 1.1x3 Ϫ 4.4x4 ϭ

1.1

Row 2 Ϫ 0.2 Row 1

Ϫ1.1

Row 3 Ϫ 0.4 Row 1

Ϫ1.1x2 Ϫ 1.1x3 ϩ 4.4x4 ϭ Ϫ1.1.

Step 2. Elimination of x2 from the third equation of (6) by adding
1.1>1.1 ϭ 1 times the second equation to the third equation.
This gives
3.0
(7)

D0
0

2.0

2.0

Ϫ5.0

1.1

1.1

Ϫ4.4

0

0

0

|
|
|
|
|

3.0x 1 ϩ 2.0x 2 ϩ 2.0x 3 Ϫ 5.0x 4 ϭ 8.0

8.0
1.1T
0

1.1x 2 ϩ 1.1x 3 Ϫ 4.4x 4 ϭ 1.1
Row 3 ϩ Row 2

0 ϭ 0.

Back Substitution. From the second equation, x 2 ϭ 1 Ϫ x 3 ϩ 4x 4. From this and the first equation,

x 1 ϭ 2 Ϫ x 4. Since x 3 and x 4 remain arbitrary, we have infinitely many solutions. If we choose a value of x 3
and a value of x 4, then the corresponding values of x 1 and x 2 are uniquely determined.

On Notation. If unknowns remain arbitrary, it is also customary to denote them by other letters t 1, t 2, Á .
In this example we may thus write x 1 ϭ 2 Ϫ x 4 ϭ 2 Ϫ t 2, x 2 ϭ 1 Ϫ x 3 ϩ 4x 4 ϭ 1 Ϫ t 1 ϩ 4t 2, x 3 ϭ t 1 (first
arbitrary unknown), x 4 ϭ t 2 (second arbitrary unknown).
᭿
EXAMPLE 4

Gauss Elimination if no Solution Exists
What will happen if we apply the Gauss elimination to a linear system that has no solution? The answer is that
in this case the method will show this fact by producing a contradiction. For instance, consider
3

2

1

D2

1

1

6

2

4

|
|
|
|
|

3

3x 1 ϩ 2x 2 ϩ x 3 ϭ 3

0T

2x 1 ϩ x 2 ϩ x 3 ϭ 0

6

6x 1 ϩ 2x 2 ϩ 4x 3 ϭ 6.

Step 1. Elimination of x1 from the second and third equations by adding
Ϫ23 times the first equation to the second equation,
Ϫ63 ϭ Ϫ2 times the first equation to the third equation.

10/28/10

7:30 PM

Page 279

SEC. 7.3 Linear Systems of Equations. Gauss Elimination

279

This gives
3

2

1

D0

Ϫ13

1
3

0

Ϫ2

2

|
|
|
|
|

3x 1 ϩ 2x 2 ϩ x 3 ϭ

3

3

Ϫ2T

Row 2 Ϫ _32 Row 1

Ϫ

0

Row 3 Ϫ 2 Row 1

Ϫ 2x 2 ϩ 2x 3 ϭ

0.

3x 1 ϩ 2x 2 ϩ x 3 ϭ

3

1
3 x2

1
3 x3

ϩ

ϭ Ϫ2

Step 2. Elimination of x2 from the third equation gives
3

2

1

D0

Ϫ13

1
3

0

0

0

|
|
|
|
|

3
Ϫ2T
12

Ϫ

1
3 x2

1
3x 3

ϩ

Row 3 Ϫ 6 Row 2

ϭϪ 2

0ϭ

12.

᭿

The false statement 0 ϭ 12 shows that the system has no solution.

Row Echelon Form and Information From It
At the end of the Gauss elimination the form of the coefficient matrix, the augmented
matrix, and the system itself are called the row echelon form. In it, rows of zeros, if
present, are the last rows, and, in each nonzero row, the leftmost nonzero entry is farther
to the right than in the previous row. For instance, in Example 4 the coefficient matrix
and its augmented in row echelon form are

(8)

3

2

D0

Ϫ13

0

0

1
1
3T

and

3

2

1

D0

Ϫ13

1
3

0

0

0

0

|
|
|
|
|
|

3
Ϫ2T .
12

Note that we do not require that the leftmost nonzero entries be 1 since this would have
no theoretic or numeric advantage. (The so-called reduced echelon form, in which those
entries are 1, will be discussed in Sec. 7.8.)
The original system of m equations in n unknowns has augmented matrix 3A | b4. This
is to be row reduced to matrix 3R | f 4. The two systems Ax ϭ b and Rx ϭ f are equivalent:
if either one has a solution, so does the other, and the solutions are identical.
At the end of the Gauss elimination (before the back substitution), the row echelon form
of the augmented matrix will be
.
.
.
.
.

r11 r12

.
rrr

.
.
.

..

(9)

.
.
.
.
.

r22

X

c07.qxd

r1n
r2n
.
.
.
rrn

f1
f
.2
.
.
fr
.
fr+1
.
.
.
fm

X

Here, r Ϲ m, r11 0, and all entries in the blue triangle and blue rectangle are zero.
The number of nonzero rows, r, in the row-reduced coefficient matrix R is called the
rank of R and also the rank of A. Here is the method for determining whether Ax ϭ b
has solutions and what they are:
(a) No solution. If r is less than m (meaning that R actually has at least one row of
all 0s) and at least one of the numbers frϩ1, frϩ2, Á , fm is not zero, then the system

c07.qxd

10/28/10

7:30 PM

Page 280

280

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

Rx ϭ f is inconsistent: No solution is possible. Therefore the system Ax ϭ b is
inconsistent as well. See Example 4, where r ϭ 2 Ͻ m ϭ 3 and frϩ1 ϭ f3 ϭ 12.
If the system is consistent (either r ϭ m, or r Ͻ m and all the numbers frϩ1, frϩ2, Á , fm
are zero), then there are solutions.
(b) Unique solution. If the system is consistent and r ϭ n, there is exactly one
solution, which can be found by back substitution. See Example 2, where r ϭ n ϭ 3
and m ϭ 4.
(c) Infinitely many solutions. To obtain any of these solutions, choose values of
x rϩ1, Á , x n arbitrarily. Then solve the rth equation for x r (in terms of those
arbitrary values), then the (r Ϫ 1)st equation for x r؊1, and so on up the line. See
Example 3.
Orientation. Gauss elimination is reasonable in computing time and storage demand.
We shall consider those aspects in Sec. 20.1 in the chapter on numeric linear algebra.
Section 7.4 develops fundamental concepts of linear algebra such as linear independence
and rank of a matrix. These in turn will be used in Sec. 7.5 to fully characterize the
behavior of linear systems in terms of existence and uniqueness of solutions.

PROBLEM SET 7.3
GAUSS ELIMINATION

1–14

Ϫ3x ϩ 8y ϭ

xϩ yϪ zϭ

3.

5.

c

10
9

1.5

4.

4.5

4

1

6.0
0

8y ϩ 6z ϭ Ϫ6

D 5

Ϫ3

1

Ϫ2x ϩ 4y Ϫ 6z ϭ 40

Ϫ9

2

Ϫ1

4

Ϫ8

3

DϪ1

2

Ϫ5

3

Ϫ6

1

13

12

Ϫ6

DϪ4

7

Ϫ73T

11

Ϫ13

7.

6.

157

2

4

1

DϪ1

1

Ϫ2

4

0

6

0

Ϫ2y Ϫ 2z ϭ Ϫ8

9.

3x ϩ 2y
10.

3x ϩ 4y Ϫ 5z ϭ 13
11. 0

5

5

Ϫ10

D2

Ϫ3

Ϫ3

6

4

1

1

Ϫ2

0
2T
4

c

Ϫ2

4

0

0

DϪ3

3

Ϫ6

5

15T

1

Ϫ1

2

0

0

10x ϩ 4y Ϫ 2z ϭ Ϫ4

13.

Ϫ3w Ϫ 17x ϩ

4
2T
5

7

ϭ5
17

Ϫ15

21

Ϫ9

50

8w Ϫ 34x ϩ 16y Ϫ 10z ϭ

4

1

Ϫ11

1

5

Ϫ2

5

Ϫ4

5

1

Ϫ1

3

Ϫ3

3

3

4

Ϫ7

2

Ϫ7

E

Ϫ21T

3

6

d

y

3

16

Ϫ7

2

ϭ

xϩ

2

14.

5

y ϩ 2z ϭ

wϩ

Ϫ zϭ2

2x

0

d

4y ϩ 3z ϭ 8

8.

0T

2

12.

Solve the linear system given explicitly or by its augmented
matrix. Show details.
1. 4x Ϫ 6y ϭ Ϫ11
0.6
2. 3.0 Ϫ0.5

U

15. Equivalence relation. By definition, an equivalence
relation on a set is a relation satisfying three conditions:
(named as indicated)
(i) Each element A of the set is equivalent to itself
(Reflexivity).
(ii) If A is equivalent to B, then B is equivalent to A
(Symmetry).
(iii) If A is equivalent to B and B is equivalent to C,
then A is equivalent to C (Transitivity).
Show that row equivalence of matrices satisfies these
three conditions. Hint. Show that for each of the three
elementary row operations these conditions hold.

c07.qxd

10/28/10

7:30 PM

Page 281

SEC. 7.3 Linear Systems of Equations. Gauss Elimination
16. CAS PROJECT. Gauss Elimination and Back
Substitution. Write a program for Gauss elimination
and back substitution (a) that does not include pivoting
and (b) that does include pivoting. Apply the programs
to Probs. 11–14 and to some larger systems of your
choice.

MODELS OF NETWORKS

17–21

In Probs. 17–19, using Kirchhoff’s laws (see Example 2)
and showing the details, find the currents:
17.
16 V
I1

2Ω

1Ω

2Ω
I3

4Ω

I2

32 V

19.

8Ω

12 Ω
24 V

12 V

I2
I1

I3

I1

I3
I2

E0 V

R2 Ω

R1 Ω

20. Wheatstone bridge. Show that if Rx>R3 ϭ R1>R2 in
the figure, then I ϭ 0. (R0 is the resistance of the
instrument by which I is measured.) This bridge is a
method for determining Rx. R1, R2, R3 are known. R3
is variable. To get Rx, make I ϭ 0 by varying R3. Then
calculate Rx ϭ R3R1>R2.
400
Rx

R3

D1 ϭ 40 Ϫ 2P1 Ϫ P2,

S1 ϭ 4P1 Ϫ P2 ϩ 4,

D2 ϭ 5P1 Ϫ 2P2 ϩ 16,

S2 ϭ 3P2 Ϫ 4.

24. PROJECT. Elementary Matrices. The idea is that
elementary operations can be accomplished by matrix
multiplication. If A is an m ϫ n matrix on which we
want to do an elementary operation, then there is a
matrix E such that EA is the new matrix after the
operation. Such an E is called an elementary matrix.
This idea can be helpful, for instance, in the design
of algorithms. (Computationally, it is generally preferable to do row operations directly, rather than by
multiplication by E.)
(a) Show that the following are elementary matrices,
for interchanging Rows 2 and 3, for adding Ϫ5 times
the first row to the third, and for multiplying the fourth
row by 8.
1

0

0

0

0

0

1

0

0

1

0

0

0

0

0

1

800

1

0

0

0

1

0

0

1200

0
E2 ϭ E
Ϫ5

0

1

0

0

0

0

1

1

0

0

0

0

1

0

0

0

0

1

0

0

0

0

8

E1 ϭ E

x4

x2
x3

1000

U,

800

600

R2

22. Models of markets. Determine the equilibrium
solution (D1 ϭ S1, D2 ϭ S2) of the two-commodity
market with linear model (D, S, P ϭ demand, supply,
price; index 1 ϭ first commodity, index 2 ϭ second
commodity)

x1

R0
R1

the analog of Kirchhoff’s Current Law, find the traffic
flow (cars per hour) in the net of one-way streets (in
the directions indicated by the arrows) shown in the
figure. Is the solution unique?

23. Balancing a chemical equation x 1C3H 8 ϩ x 2O2 :
x 3CO2 ϩ x 4H 2O means finding integer x 1, x 2, x 3, x 4
such that the numbers of atoms of carbon (C), hydrogen
(H), and oxygen (O) are the same on both sides of this
reaction, in which propane C3H 8 and O2 give carbon
dioxide and water. Find the smallest positive integers
x 1, Á , x 4.

18.
4Ω

281

600

1000

Wheatstone bridge

Net of one-way streets

Problem 20

Problem 21

21. Traffic flow. Methods of electrical circuit analysis
have applications to other fields. For instance, applying

E3 ϭ E

U,

U.

c07.qxd

10/28/10

282

7:30 PM

Page 282

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
Apply E1, E2, E3 to a vector and to a 4 ϫ 3 matrix of
your choice. Find B ϭ E3E2E1A, where A ϭ 3ajk4 is
the general 4 ϫ 2 matrix. Is B equal to C ϭ E1E2E3A?
(b) Conclude that E1, E2, E3 are obtained by doing
the corresponding elementary operations on the 4 ϫ 4

7.4

unit matrix. Prove that if M is obtained from A by an
elementary row operation, then
M ϭ EA,
where E is obtained from the n ϫ n unit matrix I n by
the same row operation.

Linear Independence. Rank of a Matrix.
Vector Space
Since our next goal is to fully characterize the behavior of linear systems in terms
of existence and uniqueness of solutions (Sec. 7.5), we have to introduce new
fundamental linear algebraic concepts that will aid us in doing so. Foremost among
these are linear independence and the rank of a matrix. Keep in mind that these
concepts are intimately linked with the important Gauss elimination method and how
it works.

Linear Independence and Dependence of Vectors
Given any set of m vectors a(1), Á , a(m) (with the same number of components), a linear
combination of these vectors is an expression of the form
c1a(1) ϩ c2a(2) ϩ Á ϩ cma(m)
where c1, c2, Á , cm are any scalars. Now consider the equation
(1)

c1a(1) ϩ c2a(2) ϩ Á ϩ cma(m) ϭ 0.

Clearly, this vector equation (1) holds if we choose all cj’s zero, because then it becomes
0 ϭ 0. If this is the only m-tuple of scalars for which (1) holds, then our vectors
a(1), Á , a(m) are said to form a linearly independent set or, more briefly, we call them
linearly independent. Otherwise, if (1) also holds with scalars not all zero, we call these
vectors linearly dependent. This means that we can express at least one of the vectors
as a linear combination of the other vectors. For instance, if (1) holds with, say,
c1 0, we can solve (1) for a(1):
a(1) ϭ k 2a(2) ϩ Á ϩ k ma(m)

where k j ϭ Ϫcj>c1.

(Some k j’s may be zero. Or even all of them, namely, if a(1) ϭ 0.)
Why is linear independence important? Well, if a set of vectors is linearly
dependent, then we can get rid of at least one or perhaps more of the vectors until we
get a linearly independent set. This set is then the smallest “truly essential” set with
which we can work. Thus, we cannot express any of the vectors, of this set, linearly
in terms of the others.

c07.qxd

10/28/10

7:30 PM

Page 283

SEC. 7.4 Linear Independence. Rank of a Matrix. Vector Space
EXAMPLE 1

283

Linear Independence and Dependence
The three vectors
a(1) ϭ 3 3

a(2) ϭ 3Ϫ6
a(3) ϭ 3 21

0

2

24

42

24

544

Ϫ21

0

Ϫ154

are linearly dependent because
6a(1) Ϫ 12 a(2) Ϫ a(3) ϭ 0.
Although this is easily checked by vector arithmetic (do it!), it is not so easy to discover. However, a systematic
method for finding out about linear independence and dependence follows below.
The first two of the three vectors are linearly independent because c1a(1) ϩ c2a(2) ϭ 0 implies c2 ϭ 0 (from
the second components) and then c1 ϭ 0 (from any other component of a(1).
᭿

Rank of a Matrix
The rank of a matrix A is the maximum number of linearly independent row vectors
of A. It is denoted by rank A.

DEFINITION

Our further discussion will show that the rank of a matrix is an important key concept for
understanding general properties of matrices and linear systems of equations.
EXAMPLE 2

Rank
The matrix

(2)

3

0

2

2

A ϭ DϪ6

42

24

54T

21

Ϫ21

0

Ϫ15

has rank 2, because Example 1 shows that the first two row vectors are linearly independent, whereas all three
row vectors are linearly dependent.
Note further that rank A ϭ 0 if and only if A ϭ 0. This follows directly from the definition.
᭿

We call a matrix A 1 row-equivalent to a matrix A 2 if A 1 can be obtained from A 2 by
(finitely many!) elementary row operations.
Now the maximum number of linearly independent row vectors of a matrix does not
change if we change the order of rows or multiply a row by a nonzero c or take a linear
combination by adding a multiple of a row to another row. This shows that rank is
invariant under elementary row operations:
THEOREM 1

Row-Equivalent Matrices

Row-equivalent matrices have the same rank.
Hence we can determine the rank of a matrix by reducing the matrix to row-echelon
form, as was done in Sec. 7.3. Once the matrix is in row-echelon form, we count the
number of nonzero rows, which is precisely the rank of the matrix.

c07.qxd

10/28/10

7:30 PM

284

Page 284

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

EXAMPLE 3

Determination of Rank
For the matrix in Example 2 we obtain successively
3

0

2

2

A ϭ DϪ6

42

24

21

Ϫ21

0

Ϫ15

3

0

2

2

D 0

42

28

0

Ϫ21

Ϫ14

Ϫ29

3

0

2

2

D 0

42

28

58 T

0

0

0

0

54 T (given)

58 T Row 2 ϩ 2 Row 1
Row 3 Ϫ 7 Row 1

Row 3 ϩ 12 Row 2.

The last matrix is in row-echelon form and has two nonzero rows. Hence rank A ϭ 2, as before.

᭿

Examples 1–3 illustrate the following useful theorem (with p ϭ 3, n ϭ 3, and the rank of
the matrix ϭ 2).
THEOREM 2

Linear Independence and Dependence of Vectors

Consider p vectors that each have n components. Then these vectors are linearly
independent if the matrix formed, with these vectors as row vectors, has rank p.
However, these vectors are linearly dependent if that matrix has rank less than p.

Further important properties will result from the basic
THEOREM 3

Rank in Terms of Column Vectors

The rank r of a matrix A equals the maximum number of linearly independent
column vectors of A.
Hence A and its transpose AT have the same rank.

PROOF

In this proof we write simply “rows” and “columns” for row and column vectors. Let A
be an m ϫ n matrix of rank A ϭ r. Then by definition of rank, A has r linearly independent
rows which we denote by v(1), Á , v(r) (regardless of their position in A), and all the rows
a(1), Á , a(m) of A are linear combinations of those, say,
a(1) ϭ c11v(1) ϩ c12v(2) ϩ Á ϩ c1rv(r)
(3)

a(2) ϭ c21v(1) ϩ c22v(2) ϩ Á ϩ c2rv(r)
.
.
.
.
.
.
.
.
.
.
.
.
a(m) ϭ cm1v(1) ϩ cm2v(2) ϩ Á ϩ cmrv(r).

c07.qxd

10/28/10

7:30 PM

Page 285

SEC. 7.4 Linear Independence. Rank of a Matrix. Vector Space

285

These are vector equations for rows. To switch to columns, we write (3) in terms of
components as n such systems, with k ϭ 1, Á , n,
a1k ϭ c11v1k ϩ c12v2k ϩ Á ϩ c1rvrk
a2k ϭ c21v1k ϩ c22v2k ϩ Á ϩ c2rvrk
.
.
.
.
.
.
.
.
.
.
.
.
amk ϭ cm1v1k ϩ cm2v2k ϩ Á ϩ cmrvrk

(4)

and collect components in columns. Indeed, we can write (4) as

(5)

a1k

c11

c12

c1r

a2k

c21

c22

c2r

.
.

.
.

.
.

.
.

amk

cm1

cm2

cmr

E . U ϭ v1k E . U ϩ v2k E . U ϩ Á ϩ vrk E . U

where k ϭ 1, Á , n. Now the vector on the left is the kth column vector of A. We see that
each of these n columns is a linear combination of the same r columns on the right. Hence
A cannot have more linearly independent columns than rows, whose number is rank A ϭ r.
Now rows of A are columns of the transpose AT. For AT our conclusion is that AT cannot
have more linearly independent columns than rows, so that A cannot have more linearly
independent rows than columns. Together, the number of linearly independent columns
᭿
of A must be r, the rank of A. This completes the proof.
EXAMPLE 4

Illustration of Theorem 3
The matrix in (2) has rank 2. From Example 3 we see that the first two row vectors are linearly independent
and by “working backward” we can verify that Row 3 ϭ 6 Row 1 Ϫ 12 Row 2. Similarly, the first two columns
are linearly independent, and by reducing the last matrix in Example 3 by columns we find that
Column 3 ϭ 23 Column 1 ϩ 23 Column 2

and

Column 4 ϭ 23 Column 1 ϩ 29
21 Column 2.

᭿

Combining Theorems 2 and 3 we obtain
THEOREM 4

Linear Dependence of Vectors

Consider p vectors each having n components. If n Ͻ p, then these vectors are
linearly dependent.
PROOF

The matrix A with those p vectors as row vectors has p rows and n Ͻ p columns; hence
by Theorem 3 it has rank A Ϲ n Ͻ p, which implies linear dependence by Theorem 2. ᭿

Vector Space
The following related concepts are of general interest in linear algebra. In the present
context they provide a clarification of essential properties of matrices and their role in
connection with linear systems.

c07.qxd

10/28/10

7:30 PM

286

Page 286

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

Consider a nonempty set V of vectors where each vector has the same number of
components. If, for any two vectors a and b in V, we have that all their linear combinations
aa ϩ bb (a, b any real numbers) are also elements of V, and if, furthermore, a and b satisfy
the laws (3a), (3c), (3d), and (4) in Sec. 7.1, as well as any vectors a, b, c in V satisfy (3b)
then V is a vector space. Note that here we wrote laws (3) and (4) of Sec. 7.1 in lowercase
letters a, b, c, which is our notation for vectors. More on vector spaces in Sec. 7.9.
The maximum number of linearly independent vectors in V is called the dimension of
V and is denoted by dim V. Here we assume the dimension to be finite; infinite dimension
will be defined in Sec. 7.9.
A linearly independent set in V consisting of a maximum possible number of vectors
in V is called a basis for V. In other words, any largest possible set of independent vectors
in V forms basis for V. That means, if we add one or more vector to that set, the set will
be linearly dependent. (See also the beginning of Sec. 7.4 on linear independence and
dependence of vectors.) Thus, the number of vectors of a basis for V equals dim V.
The set of all linear combinations of given vectors a(1), Á , a(p) with the same number
of components is called the span of these vectors. Obviously, a span is a vector space. If
in addition, the given vectors a(1), Á , a(p) are linearly independent, then they form a basis
for that vector space.
This then leads to another equivalent definition of basis. A set of vectors is a basis for
a vector space V if (1) the vectors in the set are linearly independent, and if (2) any vector
in V can be expressed as a linear combination of the vectors in the set. If (2) holds, we
also say that the set of vectors spans the vector space V.
By a subspace of a vector space V we mean a nonempty subset of V (including V itself)
that forms a vector space with respect to the two algebraic operations (addition and scalar
multiplication) defined for the vectors of V.
EXAMPLE 5

Vector Space, Dimension, Basis
The span of the three vectors in Example 1 is a vector space of dimension 2. A basis of this vector space consists
of any two of those three vectors, for instance, a(1), a(2), or a(1), a(3), etc.
᭿

We further note the simple
THEOREM 5

Vector Space Rn

The vector space Rn consisting of all vectors with n components (n real numbers)
has dimension n.
PROOF

A basis of n vectors is a(1) ϭ 31 0
a(n) ϭ 30 Á 0 14.

Á

04, a(2) ϭ 30 1

0

Á

04,

Á,
᭿

For a matrix A, we call the span of the row vectors the row space of A. Similarly, the
span of the column vectors of A is called the column space of A.
Now, Theorem 3 shows that a matrix A has as many linearly independent rows as
columns. By the definition of dimension, their number is the dimension of the row space
or the column space of A. This proves
THEOREM 6

Row Space and Column Space

The row space and the column space of a matrix A have the same dimension, equal
to rank A.

c07.qxd

10/28/10

7:30 PM

Page 287

SEC. 7.4 Linear Independence. Rank of a Matrix. Vector Space

287

Finally, for a given matrix A the solution set of the homogeneous system Ax ϭ 0 is a
vector space, called the null space of A, and its dimension is called the nullity of A. In
the next section we motivate and prove the basic relation
rank A ϩ nullity A ϭ Number of columns of A.

(6)

PROBLEM SET 7.4
RANK, ROW SPACE, COLUMN SPACE

1–10

Find the rank. Find a basis for the row space. Find a basis
for the column space. Hint. Row-reduce the matrix and its
transpose. (You may omit obvious factors from the vectors
of these bases.)
1.

c

4

Ϫ2

6

1

Ϫ3

Ϫ2
0

3

5

3. D3

5

0T

5

0

0.2

Ϫ0.1

5. D0
0.1

2.

c

a

b

b

a

Ϫ0.3T

0

Ϫ2.1

0

4

0

7. D0

2

0

4T

4

0

2

12. rank BTAT ϭ rank AB. (Note the order!)
13. rank A ϭ rank B does not imply rank A2 ϭ rank B2.
(Give a counterexample.)

15. If the row vectors of a square matrix are linearly
independent, so are the column vectors, and conversely.

4. DϪ4

0

2T

0

2

6

0

1

0

0

16. Give examples showing that the rank of a product of
matrices cannot exceed the rank of either factor.
17–25

6. DϪ1

0

Ϫ4T

0

4

0

LINEAR INDEPENDENCE

Are the following sets of vectors linearly independent?
Show the details of your work.

2

4

8

16

16
8. E
4

8

4

2

8

16

2

2

16

8

4

17. 33 4 0 24, 32 Ϫ1 3 74,
31 16 Ϫ12 Ϫ224
18. 31
314

U

19. 30
20. 31
34

0

9

0

1

0

5

Ϫ2

1

0

0

0

1

0

0

Ϫ4

1

1

1

1

1

Ϫ2
10. E
1

Ϫ4

Ϫ11

2

0

0

1

0

0

1

2

0

U

Show the following:

14. If A is not square, either the row vectors or the column
vectors of A are linearly dependent.

Ϫ4

0.4

1.1

d

6

10

8

9. E

d

GENERAL PROPERTIES OF RANK

12–16

U

11. CAS Experiment. Rank. (a) Show experimentally
that the n ϫ n matrix A ϭ 3ajk4 with ajk ϭ j ϩ k Ϫ 1
has rank 2 for any n. (Problem 20 shows n ϭ 4.) Try
to prove it.
(b) Do the same when ajk ϭ j ϩ k ϩ c, where c is any
positive integer.
(c) What is rank A if ajk ϭ 2 jϩk؊2? Try to find other
large matrices of low rank independent of n.

21. 32
32

1
2
1
5

1
3
1
6

1

14,

2
5

3
6

0
0

0
1

1
4 4,
1
74

312

31 1

74, 32
04

8

7

6

1
5 4,

313

30 0

1
4

1
6 4,

1
5

14

3

4

54,

33 4

5

64,

0

0

84,

32 0

0

94,

30 0

04,

33.0 Ϫ0.6 1.54

39 7

54,

24. 34 Ϫ1 34, 30
32 6 14

1
4

14,

44, 32
74

22. 30.4 Ϫ0.2 0.24,
23. 39

1
3

8

5 3

14, 31

25. 36 0 Ϫ1 3], 32 2
3Ϫ4 Ϫ4 Ϫ4 Ϫ44

5

3

14
Ϫ54,

04,

26. Linearly independent subset.
last of the vectors 33 0 1
312 1 2 44, 36 0 2 44,
omit one after another until
independent set.

Beginning with the
24, 36 1 0 04,
and [9 0 1 2],
you get a linearly

c07.qxd

10/28/10

7:30 PM

288
27–35

Page 288

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

VECTOR SPACE

31. All vectors in R 5 with positive components

Is the given set of vectors a vector space? Give reasons. If
your answer is yes, determine the dimension and find a
basis. (v1, v2, Á denote components.)
27. All vectors in R3 with v1 Ϫ v2 ϩ 2v3 ϭ 0
28. All vectors in R 3 with 3v2 ϩ v3 ϭ k
29. All vectors in R2 with v1 м v2
30. All vectors in R n with the first n Ϫ 2 components zero

7.5

32. All vectors in R3 with 3v1 Ϫ 2v2 ϩ v3 ϭ 0,
4v1 ϩ 5v2 ϭ 0
33. All vectors in R 3 with 3v1 Ϫ v3 ϭ 0,
2v1 ϩ 3v2 Ϫ 4v3 ϭ 0
34. All vectors in Rn with ƒ vj ƒ ϭ 1 for j ϭ 1, Á , n
35. All vectors in R 4 with v1 ϭ 2v2 ϭ 3v3 ϭ 4v4

Solutions of Linear Systems:
Existence, Uniqueness
Rank, as just defined, gives complete information about existence, uniqueness, and general
structure of the solution set of linear systems as follows.
A linear system of equations in n unknowns has a unique solution if the coefficient
matrix and the augmented matrix have the same rank n, and infinitely many solutions if
that common rank is less than n. The system has no solution if those two matrices have
different rank.
To state this precisely and prove it, we shall use the generally important concept of a
submatrix of A. By this we mean any matrix obtained from A by omitting some rows or
columns (or both). By definition this includes A itself (as the matrix obtained by omitting
no rows or columns); this is practical.

THEOREM 1

Fundamental Theorem for Linear Systems

(a) Existence. A linear system of m equations in n unknowns x1, Á , xn
a11 x 1 ϩ a12 x 2 ϩ Á ϩ a1n xn ϭ b1
a21x 1 ϩ a22x 2 ϩ Á ϩ a2nx n ϭ b2

(1)

#################################
am1x 1 ϩ am2 x 2 ϩ Á ϩ amnx n ϭ bm
is consistent, that is, has solutions, if and only if the coefficient matrix A and the
ෂ
augmented matrix A have the same rank. Here,

AϭE

a11

Á

a1n

#

Á

#

#

Á

#

am1

Á

amn

ෂ
U and A ϭ E

a11

Á

a1n

b1

#

Á

#

#

#

Á

#

#

am1

Á

amn

U

bm

(b) Uniqueness. The system (1) has precisely one solution if and only if this
ෂ equals n.
common rank r of A and A

c07.qxd

10/28/10

7:30 PM

Page 289

SEC. 7.5 Solutions of Linear Systems: Existence, Uniqueness

289

(c) Infinitely many solutions. If this common rank r is less than n, the system
(1) has infinitely many solutions. All of these solutions are obtained by determining
r suitable unknowns (whose submatrix of coefficients must have rank r) in terms of
the remaining n Ϫ r unknowns, to which arbitrary values can be assigned. (See
Example 3 in Sec. 7.3.)
(d) Gauss elimination (Sec. 7.3). If solutions exist, they can all be obtained by
the Gauss elimination. (This method will automatically reveal whether or not
solutions exist; see Sec. 7.3.)

PROOF

(a) We can write the system (1) in vector form Ax ϭ b or in terms of column vectors
c(1), Á , c(n) of A:
(2)

c (1) x 1 ϩ c (2) x 2 ϩ Á ϩ c(n) x n ϭ b.

ෂ
A is obtained by augmenting A by a single column b. Hence, by Theorem 3 in Sec. 7.4,
ෂ
rank A equals rank A or rank A ϩ 1. Now if (1) has a solution x, then (2) shows that b
ෂ
must be a linear combination of those column vectors, so that A and A have the same
maximum number of linearly independent column vectors and thus the same rank.
ෂ
Conversely, if rank A ϭ rank A, then b must be a linear combination of the column
vectors of A, say,
(2*)

b ϭ a1c (1) ϩ Á ϩ a nc (n)

ෂ
since otherwise rank A ϭ rank A ϩ 1. But (2*) means that (1) has a solution, namely,
Á
x 1 ϭ a1, , x n ϭ an, as can be seen by comparing (2*) and (2).
(b) If rank A ϭ n, the n column vectors in (2) are linearly independent by Theorem 3
in Sec. 7.4. We claim that then the representation (2) of b is unique because otherwise
c(1) x 1 ϩ Á ϩ c (n) x n ϭ c (1) ෂ
x1 ϩ Á ϩ c(n) ෂ
xn.
This would imply (take all terms to the left, with a minus sign)
(x 1 Ϫ ෂ
x1)c (1) ϩ Á ϩ (x n Ϫ ෂ
xn)c (n) ϭ 0
and x 1 Ϫ ෂ
x1 ϭ 0, Á , x n Ϫ ෂ
xn ϭ 0 by linear independence. But this means that the scalars
Á
x 1, , x n in (2) are uniquely determined, that is, the solution of (1) is unique.
ෂ
(c) If rank A ϭ rank A ϭ r Ͻ n, then by Theorem 3 in Sec. 7.4 there is a linearly
independent set K of r column vectors of A such that the other n Ϫ r column vectors of
A are linear combinations of those vectors. We renumber the columns and unknowns,
denoting the renumbered quantities by ˆ, so that {cˆ (1), Á , cˆ (r) } is that linearly independent
set K. Then (2) becomes
cˆ (1) xˆ 1 ϩ Á ϩ cˆ (r) xˆ r ϩ cˆ (rϩ1) xˆ rϩ1 ϩ Á ϩ cˆ (n) xˆ n ϭ b,
cˆ (rϩ1), Á , cˆ (n) are linear combinations of the vectors of K, and so are the vectors
xˆ rϩ1cˆ (rϩ1), Á , xˆ ncˆ (n). Expressing these vectors in terms of the vectors of K and collecting terms, we can thus write the system in the form
(3)

cˆ (1) y1 ϩ Á ϩ cˆ (r) yr ϭ b

c07.qxd

10/28/10

7:30 PM

290

Page 290

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

with yj ϭ xˆ j ϩ bj, where bj results from the n Ϫ r terms cˆ (rϩ1) xˆ rϩ1, Á , cˆ(n) xˆ n; here,
j ϭ 1, Á , r. Since the system has a solution, there are y1, Á , yr satisfying (3). These
scalars are unique since K is linearly independent. Choosing xˆ rϩ1, Á , xˆ n fixes the bj and
corresponding xˆ j ϭ yj Ϫ bj, where j ϭ 1, Á , r.
(d) This was discussed in Sec. 7.3 and is restated here as a reminder.
᭿
The theorem is illustrated in Sec. 7.3. In Example 2 there is a unique solution since rank
ෂ
A ϭ rank A ϭ n ϭ 3 (as can be seen from the last matrix in the example). In Example 3
ෂ
we have rank A ϭ rank A ϭ 2 Ͻ n ϭ 4 and can choose x 3 and x 4 arbitrarily. In
ෂ
Example 4 there is no solution because rank A ϭ 2 Ͻ rank A ϭ 3.

Homogeneous Linear System
Recall from Sec. 7.3 that a linear system (1) is called homogeneous if all the bj’s are
zero, and nonhomogeneous if one or several bj’s are not zero. For the homogeneous
system we obtain from the Fundamental Theorem the following results.
THEOREM 2

Homogeneous Linear System

A homogeneous linear system
a11x1 ϩ a12 x2 ϩ Á ϩ a1n xn ϭ 0
(4)

a21x1 ϩ a22 x2 ϩ Á ϩ a2n xn ϭ 0

# # # # # # # # # # # # # # # #
am1x1 ϩ am2 x2 ϩ Á ϩ amn xn ϭ 0
always has the trivial solution x 1 ϭ 0, Á , xn ϭ 0. Nontrivial solutions exist if and
only if rank A Ͻ n. If rank A ϭ r Ͻ n, these solutions, together with x ϭ 0, form a
vector space (see Sec. 7.4) of dimension n Ϫ r called the solution space of (4).
In particular, if x (1) and x (2) are solution vectors of (4), then x ϭ c1x (1) ϩ c2x(2)
with any scalars c1 and c2 is a solution vector of (4). (This does not hold for
nonhomogeneous systems. Also, the term solution space is used for homogeneous
systems only.)
PROOF

The first proposition can be seen directly from the system. It agrees with the fact that
ෂ
b ϭ 0 implies that rank A ϭ rank A, so that a homogeneous system is always consistent.
If rank A ϭ n, the trivial solution is the unique solution according to (b) in Theorem 1.
If rank A Ͻ n, there are nontrivial solutions according to (c) in Theorem 1. The solutions
form a vector space because if x(1) and x(2) are any of them, then Ax(1) ϭ 0, Ax(2) ϭ 0,
and this implies A(x(1) ϩ x(2)) ϭ Ax(1) ϩ Ax(2) ϭ 0 as well as A(cx (1)) ϭ cAx (1) ϭ 0,
where c is arbitrary. If rank A ϭ r Ͻ n, Theorem 1 (c) implies that we can choose n Ϫ r
suitable unknowns, call them x rϩ1, Á , x n, in an arbitrary fashion, and every solution is
obtained in this way. Hence a basis for the solution space, briefly called a basis of
solutions of (4), is y(1), Á , y(n؊r), where the basis vector y( j) is obtained by choosing
x rϩj ϭ 1 and the other x rϩ1, Á , x n zero; the corresponding first r components of this
solution vector are then determined. Thus the solution space of (4) has dimension n Ϫ r.
This proves Theorem 2.
᭿

c07.qxd

10/28/10

7:30 PM

Page 291

SEC. 7.6 For Reference: Second- and Third-Order Determinants

291

The solution space of (4) is also called the null space of A because Ax ϭ 0 for every x in
the solution space of (4). Its dimension is called the nullity of A. Hence Theorem 2 states that
rank A ϩ nullity A ϭ n

(5)

where n is the number of unknowns (number of columns of A).
Furthermore, by the definition of rank we have rank A Ϲ m in (4). Hence if m Ͻ n,
then rank A Ͻ n. By Theorem 2 this gives the practically important
Homogeneous Linear System with Fewer Equations Than Unknowns

THEOREM 3

A homogeneous linear system with fewer equations than unknowns always has
nontrivial solutions.

Nonhomogeneous Linear Systems
The characterization of all solutions of the linear system (1) is now quite simple, as follows.
THEOREM 4

Nonhomogeneous Linear System

If a nonhomogeneous linear system (1) is consistent, then all of its solutions are
obtained as
x ϭ x0 ϩ xh

(6)

where x0 is any (fixed) solution of (1) and xh runs through all the solutions of the
corresponding homogeneous system (4).
PROOF

The difference xh ϭ x Ϫ x0 of any two solutions of (1) is a solution of (4) because
Axh ϭ A(x Ϫ x0) ϭ Ax Ϫ Ax0 ϭ b Ϫ b ϭ 0. Since x is any solution of (1), we get all
the solutions of (1) if in (6) we take any solution x0 of (1) and let xh vary throughout the
solution space of (4).
᭿
This covers a main part of our discussion of characterizing the solutions of systems of
linear equations. Our next main topic is determinants and their role in linear equations.

7.6

For Reference:
Second- and Third-Order Determinants
We created this section as a quick general reference section on second- and third-order
determinants. It is completely independent of the theory in Sec. 7.7 and suffices as a
reference for many of our examples and problems. Since this section is for reference, go
on to the next section, consulting this material only when needed.
A determinant of second order is denoted and defined by
(1)

D ϭ det A ϭ 2

a11

a12

a21

a22

2 ϭ a11a22 Ϫ a12a21.

So here we have bars (whereas a matrix has brackets).

c07.qxd

10/28/10

7:30 PM

292

Page 292

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

Cramer’s rule for solving linear systems of two equations in two unknowns
(a) a11x 1 ϩ a12x 2 ϭ b1

(2)

(b) a21x 1 ϩ a22x 2 ϭ b2

is

2
x1 ϭ

a12

b2

a22

2
ϭ

D

(3)

2
x2 ϭ

b1

a11

b1

a21

b2

2
ϭ

D

b1a22 Ϫ a12b2
D

,

a11b2 Ϫ b1a21
D

with D as in (1), provided
D

0.

The value D ϭ 0 appears for homogeneous systems with nontrivial solutions.
PROOF

We prove (3). To eliminate x 2 multiply (2a) by a22 and (2b) by Ϫa12 and add,
(a11a22 Ϫ a12a21)x 1 ϭ b1a22 Ϫ a12b2.
Similarly, to eliminate x 1 multiply (2a) by Ϫa21 and (2b) by a11 and add,
(a11a22 Ϫ a12a21)x 2 ϭ a11b2 Ϫ b1a21.
Assuming that D ϭ a11a22 Ϫ a12a21 0, dividing, and writing the right sides of these
two equations as determinants, we obtain (3).
᭿

EXAMPLE 1

Cramer’s Rule for Two Equations

If

4x 1 ϩ 3x 2 ϭ 12
2x 1 ϩ 5x 2 ϭ Ϫ8

2
then

x1 ϭ

2

12

3

Ϫ8

5

4

3

2

5

2

2
ϭ

2

84
14

ϭ 6,

x2 ϭ

2

4

12

2

Ϫ8

4

3

2

5

2
ϭ

2

Ϫ56
14

ϭ Ϫ4.

Third-Order Determinants
A determinant of third order can be defined by

(4)

a11

a12

a13

D ϭ 3 a21

a22

a23 3 ϭ a11 2

a31

a32

a33

a22

a23

a32

a33

2 Ϫ a21 2

a12

a13

a32

a33

2 ϩ a31 2

a12

a13

a22

a23

2.

᭿

c07.qxd

10/28/10

7:30 PM

Page 293

SEC. 7.7 Determinants. Cramer’s Rule

293

Note the following. The signs on the right are ϩ Ϫ ϩ. Each of the three terms on the
right is an entry in the first column of D times its minor, that is, the second-order
determinant obtained from D by deleting the row and column of that entry; thus, for a11
delete the first row and first column, and so on.
If we write out the minors in (4), we obtain
(4*)

D ϭ a11a22a33 Ϫ a11a23a32 ϩ a21a13a32 Ϫ a21a12a33 ϩ a31a12a23 Ϫ a31a13a22.

Cramer’s Rule for Linear Systems of Three Equations
a11x 1 ϩ a12x 2 ϩ a13x 3 ϭ b1
a21x 1 ϩ a22x 2 ϩ a23x 3 ϭ b2

(5)

a31x 1 ϩ a32x 2 ϩ a33x 3 ϭ b3
is
x1 ϭ

(6)

D1
D

,

x2 ϭ

D2
D

x3 ϭ

,

D3

(D

D

0)

with the determinant D of the system given by (4) and
b1

a12

a13

D1 ϭ 3 b2

a22

a23 3 ,

b3

a32

a33

a11

b1

a13

D2 ϭ 3 a21

b2

a23 3 ,

a31

b3

a33

a11

a12

b1

D3 ϭ 3 a21

a22

b2 3 .

a31

a32

b3

Note that D1, D2, D3 are obtained by replacing Columns 1, 2, 3, respectively, by the
column of the right sides of (5).
Cramer’s rule (6) can be derived by eliminations similar to those for (3), but it also
follows from the general case (Theorem 4) in the next section.

7.7

Determinants. Cramer’s Rule
Determinants were originally introduced for solving linear systems. Although impractical
in computations, they have important engineering applications in eigenvalue problems
(Sec. 8.1), differential equations, vector algebra (Sec. 9.3), and in other areas. They can
be introduced in several equivalent ways. Our definition is particularly for dealing with
linear systems.
A determinant of order n is a scalar associated with an n ϫ n (hence square!) matrix
A ϭ 3ajk4, and is denoted by

(1)

a11

a12

Á

a1n

a21

a22

Á

a2n

D ϭ det A ϭ 7 #

#

Á

# 7.

#

#

Á

#

an1

an2

Á

ann

c07.qxd

10/28/10

7:30 PM

294

Page 294

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

For n ϭ 1, this determinant is defined by
D ϭ a11.

(2)
For n м 2 by
(3a)

D ϭ aj1Cj1 ϩ aj2Cj2 ϩ Á ϩ ajnCjn

( j ϭ 1, 2, Á , or n)

or
(3b)

D ϭ a1kC1k ϩ a2kC2k ϩ Á ϩ ankCnk (k ϭ 1, 2, Á , or n).

Here,
Cjk ϭ (Ϫ1) jϩkM jk
and M jk is a determinant of order n Ϫ 1, namely, the determinant of the submatrix of A
obtained from A by omitting the row and column of the entry ajk, that is, the jth row and
the kth column.
In this way, D is defined in terms of n determinants of order n Ϫ 1, each of which is,
in turn, defined in terms of n Ϫ 1 determinants of order n Ϫ 2, and so on—until we
finally arrive at second-order determinants, in which those submatrices consist of single
entries whose determinant is defined to be the entry itself.
From the definition it follows that we may expand D by any row or column, that is, choose
in (3) the entries in any row or column, similarly when expanding the Cjk’s in (3), and so on.
This definition is unambiguous, that is, it yields the same value for D no matter which
columns or rows we choose in expanding. A proof is given in App. 4.
Terms used in connection with determinants are taken from matrices. In D we have n 2
entries ajk, also n rows and n columns, and a main diagonal on which a11, a22, Á , ann
stand. Two terms are new:
M jk is called the minor of ajk in D, and Cjk the cofactor of ajk in D.
For later use we note that (3) may also be written in terms of minors
n

D ϭ a (Ϫ1) jϩkajkM jk

(4a)

( j ϭ 1, 2, Á , or n)

kϭ1
n

D ϭ a (Ϫ1) jϩkajkM jk

(4b)

(k ϭ 1, 2, Á , or n).

jϭ1

EXAMPLE 1

Minors and Cofactors of a Third-Order Determinant
In (4) of the previous section the minors and cofactors of the entries in the first column can be seen directly.
For the entries in the second row the minors are
M 21 ϭ 2

a12

a13

a32

a33

2,

M 22 ϭ 2

a11

a13

a31

a33

2,

M 23 ϭ 2

a11

a12

a31

a32

2

and the cofactors are C21 ϭ ϪM 21, C22 ϭ ϩM 22, and C23 ϭ ϪM 23. Similarly for the third row—write these
down yourself. And verify that the signs in Cjk form a checkerboard pattern
ϩ

Ϫ

ϩ

Ϫ

ϩ

Ϫ

ϩ

Ϫ

ϩ

᭿

c07.qxd

10/28/10

7:30 PM

Page 295

SEC. 7.7 Determinants. Cramer’s Rule
EXAMPLE 2

295

Expansions of a Third-Order Determinant
Dϭ3

1

3

0

2

6

43 ϭ 12

Ϫ1

0

6

4

0

2

2 Ϫ32

2

2

4

Ϫ1

2

2 ϩ02

2

6

Ϫ1

0

2

ϭ 1(12 Ϫ 0) Ϫ 3(4 ϩ 4) ϩ 0(0 ϩ 6) ϭ Ϫ12.
This is the expansion by the first row. The expansion by the third column is
Dϭ02

2

6

Ϫ1

0

2Ϫ42

1

3

Ϫ1

0

2ϩ22

1

3

2

6

2 ϭ 0 Ϫ 12 ϩ 0 ϭ Ϫ12.

᭿

Verify that the other four expansions also give the value Ϫ12.

EXAMPLE 3

Determinant of a Triangular Matrix
Ϫ3

0

0

3 6

4

0 3 ϭ Ϫ3 2

Ϫ1

2

4

0

2

5

2 ϭ Ϫ3 # 4 # 5 ϭ Ϫ60.

5

Inspired by this, can you formulate a little theorem on determinants of triangular matrices? Of diagonal
matrices?
᭿

General Properties of Determinants
There is an attractive way of finding determinants (1) that consists of applying elementary
row operations to (1). By doing so we obtain an “upper triangular” determinant (see
Sec. 7.1, for definition with “matrix” replaced by “determinant”) whose value is then very
easy to compute, being just the product of its diagonal entries. This approach is similar
(but not the same!) to what we did to matrices in Sec. 7.3. In particular, be aware that
interchanging two rows in a determinant introduces a multiplicative factor of Ϫ1 to the
value of the determinant! Details are as follows.

THEOREM 1

Behavior of an nth-Order Determinant under Elementary Row Operations

(a) Interchange of two rows multiplies the value of the determinant by Ϫ1.
(b) Addition of a multiple of a row to another row does not alter the value of the
determinant.
(c) Multiplication of a row by a nonzero constant c multiplies the value of the
determinant by c. (This holds also when c ϭ 0, but no longer gives an elementary
row operation.)

PROOF

(a) By induction. The statement holds for n ϭ 2 because

2

a
c

b
d

2 ϭ ad Ϫ bc,

but

2

c

d

a

b

2 ϭ bc Ϫ ad.

c07.qxd

10/28/10

7:30 PM

296

Page 296

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

We now make the induction hypothesis that (a) holds for determinants of order n Ϫ 1 м 2
and show that it then holds for determinants of order n. Let D be of order n. Let E be
obtained from D by the interchange of two rows. Expand D and E by a row that is not
one of those interchanged, call it the jth row. Then by (4a),
n

(5)

n

D ϭ a (Ϫ1) jϩkajkM jk,

E ϭ a (Ϫ1) jϩkajkNjk

kϭ1

kϭ1

where Njk is obtained from the minor M jk of ajk in D by the interchange of those two
rows which have been interchanged in D (and which Njk must both contain because we
expand by another row!). Now these minors are of order n Ϫ 1. Hence the induction
hypothesis applies and gives Njk ϭ ϪM jk. Thus E ϭ ϪD by (5).
ෂ be the new determinant. Its entries in Row j
(b) Add c times Row i to Row j. Let D
ෂ
are ajk ϩ caik. If we expand D by this Row j, we see that we can write it as
ෂ ϭ D ϩ cD , where D ϭ D has in Row j the a , whereas D has in that Row j the
D
1
2
1
jk
2
ajk from the addition. Hence D2 has ajk in both Row i and Row j. Interchanging these
two rows gives D2 back, but on the other hand it gives ϪD2 by (a). Together
ෂ ϭ D ϭ D.
D2 ϭ ϪD2 ϭ 0, so that D
1
(c) Expand the determinant by the row that has been multiplied.
CAUTION! det (cA) ϭ c n det A (not c det A). Explain why.
EXAMPLE 4

᭿

Evaluation of Determinants by Reduction to Triangular Form
Because of Theorem 1 we may evaluate determinants by reduction to triangular form, as in the Gauss elimination
for a matrix. For instance (with the blue explanations always referring to the preceding determinant)

Dϭ5

ϭ5

ϭ5

ϭ5

2

0

Ϫ4

6

4

5

1

0

0

2

6

Ϫ1

Ϫ3

8

9

1

2

0

Ϫ4

6

0

5

9

Ϫ12

0

2

6

Ϫ1

0

8

3

10

2

0

Ϫ4

6

0

5

9

Ϫ12

0

0

2.4

3.8

0

0

Ϫ11.4

29.2

2

0

Ϫ4

6

0

5

9

Ϫ12

0

0

2.4

0

0

Ϫ0

3.8
47.25

ϭ 2 # 5 # 2.4 # 47.25 ϭ 1134.

5

5

Row 2 Ϫ 2 Row 1

Row 4 ϩ 1.5 Row 1

5

Row 3 Ϫ 0.4 Row 2
Row 4 Ϫ 1.6 Row 2

5
Row 4 ϩ 4.75 Row 3

᭿

c07.qxd

10/28/10

7:30 PM

Page 297

SEC. 7.7 Determinants. Cramer’s Rule

THEOREM 2

297

Further Properties of nth-Order Determinants

(a)–(c) in Theorem 1 hold also for columns.
(d) Transposition leaves the value of a determinant unaltered.
(e) A zero row or column renders the value of a determinant zero.
(f ) Proportional rows or columns render the value of a determinant zero. In
particular, a determinant with two identical rows or columns has the value zero.
PROOF

(a)–(e) follow directly from the fact that a determinant can be expanded by any row
column. In (d), transposition is defined as for matrices, that is, the jth row becomes the
jth column of the transpose.
(f) If Row j ϭ c times Row i, then D ϭ cD1, where D1 has Row j ϭ Row i. Hence
an interchange of these rows reproduces D1, but it also gives ϪD1 by Theorem 1(a).
Hence D1 ϭ 0 and D ϭ cD1 ϭ 0. Similarly for columns.
᭿
It is quite remarkable that the important concept of the rank of a matrix A, which is the
maximum number of linearly independent row or column vectors of A (see Sec. 7.4), can
be related to determinants. Here we may assume that rank A Ͼ 0 because the only matrices
with rank 0 are the zero matrices (see Sec. 7.4).

THEOREM 3

Rank in Terms of Determinants

Consider an m ϫ n matrix A ϭ 3ajk4:
(1) A has rank r м 1 if and only if A has an r ϫ r submatrix with a nonzero
determinant.
(2) The determinant of any square submatrix with more than r rows, contained
in A (if such a matrix exists!) has a value equal to zero.
Furthermore, if m ϭ n, we have:
(3) An n ϫ n square matrix A has rank n if and only if
det A

PROOF

0.

The key idea is that elementary row operations (Sec. 7.3) alter neither rank (by Theorem
1 in Sec. 7.4) nor the property of a determinant being nonzero (by Theorem 1 in this
section). The echelon form Â of A (see Sec. 7.3) has r nonzero row vectors (which are
the first r row vectors) if and only if rank A ϭ r. Without loss of generality, we can
ˆ be the r ϫ r submatrix in the left upper corner of Â (so that
assume that r м 1. Let R
ˆ
ˆ is triangular,
the entries of R are in both the first r rows and r columns of Â). Now R
Á
ˆ
with all diagonal entries rjj nonzero. Thus, det R ϭ r11 rrr 0. Also det R 0 for
ˆ results from R by elementary row
the corresponding r ϫ r submatrix R of A because R
operations. This proves part (1).
Similarly, det S ϭ 0 for any square submatrix S of r ϩ 1 or more rows perhaps
contained in A because the corresponding submatrix Sˆ of Â must contain a row of zeros
(otherwise we would have rank A м r ϩ 1), so that det Sˆ ϭ 0 by Theorem 2. This proves
part (2). Furthermore, we have proven the theorem for an m ϫ n matrix.

c07.qxd

10/28/10

7:30 PM

298

Page 298

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

For an n ϫ n square matrix A we proceed as follows. To prove (3), we apply part (1)
(already proven!). This gives us that rank A ϭ n м 1 if and only if A contains an n ϫ n
submatrix with nonzero determinant. But the only such submatrix contained in our square
matrix A, is A itself, hence det A 0. This proves part (3).
᭿

Cramer’s Rule
Theorem 3 opens the way to the classical solution formula for linear systems known as
Cramer’s rule,2 which gives solutions as quotients of determinants. Cramer’s rule is not
practical in computations for which the methods in Secs. 7.3 and 20.1–20.3 are suitable.
However, Cramer’s rule is of theoretical interest in differential equations (Secs. 2.10 and
3.3) and in other theoretical work that has engineering applications.
THEOREM 4

Cramer’s Theorem (Solution of Linear Systems by Determinants)

(a) If a linear system of n equations in the same number of unknowns x 1, Á , x n
a11x 1 ϩ a12x 2 ϩ Á ϩ a1nx n ϭ b1
(6)

a21x 1 ϩ a22x 2 ϩ Á ϩ a2nx n ϭ b2

# # # # # # # # # # # # # # # # #
an1x 1 ϩ an2x 2 ϩ Á ϩ annx n ϭ bn
has a nonzero coefficient determinant D ϭ det A, the system has precisely one
solution. This solution is given by the formulas
(7)

x1 ϭ

D1
D

,

x2 ϭ

D2
D

, Á , xn ϭ

Dn
D

(Cramer’s rule)

where Dk is the determinant obtained from D by replacing in D the kth column by
the column with the entries b1, Á , bn.
(b) Hence if the system (6) is homogeneous and D 0, it has only the trivial
solution x 1 ϭ 0, x 2 ϭ 0, Á , x n ϭ 0. If D ϭ 0, the homogeneous system also has
nontrivial solutions.
PROOF

ෂ of the system (6) is of size n ϫ (n ϩ 1). Hence its rank can be
The augmented matrix A
at most n. Now if

(8)

2

D ϭ det A ϭ 5

a11

Á

a1n

#

Á

#

#

Á

#

an1

Á

ann

GABRIEL CRAMER (1704–1752), Swiss mathematician.

5

0,

c07.qxd

10/28/10

7:30 PM

Page 299

SEC. 7.7 Determinants. Cramer’s Rule

299

~
then rank A ϭ n by Theorem 3. Thus rank A ϭ rank A. Hence, by the Fundamental
Theorem in Sec. 7.5, the system (6) has a unique solution.
Let us now prove (7). Expanding D by its kth column, we obtain
(9)

D ϭ a1kC1k ϩ a2kC2k ϩ Á ϩ ankCnk,

where Cik is the cofactor of entry aik in D. If we replace the entries in the kth column of
D by any other numbers, we obtain a new determinant, say, Dˆ. Clearly, its expansion by
the kth column will be of the form (9), with a1k, Á , ank replaced by those new numbers
and the cofactors Cik as before. In particular, if we choose as new numbers the entries
ˆ which
a1l, Á , anl of the lth column of D (where l k), we have a new determinant D
T
Á
has the column 3a1l
anl4 twice, once as its lth column, and once as its kth because
ˆ ϭ 0 by Theorem 2(f). If we now expand D
ˆ by the column
of the replacement. Hence D
that has been replaced (the kth column), we thus obtain
(10)

a1lC1k ϩ a2lC2k ϩ Á ϩ anlCnk ϭ 0

(l

k).

We now multiply the first equation in (6) by C1k on both sides, the second by C2k, Á ,
the last by Cnk, and add the resulting equations. This gives
(11)

C1k(a11x 1 ϩ Á ϩ a1nx n) ϩ Á ϩ Cnk(an1x 1 ϩ Á ϩ annx n)
ϭ b1C1k ϩ Á ϩ bnCnk.

Collecting terms with the same xj, we can write the left side as
x 1(a11C1k ϩ a21C2k ϩ Á ϩ an1Cnk) ϩ Á ϩ x n(a1nC1k ϩ a2nC2k ϩ Á ϩ annCnk).
From this we see that x k is multiplied by
a1kC1k ϩ a2kC2k ϩ Á ϩ ankCnk.
Equation (9) shows that this equals D. Similarly, x 1 is multiplied by
a1lC1k ϩ a2lC2k ϩ Á ϩ anlCnk.
Equation (10) shows that this is zero when l
simply x kD, so that (11) becomes

k. Accordingly, the left side of (11) equals

x kD ϭ b1C1k ϩ b2C2k ϩ Á ϩ bnCnk.
Now the right side of this is Dk as defined in the theorem, expanded by its kth column,
so that division by D gives (7). This proves Cramer’s rule.
If (6) is homogeneous and D 0, then each Dk has a column of zeros, so that Dk ϭ 0
by Theorem 2(e), and (7) gives the trivial solution.
Finally, if (6) is homogeneous and D ϭ 0, then rank A Ͻ n by Theorem 3, so that
nontrivial solutions exist by Theorem 2 in Sec. 7.5.
᭿
EXAMPLE 5

Illustration of Cramer’s Rule (Theorem 4)
For n ϭ 2, see Example 1 of Sec. 7.6. Also, at the end of that section, we give Cramer’s rule for a general
linear system of three equations.
᭿

c07.qxd

10/28/10

7:30 PM

Page 300

300

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

Finally, an important application for Cramer’s rule dealing with inverse matrices will
be given in the next section.

PROBLEM SET 7.7
GENERAL PROBLEMS

1–6

1. General Properties of Determinants. Illustrate each
statement in Theorems 1 and 2 with an example of
your choice.
2. Second-Order Determinant. Expand a general
second-order determinant in four possible ways and
show that the results agree.
3. Third-Order Determinant. Do the task indicated in
Theorem 2. Also evaluate D by reduction to triangular
form.
4. Expansion Numerically Impractical. Show that the
computation of an nth-order determinant by expansion
involves n! multiplications, which if a multiplication
takes 10؊9 sec would take these times:

n

10

15

20

25

Time

0.004
sec

22
min

77
years

0.5 # 109
years

15. 6

1

2

0

0

2

4

2

0

0

2

9

2

0

0

2

16

16. CAS EXPERIMENT. Determinant of Zeros and
Ones. Find the value of the determinant of the n ϫ n
matrix An with main diagonal entries all 0 and all
others 1. Try to find a formula for this. Try to prove it
by induction. Interpret A3 and A4 as incidence matrices
(as in Problem Set 7.1 but without the minuses) of a
triangle and a tetrahedron, respectively; similarly for an
n-simplex, having n vertices and n (n Ϫ 1)>2 edges (and
spanning Rn؊1, n ϭ 5, 6, Á ).

RANK BY DETERMINANTS

17–19

Find the rank by Theorem 3 (which is not very practical)
and check by row reduction. Show details.
4

5. Multiplication by Scalar. Show that det (kA) ϭ
k n det A (not k det A). Give an example.
6. Minors, cofactors. Complete the list in Example 1.

EVALUATION OF DETERMINANTS

7–15

Showing the details, evaluate:
7. 2
9. 2

cos a

sin a

sin b

cos b

2

8. 2

cos nu

sin nu

Ϫsin nu

cos nu

2

10. 2

0.4

4.9

1.5

Ϫ1.3

2

cosh t

sinh t

sinh t

cosh t

Ϫ1

8

a

b

c

11. 3 0

2

33

12. 3 c

a

b3

0

0

5

b

c

a

4

7

0

0

2

8

0

0

13. 6

0

4

Ϫ1

5

Ϫ4

0

3

Ϫ2

1

Ϫ3

0

1

0

0

1

5

Ϫ5

2

Ϫ1

0

0

0

Ϫ2

2

6

14. 6

17. D Ϫ8

9
Ϫ6 T

16

12

1

5

2

2

19. D 1

3

2

6T

4

0

8

Ϫ6

0

4

18. D 4

0

10 T

Ϫ6

10

0

48

20. TEAM PROJECT. Geometric Applications: Curves
and Surfaces Through Given Points. The idea is to
get an equation from the vanishing of the determinant
of a homogeneous linear system as the condition for a
nontrivial solution in Cramer’s theorem. We explain
the trick for obtaining such a system for the case of
a line L through two given points P1: (x 1, y1) and
P2: (x 2, y2). The unknown line is ax ϩ by ϭ Ϫc,
say. We write it as ax ϩ by ϩ c # 1 ϭ 0. To get a
nontrivial solution a, b, c, the determinant of the
“coefficients” x, y, 1 must be zero. The system is

2

4

6

ax ϩ by ϩ c # 1 ϭ 0 (Line L)

6

(12)

ax 1 ϩ by1 ϩ c # 1 ϭ 0 (P1 on L)
ax 2 ϩ by2 ϩ c # 1 ϭ 0 (P2 on L).

c07.qxd

10/28/10

7:30 PM

Page 301

SEC. 7.8 Inverse of a Matrix. Gauss–Jordan Elimination
(a) Line through two points. Derive from D ϭ 0 in
(12) the familiar formula
y Ϫ y1
x Ϫ x1
x 1 Ϫ x 2 ϭ y1 Ϫ y2 .

CRAMER’S RULE

21–25

Solve by Cramer’s rule. Check by Gauss elimination and
back substitution. Show details.
21. 3x Ϫ 5y ϭ 15.5

(b) Plane. Find the analog of (12) for a plane through
three given points. Apply it when the points are
(1, 1, 1), (3, 2, 6), (5, 0, 5).
(c) Circle. Find a similar formula for a circle in the
plane through three given points. Find and sketch the
circle through (2, 6), (6, 4), (7, 1).
(d) Sphere. Find the analog of the formula in (c) for
a sphere through four given points. Find the sphere
through (0, 0, 5), (4, 0, 1), (0, 4, 1), (0, 0, Ϫ3) by this
formula or by inspection.
(e) General conic section. Find a formula for a
general conic section (the vanishing of a determinant
of 6th order). Try it out for a quadratic parabola and
for a more general conic section of your own choice.

7.8

301

22. 2x Ϫ 4y ϭ Ϫ24

6x ϩ 16y ϭ 5.0

5x ϩ 2y ϭ

3y Ϫ 4z ϭ

23.

16

Ϫx

Ϫ 9z ϭ

25. Ϫ4w ϩ x ϩ y
w Ϫ 4x
w

3x Ϫ 2y ϩ z ϭ

13

Ϫ2x ϩ y ϩ 4z ϭ

11

24.

2x Ϫ 5y ϩ 7z ϭ Ϫ27

0

x ϩ 4y Ϫ 5z ϭ Ϫ31

9
ϭ Ϫ10

ϩ zϭ

1

Ϫ 4y ϩ z ϭ Ϫ7
x ϩ y Ϫ 4z ϭ

10

Inverse of a Matrix.
Gauss–Jordan Elimination
In this section we consider square matrices exclusively.
The inverse of an n ϫ n matrix A ϭ 3ajk4 is denoted by A؊1 and is an n ϫ n matrix
such that
AA؊1 ϭ A؊1A ϭ I

(1)

where I is the n ϫ n unit matrix (see Sec. 7.2).
If A has an inverse, then A is called a nonsingular matrix. If A has no inverse, then
A is called a singular matrix.
If A has an inverse, the inverse is unique.
Indeed, if both B and C are inverses of A, then AB ϭ I and CA ϭ I, so that we obtain
the uniqueness from
B ϭ IB ϭ (CA)B ϭ C(AB) ϭ CI ϭ C.
We prove next that A has an inverse (is nonsingular) if and only if it has maximum
possible rank n. The proof will also show that Ax ϭ b implies x ϭ A؊1b provided A؊1
exists, and will thus give a motivation for the inverse as well as a relation to linear systems.
(But this will not give a good method of solving Ax ϭ b numerically because the Gauss
elimination in Sec. 7.3 requires fewer computations.)
THEOREM 1

Existence of the Inverse

The inverse A؊1 of an n ϫ n matrix A exists if and only if rank A ϭ n, thus (by
Theorem 3, Sec. 7.7) if and only if det A 0. Hence A is nonsingular if rank A ϭ n,
and is singular if rank A Ͻ n.

c07.qxd

10/28/10

7:30 PM

302

Page 302

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

PROOF

Let A be a given n ϫ n matrix and consider the linear system
Ax ϭ b.

(2)

If the inverse A؊1 exists, then multiplication from the left on both sides and use of (1)
gives
A؊1Ax ϭ x ϭ A؊1b.
This shows that (2) has a solution x, which is unique because, for another solution u, we
have Au ϭ b, so that u ϭ A؊1b ϭ x. Hence A must have rank n by the Fundamental
Theorem in Sec. 7.5.
Conversely, let rank A ϭ n. Then by the same theorem, the system (2) has a unique
solution x for any b. Now the back substitution following the Gauss elimination (Sec. 7.3)
shows that the components x j of x are linear combinations of those of b. Hence we can
write
x ϭ Bb

(3)

with B to be determined. Substitution into (2) gives
Ax ϭ A(Bb) ϭ (AB)b ϭ Cb ϭ b

(C ϭ AB)

for any b. Hence C ϭ AB ϭ I, the unit matrix. Similarly, if we substitute (2) into (3) we get
x ϭ Bb ϭ B(Ax) ϭ (BA)x
for any x (and b ϭ Ax). Hence BA ϭ I. Together, B ϭ A؊1 exists.

᭿

Determination of the Inverse by the
Gauss–Jordan Method
To actually determine the inverse A؊1 of a nonsingular n ϫ n matrix A, we can use a
variant of the Gauss elimination (Sec. 7.3), called the Gauss–Jordan elimination.3 The
idea of the method is as follows.
Using A, we form n linear systems
Ax(1) ϭ e (1),

Á,

Ax(n) ϭ e (n)

where the vectors e (1), Á , e (n) are the columns of the n ϫ n unit matrix I; thus,
e (1) ϭ 31 0 Á 04T, e (2) ϭ 30 1 0 Á 04T, etc. These are n vector equations
in the unknown vectors x(1), Á , x(n). We combine them into a single matrix equation

3

WILHELM JORDAN (1842–1899), German geodesist and mathematician. He did important geodesic work
in Africa, where he surveyed oases. [See Althoen, S.C. and R. McLaughlin, Gauss–Jordan reduction: A brief
history. American Mathematical Monthly, Vol. 94, No. 2 (1987), pp. 130–142.]
We do not recommend it as a method for solving systems of linear equations, since the number of operations
in addition to those of the Gauss elimination is larger than that for back substitution, which the Gauss–Jordan
elimination avoids. See also Sec. 20.1.

c07.qxd

10/28/10

7:30 PM

Page 303

SEC. 7.8 Inverse of a Matrix. Gauss–Jordan Elimination

303

AX ϭ I, with the unknown matrix X having the columns x(1), Á , x(n). Correspondingly,
we combine the n augmented matrices 3A e(1)4, Á , 3A e(n)4 into one wide n ϫ 2n
ෂ ϭ 3A I4. Now multiplication of AX ϭ I by A؊1 from the left
“augmented matrix” A
؊1
gives X ϭ A I ϭ A؊1. Hence, to solve AX ϭ I for X, we can apply the Gauss
ෂ ϭ 3A I4. This gives a matrix of the form 3U H4 with upper triangular
elimination to A
U because the Gauss elimination triangularizes systems. The Gauss–Jordan method
reduces U by further elementary row operations to diagonal form, in fact to the unit matrix
I. This is done by eliminating the entries of U above the main diagonal and making the
diagonal entries all 1 by multiplication (see Example 1). Of course, the method operates
on the entire matrix 3U H4, transforming H into some matrix K, hence the entire 3U H4
to 3I K4. This is the “augmented matrix” of IX ϭ K. Now IX ϭ X ϭ A؊1, as shown
before. By comparison, K ϭ A؊1, so that we can read A؊1 directly from 3I K4.
The following example illustrates the practical details of the method.
EXAMPLE 1

Finding the Inverse of a Matrix by Gauss–Jordan Elimination
Determine the inverse A؊1 of
Ϫ1

1

AϭD 3

Ϫ1

Ϫ1

3

2
1T .
4

Solution. We apply the Gauss elimination (Sec. 7.3) to the following n ϫ 2n ϭ 3 ϫ 6 matrix, where BLUE
always refers to the previous matrix.
Ϫ1

1

3A I4 ϭ D 3

Ϫ1

Ϫ1

2

1

0

0

13

0

1

0T

3

4

0

0

1

Ϫ1

1

2

1

0

0

D 0

2

73

3

1

0T

Row 2 ϩ 3 Row 1

0

2

2

Ϫ1

0

1

Row 3 Ϫ Row 1

Ϫ1

1

2

1

0

0

D 0

2

73

3

1

0T

0

0

Ϫ4

Ϫ1

Ϫ5

1

Row 3 Ϫ Row 2

This is 3U H4 as produced by the Gauss elimination. Now follow the additional Gauss–Jordan steps, reducing
U to I, that is, to diagonal form with entries 1 on the main diagonal.
1

Ϫ1

Ϫ2

Ϫ1

ϪRow 1

0

0

1.5

0.5

0 T 0.5 Row 2

D0

1

3.5 3

0

0

1

0.8

0.2

Ϫ0.2

Ϫ0.2 Row 3

1

Ϫ1

0

0.6

0.4

Ϫ0.4

Row 1 ϩ 2 Row 3

Ϫ1.3

Ϫ0.2

D0

1

0 3

0.7T

0

0

1

0.8

0.2

Ϫ0.2

1

0

0

Ϫ0.7

0.2

0.3

D0

1

0 3

Ϫ1.3

Ϫ0.2

0

0

1

0.8

0.2

0.7T
Ϫ0.2

Row 2 – 3.5 Row 3

Row 1 ϩ Row 2

c07.qxd

10/28/10

7:30 PM

304

Page 304

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
The last three columns constitute A؊1. Check:
Ϫ1

1

D 3

Ϫ1

Ϫ1

3

Ϫ0.7

0.2

1T DϪ1.3

Ϫ0.2

2

4

0.8

0.2

0.3

1

0

0

0.7T ϭ D0

1

0T .

0

1

Ϫ0.2

0

᭿

Hence AA؊1 ϭ I. Similarly, A؊1A ϭ I.

Formulas for Inverses
Since finding the inverse of a matrix is really a problem of solving a system of linear
equations, it is not surprising that Cramer’s rule (Theorem 4, Sec. 7.7) might come into
play. And similarly, as Cramer’s rule was useful for theoretical study but not for
computation, so too is the explicit formula (4) in the following theorem useful for
theoretical considerations but not recommended for actually determining inverse matrices,
except for the frequently occurring 2 ϫ 2 case as given in (4*).

THEOREM 2

Inverse of a Matrix by Determinants

The inverse of a nonsingular n ϫ n matrix A ϭ 3ajk4 is given by

(4)

A؊1 ϭ

C11

C21

Á

Cn1

C12
1
1
3Cjk4T ϭ
E
det A
det A
#

C22

Á

Cn2

#

Á

#

C1n

C2n

Á

Cnn

U,

where Cjk is the cofactor of ajk in det A (see Sec. 7.7). (CAUTION! Note well that
in A؊1, the cofactor Cjk occupies the same place as akj (not ajk) does in A.)
In particular, the inverse of
(4*)

PROOF

Aϭ

c

a11

a12

a21

a22

d

is

A؊1 ϭ

a22
1
c
det A Ϫa21

Ϫa12
a11

d.

We denote the right side of (4) by B and show that BA ϭ I. We first write
BA ϭ G ϭ 3gkl4

(5)

and then show that G ϭ I. Now by the definition of matrix multiplication and because of
the form of B in (4), we obtain (CAUTION! Csk, not Cks)
n

(6)

Csk
1
gkl ϭ a
asl ϭ
(a1lC1k ϩ Á ϩ anlCnk).
det
A
det
A
sϭ1

c07.qxd

10/28/10

7:30 PM

Page 305

SEC. 7.8 Inverse of a Matrix. Gauss–Jordan Elimination

305

Now (9) and (10) in Sec. 7.7 show that the sum ( Á ) on the right is D ϭ det A when
l ϭ k, and is zero when l k. Hence
gkk ϭ

1
det A ϭ 1,
det A

gkl ϭ 0 (l

k).

In particular, for n ϭ 2 we have in (4), in the first row, C11 ϭ a22, C21 ϭ Ϫa12 and,
in the second row, C12 ϭ Ϫa21, C22 ϭ a11. This gives (4*).
᭿
The special case n ϭ 2 occurs quite frequently in geometric and other applications. You
may perhaps want to memorize formula (4*). Example 2 gives an illustration of (4*).
EXAMPLE 2

Inverse of a 2 ؋ 2 Matrix by Determinants

c

Aϭ

EXAMPLE 3

3

1

2

4

d,

A؊1 ϭ

c
10

4

Ϫ1

Ϫ2

3

1

d

ϭ

c

0.4

Ϫ0.1

Ϫ0.2

0.3

d

᭿

Further Illustration of Theorem 2
Using (4), find the inverse of
Ϫ1

1

AϭD 3

Ϫ1

Ϫ1

3

2
1T .
4

Solution. We obtain det A ϭ Ϫ1(Ϫ7) Ϫ 1 # 13 ϩ 2 # 8 ϭ 10, and in (4),
C11 ϭ 2

Ϫ1

1

3

4

C12 ϭ Ϫ 2
C13 ϭ 2

2 ϭ Ϫ7,

3

1

Ϫ1

4

3

Ϫ1

Ϫ1

3

1

2

3

4

Ϫ1

2

Ϫ1

4

C21 ϭ Ϫ 2

2 ϭ Ϫ13,

C22 ϭ 2

2 ϭ 8,

C23 ϭ Ϫ 2

1

2

Ϫ1

1

2 ϭ 2,

C31 ϭ 2

2 ϭ Ϫ2,

C32 ϭ Ϫ 2

Ϫ1

1

Ϫ1

3

2 ϭ 2,

C33 ϭ 2

2 ϭ 3,

Ϫ1

2

3

1

Ϫ1

1

3

Ϫ1

2 ϭ 7,
2 ϭ Ϫ2,

so that by (4), in agreement with Example 1,

؊1

A

Ϫ0.7

0.2

ϭ DϪ1.3

Ϫ0.2

0.8

0.2

0.3

᭿

0.7T .
Ϫ0.2

Diagonal matrices A ϭ [ajk], ajk ϭ 0 when j k, have an inverse if and only if all
ajj 0. Then A؊1 is diagonal, too, with entries 1>a11, Á , 1>ann.
PROOF

For a diagonal matrix we have in (4)
a22 Á ann
1
ϭ a a Áa ϭ a ,
11 22
nn
11
D

C11

etc.

᭿

c07.qxd

10/28/10

7:30 PM

306

Page 306

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

EXAMPLE 4

Inverse of a Diagonal Matrix
Let
Ϫ0.5

0

0

AϭD 0

4

0T .

0

0

1

Then we obtain the inverse A؊1 by inverting each individual diagonal element of A, that is, by taking 1>(Ϫ0.5), 14 ,
and 11 as the diagonal entries of A؊1, that is,
Ϫ2
؊1

A

ϭD 0
0

0

0

0.25

0T .

0

1

᭿

Products can be inverted by taking the inverse of each factor and multiplying these
inverses in reverse order,
(AC)؊1 ϭ C ؊1A؊1.

(7)
Hence for more than two factors,
(8)

PROOF

(AC Á PQ)؊1 ϭ Q ؊1P ؊1 Á C ؊1A؊1.

The idea is to start from (1) for AC instead of A, that is, AC(AC)؊1 ϭ I, and multiply
it on both sides from the left, first by A؊1, which because of A؊1A ϭ I gives
A؊1AC(AC)؊1 ϭ C(AC)؊1
ϭ A؊1I ϭ A؊1,
and then multiplying this on both sides from the left, this time by C ؊1 and by using
C ؊1C ϭ I,
C ؊1C(AC)؊1 ϭ (AC)؊1 ϭ C ؊1A؊1.
This proves (7), and from it, (8) follows by induction.

᭿

We also note that the inverse of the inverse is the given matrix, as you may prove,
(9)

(A؊1)؊1 ϭ A.

Unusual Properties of Matrix Multiplication.
Cancellation Laws
Section 7.2 contains warnings that some properties of matrix multiplication deviate from
those for numbers, and we are now able to explain the restricted validity of the so-called
cancellation laws [2] and [3] below, using rank and inverse, concepts that were not yet

c07.qxd

10/28/10

7:30 PM

Page 307

SEC. 7.8 Inverse of a Matrix. Gauss–Jordan Elimination

307

available in Sec. 7.2. The deviations from the usual are of great practical importance and
must be carefully observed. They are as follows.
[1] Matrix multiplication is not commutative, that is, in general we have
AB

BA.

[2] AB ϭ 0 does not generally imply A ϭ 0 or B ϭ 0 (or BA ϭ 0); for example,

c

1

1

2

2

dc

Ϫ1

1

1

Ϫ1

d

ϭ

c

0

0

0

0

d.

[3] AC ϭ AD does not generally imply C ϭ D (even when A

0).

Complete answers to [2] and [3] are contained in the following theorem.
THEOREM

3

Cancellation Laws

Let A, B, C be n ϫ n matrices. Then:
(a) If rank A ϭ n and AB ϭ AC, then B ϭ C.
(b) If rank A ϭ n, then AB ϭ 0 implies B ϭ 0. Hence if AB ϭ 0, but A
as well as B 0, then rank A Ͻ n and rank B Ͻ n.
(c) If A is singular, so are BA and AB.

PROOF

0

(a) The inverse of A exists by Theorem 1. Multiplication by A؊1 from the left gives
A؊1AB ϭ A؊1AC, hence B ϭ C.
(b) Let rank A ϭ n. Then A؊1 exists, and AB ϭ 0 implies A؊1AB ϭ B ϭ 0. Similarly
when rank B ϭ n. This implies the second statement in (b).
(c1) Rank A Ͻ n by Theorem 1. Hence Ax ϭ 0 has nontrivial solutions by Theorem 2
in Sec. 7.5. Multiplication by B shows that these solutions are also solutions of BAx ϭ 0,
so that rank (BA) Ͻ n by Theorem 2 in Sec. 7.5 and BA is singular by Theorem 1.
(c2) AT is singular by Theorem 2(d) in Sec. 7.7. Hence BTAT is singular by part (c1),
and is equal to (AB)T by (10d) in Sec. 7.2. Hence AB is singular by Theorem 2(d) in
Sec. 7.7.
᭿

Determinants of Matrix Products
The determinant of a matrix product AB or BA can be written as the product of the
determinants of the factors, and it is interesting that det AB ϭ det BA, although AB BA
in general. The corresponding formula (10) is needed occasionally and can be obtained
by Gauss–Jordan elimination (see Example 1) and from the theorem just proved.
THEOREM 4

Determinant of a Product of Matrices

For any n ϫ n matrices A and B,
(10)

det (AB) ϭ det (BA) ϭ det A det B.

c07.qxd

10/28/10

7:30 PM

Page 308

308

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

If A or B is singular, so are AB and BA by Theorem 3(c), and (10) reduces to 0 ϭ 0 by
Theorem 3 in Sec. 7.7.
Now let A and B be nonsingular. Then we can reduce A to a diagonal matrix Â ϭ [ajk]
by Gauss–Jordan steps. Under these operations, det A retains its value, by Theorem 1 in
Sec. 7.7, (a) and (b) [not (c)] except perhaps for a sign reversal in row interchanging when
pivoting. But the same operations reduce AB to ÂB with the same effect on det (AB).
Hence it remains to prove (10) for ÂB; written out,

PROOF

aˆ11

0

Á

0

0

aˆ22

Á

0

ÂB ϭ E

0

0

ϭE

b12

Á

b1n

b21

b22

b2n

bn1

bn2

Á
.
.
.
Á

U E

..

.
Á

b11

aˆnn

aˆ11b11

aˆ11b12

Á

aˆ11b1n

aˆ22b21

aˆ22b22

aˆ22b2n

aˆnnbn1

aˆnnbn2

Á
.
.
.
Á

U

bnn

U.

aˆnnbnn

We now take the determinant det (ÂB). On the right we can take out a factor aˆ11 from
the first row, aˆ22 from the second, Á , aˆ nn from the nth. But this product aˆ11aˆ22 Á aˆnn
equals det Â because Â is diagonal. The remaining determinant is det B. This proves (10)
for det (AB), and the proof for det (BA) follows by the same idea.
᭿
This completes our discussion of linear systems (Secs. 7.3–7.8). Section 7.9 on vector
spaces and linear transformations is optional. Numeric methods are discussed in Secs.
20.1–20.4, which are independent of other sections on numerics.

PROBLEM SET 7.8
INVERSE

1–10

Find the inverse by Gauss–Jordan (or by (4*) if n ϭ 2).
Check by using (1).
1.

c

Ϫ2.32

1.80
Ϫ0.25

0.60

d

Ϫ0.1

0.5

3. D2

6

4 T

5

0

9

0.3

1

0

0

5. D2

1

0T

5

4

1

2.

c

cos 2u

sin 2u

Ϫsin 2u

cos 2u

0
4. D0

d

0

0.1

Ϫ0.4

0 T

2.5

0

Ϫ4

0

0

6. D 0

8

13T

0

3

5

0

1

0

7. D1

0

0T

0

0

1

0

8

0

9. D0

0

4T

2

0

0

1

2

3

8. D4

5

6T

7

8

9

2
3

1
3

2
3

10. DϪ23

2
3

1
3T

1
3

2
3

Ϫ23

0
11–18

SOME GENERAL FORMULAS

11. Inverse of the square. Verify (A2)؊1 ϭ (A؊1)2 for A
in Prob. 1.
12. Prove the formula in Prob. 11.

c07.qxd

10/28/10

7:30 PM

Page 309

SEC. 7.9 Vector Spaces, Inner Product Spaces, Linear Transformations Optional
13. Inverse of the transpose. Verify (AT)؊1 ϭ (A؊1)T for
A in Prob. 1.

309

18. Row interchange. Same task as in Prob. 16 for the
matrix in Prob. 7.

14. Prove the formula in Prob. 13.
15. Inverse of the inverse. Prove that (A؊1)؊1 ϭ A.
16. Rotation. Give an application of the matrix in Prob. 2
that makes the form of the inverse obvious.
17. Triangular matrix. Is the inverse of a triangular
matrix always triangular (as in Prob. 5)? Give reason.

7.9

19–20

FORMULA (4)

Formula (4) is occasionally needed in theory. To understand
it, apply it and check the result by Gauss–Jordan:
19. In Prob. 3
20. In Prob. 6

Vector Spaces, Inner Product Spaces,
Linear Transformations Optional
We have captured the essence of vector spaces in Sec. 7.4. There we dealt with special
vector spaces that arose quite naturally in the context of matrices and linear systems. The
elements of these vector spaces, called vectors, satisfied rules (3) and (4) of Sec. 7.1
(which were similar to those for numbers). These special vector spaces were generated
by spans, that is, linear combination of finitely many vectors. Furthermore, each such
vector had n real numbers as components. Review this material before going on.
We can generalize this idea by taking all vectors with n real numbers as components
and obtain the very important real n-dimensional vector space Rn. The vectors are known
as “real vectors.” Thus, each vector in Rn is an ordered n-tuple of real numbers.
Now we can consider special values for n. For n ϭ 2, we obtain R2, the vector space
of all ordered pairs, which correspond to the vectors in the plane. For n ϭ 3, we obtain
R3, the vector space of all ordered triples, which are the vectors in 3-space. These vectors
have wide applications in mechanics, geometry, and calculus and are basic to the engineer
and physicist.
Similarly, if we take all ordered n-tuples of complex numbers as vectors and complex
numbers as scalars, we obtain the complex vector space C n, which we shall consider in
Sec. 8.5.
Furthermore, there are other sets of practical interest consisting of matrices, functions,
transformations, or others for which addition and scalar multiplication can be defined in
an almost natural way so that they too form vector spaces.
It is perhaps not too great an intellectual jump to create, from the concrete model R n,
the abstract concept of a real vector space V by taking the basic properties (3) and (4)
in Sec. 7.1 as axioms. In this way, the definition of a real vector space arises.

DEFINITION

Real Vector Space

A nonempty set V of elements a, b, • • • is called a real vector space (or real linear
space), and these elements are called vectors (regardless of their nature, which will
come out from the context or will be left arbitrary) if, in V, there are defined two
algebraic operations (called vector addition and scalar multiplication) as follows.
I. Vector addition associates with every pair of vectors a and b of V a unique
vector of V, called the sum of a and b and denoted by a ϩ b, such that the following
axioms are satisfied.

c07.qxd

10/28/10

310

7:30 PM

Page 310

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

I.1 Commutativity. For any two vectors a and b of V,
a ϩ b ϭ b ϩ a.
I.2 Associativity. For any three vectors a, b, c of V,
(a ϩ b) ϩ c ϭ a ϩ (b ϩ c)

(written a ϩ b ϩ c).

I.3 There is a unique vector in V, called the zero vector and denoted by 0, such
that for every a in V,
a ϩ 0 ϭ a.
I.4 For every a in V there is a unique vector in V that is denoted by Ϫa and is
such that
a ϩ (Ϫa) ϭ 0.
II. Scalar multiplication. The real numbers are called scalars. Scalar
multiplication associates with every a in V and every scalar c a unique vector of V,
called the product of c and a and denoted by ca (or a c) such that the following
axioms are satisfied.
II.1 Distributivity. For every scalar c and vectors a and b in V,
c(a ϩ b) ϭ ca ϩ cb.
II.2 Distributivity. For all scalars c and k and every a in V,
(c ϩ k)a ϭ ca ϩ ka.
II.3 Associativity. For all scalars c and k and every a in V,
c(ka) ϭ (ck)a

(written cka).

II.4 For every a in V,
1a ϭ a.

If, in the above definition, we take complex numbers as scalars instead of real numbers,
we obtain the axiomatic definition of a complex vector space.
Take a look at the axioms in the above definition. Each axiom stands on its own: It
is concise, useful, and it expresses a simple property of V. There are as few axioms as
possible and together they express all the desired properties of V. Selecting good axioms
is a process of trial and error that often extends over a long period of time. But once
agreed upon, axioms become standard such as the ones in the definition of a real vector
space.

c07.qxd

10/28/10

7:30 PM

Page 311

SEC. 7.9 Vector Spaces, Inner Product Spaces, Linear Transformations Optional

311

The following concepts related to a vector space are exactly defined as those given in
Sec. 7.4. Indeed, a linear combination of vectors a(1), Á , a(m) in a vector space V is an
expression
c1a(1) ϩ Á ϩ cmam

(c1, Á , cm any scalars).

These vectors form a linearly independent set (briefly, they are called linearly
independent) if
c1a(1) ϩ Á ϩ cma(m) ϭ 0

(1)

implies that c1 ϭ 0, Á , cm ϭ 0. Otherwise, if (1) also holds with scalars not all zero, the
vectors are called linearly dependent.
Note that (1) with m ϭ 1 is ca ϭ 0 and shows that a single vector a is linearly
independent if and only if a 0.
V has dimension n, or is n-dimensional, if it contains a linearly independent set of n
vectors, whereas any set of more than n vectors in V is linearly dependent. That set of
n linearly independent vectors is called a basis for V. Then every vector in V can be
written as a linear combination of the basis vectors. Furthermore, for a given basis, this
representation is unique (see Prob. 2).
EXAMPLE 1

Vector Space of Matrices
The real 2 ϫ 2 matrices form a four-dimensional real vector space. A basis is
B11 ϭ

c

1

0

0

0

d,

B12 ϭ

c

0

1

0

0

d,

B 21 ϭ

c

0

0

1

0

d,

B22 ϭ

c

0

0

0

1

d

because any 2 ϫ 2 matrix A ϭ [ajk] has a unique representation A ϭ a11B11 ϩ a12B12 ϩ a21B21 ϩ a22B22.
Similarly, the real m ϫ n matrices with fixed m and n form an mn-dimensional vector space. What is the
᭿
dimension of the vector space of all 3 ϫ 3 skew-symmetric matrices? Can you find a basis?

EXAMPLE 2

Vector Space of Polynomials
The set of all constant, linear, and quadratic polynomials in x together is a vector space of dimension 3 with
basis {1, x, x 2 } under the usual addition and multiplication by real numbers because these two operations give
polynomials not exceeding degree 2. What is the dimension of the vector space of all polynomials of degree
᭿
not exceeding a given fixed n? Can you find a basis?

If a vector space V contains a linearly independent set of n vectors for every n, no matter
how large, then V is called infinite dimensional, as opposed to a finite dimensional
(n-dimensional) vector space just defined. An example of an infinite dimensional vector
space is the space of all continuous functions on some interval [a, b] of the x-axis, as we
mention without proof.

Inner Product Spaces
If a and b are vectors in Rn, regarded as column vectors, we can form the product aTb.
This is a 1 ϫ 1 matrix, which we can identify with its single entry, that is, with a number.

c07.qxd

10/28/10

7:30 PM

312

Page 312

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

This product is called the inner product or dot product of a and b. Other notations for
it are (a, b) and a • b. Thus
b1

n

aTb ϭ (a, b) ϭ a • b ϭ 3a1 Á an4 D o T ϭ a albl ϭ a1b1 ϩ Á ϩ anbn.
bn

iϭ1

We now extend this concept to general real vector spaces by taking basic properties of
(a, b) as axioms for an “abstract inner product” (a, b) as follows.

DEFINITION

Real Inner Product Space

A real vector space V is called a real inner product space (or real pre-Hilbert 4
space) if it has the following property. With every pair of vectors a and b in V there
is associated a real number, which is denoted by (a, b) and is called the inner
product of a and b, such that the following axioms are satisfied.
I. For all scalars q1 and q2 and all vectors a, b, c in V,
(q1a ϩ q2b, c) ϭ q1(a, c) ϩ q2(b, c)

(Linearity).

II. For all vectors a and b in V,
(a, b) ϭ (b, a)

(Symmetry).

III. For every a in V,
(a, a) м 0,
(a, a) ϭ 0 if and only if a ϭ 0

r

(Positive-definiteness).

Vectors whose inner product is zero are called orthogonal.
The length or norm of a vector in V is defined by
(2)

ʈ a ʈ ϭ 2(a, a) (м 0).

A vector of norm 1 is called a unit vector.

4
DAVID HILBERT (1862–1943), great German mathematician, taught at Königsberg and Göttingen and was
the creator of the famous Göttingen mathematical school. He is known for his basic work in algebra, the calculus
of variations, integral equations, functional analysis, and mathematical logic. His “Foundations of Geometry”
helped the axiomatic method to gain general recognition. His famous 23 problems (presented in 1900 at the
International Congress of Mathematicians in Paris) considerably influenced the development of modern
mathematics.
If V is finite dimensional, it is actually a so-called Hilbert space; see [GenRef7], p. 128, listed in App. 1.

c07.qxd

10/28/10

7:30 PM

Page 313

SEC. 7.9 Vector Spaces, Inner Product Spaces, Linear Transformations Optional

313

From these axioms and from (2) one can derive the basic inequality
ƒ (a, b) ƒ Ϲ ʈ a ʈ ʈ b ʈ

(3)

(Cauchy–Schwarz5 inequality).

From this follows
ʈa ϩ bʈ Ϲ ʈaʈ ϩ ʈbʈ

(4)

(Triangle inequality).

A simple direct calculation gives
(5)
EXAMPLE 3

ʈ a ϩ b ʈ2 ϩ ʈ a Ϫ b ʈ2 ϭ 2(ʈ a ʈ2 ϩ ʈ b ʈ2)

(Parallelogram equality).

n-Dimensional Euclidean Space
Rn with the inner product
(6)

(a, b) ϭ aTb ϭ a1b1 ϩ Á ϩ anbn

(where both a and b are column vectors) is called the n-dimensional Euclidean space and is denoted by En or
again simply by Rn. Axioms I–III hold, as direct calculation shows. Equation (2) gives the “Euclidean norm”
(7)

EXAMPLE 4

ʈ a ʈ ϭ 2(a, a) ϭ 2aTa ϭ 2a12 ϩ Á ϩ an2.

᭿

An Inner Product for Functions. Function Space
The set of all real-valued continuous functions f (x), g (x), Á on a given interval a Ϲ x Ϲ b is a real vector
space under the usual addition of functions and multiplication by scalars (real numbers). On this “function
space” we can define an inner product by the integral
b

(8)

( f, g) ϭ

Ύ f (x) g (x) dx.
a

Axioms I–III can be verified by direct calculation. Equation (2) gives the norm
b

(9)

ʈ f ʈ ϭ 2( f, f ) ϭ

Ύ f (x)
G

2

dx.

᭿

a

Our examples give a first impression of the great generality of the abstract concepts of
vector spaces and inner product spaces. Further details belong to more advanced courses
(on functional analysis, meaning abstract modern analysis; see [GenRef7] listed in App.
1) and cannot be discussed here. Instead we now take up a related topic where matrices
play a central role.

Linear Transformations
Let X and Y be any vector spaces. To each vector x in X we assign a unique vector y in
Y. Then we say that a mapping (or transformation or operator) of X into Y is given.
Such a mapping is denoted by a capital letter, say F. The vector y in Y assigned to a vector
x in X is called the image of x under F and is denoted by F (x) [or Fx, without parentheses].
5

HERMANN AMANDUS SCHWARZ (1843–1921). German mathematician, known by his work in complex
analysis (conformal mapping) and differential geometry. For Cauchy see Sec. 2.5.

c07.qxd

10/28/10

314

7:30 PM

Page 314

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

F is called a linear mapping or linear transformation if, for all vectors v and x in X
and scalars c,
F (v ϩ x) ϭ F (v) ϩ F (x)

(10)

F (cx) ϭ cF (x).

Linear Transformation of Space Rn into Space Rm
From now on we let X ϭ Rn and Y ϭ Rm. Then any real m ϫ n matrix A ϭ [ajk] gives
a transformation of Rn into Rm,
y ϭ Ax.

(11)

Since A(u ϩ x) ϭ Au ϩ Ax and A(cx) ϭ cAx, this transformation is linear.
We show that, conversely, every linear transformation F of Rn into Rm can be given
in terms of an m ϫ n matrix A, after a basis for Rn and a basis for Rm have been chosen.
This can be proved as follows.
Let e (1), Á , e (n) be any basis for Rn. Then every x in Rn has a unique representation
x ϭ x 1e (1) ϩ Á ϩ x ne (n).
Since F is linear, this representation implies for the image F (x):
F (x) ϭ F (x 1e (1) ϩ Á ϩ x ne (n)) ϭ x 1F (e (1)) ϩ Á ϩ x nF (e (n)).
Hence F is uniquely determined by the images of the vectors of a basis for Rn. We now
choose for Rn the “standard basis”

(12)

1

0

0

0

1

0

e (1) ϭ G0W,
.
.
.
0

e (2) ϭ G0W,
.
.
.
0

Á,

e (n) ϭ G0W
.
.
.
1

where e ( j) has its jth component equal to 1 and all others 0. We show that we can now
determine an m ϫ n matrix A ϭ [ajk] such that for every x in Rn and image y ϭ F (x) in
Rm,
y ϭ F (x) ϭ Ax.
Indeed, from the image y (1) ϭ F (e (1)) of e (1) we get the condition
y1(1)
y

(1)

a11

Á

a1n

y2(1)
a21
ϭF . VϭF .
.
.
.
.

Á

a2n
0
. V F.V
.
.
.
.

Á

amm

(1)
ym

am1

1

0

c07.qxd

10/28/10

7:30 PM

Page 315

SEC. 7.9 Vector Spaces, Inner Product Spaces, Linear Transformations Optional

315

(1) Á
from which we can determine the first column of A, namely a11 ϭ y(1)
,
1 , a21 ϭ y2 ,
(1)
am1 ϭ ym . Similarly, from the image of e (2) we get the second column of A, and so on.
This completes the proof.
᭿

We say that A represents F, or is a representation of F, with respect to the bases for Rn
and Rm. Quite generally, the purpose of a “representation” is the replacement of one
object of study by another object whose properties are more readily apparent.
In three-dimensional Euclidean space E 3 the standard basis is usually written e (1) ϭ i,
e (2) ϭ j, e (3) ϭ k. Thus,
1

0

i ϭ D0T ,

(13)

0

j ϭ D1T ,

0

k ϭ D0T .

0

1

These are the three unit vectors in the positive directions of the axes of the Cartesian
coordinate system in space, that is, the usual coordinate system with the same scale of
measurement on the three mutually perpendicular coordinate axes.
EXAMPLE 5

Linear Transformations
Interpreted as transformations of Cartesian coordinates in the plane, the matrices

c

0

1

1

0

d, c

1

0

0

Ϫ1

d, c

Ϫ1

0

0

Ϫ1

d, c

a

0

0

1

d

represent a reflection in the line x2 ϭ x1, a reflection in the x1-axis, a reflection in the origin, and a stretch
(when a Ͼ 1, or a contraction when 0 Ͻ a Ͻ 1) in the x1-direction, respectively.
᭿

EXAMPLE 6

Linear Transformations
Our discussion preceding Example 5 is simpler than it may look at first sight. To see this, find A representing
the linear transformation that maps (x1, x2) onto (2x1 Ϫ 5x2, 3x1 ϩ 4x2).

Solution. Obviously, the transformation is
y1 ϭ 2x 1 Ϫ 5x 2
y2 ϭ 3x 1 ϩ 4x 2.
From this we can directly see that the matrix is
Aϭ

c

2
3

Ϫ5
4

d.

Check:

c d
y1
y2

ϭ

c

2

Ϫ5

3

4

dc d
x1
x2

ϭ

c

2x 1 Ϫ 5x 2
3x 1 ϩ 4x 2

d.

᭿

If A in (11) is square, n ϫ n, then (11) maps Rn into Rn. If this A is nonsingular, so that
A؊1 exists (see Sec. 7.8), then multiplication of (11) by A؊1 from the left and use of
A؊1A ϭ I gives the inverse transformation
(14)

x ϭ A؊1y.

It maps every y ϭ y0 onto that x, which by (11) is mapped onto y0. The inverse of a linear
transformation is itself linear, because it is given by a matrix, as (14) shows.

c07.qxd

11/4/10

12:30 PM

316

Page 316

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

Composition of Linear Transformations
We want to give you a flavor of how linear transformations in general vector spaces work.
You will notice, if you read carefully, that definitions and verifications (Example 7) strictly
follow the given rules and you can think your way through the material by going in a
slow systematic fashion.
The last operation we want to discuss is composition of linear transformations. Let X,
Y, W be general vector spaces. As before, let F be a linear transformation from X to Y.
Let G be a linear transformation from W to X. Then we denote, by H, the composition
of F and G, that is,
H ϭ F ‫ ؠ‬G ϭ FG ϭ F(G),
which means we take transformation G and then apply transformation F to it (in that
order!, i.e. you go from left to right).
Now, to give this a more concrete meaning, if we let w be a vector in W, then G (w)
is a vector in X and F (G (w)) is a vector in Y. Thus, H maps W to Y, and we can write
(15)

H (w) ϭ (F ‫ ؠ‬G) (w) ϭ (FG) (w) ϭ F(G(w)),

which completes the definition of composition in a general vector space setting. But is
composition really linear? To check this we have to verify that H, as defined in (15), obeys
the two equations of (10).
EXAMPLE 7

The Composition of Linear Transformations Is Linear
To show that H is indeed linear we must show that (10) holds. We have, for two vectors w1, w2 in W,
H (w1 ϩ w2) ϭ (F ‫ ؠ‬G)(w1 ϩ w2)
ϭ F (G (w1 ϩ w2))
ϭ F (G (w1) ϩ G (w2))

(by linearity of G)

ϭ F (G (w1)) ϩ F (G (w2))

(by linearity of F)

ϭ (F ‫ ؠ‬G)(w1) ϩ (F ‫ ؠ‬G)(w2)

(by (15))

ϭ H (w1) ϩ H (w2)

(by definition of H).

Similarly, H (cw2) ϭ (F ‫ ؠ‬G)(cw2) ϭ F (G (cw2)) ϭ F (c (G (w2))
ϭ cF (G (w2)) ϭ c (F ‫ ؠ‬G)(w2) ϭ cH(w2).

᭿

We defined composition as a linear transformation in a general vector space setting and
showed that the composition of linear transformations is indeed linear.
Next we want to relate composition of linear transformations to matrix multiplication.
To do so we let X ϭ Rn, Y ϭ Rm, and W ϭ Rp. This choice of particular vector spaces
allows us to represent the linear transformations as matrices and form matrix equations,
as was done in (11). Thus F can be represented by a general real m ϫ n matrix A ϭ 3ajk4
and G by an n ϫ p matrix B ϭ 3bjk4. Then we can write for F, with column vectors x
with n entries, and resulting vector y, with m entries
(16)

y ϭ Ax

c07.qxd

11/9/10

7:34 PM

Page 317

SEC. 7.9 Vector Spaces, Inner Product Spaces, Linear Transformations Optional

317

and similarly for G, with column vector w with p entries,
x ϭ Bw.

(17)
Substituting (17) into (16) gives
(18)

y ϭ Ax ϭ A(Bw) ϭ (AB)w ϭ ABw ϭ Cw

where C ϭ AB.

This is (15) in a matrix setting, this is, we can define the composition of linear transformations in the Euclidean spaces as multiplication by matrices. Hence, the real m ϫ p
matrix C represents a linear transformation H which maps R p to Rn with vector w, a
column vector with p entries.
Remarks. Our discussion is similar to the one in Sec. 7.2, where we motivated the
“unnatural” matrix multiplication of matrices. Look back and see that our current, more
general, discussion is written out there for the case of dimension m ϭ 2, n ϭ 2, and p ϭ 2.
(You may want to write out our development by picking small distinct dimensions, such
as m ϭ 2, n ϭ 3, and p ϭ 4, and writing down the matrices and vectors. This is a trick
of the trade of mathematicians in that we like to develop and test theories on smaller
examples to see that they work.)
EXAMPLE 8

Linear Transformations. Composition
In Example 5 of Sec. 7.9, let A be the first matrix and B be the fourth matrix with a Ͼ 1. Then, applying B to
a vector w ϭ [w1 w2]T, stretches the element w1 by a in the x1 direction. Next, when we apply A to the
“stretched” vector, we reflect the vector along the line x1 ϭ x2, resulting in a vector y ϭ [w2 aw1]T. But this
represents, precisely, a geometric description for the composition H of two linear transformations F and G
represented by matrices A and B. We now show that, for this example, our result can be obtained by
straightforward matrix multiplication, that is,
AB ϭ

c

0

1

1

0

dc

d

ϭ

c

0

1

a

0

dc d

ϭ

c

w2

d,

a

0

0

1

d

and as in (18) calculate
ABw ϭ

c

0

1

a

0

w1
w2

aw1

which is the same as before. This shows that indeed AB ϭ C, and we see the composition of linear
transformations can be represented by a linear transformation. It also shows that the order of matrix multiplication
is important (!). You may want to try applying A first and then B, resulting in BA. What do you see? Does it
᭿
make geometric sense? Is it the same result as AB?

We have learned several abstract concepts such as vector space, inner product space,
and linear transformation. The introduction of such concepts allows engineers and
scientists to communicate in a concise and common language. For example, the concept
of a vector space encapsulated a lot of ideas in a very concise manner. For the student,
learning such concepts provides a foundation for more advanced studies in engineering.
This concludes Chapter 7. The central theme was the Gaussian elimination of Sec. 7.3
from which most of the other concepts and theory flowed. The next chapter again has a
central theme, that is, eigenvalue problems, an area very rich in applications such as in
engineering, modern physics, and other areas.

c07.qxd

10/28/10

318

7:30 PM

Page 318

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

PROBLEM SET 7.9
1. Basis. Find three bases of R2.
2. Uniqueness. Show that the representation v ϭ c1a(1)
ϩ Á ϩ cna(n) of any given vector in an n-dimensional
vector space V in terms of a given basis a(1), Á , a(n)
for V is unique. Hint. Take two representations and
consider the difference.
3–10

LINEAR TRANSFORMATIONS

Find the inverse transformation. Show the details.
11. y1 ϭ 0.5x 1 Ϫ 0.5x 2
12. y1 ϭ 3x 1 ϩ 2x 2
y2 ϭ 1.5x 1 Ϫ 2.5x 2

y2 ϭ 3x 1 ϩ 2x 2 Ϫ 2x 3
y3 ϭ 2x 1 Ϫ x 2 ϩ 2x 3
14. y1 ϭ 0.2x 1 Ϫ 0.1x 2
y2 ϭ

VECTOR SPACE

(More problems in Problem Set 9.4.) Is the given set, taken
with the usual addition and scalar multiplication, a vector
space? Give reason. If your answer is yes, find the dimension and a basis.
3. All vectors in R3 satisfying Ϫv1 ϩ 2v2 ϩ 3v3 ϭ 0,
Ϫ4v1 ϩ v2 ϩ v3 ϭ 0.
4. All skew-symmetric 3 ϫ 3 matrices.
5. All polynomials in x of degree 4 or less with
nonnegative coefficients.
6. All functions y (x) ϭ a cos 2x ϩ b sin 2x with arbitrary
constants a and b.
7. All functions y (x) ϭ (ax ϩ b)e؊x with any constant a
and b.
8. All n ϫ n matrices A with fixed n and det A ϭ 0.
9. All 2 ϫ 2 matrices [ajk] with a11 ϩ a22 ϭ 0.
10. All 3 ϫ 2 matrices [ajk] with first column any multiple
of [3
0 Ϫ5]T.
11–14

13. y1 ϭ 5x 1 ϩ 3x 2 Ϫ 3x 3

Ϫ 0.2x 2 ϩ 0.1x 3

y3 ϭ 0.1x 1
15–20

ϩ 0.1x 3

EUCLIDEAN NORM

Find the Euclidean norm of the vectors:
1
15. 33
16. 312
1 Ϫ44T
3
17. 31
0
0
1 Ϫ1
0 Ϫ1
2
T
18. 3Ϫ4
19.
8 Ϫ14
323
3
1 T
20. 312 Ϫ12 Ϫ12
24
21–25

Ϫ12 Ϫ134T
14T
1
04T
3

INNER PRODUCT. ORTHOGONALITY

21. Orthogonality. For what value(s) of k are the vectors
1
32
Ϫ4
04T and 35 k 0 144T orthogonal?
2
22. Orthogonality. Find all vectors in R3 orthogonal to
32 0 14. Do they form a vector space?
23. Triangle inequality. Verify (4) for the vectors in
Probs. 15 and 18.
24. Cauchy–Schwarz inequality. Verify (3) for the
vectors in Probs. 16 and 19.
25. Parallelogram equality. Verify (5) for the first two
column vectors of the coefficient matrix in Prob. 13.

y2 ϭ 4x 1 ϩ x 2

CHAPTER 7 REVIEW QUESTIONS AND PROBLEMS
1. What properties of matrix multiplication differ from
those of the multiplication of numbers?
2. Let A be a 100 ϫ 100 matrix and B a 100 ϫ 50 matrix.
Are the following expressions defined or not? A ϩ B,
A2, B2, AB, BA, AAT, BTA, BTB, BBT, BT AB. Give
reasons.
3. Are there any linear systems without solutions? With
one solution? With more than one solution? Give
simple examples.
4. Let C be 10 ϫ 10 matrix and a a column vector with
10 components. Are the following expressions defined
or not? Ca, C Ta, CaT, aC, aTC, (CaT)T.

5. Motivate the definition of matrix multiplication.
6. Explain the use of matrices in linear transformations.
7. How can you give the rank of a matrix in terms of row
vectors? Of column vectors? Of determinants?
8. What is the role of rank in connection with solving
linear systems?
9. What is the idea of Gauss elimination and back
substitution?
10. What is the inverse of a matrix? When does it exist?
How would you determine it?

c07.qxd

10/28/10

7:30 PM

Page 319

Chapter 7 Review Questions and Problems
11–20

319

MATRIX AND VECTOR CALCULATIONS

Showing the details, calculate the following expressions or
give reason why they are not defined, when
3

1

AϭD 1
Ϫ3

Ϫ3

0

4

1

4

2T , B ϭ DϪ4

0

Ϫ2T ,

2

5

Ϫ1

2

0

2

7

u ϭ D 0T ,

v ϭ DϪ3T

Ϫ5
11.
13.
15.
17.
18.
20.

3

AB, BA
12.
T
14.
Au, u A
16.
uTAu, v TBv
det A, det A2, (det A)2,
19.
(A2)؊1, (A؊1)2
T
T
(A ϩ A )(B Ϫ B )

21–28

AT, BT
uTv, uv T
A؊1, B؊1
det B
AB Ϫ BA

LINEAR SYSTEMS

6

3x ϩ 5y ϭ

20

Ϫ4x ϩ y ϭ Ϫ42
28. Ϫ8x

ϩ 2z ϭ 1
6y ϩ 4z ϭ 3

12x ϩ 2y

ϭ2

RANK

29–32

Determine the ranks of the coefficient matrix and the
augmented matrix and state how many solutions the linear
system will have.
29. In Prob. 23
30. In Prob. 24
31. In Prob. 27
32. In Prob. 26

NETWORKS

33–35

Find the currents.
33.
20 Ω

Showing the details, find all solutions or indicate that no
solution exists.
21.
4y ϩ z ϭ 0

I3

I1

12x Ϫ 5y Ϫ 3z ϭ 34
Ϫ6x

x ϩ 2y ϭ

27.

I2

ϩ 4z ϭ 8

10 Ω

110 V

22. 5x Ϫ 3y ϩ z ϭ 7

34.

220 V

2x ϩ 3y Ϫ z ϭ 0
5Ω

8x ϩ 9y Ϫ 3z ϭ 2
23. 9x ϩ 3y Ϫ 6z ϭ 60

I2

2x Ϫ 4y ϩ 8z ϭ 4
24. Ϫ6x ϩ 39y Ϫ 9z ϭ Ϫ12
2x Ϫ 13y ϩ 3z ϭ
25. 0.3x Ϫ 0.7y ϩ 1.3z ϭ

4
3.24

26.

2x ϩ 3y Ϫ 7z ϭ 3
Ϫ4x Ϫ 6y ϩ 14z ϭ 7

1.19

I3

10 Ω

240 V

35.

20 Ω

10 Ω
10 V

I1

0.9y Ϫ 0.8z ϭ Ϫ2.53
0.7z ϭ

I1

I2

I3

30 Ω

20 Ω

130 V

c07.qxd

10/28/10

320

7:30 PM

Page 320

CHAP. 7 Linear Algebra: Matrices, Vectors, Determinants. Linear Systems

SUMMARY OF CHAPTER

7

Linear Algebra: Matrices, Vectors, Determinants.
Linear Systems
An m ϫ n matrix A ϭ [ajk] is a rectangular array of numbers or functions
(“entries,” “elements”) arranged in m horizontal rows and n vertical columns. If
m ϭ n, the matrix is called square. A 1 ϫ n matrix is called a row vector and an
m ϫ 1 matrix a column vector (Sec. 7.1).
The sum A ϩ B of matrices of the same size (i.e., both m ϫ n) is obtained by
adding corresponding entries. The product of A by a scalar c is obtained by
multiplying each ajk by c (Sec. 7.1).
The product C ϭ AB of an m ϫ n matrix A by an r ϫ p matrix B ϭ [bjk] is
defined only when r ϭ n, and is the m ϫ p matrix C ϭ 3cjk4 with entries
(row j of A times
column k of B).

cjk ϭ aj1b1k ϩ aj2b2k ϩ Á ϩ ajnbnk

(1)

This multiplication is motivated by the composition of linear transformations
(Secs. 7.2, 7.9). It is associative, but is not commutative: if AB is defined, BA may
not be defined, but even if BA is defined, AB BA in general. Also AB ϭ 0 may
not imply A ϭ 0 or B ϭ 0 or BA ϭ 0 (Secs. 7.2, 7.8). Illustrations:

c
c
[1

1

1

2

2

dc

Ϫ1

1

1

Ϫ1

dc

Ϫ1

1

1

Ϫ1

d

ϭ [11],

2] c

3
4

1

1

2

2

d

d

ϭ

c d [1
3
4

c

ϭ

c

0

0

0

0

d

1

1

Ϫ1

Ϫ1

2] ϭ

c

d

3

6

4

8

d.

The transpose AT of a matrix A ϭ 3ajk4 is AT ϭ 3akj4; rows become columns
and conversely (Sec. 7.2). Here, A need not be square. If it is and A ϭ AT, then A
is called symmetric; if A ϭ ϪAT, it is called skew-symmetric. For a product,
(AB)T ϭ BTAT (Sec. 7.2).
A main application of matrices concerns linear systems of equations
(2)

Ax ϭ b

(Sec. 7.3)

(m equations in n unknowns x 1, Á , x n; A and b given). The most important method
of solution is the Gauss elimination (Sec. 7.3), which reduces the system to
“triangular” form by elementary row operations, which leave the set of solutions
unchanged. (Numeric aspects and variants, such as Doolittle’s and Cholesky’s
methods, are discussed in Secs. 20.1 and 20.2.)

c07.qxd

10/28/10

7:30 PM

Page 321

Summary of Chapter 7

321

Cramer’s rule (Secs. 7.6, 7.7) represents the unknowns in a system (2) of n
equations in n unknowns as quotients of determinants; for numeric work it is
impractical. Determinants (Sec. 7.7) have decreased in importance, but will retain
their place in eigenvalue problems, elementary geometry, etc.
The inverse A؊1 of a square matrix satisfies AA؊1 ϭ A؊1A ϭ I. It exists if and
only if det A 0. It can be computed by the Gauss–Jordan elimination (Sec. 7.8).
The rank r of a matrix A is the maximum number of linearly independent rows
or columns of A or, equivalently, the number of rows of the largest square submatrix
of A with nonzero determinant (Secs. 7.4, 7.7).
The system (2) has solutions if and only if rank A ϭ rank [A b], where [A b]
is the augmented matrix (Fundamental Theorem, Sec. 7.5).
The homogeneous system
(3)

Ax ϭ 0

has solutions x 0 (“nontrivial solutions”) if and only if rank A Ͻ n, in the case
m ϭ n equivalently if and only if det A ϭ 0 (Secs. 7.6, 7.7).
Vector spaces, inner product spaces, and linear transformations are discussed in
Sec. 7.9. See also Sec. 7.4.

c08.qxd

10/30/10

10:56 AM

Page 322

CHAPTER

8

Linear Algebra:
Matrix Eigenvalue Problems
A matrix eigenvalue problem considers the vector equation
(1)

Ax ϭ lx.

Here A is a given square matrix, l an unknown scalar, and x an unknown vector. In a
matrix eigenvalue problem, the task is to determine l’s and x’s that satisfy (1). Since
x ϭ 0 is always a solution for any l and thus not interesting, we only admit solutions
with x 0.
The solutions to (1) are given the following names: The l’s that satisfy (1) are called
eigenvalues of A and the corresponding nonzero x’s that also satisfy (1) are called
eigenvectors of A.
From this rather innocent looking vector equation flows an amazing amount of relevant
theory and an incredible richness of applications. Indeed, eigenvalue problems come up
all the time in engineering, physics, geometry, numerics, theoretical mathematics, biology,
environmental science, urban planning, economics, psychology, and other areas. Thus, in
your career you are likely to encounter eigenvalue problems.
We start with a basic and thorough introduction to eigenvalue problems in Sec. 8.1 and
explain (1) with several simple matrices. This is followed by a section devoted entirely
to applications ranging from mass–spring systems of physics to population control models
of environmental science. We show you these diverse examples to train your skills in
modeling and solving eigenvalue problems. Eigenvalue problems for real symmetric,
skew-symmetric, and orthogonal matrices are discussed in Sec. 8.3 and their complex
counterparts (which are important in modern physics) in Sec. 8.5. In Sec. 8.4 we show
how by diagonalizing a matrix, we obtain its eigenvalues.
COMMENT. Numerics for eigenvalues (Secs. 20.6–20.9) can be studied immediately
after this chapter.
Prerequisite: Chap. 7.
Sections that may be omitted in a shorter course: 8.4, 8.5.
References and Answers to Problems: App. 1 Part B, App. 2.

322

c08.qxd

11/9/10

3:07 PM

Page 323

SEC. 8.1 The Matrix Eigenvalue Problem. Determining Eigenvalues and Eigenvectors

323

The following chart identifies where different types of eigenvalue problems appear in the
book.

8.1

Topic

Where to find it

Matrix Eigenvalue Problem (algebraic eigenvalue problem)
Eigenvalue Problems in Numerics
Eigenvalue Problem for ODEs (Sturm–Liouville problems)
Eigenvalue Problems for Systems of ODEs
Eigenvalue Problems for PDEs

Chap. 8
Secs. 20.6–20.9
Secs. 11.5, 11.6
Chap. 4
Secs. 12.3–12.11

The Matrix Eigenvalue Problem. Determining
Eigenvalues and Eigenvectors
Consider multiplying nonzero vectors by a given square matrix, such as

c

6

3

4

7

dc d
5
1

ϭ

c d,
33
27

c

6

3

4

7

dc d
3
4

ϭ

c d.
30
40

We want to see what influence the multiplication of the given matrix has on the vectors.
In the first case, we get a totally new vector with a different direction and different length
when compared to the original vector. This is what usually happens and is of no interest
here. In the second case something interesting happens. The multiplication produces a
vector [30 40]T ϭ 10 [3 4]T, which means the new vector has the same direction as
the original vector. The scale constant, which we denote by l is 10. The problem of
systematically finding such l’s and nonzero vectors for a given square matrix will be the
theme of this chapter. It is called the matrix eigenvalue problem or, more commonly, the
eigenvalue problem.
We formalize our observation. Let A ϭ [ajk] be a given nonzero square matrix of
dimension n ϫ n. Consider the following vector equation:
(1)

Ax ϭ lx.

The problem of finding nonzero x’s and l’s that satisfy equation (1) is called an eigenvalue
problem.
Remark. So A is a given square (!) matrix, x is an unknown vector, and l is an
unknown scalar. Our task is to find l’s and nonzero x’s that satisfy (1). Geometrically,
we are looking for vectors, x, for which the multiplication by A has the same effect as
the multiplication by a scalar l; in other words, Ax should be proportional to x. Thus,
the multiplication has the effect of producing, from the original vector x, a new vector
lx that has the same or opposite (minus sign) direction as the original vector. (This was
all demonstrated in our intuitive opening example. Can you see that the second equation in
that example satisfies (1) with l ϭ 10 and x ϭ [3 4]T, and A the given 2 ϫ 2 matrix?
Write it out.) Now why do we require x to be nonzero? The reason is that x ϭ 0 is
always a solution of (1) for any value of l, because A0 ϭ 0. This is of no interest.

c08.qxd

10/30/10

10:56 AM

324

Page 324

CHAP. 8 Linear Algebra: Matrix Eigenvalue Problems

We introduce more terminology. A value of l, for which (1) has a solution x 0, is
called an eigenvalue or characteristic value of the matrix A. Another term for l is a latent
root. (“Eigen” is German and means “proper” or “characteristic.”). The corresponding
solutions x 0 of (1) are called the eigenvectors or characteristic vectors of A
corresponding to that eigenvalue l. The set of all the eigenvalues of A is called the
spectrum of A. We shall see that the spectrum consists of at least one eigenvalue and at
most of n numerically different eigenvalues. The largest of the absolute values of the
eigenvalues of A is called the spectral radius of A, a name to be motivated later.

How to Find Eigenvalues and Eigenvectors
Now, with the new terminology for (1), we can just say that the problem of determining
the eigenvalues and eigenvectors of a matrix is called an eigenvalue problem. (However,
more precisely, we are considering an algebraic eigenvalue problem, as opposed to an
eigenvalue problem involving an ODE or PDE, as considered in Secs. 11.5 and 12.3, or
an integral equation.)
Eigenvalues have a very large number of applications in diverse fields such as in
engineering, geometry, physics, mathematics, biology, environmental science, economics,
psychology, and other areas. You will encounter applications for elastic membranes,
Markov processes, population models, and others in this chapter.
Since, from the viewpoint of engineering applications, eigenvalue problems are the most
important problems in connection with matrices, the student should carefully follow our
discussion.
Example 1 demonstrates how to systematically solve a simple eigenvalue problem.
EXAMPLE 1

Determination of Eigenvalues and Eigenvectors
We illustrate all the steps in terms of the matrix
Aϭ

c

Ϫ5

2

2

Ϫ2

d.

Solution. (a) Eigenvalues. These must be determined first. Equation (1) is
Ax ϭ

c

Ϫ5

2

2

Ϫ2

dc d
x1
x2

ϭ lc

x1
x2

d;

in components,

Ϫ5x 1 ϩ 2x 2 ϭ lx 1
2x 1 Ϫ 2x 2 ϭ lx 2.

Transferring the terms on the right to the left, we get
(Ϫ5 Ϫ l)x 1 ϩ

(2*)

2x 2

ϭ0

ϩ (Ϫ2 Ϫ l)x 2 ϭ 0.

2x 1
This can be written in matrix notation

(A Ϫ lI)x ϭ 0

(3*)

because (1) is Ax Ϫ lx ϭ Ax Ϫ lIx ϭ (A Ϫ lI)x ϭ 0, which gives (3*). We see that this is a homogeneous
linear system. By Cramer’s theorem in Sec. 7.7 it has a nontrivial solution x 0 (an eigenvector of A we are
looking for) if and only if its coefficient determinant is zero, that is,
(4*)

D (l) ϭ det (A Ϫ lI) ϭ 2

Ϫ5 Ϫ l

2

2

Ϫ2 Ϫ l

2 ϭ (Ϫ5 Ϫ l)(Ϫ2 Ϫ l) Ϫ 4 ϭ l2 ϩ 7l ϩ 6 ϭ 0.

c08.qxd

10/30/10

10:56 AM

Page 325

SEC. 8.1 The Matrix Eigenvalue Problem. Determining Eigenvalues and Eigenvectors

325

We call D (l) the characteristic determinant or, if expanded, the characteristic polynomial, and D (l) ϭ 0
the characteristic equation of A. The solutions of this quadratic equation are l1 ϭ Ϫ1 and l2 ϭ Ϫ6. These
are the eigenvalues of A.
(b1) Eigenvector of A corresponding to l1. This vector is obtained from (2*) with l ϭ l1 ϭ Ϫ1, that is,
Ϫ4x 1 ϩ 2x 2 ϭ 0
2x 1 Ϫ x 2 ϭ 0.
A solution is x 2 ϭ 2x 1, as we see from either of the two equations, so that we need only one of them. This
determines an eigenvector corresponding to l1 ϭ Ϫ1 up to a scalar multiple. If we choose x 1 ϭ 1, we obtain
the eigenvector

x1 ϭ

c d,
1

Check:

2

Ax1 ϭ

c

Ϫ5

2

2

Ϫ2

dc d
1
2

ϭ

c

Ϫ1
Ϫ2

d

ϭ (Ϫ1)x1 ϭ l1x1.

(b2) Eigenvector of A corresponding to l2. For l ϭ l2 ϭ Ϫ6, equation (2*) becomes
x 1 ϩ 2x 2 ϭ 0
2x 1 ϩ 4x 2 ϭ 0.
A solution is x 2 ϭ Ϫx 1>2 with arbitrary x1. If we choose x 1 ϭ 2, we get x 2 ϭ Ϫ1. Thus an eigenvector of A
corresponding to l2 ϭ Ϫ6 is

x2 ϭ

c

2
Ϫ1

d,

Ax2 ϭ

Check:

c

Ϫ5

2

2

Ϫ2

dc

2
Ϫ1

d

ϭ

c

Ϫ12
6

d

ϭ (Ϫ6)x2 ϭ l2x2.

For the matrix in the intuitive opening example at the start of Sec. 8.1, the characteristic equation is
l2 Ϫ 13l ϩ 30 ϭ (l Ϫ 10)(l Ϫ 3) ϭ 0. The eigenvalues are {10, 3}. Corresponding eigenvectors are
[3 4]T and [Ϫ1 1]T , respectively. The reader may want to verify this.
᭿

This example illustrates the general case as follows. Equation (1) written in components is
a11x 1 ϩ Á ϩ a1nx n ϭ lx 1
a21x 1 ϩ Á ϩ a2nx n ϭ lx 2

#######################
an1x 1 ϩ Á ϩ annx n ϭ lx n.
Transferring the terms on the right side to the left side, we have
(a11 Ϫ l)x 1 ϩ
a21x 1

(2)

ϩ Á ϩ

a1nx n

ϭ0

ϩ (a22 Ϫ l)x 2 ϩ Á ϩ

a2nx n

ϭ0

a12x 2

. . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
an1x 1

ϩ

an2x 2

ϩ Á ϩ (ann Ϫ l)x n ϭ 0.

In matrix notation,
(3)

(A Ϫ lI)x ϭ 0.

c08.qxd

10/30/10

10:56 AM

326

Page 326

CHAP. 8 Linear Algebra: Matrix Eigenvalue Problems

By Cramer’s theorem in Sec. 7.7, this homogeneous linear system of equations has a
nontrivial solution if and only if the corresponding determinant of the coefficients is zero:

(4)

D(l) ϭ det (A Ϫ lI) ϭ 5

a11 Ϫ l

a12

Á

a1n

a21

a22 Ϫ l

Á

a2n

#

#

Á

#

an1

an2

Á

ann Ϫ l

5 ϭ 0.

A Ϫ lI is called the characteristic matrix and D (l) the characteristic determinant of
A. Equation (4) is called the characteristic equation of A. By developing D(l) we obtain
a polynomial of nth degree in l. This is called the characteristic polynomial of A.
This proves the following important theorem.

THEOREM 1

Eigenvalues

The eigenvalues of a square matrix A are the roots of the characteristic equation
(4) of A.
Hence an n ϫ n matrix has at least one eigenvalue and at most n numerically
different eigenvalues.

For larger n, the actual computation of eigenvalues will, in general, require the use
of Newton’s method (Sec. 19.2) or another numeric approximation method in Secs.
20.7–20.9.
The eigenvalues must be determined first. Once these are known, corresponding
eigenvectors are obtained from the system (2), for instance, by the Gauss elimination,
where l is the eigenvalue for which an eigenvector is wanted. This is what we did in
Example 1 and shall do again in the examples below. (To prevent misunderstandings:
numeric approximation methods, such as in Sec. 20.8, may determine eigenvectors first.)
Eigenvectors have the following properties.

THEOREM 2

Eigenvectors, Eigenspace

If w and x are eigenvectors of a matrix A corresponding to the same eigenvalue l,
so are w ϩ x (provided x Ϫw) and kx for any k 0.
Hence the eigenvectors corresponding to one and the same eigenvalue l of A,
together with 0, form a vector space (cf. Sec. 7.4), called the eigenspace of A
corresponding to that l.

PROOF

Aw ϭ lw and Ax ϭ lx imply A(w ϩ x) ϭ Aw ϩ Ax ϭ lw ϩ lx ϭ l(w ϩ x) and
A (kw) ϭ k (Aw) ϭ k (lw) ϭ l (kw); hence A (kw ϩ /x) ϭ l (kw ϩ /x).
᭿
In particular, an eigenvector x is determined only up to a constant factor. Hence we
can normalize x, that is, multiply it by a scalar to get a unit vector (see Sec. 7.9). For
instance, x1 ϭ [1 2]T in Example 1 has the length ʈx1ʈ ϭ 212 ϩ 22 ϭ 15; hence
[1> 15 2> 15]T is a normalized eigenvector (a unit eigenvector).

c08.qxd

10/30/10

10:56 AM

Page 327

SEC. 8.1 The Matrix Eigenvalue Problem. Determining Eigenvalues and Eigenvectors

327

Examples 2 and 3 will illustrate that an n ϫ n matrix may have n linearly independent
eigenvectors, or it may have fewer than n. In Example 4 we shall see that a real matrix
may have complex eigenvalues and eigenvectors.
EXAMPLE 2

Multiple Eigenvalues
Find the eigenvalues and eigenvectors of
Ϫ2

2

Ϫ3

AϭD 2

1

Ϫ6T .

Ϫ1

Ϫ2

0

Solution. For our matrix, the characteristic determinant gives the characteristic equation
Ϫl3 Ϫ l2 ϩ 21l ϩ 45 ϭ 0.
The roots (eigenvalues of A) are l1 ϭ 5, l2 ϭ l3 ϭ Ϫ3. (If you have trouble finding roots, you may want to
use a root finding algorithm such as Newton’s method (Sec. 19.2). Your CAS or scientific calculator can find
roots. However, to really learn and remember this material, you have to do some exercises with paper and pencil.)
To find eigenvectors, we apply the Gauss elimination (Sec. 7.3) to the system (A Ϫ lI)x ϭ 0, first with l ϭ 5
and then with l ϭ Ϫ3. For l ϭ 5 the characteristic matrix is
Ϫ7

2

Ϫ3

A Ϫ lI ϭ A Ϫ 5I ϭ D 2

Ϫ4

Ϫ6T .

Ϫ1

Ϫ2

Ϫ5

It row-reduces to

Ϫ7

2

D 0

Ϫ 24
7

0

0

Ϫ3
Ϫ 48
7 T.
0

48
Hence it has rank 2. Choosing x 3 ϭ Ϫ1 we have x 2 ϭ 2 from Ϫ 24
7 x 2 Ϫ 7 x 3 ϭ 0 and then x 1 ϭ 1 from
Ϫ7x 1 ϩ 2x 2 Ϫ 3x 3 ϭ 0. Hence an eigenvector of A corresponding to l ϭ 5 is x1 ϭ [1 2 Ϫ1]T.
For l ϭ Ϫ3 the characteristic matrix

1

2

Ϫ3

A Ϫ lI ϭ A ϩ 3I ϭ D 2

4

Ϫ6T

Ϫ1

Ϫ2

row-reduces to

3

Ϫ3

1

2

D0

0

0T .

0

0

0

Hence it has rank 1. From x 1 ϩ 2x 2 Ϫ 3x 3 ϭ 0 we have x 1 ϭ Ϫ2x 2 ϩ 3x 3. Choosing x 2 ϭ 1, x 3 ϭ 0 and
x 2 ϭ 0, x 3 ϭ 1, we obtain two linearly independent eigenvectors of A corresponding to l ϭ Ϫ3 [as they must
exist by (5), Sec. 7.5, with rank ϭ 1 and n ϭ 3],
Ϫ2
x2 ϭ D 1T
0
and
3
x3 ϭ D0T .

᭿

1

The order M l of an eigenvalue l as a root of the characteristic polynomial is called the
algebraic multiplicity of l. The number m l of linearly independent eigenvectors
corresponding to l is called the geometric multiplicity of l. Thus m l is the dimension
of the eigenspace corresponding to this l.

c08.qxd

10/30/10

10:56 AM

328

Page 328

CHAP. 8 Linear Algebra: Matrix Eigenvalue Problems

Since the characteristic polynomial has degree n, the sum of all the algebraic
multiplicities must equal n. In Example 2 for l ϭ Ϫ3 we have m l ϭ M l ϭ 2. In general,
m l Ϲ M l, as can be shown. The difference ¢ l ϭ M l Ϫ m l is called the defect of l.
Thus ¢ Ϫ3 ϭ 0 in Example 2, but positive defects ¢ l can easily occur:
EXAMPLE 3

Algebraic Multiplicity, Geometric Multiplicity. Positive Defect
The characteristic equation of the matrix

Aϭ

c

0

1

0

0

d

det (A Ϫ lI) ϭ 2

is

Ϫl

1

0

Ϫl

2 ϭ l2 ϭ 0.

Hence l ϭ 0 is an eigenvalue of algebraic multiplicity M 0 ϭ 2. But its geometric multiplicity is only m 0 ϭ 1,
since eigenvectors result from Ϫ0x 1 ϩ x 2 ϭ 0, hence x 2 ϭ 0, in the form [x 1 0]T. Hence for l ϭ 0 the defect
is ¢ 0 ϭ 1.
Similarly, the characteristic equation of the matrix

Aϭ

c

3
0

2
3

d

is

det (A Ϫ lI) ϭ 2

3Ϫl

2

0

3Ϫl

2 ϭ (3 Ϫ l)2 ϭ 0.

Hence l ϭ 3 is an eigenvalue of algebraic multiplicity M 3 ϭ 2, but its geometric multiplicity is only m 3 ϭ 1,
since eigenvectors result from 0x 1 ϩ 2x 2 ϭ 0 in the form [x 1 0]T.
᭿

EXAMPLE 4

Real Matrices with Complex Eigenvalues and Eigenvectors
Since real polynomials may have complex roots (which then occur in conjugate pairs), a real matrix may have
complex eigenvalues and eigenvectors. For instance, the characteristic equation of the skew-symmetric matrix

Aϭ

c

0
Ϫ1

1
0

d

det (A Ϫ lI) ϭ 2

is

Ϫl

1

Ϫ1

Ϫl

2 ϭ l2 ϩ 1 ϭ 0.

It gives the eigenvalues l1 ϭ i (ϭ 1Ϫ1), l2 ϭ Ϫi. Eigenvectors are obtained from Ϫix 1 ϩ x 2 ϭ 0 and
ix 1 ϩ x 2 ϭ 0, respectively, and we can choose x 1 ϭ 1 to get

c d
1
i

and

c

1
Ϫi

d.

᭿

In the next section we shall need the following simple theorem.

THEOREM 3

Eigenvalues of the Transpose

The transpose AT of a square matrix A has the same eigenvalues as A.

PROOF

Transposition does not change the value of the characteristic determinant, as follows from
Theorem 2d in Sec. 7.7.
᭿
Having gained a first impression of matrix eigenvalue problems, we shall illustrate their
importance with some typical applications in Sec. 8.2.

c08.qxd

10/30/10

10:56 AM

Page 329

SEC. 8.2 Some Applications of Eigenvalue Problems

329

PROBLEM SET 8.1
1–16
EIGENVALUES, EIGENVECTORS
Find the eigenvalues. Find the corresponding eigenvectors.
Use the given l or factor in Probs. 11 and 15.
1.

3.

5.

7.

9.

c

3.0

c

5

Ϫ2

9

Ϫ6

c

0
Ϫ0.6

0

0

0

4.

d

c

0
0

0

c

0.8

Ϫ0.6

1

0.6

2.

d
3

Ϫ3

d

6.

d

8.

0.8

d

10.

0

0

0

0

c

1

2

2

4

c

1

2

0

3

c
c

2

11. D 2

5

0T ,

Ϫ2

0

7

3

5

3

12. D0

4

6T

0

0

1

2

0

Ϫ1

14. D0

1
2

0T

1

0

4

0

12

0

0

Ϫ1

0

12

0

0

Ϫ1

Ϫ4

0

0

Ϫ4

Ϫ1

Ϫ3

0

4

2

0

1

Ϫ2

4

2

4

Ϫ1

Ϫ2

0

2

Ϫ2

3

15. E

d
d

16. E

d

17–20

d

a

b

Ϫb

a

cos u

Ϫsin u

sin u

cos u

d

Ϫ2

6

8.2

c

Ϫ1

lϭ3

5

2

13. D 2

7

Ϫ8T

5

4

7

U

LINEAR TRANSFORMATIONS
AND EIGENVALUES

Find the matrix A in the linear transformation y ϭ Ax,
where x ϭ [x 1 x 2]T (x ϭ [x 1 x 2 x 3]T) are Cartesian
coordinates. Find the eigenvalues and eigenvectors and
explain their geometric meaning.
17. Counterclockwise rotation through the angle p>2 about
the origin in R2.
18. Reflection about the x 1-axis in R2.
19. Orthogonal projection (perpendicular projection) of R2
onto the x 2-axis.
20. Orthogonal projection of R 3 onto the plane x 2 ϭ x 1.
21–25

13

U , (l ϩ 1)2

GENERAL PROBLEMS

21. Nonzero defect. Find further 2 ϫ 2 and 3 ϫ 3
matrices with positive defect. See Example 3.
22. Multiple eigenvalues. Find further 2 ϫ 2 and 3 ϫ 3
matrices with multiple eigenvalues. See Example 2.
23. Complex eigenvalues. Show that the eigenvalues of a
real matrix are real or complex conjugate in pairs.
24. Inverse matrix. Show that A؊1 exists if and only if
the eigenvalues l1, Á , ln are all nonzero, and then
A؊1 has the eigenvalues 1>l1, Á , 1>ln.
25. Transpose. Illustrate Theorem 3 with examples of your
own.

Some Applications of Eigenvalue Problems
We have selected some typical examples from the wide range of applications of matrix
eigenvalue problems. The last example, that is, Example 4, shows an application involving
vibrating springs and ODEs. It falls into the domain of Chapter 4, which covers matrix
eigenvalue problems related to ODE’s modeling mechanical systems and electrical

c08.qxd

10/30/10

10:56 AM

330

Page 330

CHAP. 8 Linear Algebra: Matrix Eigenvalue Problems

networks. Example 4 is included to keep our discussion independent of Chapter 4.
(However, the reader not interested in ODEs may want to skip Example 4 without loss
of continuity.)
EXAMPLE 1

Stretching of an Elastic Membrane
An elastic membrane in the x 1x 2-plane with boundary circle x 21 ϩ x 22 ϭ 1 (Fig. 160) is stretched so that a point
P: (x 1, x 2) goes over into the point Q: ( y1, y2) given by

(1)

yϭ

c d
y1
y2

ϭ Ax ϭ

c

5
3

3
5

d c d;
x1

y1 ϭ 5x 1 ϩ 3x 2

in components,

y2 ϭ 3x 1 ϩ 5x 2.

x2

Find the principal directions, that is, the directions of the position vector x of P for which the direction of the
position vector y of Q is the same or exactly opposite. What shape does the boundary circle take under this
deformation?

Solution. We are looking for vectors x such that y ϭ lx. Since y ϭ Ax, this gives Ax ϭ lx, the equation
of an eigenvalue problem. In components, Ax ϭ lx is

(2)

5x 1 ϩ 3x 2 ϭ lx 1

(5 Ϫ l)x 1 ϩ

or

3x 1 ϩ 5x 2 ϭ lx 2

3x 1

3x 2

ϭ0

ϩ (5 Ϫ l)x 2 ϭ 0.

The characteristic equation is

(3)

2

5Ϫl

3

3

5Ϫl

2 ϭ (5 Ϫ l)2 Ϫ 9 ϭ 0.

Its solutions are l1 ϭ 8 and l2 ϭ 2. These are the eigenvalues of our problem. For l ϭ l1 ϭ 8, our system (2)
becomes
Ϫ3x 1 ϩ 3x 2 ϭ 0,

Solution x 2 ϭ x 1, x 1 arbitrary,

2

3x 1 Ϫ 3x 2 ϭ 0.

for instance, x 1 ϭ x 2 ϭ 1.

For l2 ϭ 2, our system (2) becomes
3x 1 ϩ 3x 2 ϭ 0,
3x 1 ϩ 3x 2 ϭ 0.

2

Solution x 2 ϭ Ϫx 1, x 1 arbitrary,
for instance, x 1 ϭ 1, x 2 ϭ Ϫ1.

We thus obtain as eigenvectors of A, for instance, [1 1]T corresponding to l1 and [1 Ϫ1]T corresponding to
l2 (or a nonzero scalar multiple of these). These vectors make 45° and 135° angles with the positive x1-direction.
They give the principal directions, the answer to our problem. The eigenvalues show that in the principal
directions the membrane is stretched by factors 8 and 2, respectively; see Fig. 160.
Accordingly, if we choose the principal directions as directions of a new Cartesian u 1u 2-coordinate system,
say, with the positive u 1-semi-axis in the first quadrant and the positive u 2-semi-axis in the second quadrant of
the x 1x 2-system, and if we set u 1 ϭ r cos ␾, u 2 ϭ r sin ␾, then a boundary point of the unstretched circular
membrane has coordinates cos ␾, sin ␾. Hence, after the stretch we have
z 1 ϭ 8 cos ␾,

z 2 ϭ 2 sin ␾.

Since cos2 ␾ ϩ sin2 ␾ ϭ 1, this shows that the deformed boundary is an ellipse (Fig. 160)

(4)

z 21
82

ϩ

z 22
22

ϭ 1.

᭿

10/30/10

10:56 AM

Page 331

SEC. 8.2 Some Applications of Eigenvalue Problems

331
x2
l
pa
ci on
in ti
Pr irec
d

Pr
di inc
re ip
ct al
io
n

c08.qxd

x1

Fig. 160. Undeformed and deformed membrane in Example 1

EXAMPLE 2

Eigenvalue Problems Arising from Markov Processes
Markov processes as considered in Example 13 of Sec. 7.2 lead to eigenvalue problems if we ask for the limit
state of the process in which the state vector x is reproduced under the multiplication by the stochastic matrix
A governing the process, that is, Ax ϭ x. Hence A should have the eigenvalue 1, and x should be a corresponding
eigenvector. This is of practical interest because it shows the long-term tendency of the development modeled
by the process.
In that example,
0.7

0.1

0

A ϭ D0.2

0.9

0.2T .

0

0.8

0.1

For the transpose,

0.7

0.2

0.1

1

1

D0.1

0.9

0 T D1T ϭ D1T .

0

0.2

0.8

1

1

Hence AT has the eigenvalue 1, and the same is true for A by Theorem 3 in Sec. 8.1. An eigenvector x of A
for l ϭ 1 is obtained from
Ϫ0.3
A Ϫ I ϭ D 0.2
0.1

0.1
Ϫ0.1
0

3
Ϫ10

0
0.2T ,

row-reduced to

Ϫ0.2

D

1
10

0

1
Ϫ30

0

0

0
1
5T

.

0

Taking x 3 ϭ 1, we get x 2 ϭ 6 from Ϫx 2>30 ϩ x 3>5 ϭ 0 and then x 1 ϭ 2 from Ϫ3x 1>10 ϩ x 2>10 ϭ 0. This
gives x ϭ [2 6 1]T. It means that in the long run, the ratio Commercial:Industrial:Residential will approach
2:6:1, provided that the probabilities given by A remain (about) the same. (We switched to ordinary fractions
᭿
to avoid rounding errors.)

EXAMPLE 3

Eigenvalue Problems Arising from Population Models. Leslie Model
The Leslie model describes age-specified population growth, as follows. Let the oldest age attained by the
females in some animal population be 9 years. Divide the population into three age classes of 3 years each. Let
the “Leslie matrix” be
0
(5)

L ϭ [l jk] ϭ D0.6
0

2.3

0.4

0

0 T

0.3

0

where l 1k is the average number of daughters born to a single female during the time she is in age class k, and
l j, j؊1( j ϭ 2, 3) is the fraction of females in age class j Ϫ 1 that will survive and pass into class j. (a) What is the
number of females in each class after 3, 6, 9 years if each class initially consists of 400 females? (b) For what initial
distribution will the number of females in each class change by the same proportion? What is this rate of change?

c08.qxd

10/30/10

10:56 AM

332

Page 332

CHAP. 8 Linear Algebra: Matrix Eigenvalue Problems

Solution. (a) Initially, x T(0) ϭ [400 400 400]. After 3 years,
0
x(3) ϭ Lx(0) ϭ D0.6
0

2.3

0.4

400

1080

0

0 T D400T ϭ D 240T .

0.3

0

400

120

Similarly, after 6 years the number of females in each class is given by x T(6) ϭ (Lx(3))T ϭ [600 648 72], and
after 9 years we have x T(9) ϭ (Lx(6))T ϭ [1519.2 360 194.4].
(b) Proportional change means that we are looking for a distribution vector x such that Lx ϭ lx, where l is
the rate of change (growth if l Ͼ 1, decrease if l Ͻ 1). The characteristic equation is (develop the characteristic
determinant by the first column)
det (L Ϫ lI) ϭ Ϫl3 Ϫ 0.6(Ϫ2.3l Ϫ 0.3 # 0.4) ϭ Ϫl3 ϩ 1.38l ϩ 0.072 ϭ 0.
A positive root is found to be (for instance, by Newton’s method, Sec. 19.2) l ϭ 1.2. A corresponding eigenvector
x can be determined from the characteristic matrix
Ϫ1.2

2.3

A Ϫ 1.2I ϭ D 0.6

Ϫ1.2

0

0.3

1

0.4
0 T,

x ϭ D 0.5 T

say,

Ϫ1.2

0.125

where x 3 ϭ 0.125 is chosen, x 2 ϭ 0.5 then follows from 0.3x 2 Ϫ 1.2x 3 ϭ 0, and x 1 ϭ 1 from
Ϫ1.2x 1 ϩ 2.3x 2 ϩ 0.4x 3 ϭ 0. To get an initial population of 1200 as before, we multiply x by
1200>(1 ϩ 0.5 ϩ 0.125) ϭ 738. Answer: Proportional growth of the numbers of females in the three classes
will occur if the initial values are 738, 369, 92 in classes 1, 2, 3, respectively. The growth rate will be 1.2 per
᭿
3 years.

EXAMPLE 4

Vibrating System of Two Masses on Two Springs (Fig. 161)
Mass–spring systems involving several masses and springs can be treated as eigenvalue problems. For instance,
the mechanical system in Fig. 161 is governed by the system of ODEs
(6)

y1s ϭ Ϫ3y1 Ϫ 2( y1 Ϫ y2) ϭ Ϫ5y1 ϩ 2y2
y2s ϭ

Ϫ2( y2 Ϫ y1) ϭ

2y1 Ϫ 2y2

where y1 and y2 are the displacements of the masses from rest, as shown in the figure, and primes denote
derivatives with respect to time t. In vector form, this becomes
(7)

ys ϭ

c d
y1s
y2s

ϭ Ay ϭ

c

Ϫ5

2

2

Ϫ2

d c d.
y1
y2

k1 = 3
m1 = 1

(y1 = 0)

y1

y1
k2 = 2
(y2 = 0)

(Net change in
spring length
= y2 – y1)

m2 = 1
y2
System in
static
equilibrium

y2

System in
motion

Fig. 161. Masses on springs in Example 4

c08.qxd

10/30/10

10:56 AM

Page 333

SEC. 8.2 Some Applications of Eigenvalue Problems

333

We try a vector solution of the form
y ϭ xevt.

(8)

This is suggested by a mechanical system of a single mass on a spring (Sec. 2.4), whose motion is given by
exponential functions (and sines and cosines). Substitution into (7) gives
v2xevt ϭ Axevt.
Dividing by evt and writing v2 ϭ l, we see that our mechanical system leads to the eigenvalue problem
Ax ϭ lx

(9)

where l ϭ v2.

From Example 1 in Sec. 8.1 we see that A has the eigenvalues l1 ϭ Ϫ1 and l2 ϭ Ϫ6. Consequently,
v ϭ Ϯ 1Ϫ1 ϭ Ϯi and 1Ϫ6 ϭ Ϯi 16, respectively. Corresponding eigenvectors are
x1 ϭ

(10)

c d
1

and

2

x2 ϭ

c

2
Ϫ1

d.

From (8) we thus obtain the four complex solutions [see (10), Sec. 2.2]
x1eϮit ϭ x1 (cos t Ϯ i sin t),
x2eϮi26t ϭ x2 (cos 16 t Ϯ i sin 16 t).
By addition and subtraction (see Sec. 2.2) we get the four real solutions
x1 cos t,

x2 cos 16 t,

x1 sin t,

x2 sin 16 t.

A general solution is obtained by taking a linear combination of these,
y ϭ x1 (a1 cos t ϩ b1 sin t) ϩ x2 (a2 cos 16 t ϩ b2 sin 16 t)
with arbitrary constants a1, b1, a2, b2 (to which values can be assigned by prescribing initial displacement and
initial velocity of each of the two masses). By (10), the components of y are
y1 ϭ a1 cos t ϩ b1 sin t ϩ 2a2 cos 16 t ϩ 2b2 sin 16 t
y2 ϭ 2a1 cos t ϩ 2b1 sin t Ϫ a2 cos 16 t Ϫ b2 sin 16 t.
These functions describe harmonic oscillations of the two masses. Physically, this had to be expected because
we have neglected damping.
᭿

PROBLEM SET 8.2
1–6
ELASTIC DEFORMATIONS
Given A in a deformation y ϭ Ax, find the principal
directions and corresponding factors of extension or
contraction. Show the details.
1.

3.

5.

c

3.0

c

7

16

16

2

1.5

1.5

c1

3.0

1

1
2

2

1

d

d

2.

d

4.

6.

c

2.0

0.4

0.4

2.0

c

5

2

2

13

c

1.25

0.75

0.75

1.25

d

MARKOV PROCESSES
7–9
Find the limit state of the Markov process modeled by the
given matrix. Show the details.

d
d

c

d

0.2

0.5

0.8

0.5

0.4

0.3

0.3

8. D0.3

0.6

0.1T

0.3

0.1

0.6

7.

0.6

0.1

0.2

9. D0.4

0.1

0.4T

0

0.8

0.4

c08.qxd

10/30/10

10:56 AM

Page 334

334

CHAP. 8 Linear Algebra: Matrix Eigenvalue Problems

10–12
AGE-SPECIFIC POPULATION
Find the growth rate in the Leslie model (see Example 3)
with the matrix as given. Show the details.
0

9.0

5.0

3.45

0.60

0

0 T 11. D0.90

0

0

0

0.4

0

0.45

0

0

3.0

2.0

2.0

0.5

0

0

0

0

0.5

0

0

0

0

0.1

0

10. D0.4

0

0

T

industries themselves, then instead of Ax ϭ x (as in Prob.
13), we have x Ϫ Ax ϭ y, where x ϭ [x 1 x 2 x 3]T
is produced, Ax is consumed by the industries, and, thus,
y is the net production available for other consumers.
Find for what production x a given demand vector
y ϭ [0.1 0.3 0.1]T can be achieved if the consumption matrix is
0.1

12. E

13–15

A ϭ D0.5

U

0.1
16–20

LEONTIEF MODELS1

13. Leontief input–output model. Suppose that three
industries are interrelated so that their outputs are used
as inputs by themselves, according to the 3 ϫ 3
consumption matrix
0.1
A ϭ [ajk] ϭ D0.8
0.1

0.5

0

0

0.4T

0.5

0.6

where ajk is the fraction of the output of industry k
consumed (purchased) by industry j. Let pj be the price
charged by industry j for its total output. A problem is
to find prices so that for each industry, total
expenditures equal total income. Show that this leads
to Ap ϭ p, where p ϭ [p1 p2 p3]T, and find a
solution p with nonnegative p1, p2, p3.
14. Show that a consumption matrix as considered in Prob.
13 must have column sums 1 and always has the
eigenvalue 1.
15. Open Leontief input–output model. If not the whole
output but only a portion of it is consumed by the

8.3

0.4

0.2

0

0.1T .

0.4

0.4

GENERAL PROPERTIES OF EIGENVALUE
PROBLEMS

Let A ϭ [ajk] be an n ϫ n matrix with (not necessarily
distinct) eigenvalues l1, Á , ln. Show.
16. Trace. The sum of the main diagonal entries, called
the trace of A, equals the sum of the eigenvalues of A.
17. “Spectral shift.” A Ϫ kI has the eigenvalues
l1 Ϫ k, Á , ln Ϫ k and the same eigenvectors as A.
18. Scalar multiples, powers. kA has the eigenvalues
kl1, Á , kln. Am(m ϭ 1, 2, Á ) has the eigenvalues
Á , lm
lm
1 ,
n . The eigenvectors are those of A.
19. Spectral mapping theorem. The “polynomial
matrix”
p (A) ϭ k mAm ϩ k m؊1Am؊1 ϩ Á ϩ k 1A ϩ k 0 I
has the eigenvalues
m؊1
p (lj) ϭ k mlm
ϩ Á ϩ k 1lj ϩ k 0
j ϩ k m؊1lj

where j ϭ 1, Á , n, and the same eigenvectors as A.
20. Perron’s theorem. A Leslie matrix L with positive
l 12, l 13, l 21, l 32 has a positive eigenvalue. (This is a
special case of the Perron–Frobenius theorem in Sec.
20.7, which is difficult to prove in its general form.)

Symmetric, Skew-Symmetric,
and Orthogonal Matrices
We consider three classes of real square matrices that, because of their remarkable
properties, occur quite frequently in applications. The first two matrices have already been
mentioned in Sec. 7.2. The goal of Sec. 8.3 is to show their remarkable properties.
1
WASSILY LEONTIEF (1906–1999). American economist at New York University. For his input–output
analysis he was awarded the Nobel Prize in 1973.

c08.qxd

10/30/10

3:18 PM

Page 335

SEC. 8.3 Symmetric, Skew-Symmetric, and Orthogonal Matrices

DEFINITIONS

335

Symmetric, Skew-Symmetric, and Orthogonal Matrices

A real square matrix A ϭ [ajk] is called
symmetric if transposition leaves it unchanged,
AT ϭ A,

(1)

akj ϭ ajk,

thus

skew-symmetric if transposition gives the negative of A,
AT ϭ ϪA,

(2)

akj ϭ Ϫajk,

thus

orthogonal if transposition gives the inverse of A,
AT ϭ A؊1.

(3)

EXAMPLE 1

Symmetric, Skew-Symmetric, and Orthogonal Matrices
The matrices
Ϫ3

1

5

D 1

0

Ϫ2T ,

5

Ϫ2

0

9

DϪ9

0

12

Ϫ20

4

2
3

1
3

2
3

DϪ23

2
3

1
3T

1
3

2
3

Ϫ12
20T ,
0

Ϫ23

are symmetric, skew-symmetric, and orthogonal, respectively, as you should verify. Every skew-symmetric
matrix has all main diagonal entries zero. (Can you prove this?)
᭿

Any real square matrix A may be written as the sum of a symmetric matrix R and a skewsymmetric matrix S, where
R ϭ 12 (A ϩ AT)

(4)
EXAMPLE 2

THEOREM 1

S ϭ 12 (A Ϫ AT).

and

Illustration of Formula (4)
9

5

A ϭ D2

3

5

4

2

9.0

3.5

Ϫ8T ϭ R ϩ S ϭ D3.5

3.0

3

3.5

Ϫ2.0

3.5

0

Ϫ2.0T ϩ DϪ1.5
3.0

1.5

1.5

Ϫ1.5

0

Ϫ6.0T

6.0

0

Eigenvalues of Symmetric and Skew-Symmetric Matrices

(a) The eigenvalues of a symmetric matrix are real.
(b) The eigenvalues of a skew-symmetric matrix are pure imaginary or zero.

This basic theorem (and an extension of it) will be proved in Sec. 8.5.

᭿

c08.qxd

10/30/10

10:56 AM

336

Page 336

CHAP. 8 Linear Algebra: Matrix Eigenvalue Problems

EXAMPLE 3

Eigenvalues of Symmetric and Skew-Symmetric Matrices
The matrices in (1) and (7) of Sec. 8.2 are symmetric and have real eigenvalues. The skew-symmetric matrix
in Example 1 has the eigenvalues 0, Ϫ25 i, and 25i. (Verify this.) The following matrix has the real eigenvalues
1 and 5 but is not symmetric. Does this contradict Theorem 1?

c

3

4

1

3

d

᭿

Orthogonal Transformations and Orthogonal Matrices
Orthogonal transformations are transformations
y ϭ Ax

(5)

where A is an orthogonal matrix.

With each vector x in Rn such a transformation assigns a vector y in Rn. For instance,
the plane rotation through an angle u
yϭ

(6)

c d
y1
y2

ϭ

c

cos u

Ϫsin u

sin u

cos u

dc d
x1
x2

is an orthogonal transformation. It can be shown that any orthogonal transformation in
the plane or in three-dimensional space is a rotation (possibly combined with a reflection
in a straight line or a plane, respectively).
The main reason for the importance of orthogonal matrices is as follows.
THEOREM

2

Invariance of Inner Product

An orthogonal transformation preserves the value of the inner product of vectors
a and b in R n, defined by

(7)

a • b ϭ aTb ϭ [a1

b1
.
Á an] D . T .
.
bn

That is, for any a and b in Rn, orthogonal n ϫ n matrix A, and u ϭ Aa, v ϭ Ab
we have u • v ϭ a • b.
Hence the transformation also preserves the length or norm of any vector a in
Rn given by
(8)

PROOF

ʈ a ʈ ϭ 1a • a ϭ 2aTa.

Let A be orthogonal. Let u ϭ Aa and v ϭ Ab. We must show that u • v ϭ a • b. Now
(Aa)T ϭ aTAT by (10d) in Sec. 7.2 and ATA ϭ A؊1A ϭ I by (3). Hence
(9)

u • v ϭ uTv ϭ (Aa)TAb ϭ aTATAb ϭ aTIb ϭ aTb ϭ a • b.

From this the invariance of ʈ a ʈ follows if we set b ϭ a.

᭿

c08.qxd

10/30/10

10:56 AM

Page 337

SEC. 8.3 Symmetric, Skew-Symmetric, and Orthogonal Matrices

337

Orthogonal matrices have further interesting properties as follows.

THEOREM 3

Orthonormality of Column and Row Vectors

A real square matrix is orthogonal if and only if its column vectors a1, Á , an (and
also its row vectors) form an orthonormal system, that is,
(10)

PROOF

aj • ak ϭ aTj ak ϭ e

0 if j

k

1 if j ϭ k.

(a) Let A be orthogonal. Then A؊1A ϭ ATA ϭ I. In terms of column vectors a1, Á , an,

(11)

aT1
aT1 a1
.
I ϭ A؊1A ϭ ATA ϭ D .. T [a1 Á an] ϭ D ؒ
aTn

aTna1

aT1 a2

ؒ ؒؒ

ؒ

ؒ ؒؒ

aTna2

ؒ ؒؒ

aT1 an
ؒ T.
aTnan

The last equality implies (10), by the definition of the n ϫ n unit matrix I. From (3) it
follows that the inverse of an orthogonal matrix is orthogonal (see CAS Experiment 12).
Now the column vectors of A؊1(ϭAT) are the row vectors of A. Hence the row vectors
of A also form an orthonormal system.
(b) Conversely, if the column vectors of A satisfy (10), the off-diagonal entries in (11)
must be 0 and the diagonal entries 1. Hence ATA ϭ I, as (11) shows. Similarly, AAT ϭ I.
This implies AT ϭ A؊1 because also A؊1A ϭ AA؊1 ϭ I and the inverse is unique. Hence
A is orthogonal. Similarly when the row vectors of A form an orthonormal system, by
what has been said at the end of part (a).
᭿

THEOREM 4

Determinant of an Orthogonal Matrix

The determinant of an orthogonal matrix has the value ϩ1 or Ϫ1.

PROOF

From det AB ϭ det A det B (Sec. 7.8, Theorem 4) and det AT ϭ det A (Sec. 7.7,
Theorem 2d), we get for an orthogonal matrix
1 ϭ det I ϭ det (AA؊1) ϭ det (AAT) ϭ det A det AT ϭ (det A)2.

EXAMPLE 4

᭿

Illustration of Theorems 3 and 4
The last matrix in Example 1 and the matrix in (6) illustrate Theorems 3 and 4 because their determinants are
Ϫ1 and ϩ1, as you should verify.
᭿

THEOREM 5

Eigenvalues of an Orthogonal Matrix

The eigenvalues of an orthogonal matrix A are real or complex conjugates in pairs
and have absolute value 1.

c08.qxd

10/30/10

10:56 AM

Page 338

338

CHAP. 8 Linear Algebra: Matrix Eigenvalue Problems

PROOF

The first part of the statement holds for any real matrix A because its characteristic
polynomial has real coefficients, so that its zeros (the eigenvalues of A) must be as
᭿
indicated. The claim that ƒ l ƒ ϭ 1 will be proved in Sec. 8.5.

EXAMPLE 5

Eigenvalues of an Orthogonal Matrix
The orthogonal matrix in Example 1 has the characteristic equation
Ϫl3 ϩ 23 l2 ϩ 23 l Ϫ 1 ϭ 0.
Now one of the eigenvalues must be real (why?), hence ϩ1 or Ϫ1. Trying, we find Ϫ1. Division by l ϩ 1
gives Ϫ(l2 Ϫ 5l>3 ϩ 1) ϭ 0 and the two eigenvalues (5 ϩ i111)>6 and (5 Ϫ i 111)>6, which have absolute
value 1. Verify all of this.
᭿

Looking back at this section, you will find that the numerous basic results it contains have
relatively short, straightforward proofs. This is typical of large portions of matrix
eigenvalue theory.

PROBLEM SET 8.3
1–10
SPECTRUM
Are the following matrices symmetric, skew-symmetric, or
orthogonal? Find the spectrum of each, thereby illustrating
Theorems 1 and 5. Show your work in detail.
1.

3.

c
c

0.8

d

0.6

Ϫ0.6

0.8

2

8

Ϫ8

2

2.

d

4.

6

0

0

5. D0

2

Ϫ2T

0

Ϫ2
0

5
9

7. DϪ9

0

12

Ϫ20

Ϫ12
20T
0

0

0

1

9. D 0

1

0T

Ϫ1

0

0

c
c

d

a

b

Ϫb

a

cos u

Ϫsin u

sin u

cos u

a

k

k

6. D k

a

kT

k

k

a

1

d

0

8. D0

cos u

0
Ϫsin u T

sin u

0

cos u

4
9

8
9

1
9

10. DϪ79

4
9

Ϫ49T

Ϫ49

1
9

8
9

11. WRITING PROJECT. Section Summary. Summarize the main concepts and facts in this section,
giving illustrative examples of your own.
12. CAS EXPERIMENT. Orthogonal Matrices.
(a) Products. Inverse. Prove that the product of two
orthogonal matrices is orthogonal, and so is the inverse
of an orthogonal matrix. What does this mean in terms
of rotations?

(b) Rotation. Show that (6) is an orthogonal transformation. Verify that it satisfies Theorem 3. Find the
inverse transformation.
(c) Powers. Write a program for computing powers
Am (m ϭ 1, 2, Á ) of a 2 ϫ 2 matrix A and their
spectra. Apply it to the matrix in Prob. 1 (call it A). To
what rotation does A correspond? Do the eigenvalues
of Am have a limit as m : ϱ ?
(d) Compute the eigenvalues of (0.9A)m, where A is
the matrix in Prob. 1. Plot them as points. What is their
limit? Along what kind of curve do these points
approach the limit?
(e) Find A such that y ϭ Ax is a counterclockwise
rotation through 30° in the plane.
13–20

GENERAL PROPERTIES

13. Verification. Verify the statements in Example 1.
14. Verify the statements in Examples 3 and 4.
15. Sum. Are the eigenvalues of A ϩ B sums of the
eigenvalues of A and of B?
16. Orthogonality. Prove that eigenvectors of a symmetric
matrix corresponding to different eigenvalues are
orthogonal. Give examples.
17. Skew-symmetric matrix. Show that the inverse of a
skew-symmetric matrix is skew-symmetric.
18. Do there exist nonsingular skew-symmetric n ϫ n
matrices with odd n?
19. Orthogonal matrix. Do there exist skew-symmetric
orthogonal 3 ϫ 3 matrices?
20. Symmetric matrix. Do there exist nondiagonal
symmetric 3 ϫ 3 matrices that are orthogonal?

c08.qxd

10/30/10

10:56 AM

Page 339

SEC. 8.4 Eigenbases. Diagonalization. Quadratic Forms

8.4

339

Eigenbases. Diagonalization.
Quadratic Forms
So far we have emphasized properties of eigenvalues. We now turn to general properties
of eigenvectors. Eigenvectors of an n ϫ n matrix A may (or may not!) form a basis for
Rn. If we are interested in a transformation y ϭ Ax, such an “eigenbasis” (basis of
eigenvectors)—if it exists—is of great advantage because then we can represent any x in
Rn uniquely as a linear combination of the eigenvectors x1, Á , xn, say,
x ϭ c1x1 ϩ c2x2 ϩ Á ϩ cnxn.
And, denoting the corresponding (not necessarily distinct) eigenvalues of the matrix A by
l1, Á , ln, we have Axj ϭ ljxj, so that we simply obtain
y ϭ Ax ϭ A(c1x1 ϩ Á ϩ cnxn)
ϭ c1Ax1 ϩ Á ϩ cnAxn

(1)

ϭ c1l1x1 ϩ Á ϩ cnlnxn.
This shows that we have decomposed the complicated action of A on an arbitrary vector
x into a sum of simple actions (multiplication by scalars) on the eigenvectors of A. This
is the point of an eigenbasis.
Now if the n eigenvalues are all different, we do obtain a basis:
THEOREM 1

Basis of Eigenvectors

If an n ϫ n matrix A has n distinct eigenvalues, then A has a basis of eigenvectors
x1, Á , xn for R n.

PROOF

All we have to show is that x1, Á , xn are linearly independent. Suppose they are not. Let
r be the largest integer such that {x1, Á , xr } is a linearly independent set. Then r Ͻ n
and the set {x1, Á , xr, xrϩ1 } is linearly dependent. Thus there are scalars c1, Á , crϩ1,
not all zero, such that
(2)

c1x1 ϩ Á ϩ crϩ1xrϩ1 ϭ 0

(see Sec. 7.4). Multiplying both sides by A and using Axj ϭ ljxj, we obtain
(3)

A(c1x1 ϩ Á ϩ crϩ1xrϩ1) ϭ c1l1x1 ϩ Á ϩ crϩ1lrϩ1xrϩ1 ϭ A0 ϭ 0.

To get rid of the last term, we subtract lrϩ1 times (2) from this, obtaining
c1(l1 Ϫ lrϩ1)x1 ϩ Á ϩ cr(lr Ϫ lrϩ1)xr ϭ 0.
Here c1(l1 Ϫ lrϩ1) ϭ 0, Á , cr(lr Ϫ lrϩ1) ϭ 0 since {x 1, Á , x r } is linearly independent.
Hence c1 ϭ Á ϭ cr ϭ 0, since all the eigenvalues are distinct. But with this, (2) reduces to
crϩ1xrϩ1 ϭ 0, hence crϩ1 ϭ 0, since xrϩ1 0 (an eigenvector!). This contradicts the fact
that not all scalars in (2) are zero. Hence the conclusion of the theorem must hold.
᭿

c08.qxd

10/30/10

10:56 AM

340
EXAMPLE 1

Page 340

CHAP. 8 Linear Algebra: Matrix Eigenvalue Problems
Eigenbasis. Nondistinct Eigenvalues. Nonexistence

d corresponding to the eigenvalues l1 ϭ 8,
3 5
1
Ϫ1
l2 ϭ 2. (See Example 1 in Sec. 8.2.)
Even if not all n eigenvalues are different, a matrix A may still provide an eigenbasis for R n. See Example 2
in Sec. 8.1, where n ϭ 3.
On the other hand, A may not have enough linearly independent eigenvectors to make up a basis. For
instance, A in Example 3 of Sec. 8.1 is
The matrix A ϭ

c

5

Aϭ

3

c

0
0

d

c d, c
1

has a basis of eigenvectors

1
0

d

1

c d
k

and has only one eigenvector

(k

0, arbitrary).

0

᭿

Actually, eigenbases exist under much more general conditions than those in Theorem 1.
An important case is the following.
THEOREM 2

Symmetric Matrices

A symmetric matrix has an orthonormal basis of eigenvectors for Rn.
For a proof (which is involved) see Ref. [B3], vol. 1, pp. 270–272.
EXAMPLE 2

Orthonormal Basis of Eigenvectors
The first matrix in Example 1 is symmetric, and an orthonormal basis of eigenvectors is 31> 12 1> 124T,
[1> 12 Ϫ1> 124T.
᭿

Similarity of Matrices. Diagonalization
Eigenbases also play a role in reducing a matrix A to a diagonal matrix whose entries are
the eigenvalues of A. This is done by a “similarity transformation,” which is defined as
follows (and will have various applications in numerics in Chap. 20).
DEFINITION

Similar Matrices. Similarity Transformation

ˆ is called similar to an n ϫ n matrix A if
An n ϫ n matrix A
(4)

ˆ ϭ P ؊1AP
A

ˆ from
for some (nonsingular!) n ϫ n matrix P. This transformation, which gives A
A, is called a similarity transformation.
The key property of this transformation is that it preserves the eigenvalues of A:
THEOREM 3

Eigenvalues and Eigenvectors of Similar Matrices

ˆ is similar to A, then A
ˆ has the same eigenvalues as A.
If A
ˆ
Furthermore, if x is an eigenvector of A, then y ϭ P ؊1x is an eigenvector of A
corresponding to the same eigenvalue.

c08.qxd

10/30/10

10:56 AM

Page 341

SEC. 8.4 Eigenbases. Diagonalization. Quadratic Forms

PROOF

341

From Ax ϭ lx (l an eigenvalue, x 0) we get P ؊1Ax ϭ lP ؊1x. Now I ϭ PP ؊1. By
this identity trick the equation P ؊1Ax ϭ lP ؊1x gives
ˆ (P ؊1x) ϭ lP ؊1x.
P ؊1Ax ϭ P ؊1AIx ϭ P ؊1APP ؊1x ϭ (P ؊1AP)P ؊1x ϭ A
ˆ and P ؊1x a corresponding eigenvector. Indeed, P ؊1x
Hence l is an eigenvalue of A
؊1
because P x ϭ 0 would give x ϭ Ix ϭ PP ؊1x ϭ P0 ϭ 0, contradicting x 0.

EXAMPLE 3

0
᭿

Eigenvalues and Vectors of Similar Matrices

c

Aϭ

Let,

ˆ ϭ
A

Then

c

Ϫ3

6

Ϫ1

4

4

Ϫ3

Ϫ1

1

d

dc

Pϭ

and

6

Ϫ3

4

Ϫ1

dc

1

3

1

4

d

c

1

3

1

4

ϭ

c

d.

3

0

0

2

d.

ˆ has the eigenvalues l1 ϭ 3, l2 ϭ 2.
Here P Ϫ1 was obtained from (4*) in Sec. 7.8 with det P ϭ 1. We see that A
The characteristic equation of A is (6 Ϫ l)(Ϫ1 Ϫ l) ϩ 12 ϭ l2 Ϫ 5l ϩ 6 ϭ 0. It has the roots (the eigenvalues
of A) l1 ϭ 3, l2 ϭ 2, confirming the first part of Theorem 3.
We confirm the second part. From the first component of (A Ϫ lI)x ϭ 0 we have (6 Ϫ l)x 1 Ϫ 3x 2 ϭ 0. For
l ϭ 3 this gives 3x 1 Ϫ 3x 2 ϭ 0, say, x1 ϭ 31 14T. For l ϭ 2 it gives 4x 1 Ϫ 3x 2 ϭ 0, say, x2 ϭ 33 44T. In
Theorem 3 we thus have

y1 ϭ P Ϫ1x1 ϭ

c

4

Ϫ3

Ϫ1

1

dc d
1
1

ϭ

c d,
1
0

y2 ϭ P Ϫ1x2 ϭ

c

4

Ϫ3

Ϫ1

1

dc d
3
4

ϭ

c d.
0
1

ˆ.
Indeed, these are eigenvectors of the diagonal matrix A
Perhaps we see that x1 and x2 are the columns of P. This suggests the general method of transforming a
matrix A to diagonal form D by using P ϭ X, the matrix with eigenvectors as columns.
᭿

By a suitable similarity transformation we can now transform a matrix A to a diagonal
matrix D whose diagonal entries are the eigenvalues of A:

THEOREM 4

Diagonalization of a Matrix

If an n ϫ n matrix A has a basis of eigenvectors, then
(5)

D ϭ X؊1AX

is diagonal, with the eigenvalues of A as the entries on the main diagonal. Here X
is the matrix with these eigenvectors as column vectors. Also,

(5*)

D m ϭ X؊1AmX

(m ϭ 2, 3, Á ).

c08.qxd

10/30/10

10:56 AM

342

Page 342

CHAP. 8 Linear Algebra: Matrix Eigenvalue Problems

PROOF

Let x1, Á , xn be a basis of eigenvectors of A for R n. Let the corresponding eigenvalues
of A be l1, Á , ln, respectively, so that Ax1 ϭ l1x1, Á , Axn ϭ lnxn. Then
X ϭ 3x1 Á xn4 has rank n, by Theorem 3 in Sec. 7.4. Hence X؊1 exists by Theorem 1
in Sec. 7.8. We claim that
(6)

Ax ϭ A3x1 Á xn4 ϭ 3Ax1 Á Axn4 ϭ 3l1x1

Á lnxn4 ϭ XD

where D is the diagonal matrix as in (5). The fourth equality in (6) follows by direct
calculation. (Try it for n ϭ 2 and then for general n.) The third equality uses Axk ϭ lkxk.
The second equality results if we note that the first column of AX is A times the first
column of X, which is x1, and so on. For instance, when n ϭ 2 and we write
x1 ϭ 3x 11 x 214, x2 ϭ 3x 12 x 224, we have
AX ϭ A3x1 x24 ϭ

c

a11

a12

dc

a21

a22

ϭ

c

a11x 11 ϩ a12x 21

a11x 12 ϩ a12x 22

a21x 11 ϩ a22x 21

a21x 12 ϩ a22x 22

Column 1

Column 2

x 11

x 12

x 21

x 22

d
d

ϭ 3Ax1

Ax24.

If we multiply (6) by X؊1 from the left, we obtain (5). Since (5) is a similarity
transformation, Theorem 3 implies that D has the same eigenvalues as A. Equation (5*)
follows if we note that
D 2 ϭ DD ϭ (X؊1AX)(X؊1AX) ϭ X؊1A(XX؊1)AX ϭ X؊1AAX ϭ X؊1A2X, etc. ᭿
EXAMPLE 4

Diagonalization
Diagonalize
7.3

0.2

A ϭ DϪ11.5

1.0

17.7

1.8

Ϫ3.7
5.5T .
Ϫ9.3

Solution. The characteristic determinant gives the characteristic equation Ϫl3 Ϫ l2 ϩ 12l ϭ 0. The roots
(eigenvalues of A) are l1 ϭ 3, l2 ϭ Ϫ4, l3 ϭ 0. By the Gauss elimination applied to (A Ϫ lI)x ϭ 0 with
l ϭ l1, l2, l3 we find eigenvectors and then X؊1 by the Gauss–Jordan elimination (Sec. 7.8, Example 1). The
results are
Ϫ1

1

2

D 3T , DϪ1T , D1T ,
Ϫ1

3

4

Ϫ1

1

XϭD 3

Ϫ1

Ϫ1

3

Ϫ0.7

0.2

X؊1 ϭ DϪ1.3

Ϫ0.2

0.8

0.2

2
1T ,
4

0.3
0.7T .
Ϫ0.2

Calculating AX and multiplying by X؊1 from the left, we thus obtain

DϭX

Ϫ0.7

0.2

AX ϭ DϪ1.3

Ϫ0.2

0.8

0.2

؊1

Ϫ3

Ϫ4

0.7T D 9

4

0.3

Ϫ0.2

Ϫ3

Ϫ12

0

3

0

0T ϭ D0

Ϫ4

0

0

0

0
0T .
0

᭿

c08.qxd

10/30/10

10:56 AM

Page 343

SEC. 8.4 Eigenbases. Diagonalization. Quadratic Forms

343

Quadratic Forms. Transformation to Principal Axes
By definition, a quadratic form Q in the components x 1, Á , x n of a vector x is a sum
of n 2 terms, namely,
n

n

Q ϭ x TAx ϭ a a ajkx j x k
jϭ1 kϭ1

ϭ

a11x 21

ϩ a12x 1x 2 ϩ Á ϩ a1nx 1x n

ϩ a21x 2x 1 ϩ a22x 22

(7)

ϩ Á ϩ a2nx 2x n

ϩ# # # # # # # # # # # # # # # # # # # # # # # # # # #
ϩ an1x nx 1 ϩ an2x nx 2 ϩ Á ϩ annx 2n.
A ϭ 3ajk4 is called the coefficient matrix of the form. We may assume that A is
symmetric, because we can take off-diagonal terms together in pairs and write the result
as a sum of two equal terms; see the following example.
EXAMPLE 5

Quadratic Form. Symmetric Coefficient Matrix
Let
x TAx ϭ 3x 1 x 24

c

3

4

6

2

dc d
x1
x2

ϭ 3x 21 ϩ 4x 1x 2 ϩ 6x 2x 1 ϩ 2x 22 ϭ 3x 21 ϩ 10x 1x 2 ϩ 2x 22.

Here 4 ϩ 6 ϭ 10 ϭ 5 ϩ 5. From the corresponding symmetric matrix C ϭ [cjk4, where cjk ϭ 12 (ajk ϩ akj),
thus c11 ϭ 3, c12 ϭ c21 ϭ 5, c22 ϭ 2, we get the same result; indeed,
x TCx ϭ 3x 1

x 24 c

3

5

5

2

dc d
x1
x2

ϭ 3x 21 ϩ 5x 1x 2 ϩ 5x 2x 1 ϩ 2x 22 ϭ 3x 21 ϩ 10x 1x 2 ϩ 2x 22.

᭿

Quadratic forms occur in physics and geometry, for instance, in connection with conic
sections (ellipses x 21>a 2 ϩ x 22>b 2 ϭ 1, etc.) and quadratic surfaces (cones, etc.). Their
transformation to principal axes is an important practical task related to the diagonalization
of matrices, as follows.
By Theorem 2, the symmetric coefficient matrix A of (7) has an orthonormal basis of
eigenvectors. Hence if we take these as column vectors, we obtain a matrix X that is
orthogonal, so that X؊1 ϭ XT. From (5) we thus have A ϭ XDX؊1 ϭ XDXT. Substitution
into (7) gives
(8)

Q ϭ x TXDXTx.

If we set XTx ϭ y, then, since XT ϭ X؊1, we have X؊1x ϭ y and thus obtain
(9)

x ϭ Xy.

Furthermore, in (8) we have x TX ϭ (XTx)T ϭ y T and XTx ϭ y, so that Q becomes simply
(10)

Q ϭ y TDy ϭ l1y 21 ϩ l2y 22 ϩ Á ϩ lny 2n.

c08.qxd

10/30/10

10:56 AM

344

Page 344

CHAP. 8 Linear Algebra: Matrix Eigenvalue Problems

This proves the following basic theorem.

THEOREM 5

Principal Axes Theorem

The substitution (9) transforms a quadratic form
n

n

Q ϭ x TAx ϭ a a ajkx jx k

(akj ϭ ajk)

jϭ1 kϭ1

to the principal axes form or canonical form (10), where l1, Á , ln are the (not
necessarily distinct) eigenvalues of the (symmetric!) matrix A, and X is an
orthogonal matrix with corresponding eigenvectors x1, Á , xn, respectively, as
column vectors.

EXAMPLE 6

Transformation to Principal Axes. Conic Sections
Find out what type of conic section the following quadratic form represents and transform it to principal axes:
Q ϭ 17x 21 Ϫ 30x1x 2 ϩ 17x 22 ϭ 128.

Solution. We have Q ϭ x TAx, where
Aϭ

c

17

Ϫ15

Ϫ15

17

d,

xϭ

c d.
x1
x2

This gives the characteristic equation (17 Ϫ l)2 Ϫ 152 ϭ 0. It has the roots l1 ϭ 2, l2 ϭ 32. Hence (10)
becomes
Q ϭ 2y 21 ϩ 32y 22.
We see that Q ϭ 128 represents the ellipse 2y 21 ϩ 32y 22 ϭ 128, that is,
y 21
82

ϩ

y 22
22

ϭ 1.

If we want to know the direction of the principal axes in the x 1x 2-coordinates, we have to determine normalized
eigenvectors from (A Ϫ lI)x ϭ 0 with l ϭ l1 ϭ 2 and l ϭ l2 ϭ 32 and then use (9). We get

c

1> 12
1> 12

d

and

c

Ϫ1> 12
1> 12

d,

hence

x ϭ Xy ϭ

c

1> 12

Ϫ1> 12

1> 12

1> 12

d c d,
y1
y2

x 1 ϭ y1> 12 Ϫ y2> 12

x 2 ϭ y1> 12 ϩ y2> 12.

This is a 45° rotation. Our results agree with those in Sec. 8.2, Example 1, except for the notations. See also
᭿
Fig. 160 in that example.

c08.qxd

10/30/10

10:56 AM

Page 345

SEC. 8.4 Eigenbases. Diagonalization. Quadratic Forms

345

PROBLEM SET 8.4
1–5

SIMILAR MATRICES HAVE EQUAL
EIGENVALUES

8. Orthonormal basis. Illustrate Theorem 2 with further
examples.

Verify this for A and A ϭ P ؊1AP. If y is an eigenvector
of P, show that x ϭ Py are eigenvectors of A. Show the
details of your work.

c

3

4

4

Ϫ3

c

1

0

2

Ϫ1

c

8

Ϫ4

2

2

0

0

2

4. A ϭ D0

3

2T ,

1
l1 ϭ 3

0

1

1. A ϭ

2. A ϭ

3. A ϭ

d,

Pϭ

d,

Pϭ

d,

Ϫ5

0

5. A ϭ D 3

4

Ϫ5

0

Pϭ

c

Ϫ4

2

3

Ϫ1

c

7

Ϫ5

10

Ϫ7

c

d

11.

0.28

0.96

Ϫ0.96

0.28

S

2

0

3

P ϭ D0

1

0T ,

3

0

5

15
Ϫ9T ,
15

0

1

0

P ϭ D1

0

0T

0

0

1

6. PROJECT. Similarity of Matrices. Similarity is
basic, for instance, in designing numeric methods.
(a) Trace. By definition, the trace of an n ϫ n matrix
A ϭ 3ajk4 is the sum of the diagonal entries,
trace A ϭ a11 ϩ a22 ϩ Á ϩ ann.
Show that the trace equals the sum of the eigenvalues,
each counted as often as its algebraic multiplicity
indicates. Illustrate this with the matrices A in Probs.
1, 3, and 5.
(b) Trace of product. Let B ϭ 3bjk4 be n ϫ n. Show
that similar matrices have equal traces, by first proving
n

Find an eigenbasis (a basis of eigenvectors) and diagonalize.
Show the details.
9.

d

n

trace AB ϭ a a ailbli ϭ trace BA.
iϭ1 lϭ1

ˆ in (4) and
(c) Find a relationship between A
ˆA ϭ PAP ؊1.
(d) Diagonalization. What can you do in (5) if you
want to change the order of the eigenvalues in D, for
instance, interchange d11 ϭ l1 and d22 ϭ l2?
7. No basis. Find further 2 ϫ 2 and 3 ϫ 3 matrices
without eigenbasis.

DIAGONALIZATION OF MATRICES

9–16

c

1

c

Ϫ19

7

Ϫ42

16

2

2
4

d

10.

d

12.

4

0

13. D12

Ϫ2

0T

21

Ϫ6

1

1

0

2

Ϫ1

c

Ϫ4.3

7.7

1.3

9.3

d

0

Ϫ5

Ϫ6

6

14. D Ϫ9

Ϫ8

12T ,

Ϫ12

Ϫ12

l1 ϭ Ϫ2

16

4

3

3

15. D3

6

1T ,

3

1

6

1

1

0

16. D1

1

0T

0

0

17–23

d

c

l1 ϭ 10

Ϫ4

PRINCIPAL AXES. CONIC SECTIONS

What kind of conic section (or pair of straight lines) is given
by the quadratic form? Transform it to principal axes.
Express x T ϭ 3x 1 x 24 in terms of the new coordinate
vector y T ϭ 3y1 y24, as in Example 6.
17. 7x 21 ϩ 6x1x 2 ϩ 7x 22 ϭ 200
18. 3x 21 ϩ 8x1x 2 Ϫ 3x 22 ϭ 10
19. 3x 21 ϩ 22x1x 2 ϩ 3x 22 ϭ 0
20. 9x 21 ϩ 6x1x 2 ϩ x 22 ϭ 10
21. x 21 Ϫ 12x1x 2 ϩ x 22 ϭ 70
22. 4x 21 ϩ 12x1x 2 ϩ 13x 22 ϭ 16
23. Ϫ11x 21 ϩ 84x1x 2 ϩ 24x 22 ϭ 156

c08.qxd

10/30/10

10:56 AM

Page 346

346

CHAP. 8 Linear Algebra: Matrix Eigenvalue Problems

24. Definiteness. A quadratic form Q (x) ϭ x TAx and its
(symmetric!) matrix A are called (a) positive definite
if Q (x) Ͼ 0 for all x 0, (b) negative definite if
Q (x) Ͻ 0 for all x 0, (c) indefinite if Q (x) takes
both positive and negative values. (See Fig. 162.)
3Q (x) and A are called positive semidefinite (negative
semidefinite) if Q (x) м 0 (Q (x) Ϲ 0) for all x.] Show
that a necessary and sufficient condition for (a), (b),
and (c) is that the eigenvalues of A are (a) all positive,
(b) all negative, and (c) both positive and negative.
Hint. Use Theorem 5.
25. Definiteness. A necessary and sufficient condition for
positive definiteness of a quadratic form Q (x) ϭ x TAx
with symmetric matrix A is that all the principal minors
are positive (see Ref. [B3], vol. 1, p. 306), that is,

2

a11 Ͼ 0,

a11

a12

a12

a22

Q(x)

x1
x2
(a) Positive definite form

Q(x)

x1
x2

2 Ͼ 0,
(b) Negative definite form

a11

a12

a13

3 a12

a22

a23 3 Ͼ 0,

a13

a23

a33

Á,

det A Ͼ 0.

Q(x)

Show that the form in Prob. 22 is positive definite,
whereas that in Prob. 23 is indefinite.

x1
x2
(c) Indefinite form

Fig. 162. Quadratic forms in two variables (Problem 24)

8.5

Complex Matrices and Forms.

Optional

The three classes of matrices in Sec. 8.3 have complex counterparts which are of practical
interest in certain applications, for instance, in quantum mechanics. This is mainly because
of their spectra as shown in Theorem 1 in this section. The second topic is about extending
quadratic forms of Sec. 8.4 to complex numbers. (The reader who wants to brush up on
complex numbers may want to consult Sec. 13.1.)
Notations

A ϭ 3ajk4 is obtained from A ϭ 3ajk4 by replacing each entry ajk ϭ a ϩ ib
T
(a, b real) with its complex conjugate ajk ϭ a Ϫ ib. Also, A ϭ 3akj4 is the transpose
of A, hence the conjugate transpose of A.

EXAMPLE 1

Notations
If A ϭ

c

3 ϩ 4i

1Ϫi

6

2 Ϫ 5i

d,

then A ϭ

c

3 Ϫ 4i

1ϩi

6

2 ϩ 5i

d

T

and A ϭ

c

3 Ϫ 4i

6

1ϩi

2 ϩ 5i

d.

᭿

c08.qxd

10/30/10

10:56 AM

Page 347

SEC. 8.5 Complex Matrices and Forms. Optional

DEFINITION

347

Hermitian, Skew-Hermitian, and Unitary Matrices

A square matrix A ϭ 3akj4 is called
Hermitian

if A ϭ A,

T

that is,

akj ϭ ajk

T

that is,

akj ϭ Ϫajk

skew-Hermitian

if A ϭ ϪA,

unitary

if A ϭ A؊1.
T

The first two classes are named after Hermite (see footnote 13 in Problem Set 5.8).
From the definitions we see the following. If A is Hermitian, the entries on the main
diagonal must satisfy ajj ϭ ajj; that is, they are real. Similarly, if A is skew-Hermitian,
then ajj ϭ Ϫajj. If we set ajj ϭ a ϩ ib, this becomes a Ϫ ib ϭ Ϫ(a ϩ ib). Hence a ϭ 0,
so that ajj must be pure imaginary or 0.
EXAMPLE 2

Hermitian, Skew-Hermitian, and Unitary Matrices
Aϭ

c

4
1 ϩ 3i

1 Ϫ 3i
7

d

Bϭ

c

2ϩi

3i
Ϫ2ϩi

Ϫi

d

Cϭ

c1

1
2i
2

13

1
2

13

1
2i

d

are Hermitian, skew-Hermitian, and unitary matrices, respectively, as you may verify by using the definitions.

᭿

T

If a Hermitian matrix is real, then A ϭ AT ϭ A. Hence a real Hermitian matrix is a
symmetric matrix (Sec. 8.3).
T
Similarly, if a skew-Hermitian matrix is real, then A ϭ AT ϭ ϪA. Hence a real skewHermitian matrix is a skew-symmetric matrix.
T
Finally, if a unitary matrix is real, then A ϭ AT ϭ A؊1. Hence a real unitary matrix
is an orthogonal matrix.
This shows that Hermitian, skew-Hermitian, and unitary matrices generalize symmetric,
skew-symmetric, and orthogonal matrices, respectively.

Eigenvalues
It is quite remarkable that the matrices under consideration have spectra (sets of eigenvalues;
see Sec. 8.1) that can be characterized in a general way as follows (see Fig. 163).
Im λ

Skew-Hermitian (skew-symmetric)
Unitary (orthogonal)
Hermitian (symmetric)

1

Re λ

Fig. 163. Location of the eigenvalues of Hermitian, skew-Hermitian,
and unitary matrices in the complex l-plane

c08.qxd

10/30/10

10:56 AM

348

Page 348

CHAP. 8 Linear Algebra: Matrix Eigenvalue Problems

THEOREM 1

Eigenvalues

(a) The eigenvalues of a Hermitian matrix (and thus of a symmetric matrix)
are real.
(b) The eigenvalues of a skew-Hermitian matrix (and thus of a skew-symmetric
matrix) are pure imaginary or zero.
(c) The eigenvalues of a unitary matrix (and thus of an orthogonal matrix) have
absolute value 1.
EXAMPLE 3

Illustration of Theorem 1
For the matrices in Example 2 we find by direct calculation

Matrix
A
B
C

Characteristic Equation
l2 Ϫ 11l ϩ 18 ϭ 0
l2 Ϫ 2il ϩ 8 ϭ 0
l2 Ϫ il Ϫ 1 ϭ 0

Hermitian
Skew-Hermitian
Unitary

Eigenvalues
9, 2
4i, Ϫ2i
1
1
Ϫ12 13 ϩ 12 i
2 13 ϩ 2 i,

᭿

and ƒ Ϯ12 13 ϩ 12 i ƒ 2 ϭ 34 ϩ 14 ϭ 1.

PROOF

We prove Theorem 1. Let l be an eigenvalue and x an eigenvector of A. Multiply Ax ϭ lx
from the left by x T, thus x TAx ϭ lx Tx, and divide by x Tx ϭ x 1x 1 ϩ Á ϩ xnx n ϭ
ƒ x 1 ƒ 2 ϩ Á ϩ ƒ x n ƒ 2, which is real and not 0 because x 0. This gives
lϭ

(1)

x TAx
x Tx

.

T

(a) If A is Hermitian, A ϭ A or AT ϭ A and we show that then the numerator in (1)
is real, which makes l real. x TAx is a scalar; hence taking the transpose has no effect. Thus
(2)

x TAx ϭ (x TAx)T ϭ x TATx ϭ x T Ax ϭ ( x TAx).

Hence, x TAx equals its complex conjugate, so that it must be real. (a ϩ ib ϭ a Ϫ ib
implies b ϭ 0.)
(b) If A is skew-Hermitian, AT ϭ ϪA and instead of (2) we obtain
(3)

x TAx ϭ Ϫ( x TAx)

so that x TAx equals minus its complex conjugate and is pure imaginary or 0.
(a ϩ ib ϭ Ϫ(a Ϫ ib) implies a ϭ 0.)
(c) Let A be unitary. We take Ax ϭ lx and its conjugate transpose
(Ax)T ϭ (lx)T ϭ lx T
and multiply the two left sides and the two right sides,
(Ax)TAx ϭ llx Tx ϭ ƒ l ƒ 2 x Tx.

c08.qxd

10/30/10

10:56 AM

Page 349

SEC. 8.5 Complex Matrices and Forms. Optional

349

But A is unitary, A ϭ A؊1, so that on the left we obtain
T

(Ax )TAx ϭ x T A Ax ϭ x TA؊1Ax ϭ x TIx ϭ x Tx.
T

Together, x Tx ϭ ƒ l ƒ 2 x Tx. We now divide by x Tx ( 0) to get ƒ l ƒ 2 ϭ 1. Hence ƒ l ƒ ϭ 1.
This proves Theorem 1 as well as Theorems 1 and 5 in Sec. 8.3.
᭿
Key properties of orthogonal matrices (invariance of the inner product, orthonormality of
rows and columns; see Sec. 8.3) generalize to unitary matrices in a remarkable way.
To see this, instead of R n we now use the complex vector space C n of all complex
vectors with n complex numbers as components, and complex numbers as scalars. For
such complex vectors the inner product is defined by (note the overbar for the complex
conjugate)
a • b ϭ aTb.

(4)

The length or norm of such a complex vector is a real number defined by
ʈ a ʈ ϭ 2a • a ϭ 2aTj a ϭ 2a1a1 ϩ Á ϩ anan ϭ 2 ƒ a1 ƒ 2 ϩ Á ϩ ƒ an ƒ 2.

(5)

THEOREM 2

Invariance of Inner Product

A unitary transformation, that is, y ϭ Ax with a unitary matrix A, preserves the
value of the inner product (4), hence also the norm (5).
PROOF

The proof is the same as that of Theorem 2 in Sec. 8.3, which the theorem generalizes.
In the analog of (9), Sec. 8.3, we now have bars,
T

u • v ϭ uTv ϭ (Aa)TAb ϭ aT A Ab ϭ aTIb ϭ aTb ϭ a • b.
The complex analog of an orthonormal system of real vectors (see Sec. 8.3) is defined as
follows.
DEFINITION

Unitary System

A unitary system is a set of complex vectors satisfying the relationships
(6)

aj • ak ϭ aTj ak ϭ b

0

if

j

k

1

if

j ϭ k.

Theorem 3 in Sec. 8.3 extends to complex as follows.
THEOREM 3

Unitary Systems of Column and Row Vectors

A complex square matrix is unitary if and only if its column vectors (and also its
row vectors) form a unitary system.

c08.qxd

10/30/10

10:56 AM

350

Page 350

CHAP. 8 Linear Algebra: Matrix Eigenvalue Problems

PROOF

The proof is the same as that of Theorem 3 in Sec. 8.3, except for the bars required in
T
A ϭ A؊1 and in (4) and (6) of the present section.

THEOREM 4

Determinant of a Unitary Matrix

Let A be a unitary matrix. Then its determinant has absolute value one, that is,
ƒ det A ƒ ϭ 1.
PROOF

Similarly, as in Sec. 8.3, we obtain
1 ϭ det (AA؊1) ϭ det (AA ) ϭ det A det A ϭ det A det A
T

T

ϭ det A det A ϭ ƒ det A ƒ 2.
Hence ƒ det A ƒ ϭ 1 (where det A may now be complex).
EXAMPLE 4

᭿

Unitary Matrix Illustrating Theorems 1c and 2–4
For the vectors aT ϭ 32
and with
Aϭ

c

Ϫi4 and bT ϭ 31 ϩ i

0.8i
0.6

0.6
0.8i

d

4i4 we get aT ϭ 32

Aa ϭ

also

c d

i4T and aTb ϭ 2(1 ϩ i) Ϫ 4 ϭ Ϫ2 ϩ 2i

i

and

2

Ab ϭ

c

Ϫ0.8 ϩ 3.2i
Ϫ2.6 ϩ 0.6i

d,

as one can readily verify. This gives (Aa)TAb ϭ Ϫ2 ϩ 2i, illustrating Theorem 2. The matrix is unitary. Its
columns form a unitary system,
aT1 a1 ϭ Ϫ0.8i # 0.8i ϩ 0.62 ϭ 1,

aT1 a2 ϭ Ϫ0.8i # 0.6 ϩ 0.6 # 0.8i ϭ 0,

aT2 a2 ϭ 0.62 ϩ (Ϫ0.8i)0.8i ϭ 1
and so do its rows. Also, det A ϭ Ϫ1. The eigenvalues are 0.6 ϩ 0.8i and Ϫ0.6 ϩ 0.8i, with eigenvectors 31
and 31 Ϫ14T, respectively.

14T

᭿

Theorem 2 in Sec. 8.4 on the existence of an eigenbasis extends to complex matrices as
follows.
THEOREM 5

Basis of Eigenvectors

A Hermitian, skew-Hermitian, or unitary matrix has a basis of eigenvectors for C n
that is a unitary system.
For a proof see Ref. [B3], vol. 1, pp. 270–272 and p. 244 (Definition 2).
EXAMPLE 5

Unitary Eigenbases
The matrices A, B, C in Example 2 have the following unitary systems of eigenvectors, as you should verify.
A:

B:

C:

1
135
1
130
1
12

31 Ϫ 3i

54T (l ϭ 9),

31 Ϫ 2i

Ϫ54T (l ϭ Ϫ2i),

31

14T

(l ϭ 12 (i ϩ 13)) ,

1
114
1
130
1
12

31 Ϫ 3i

Ϫ24T (l ϭ 2)

35

1 ϩ 2i4T (l ϭ 4i)

31

Ϫ14T

(l ϭ 12 (i Ϫ 13)) .

᭿

c08.qxd

10/30/10

3:18 PM

Page 351

SEC. 8.5 Complex Matrices and Forms. Optional

351

Hermitian and Skew-Hermitian Forms
The concept of a quadratic form (Sec. 8.4) can be extended to complex. We call the
numerator x TAx in (1) a form in the components x 1, Á , x n of x, which may now be
complex. This form is again a sum of n 2 terms
n

n

x TAx ϭ a a ajk x j x k
jϭ1 kϭ1

ϭ

a11x 1x 1 ϩ Á ϩ a1nx1x n
ϩ a21x 2x 1 ϩ Á ϩ a2nx 2x n

(7)

ϩ# # # # # # # # # # # # # # # # # # #
ϩ an1x nx 1 ϩ Á ϩ annx nx n.
A is called its coefficient matrix. The form is called a Hermitian or skew-Hermitian
form if A is Hermitian or skew-Hermitian, respectively. The value of a Hermitian form
is real, and that of a skew-Hermitian form is pure imaginary or zero. This can be seen
directly from (2) and (3) and accounts for the importance of these forms in physics. Note
that (2) and (3) are valid for any vectors because, in the proof of (2) and (3), we did not
use that x is an eigenvector but only that x Tx is real and not 0.
EXAMPLE 6

Hermitian Form
For A in Example 2 and, say, x ϭ 31 ϩ i
x TAx ϭ 31 Ϫ i

Ϫ5i4

c

4

1 Ϫ 3i

1 ϩ 3i

7

5i4T we get

dc

1ϩi
5i

d

ϭ 31 Ϫ i

Ϫ5i4

c

4(1 ϩ i) ϩ (1 Ϫ 3i) # 5i
(1 ϩ 3i)(1 ϩ i) ϩ 7 # 5i

d

ϭ 223.

᭿

Clearly, if A and x in (4) are real, then (7) reduces to a quadratic form, as discussed in
the last section.

PROBLEM SET 8.5
EIGENVALUES AND VECTORS

1–6

Is the given matrix Hermitian? Skew-Hermitian? Unitary?
Find its eigenvalues and eigenvectors.
1.

3.

c
c

6

i

Ϫi

6

d

2.

1
2

i234

i234

1
2

d

i

0

0

5. D0

0

iT

0

i

0

4.

c
c

i

1ϩi

Ϫ1 ϩ i

0

0

i

i

0
0

6. D2 Ϫ 2i
0

d

d
2 ϩ 2i

0

0

2 ϩ 2iT

2 Ϫ 2i

0

7. Pauli spin matrices. Find the eigenvalues and eigenvectors of the so-called Pauli spin matrices and show
that SxSy ϭ iSz, SySx ϭ ϪiSz, S2x ϭ S2y ϭ S2z ϭ I,
where
Sx ϭ

c

0

1

1

0

Sz ϭ

c

1

0

0

Ϫ1

d,

Sy ϭ

c

0

Ϫi

i

0

d,

d.

8. Eigenvectors. Find eigenvectors of A, B, C in
Examples 2 and 3.

c08.qxd

10/30/10

10:56 AM

Page 352

352

CHAP. 8 Linear Algebra: Matrix Eigenvalue Problems

9–12

COMPLEX FORMS
T

Is the matrix A Hermitian or skew-Hermitian? Find x Ax.
Show the details.
9. A ϭ

c

10. A ϭ

c

4

3 Ϫ 2i

3 ϩ 2i

Ϫ4

d,

i

Ϫ2 ϩ 3i

2 ϩ 3i

0

i

Ϫ2 ϩ i

Ϫ4i
2 ϩ 2i

d

c d
2i

xϭ

8

1

0

3i T ,

3i

i

x ϭ D iT
Ϫi
1

1

i

4

12. A ϭ DϪi

3

0T ,

4

0

2

13–20

S,

c

2ϩi

1

11. A ϭ D Ϫ1

xϭ

x ϭ D iT
Ϫi

GENERAL PROBLEMS

13. Product. Show that (ABC) T ϭ ϪC Ϫ1BA for any
n ϫ n Hermitian A, skew-Hermitian B, and unitary C.

14. Product. Show (BA) T ϭ ϪAB for A and B in
Example 2. For any n ϫ n Hermitian A and
skew-Hermitian B.
15. Decomposition. Show that any square matrix may be
written as the sum of a Hermitian and a skew-Hermitian
matrix. Give examples.
16. Unitary matrices. Prove that the product of two
unitary n ϫ n matrices and the inverse of a unitary
matrix are unitary. Give examples.
17. Powers of unitary matrices in applications may
sometimes be very simple. Show that C 12 ϭ I in
Example 2. Find further examples.
18. Normal matrix. This important concept denotes a
matrix that commutes with its conjugate transpose,
AA T ϭ A TA. Prove that Hermitian, skew-Hermitian,
and unitary matrices are normal. Give corresponding
examples of your own.
19. Normality criterion. Prove that A is normal if and
only if the Hermitian and skew-Hermitian matrices in
Prob. 18 commute.
20. Find a simple matrix that is not normal. Find a normal
matrix that is not Hermitian, skew-Hermitian, or
unitary.

CHAPTER 8 REVIEW QUESTIONS AND PROBLEMS
1. In solving an eigenvalue problem, what is given and
what is sought?
2. Give a few typical applications of eigenvalue problems.
3. Do there exist square matrices without eigenvalues?
4. Can a real matrix have complex eigenvalues? Can a
complex matrix have real eigenvalues?
5. Does a 5 ϫ 5 matrix always have a real eigenvalue?
6. What is algebraic multiplicity of an eigenvalue? Defect?
7. What is an eigenbasis? When does it exist? Why is it
important?
8. When can we expect orthogonal eigenvectors?
9. State the definitions and main properties of the three
classes of real matrices and of complex matrices that
we have discussed.
10. What is diagonalization? Transformation to principal axes?
11–15

11.

13.

2

14. D 2

7

1 T

Ϫ1

1

8.5

0

Ϫ3

15. D3

0

Ϫ6T

6

6

0

c

2.5

0.5

0.5

2.5

c

8

Ϫ1

5

2

d

d

12.

c

Ϫ7

4

Ϫ12

7

d

Ϫ6

16–17
SIMILARITY
ˆ ϭ p Ϫ1AP have the same spectrum.
Verify that A and A
16. A ϭ

c

19

12

12

1

17. A ϭ

c

7

Ϫ4

12

Ϫ7

Ϫ4

6

6

18. A ϭ D 0

2

0T ,

Ϫ1

1

1

SPECTRUM

Find the eigenvalues. Find the eigenvectors.

Ϫ1

7

d,

Pϭ

d,

Pϭ

c

c

2

4

4

2

5

3

3

5

d

d
Ϫ7

1

8

P ϭ D0

1

3T

0

0

1

c08.qxd

10/30/10

10:56 AM

Page 353

Summary of Chapter 8

353

DIAGONALIZATION

19–21

22–25

Find an eigenbasis and diagonalize.
9.

c

Ϫ1.4

1.0

Ϫ1.0

1.1

Ϫ12
21. D

d

20.

22

6

8

2

6T

Ϫ8

20

c

72

Ϫ56

Ϫ56

513

d

CONIC SECTIONS. PRINCIPAL AXES

Transform to canonical form (to principal axes). Express
3x 1 x 24T in terms of the new variables 3y1 y24T.
22. 9x 21 Ϫ 6x 1x 2 ϩ 17x 22 ϭ 36
23. 4x 21 ϩ 24x 1x 2 Ϫ 14x 22 ϭ 20
24. 5x 21 ϩ 24x 1x 2 Ϫ 5x 22 ϭ 0
25. 3.7x 21 ϩ 3.2x 1x 2 ϩ 1.3x 22 ϭ 4.5

16

SUMMARY OF CHAPTER

8

Linear Algebra: Matrix Eigenvalue Problems
The practical importance of matrix eigenvalue problems can hardly be overrated.
The problems are defined by the vector equation
Ax ϭ lx.

(1)

A is a given square matrix. All matrices in this chapter are square. l is a scalar. To
solve the problem (1) means to determine values of l, called eigenvalues (or
characteristic values) of A, such that (1) has a nontrivial solution x (that is, x 0),
called an eigenvector of A corresponding to that l. An n ϫ n matrix has at least
one and at most n numerically different eigenvalues. These are the solutions of the
characteristic equation (Sec. 8.1)

(2)

D (l) ϭ det (A Ϫ lI) ϭ 5

a11 Ϫ l

a12

Á

a1n

a21

a22 Ϫ l

Á

a2n

#

#

Á

#

an1

an2

Á

ann Ϫ l

5 ϭ 0.

D (l) is called the characteristic determinant of A. By expanding it we get the
characteristic polynomial of A, which is of degree n in l. Some typical applications
are shown in Sec. 8.2.
Section 8.3 is devoted to eigenvalue problems for symmetric (AT ϭ A), skewsymmetric (AT ϭ ϪA), and orthogonal matrices (AT ϭ A؊1). Section 8.4
concerns the diagonalization of matrices and the transformation of quadratic forms
to principal axes and its relation to eigenvalues.
Section 8.5 extends Sec. 8.3 to the complex analogs of those real matrices, called
Hermitian (AT ϭ A), skew-Hermitian (AT ϭ ϪA), and unitary matrices
(A T ϭ A؊1). All the eigenvalues of a Hermitian matrix (and a symmetric one) are
real. For a skew-Hermitian (and a skew-symmetric) matrix they are pure imaginary
or zero. For a unitary (and an orthogonal) matrix they have absolute value 1.

c09.qxd

10/30/10

3:25 PM

Page 354

CHAPTER

9

Vector Differential Calculus.
Grad, Div, Curl
Engineering, physics, and computer sciences, in general, but particularly solid mechanics,
aerodynamics, aeronautics, fluid flow, heat flow, electrostatics, quantum physics, laser
technology, robotics as well as other areas have applications that require an understanding
of vector calculus. This field encompasses vector differential calculus and vector integral
calculus. Indeed, the engineer, physicist, and mathematician need a good grounding in
these areas as provided by the carefully chosen material of Chaps. 9 and 10.
Forces, velocities, and various other quantities may be thought of as vectors. Vectors
appear frequently in the applications above and also in the biological and social sciences,
so it is natural that problems are modeled in 3-space. This is the space of three dimensions
with the usual measurement of distance, as given by the Pythagorean theorem. Within that
realm, 2-space (the plane) is a special case. Working in 3-space requires that we extend
the common differential calculus to vector differential calculus, that is, the calculus that
deals with vector functions and vector fields and is explained in this chapter.
Chapter 9 is arranged in three groups of sections. Sections 9.1–9.3 extend the basic
algebraic operations of vectors into 3-space. These operations include the inner product
and the cross product. Sections 9.4 and 9.5 form the heart of vector differential calculus.
Finally, Secs. 9.7–9.9 discuss three physically important concepts related to scalar and
vector fields: gradient (Sec. 9.7), divergence (Sec. 9.8), and curl (Sec. 9.9). They are
expressed in Cartesian coordinates in this chapter and, if desired, expressed in curvilinear
coordinates in a short section in App. A3.4.
We shall keep this chapter independent of Chaps. 7 and 8. Our present approach is in
harmony with Chap. 7, with the restriction to two and three dimensions providing for a
richer theory with basic physical, engineering, and geometric applications.
Prerequisite: Elementary use of second- and third-order determinants in Sec. 9.3.
Sections that may be omitted in a shorter course: 9.5, 9.6.
References and Answers to Problems: App. 1 Part B, App. 2.

9.1

Vectors in 2-Space and 3-Space
In engineering, physics, mathematics, and other areas we encounter two kinds of quantities.
They are scalars and vectors.
A scalar is a quantity that is determined by its magnitude. It takes on a numerical value,
i.e., a number. Examples of scalars are time, temperature, length, distance, speed, density,
energy, and voltage.

354

c09.qxd

10/30/10

3:25 PM

Page 355

SEC. 9.1 Vectors in 2-Space and 3-Space

355

In contrast, a vector is a quantity that has both magnitude and direction. We can say
that a vector is an arrow or a directed line segment. For example, a velocity vector has
length or magnitude, which is speed, and direction, which indicates the direction of motion.
Typical examples of vectors are displacement, velocity, and force, see Fig. 164 as an
illustration.
More formally, we have the following. We denote vectors by lowercase boldface letters
a, b, v, etc. In handwriting you may use arrows, for instance, aជ (in place of a), bជ, etc.
A vector (arrow) has a tail, called its initial point, and a tip, called its terminal point.
This is motivated in the translation (displacement without rotation) of the triangle in
Fig. 165, where the initial point P of the vector a is the original position of a point, and
the terminal point Q is the terminal position of that point, its position after the translation.
The length of the arrow equals the distance between P and Q. This is called the length
(or magnitude) of the vector a and is denoted by ƒ a ƒ . Another name for length is norm
(or Euclidean norm).
A vector of length 1 is called a unit vector.
Velocity
Earth

Force

Q
a

Sun

P

Fig. 164. Force and velocity

Fig. 165. Translation

Of course, we would like to calculate with vectors. For instance, we want to find the
resultant of forces or compare parallel forces of different magnitude. This motivates our
next ideas: to define components of a vector, and then the two basic algebraic operations
of vector addition and scalar multiplication.
For this we must first define equality of vectors in a way that is practical in connection
with forces and other applications.

DEFINITION

Equality of Vectors

Two vectors a and b are equal, written a ϭ b, if they have the same length and the
same direction [as explained in Fig. 166; in particular, note (B)]. Hence a vector
can be arbitrarily translated; that is, its initial point can be chosen arbitrarily.

a

b

Equal vectors,
a=b
(A)

a

b

a

b

a

b

Vectors having
the same length
but different
direction

Vectors having
the same direction
but different
length

Vectors having
different length
and different
direction

(B)

(C)

(D)

Fig. 166. (A) Equal vectors. (B)–(D) Different vectors

c09.qxd

10/30/10

3:25 PM

356

Page 356

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

Components of a Vector
We choose an xyz Cartesian coordinate system1 in space (Fig. 167), that is, a usual
rectangular coordinate system with the same scale of measurement on the three mutually
perpendicular coordinate axes. Let a be a given vector with initial point P: (x 1, y1, z 1) and
terminal point Q: (x 2, y2, z 2). Then the three coordinate differences
a1 ϭ x 2 Ϫ x 1,

(1)

a2 ϭ y2 Ϫ y1,

a3 ϭ z 2 Ϫ z 1

are called the components of the vector a with respect to that coordinate system, and we
write simply a ϭ [a1, a2, a3]. See Fig. 168.
The length ƒ a ƒ of a can now readily be expressed in terms of components because from
(1) and the Pythagorean theorem we have
ƒ a ƒ ϭ 2a 21 ϩ a 22 ϩ a 23.

(2)

EXAMPLE 1

Components and Length of a Vector
The vector a with initial point P: (4, 0, 2) and terminal point Q: (6, Ϫ1, 2) has the components
a1 ϭ 6 Ϫ 4 ϭ 2,

a2 ϭ Ϫ1 Ϫ 0 ϭ Ϫ1,

a3 ϭ 2 Ϫ 2 ϭ 0.

Hence a ϭ [2, Ϫ1, 0]. (Can you sketch a, as in Fig. 168?) Equation (2) gives the length
ƒ a ƒ ϭ 222 ϩ (Ϫ1)2 ϩ 02 ϭ 15.
If we choose (Ϫ1, 5, 8) as the initial point of a, the corresponding terminal point is (1, 4, 8).
If we choose the origin (0, 0, 0) as the initial point of a, the corresponding terminal point is (2, Ϫ1, 0); its
coordinates equal the components of a. This suggests that we can determine each point in space by a vector,
᭿
called the position vector of the point, as follows.

A Cartesian coordinate system being given, the position vector r of a point A: (x, y, z)
is the vector with the origin (0, 0, 0) as the initial point and A as the terminal point (see
Fig. 169). Thus in components, r ϭ [x, y, z]. This can be seen directly from (1) with
x 1 ϭ y1 ϭ z 1 ϭ 0.
z

z

z

a3

A

Q

1
r
P
1

a1

1
y

x

Fig. 167. Cartesian
coordinate system
1

a2

x

y

Fig. 168. Components
of a vector

x

y

Fig. 169. Position vector r
of a point A: (x, y, z)

Named after the French philosopher and mathematician RENATUS CARTESIUS, latinized for RENÉ
DESCARTES (1596–1650), who invented analytic geometry. His basic work Géométrie appeared in 1637, as
an appendix to his Discours de la méthode.

10/30/10

3:25 PM

Page 357

SEC. 9.1 Vectors in 2-Space and 3-Space

357

Furthermore, if we translate a vector a, with initial point P and terminal point Q, then
corresponding coordinates of P and Q change by the same amount, so that the differences
in (1) remain unchanged. This proves
THEOREM 1

Vectors as Ordered Triples of Real Numbers

A fixed Cartesian coordinate system being given, each vector is uniquely determined
by its ordered triple of corresponding components. Conversely, to each ordered
triple of real numbers (a1, a2, a3) there corresponds precisely one vector
a ϭ [a1, a2, a3], with (0, 0, 0) corresponding to the zero vector 0, which has length
0 and no direction.
Hence a vector equation a ϭ b is equivalent to the three equations a1 ϭ b1,
a2 ϭ b2, a3 ϭ b3 for the components.
We now see that from our “geometric” definition of a vector as an arrow we have arrived
at an “algebraic” characterization of a vector by Theorem 1. We could have started from
the latter and reversed our process. This shows that the two approaches are equivalent.

Vector Addition, Scalar Multiplication
Calculations with vectors are very useful and are almost as simple as the arithmetic for
real numbers. Vector arithmetic follows almost naturally from applications. We first define
how to add vectors and later on how to multiply a vector by a number.
DEFINITION
b
a

c=a+b

Fig. 170. Vector
addition

Addition of Vectors

The sum a ϩ b of two vectors a ϭ [a1, a2, a3] and b ϭ [b1, b2, b3] is obtained by
adding the corresponding components,
(3)

a ϩ b ϭ [a1 ϩ b1,

a2 ϩ b2,

a3 ϩ b3].

Geometrically, place the vectors as in Fig. 170 (the initial point of b at the terminal
point of a); then a ϩ b is the vector drawn from the initial point of a to the terminal
point of b.
For forces, this addition is the parallelogram law by which we obtain the resultant of two
forces in mechanics. See Fig. 171.
Figure 172 shows (for the plane) that the “algebraic” way and the “geometric way” of
vector addition give the same vector.
c
b
Resultant

c09.qxd

a

c
b

a

Fig. 171. Resultant of two forces (parallelogram law)

10/30/10

3:25 PM

358

Page 358

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

Basic Properties of Vector Addition.

(4)

Familiar laws for real numbers give immediately

(a)

aϩbϭbϩa

(b)

(u ϩ v) ϩ w ϭ u ϩ (v ϩ w)

(Commutativity)
(Associativity)

aϩ0ϭ0ϩaϭa

(c)

a ϩ (Ϫa) ϭ 0.

(d)

Properties (a) and (b) are verified geometrically in Figs. 173 and 174. Furthermore, Ϫa
denotes the vector having the length ƒ a ƒ and the direction opposite to that of a.
y
u+v

b

b2
c2
a2

c

a

b
a

c
a1

v+

u

c1

x

w

w

b

Fig. 173. Cummutativity
of vector addition

Fig. 172. Vector addition

+w

v

a

b1

u+v

c09.qxd

Fig. 174. Associativity
of vector addition

In (4b) we may simply write u ϩ v ϩ w, and similarly for sums of more than three
vectors. Instead of a ϩ a we also write 2a, and so on. This (and the notation Ϫa used
just before) motivates defining the second algebraic operation for vectors as follows.

DEFINITION

Scalar Multiplication (Multiplication by a Number)

The product ca of any vector a ϭ [a1, a2, a3] and any scalar c (real number c) is
the vector obtained by multiplying each component of a by c,
a

2a

–a

–1 a

ca ϭ [ca1, ca2, ca3].

(5)

2

Fig. 175. Scalar
multiplication
[multiplication of
vectors by scalars
(numbers)]

Geometrically, if a 0, then ca with c Ͼ 0 has the direction of a and with c Ͻ 0
the direction opposite to a. In any case, the length of ca is ƒ ca ƒ ϭ ƒ c ƒ ƒ a ƒ , and ca ϭ 0
if a ϭ 0 or c ϭ 0 (or both). (See Fig. 175.)

Basic Properties of Scalar Multiplication.

(6)

From the definitions we obtain directly

(a)

c(a ϩ b) ϭ ca ϩ cb

(b)

(c ϩ k)a ϭ ca ϩ ka

(c)

c(ka) ϭ (ck)a

(d)

1a ϭ a.

(written cka)

c09.qxd

10/30/10

3:25 PM

Page 359

SEC. 9.1 Vectors in 2-Space and 3-Space

359

You may prove that (4) and (6) imply for any vector a
(7)

(a)

0a ϭ 0

(b)

(Ϫ1)a ϭ Ϫa.

Instead of b ϩ (Ϫa) we simply write b Ϫ a (Fig. 176).
EXAMPLE 2

Vector Addition. Multiplication by Scalars
With respect to a given coordinate system, let
a ϭ [4, 0, 1]

b ϭ [2, Ϫ5, 13 ].

and

Then Ϫa ϭ [Ϫ4, 0, Ϫ1], 7a ϭ [28, 0, 7], a ϩ b ϭ [6, Ϫ5, 43 ], and

᭿

2(a Ϫ b) ϭ 2[2, 5, 23 ] ϭ [4, 10, 43 ] ϭ 2a Ϫ 2b.

Unit Vectors i, j, k. Besides a ϭ [a1, a2, a3] another popular way of writing vectors is
a ϭ a1i ϩ a2 j ϩ a3k.

(8)

In this representation, i, j, k are the unit vectors in the positive directions of the axes of
a Cartesian coordinate system (Fig. 177). Hence, in components,
i ϭ [1, 0, 0],

(9)

j ϭ [0, 1, 0],

k ϭ [0, 0, 1]

and the right side of (8) is a sum of three vectors parallel to the three axes.
EXAMPLE 3

ijk Notation for Vectors

᭿

In Example 2 we have a ϭ 4i ϩ k, b ϭ 2i Ϫ 5j ϩ 13 k, and so on.

All the vectors a ϭ [a1, a2, a3] ϭ a1i ϩ a2 j ϩ a3k (with real numbers as components)
form the real vector space R3 with the two algebraic operations of vector addition and
scalar multiplication as just defined. R3 has dimension 3. The triple of vectors i, j, k
is called a standard basis of R 3. Given a Cartesian coordinate system, the representation
(8) of a given vector is unique.
Vector space R 3 is a model of a general vector space, as discussed in Sec. 7.9, but is
not needed in this chapter.
z

z
a

k
a

i

b
–a

b–

a

–a

Fig. 176. Difference of vectors

x

a 1i

j
y

x

a2 j

Fig. 177. The unit vectors i, j, k
and the representation (8)

a3k

y

c09.qxd

10/30/10

360

3:25 PM

Page 360

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

PROBLEM SET 9.1
1–5

COMPONENTS AND LENGTH

Find the components of the vector v with initial point P
and terminal point Q. Find ƒ v ƒ . Sketch ƒ v ƒ . Find the unit
vector u in the direction of v.
1. P: (1, 1, 0), Q: (6, 2, 0)
2. P: (1, 1, 1), Q: (2, 2, 0)
3. P: (Ϫ3.0, 4,0, Ϫ0.5), Q: (5.5, 0, 1.2)
4. P: (1, 4, 2), Q: (Ϫ1, Ϫ4, Ϫ2)
5. P: (0, 0, 0), Q: (2, 1, Ϫ2)

6–10
Find the terminal point Q of the vector v with
components as given and initial point P. Find ƒ v ƒ .
6.
7.
8.
9.
10.

4, 0, 0; P: (0, 2, 13)
1
1
P: (72 , Ϫ3, 34 )
2 , 3, Ϫ4 ;
13.1, 0.8, Ϫ2.0; P: (0, 0, 0)
6, 1, Ϫ4; P: (Ϫ6, Ϫ1, Ϫ4)
0, Ϫ3, 3; P: (0, 3, Ϫ3)

11–18

ADDITION, SCALAR MULTIPLICATION

Let a ϭ [3, 2, 0] ϭ 3i ϩ 2j; b ϭ [Ϫ4, 6, 0] ϭ 4i ϩ 6j,
c ϭ [5, Ϫ1, 8] ϭ 5i Ϫ j ϩ 8k, d ϭ [0, 0, 4] ϭ 4k.
Find:
11. 2a, 12 a, Ϫa
12. (a ϩ b) ϩ c, a ϩ (b ϩ c)
13. b ϩ c, c ϩ b
14. 3c Ϫ 6d, 3(c Ϫ 2d)
15. 7(c Ϫ b), 7c Ϫ 7b
16. 92 a Ϫ 3c, 9 (12 a Ϫ 13 c)
17. (7 Ϫ 3) a, 7a Ϫ 3a
18. 4a ϩ 3b, Ϫ4a Ϫ 3b
19. What laws do Probs. 12–16 illustrate?
20. Prove Eqs. (4) and (6).

21–25

26–37

FORCES, VELOCITIES

26. Equilibrium. Find v such that p, q, u in Prob. 21 and
v are in equilibrium.
27. Find p such that u, v, w in Prob. 23 and p are in
equilibrium.
28. Unit vector. Find the unit vector in the direction of
the resultant in Prob. 24.
29. Restricted resultant. Find all v such that the resultant
of v, p, q, u with p, q, u as in Prob. 21 is parallel to
the xy-plane.
30. Find v such that the resultant of p, q, u, v with p,
q, u as in Prob. 24 has no components in x- and
y-directions.
31. For what k is the resultant of [2, 0, Ϫ7], [1, 2, Ϫ3], and
[0, 3, k] parallel to the xy-plane?
32. If ƒ p ƒ ϭ 6 and ƒ q ƒ ϭ 4, what can you say about the
magnitude and direction of the resultant? Can you think
of an application to robotics?
33. Same question as in Prob. 32 if ƒ p ƒ ϭ 9, ƒ q ƒ ϭ 6,
ƒ u ƒ ϭ 3.
34. Relative velocity. If airplanes A and B are moving
southwest with speed ƒ vA ƒ ϭ 550 mph, and northwest with speed ƒ vB ƒ ϭ 450 mph, respectively, what
is the relative velocity v ϭ vB Ϫ vA of B with respect
to A?
35. Same question as in Prob. 34 for two ships moving
northeast with speed ƒ vA ƒ ϭ 22 knots and west with
speed ƒ vB ƒ ϭ 19 knots.
36. Reflection. If a ray of light is reflected once in each
of two mutually perpendicular mirrors, what can you
say about the reflected ray?
37. Force polygon. Truss. Find the forces in the system
of two rods (truss) in the figure, where ƒ p ƒ ϭ 1000 nt.
Hint. Forces in equilibrium form a polygon, the force
polygon.

FORCES, RESULTANT

Find the resultant in terms of components and its
magnitude.
21. p ϭ [2, 3, 0], q ϭ [0, 6, 1], u ϭ [2, 0, Ϫ4]
22. p ϭ [1, Ϫ2, 3], q ϭ [3, 21, Ϫ16],
u ϭ [Ϫ4, Ϫ19, 13]
11
23. u ϭ [8, Ϫ1, 0], v ϭ [12 , 0, 43 ], w ϭ [Ϫ17
2 , 1, 3 ]
24. p ϭ [Ϫ1, 2, Ϫ3], q ϭ [1, 1, 1], u ϭ [1, Ϫ2, 2]
25. u ϭ [3, 1, Ϫ6], v ϭ [0, 2, 5], w ϭ [3, Ϫ1, Ϫ13]

y
x

45Њ
p

v

p

u
Force polygon

Truss

Problem 37

c09.qxd

10/30/10

3:25 PM

Page 361

SEC. 9.2 Inner Product (Dot Product)

361

38. TEAM PROJECT. Geometric Applications. To
increase your skill in dealing with vectors, use vectors
to prove the following (see the figures).
(a) The diagonals of a parallelogram bisect each other.
(b) The line through the midpoints of adjacent sides
of a parallelogram bisects one of the diagonals in the
ratio 1 : 3.
(c) Obtain (b) from (a).
(d) The three medians of a triangle (the segments
from a vertex to the midpoint of the opposite side)
meet at a single point, which divides the medians in
the ratio 2 : 1.
(e) The quadrilateral whose vertices are the midpoints of the sides of an arbitrary quadrilateral is a
parallelogram.
(f) The four space diagonals of a parallelepiped meet
and bisect each other.
(g) The sum of the vectors drawn from the center of
a regular polygon to its vertices is the zero vector.

9.2

b
P
a

Team Project 38(a)
b
P

Q

0

a

Team Project 38(d)

c
d

C

b

D

B
a

A

Team Project 38(e)

Inner Product (Dot Product)
Orthogonality
The inner product or dot product can be motivated by calculating work done by a constant
force, determining components of forces, or other applications. It involves the length of
vectors and the angle between them. The inner product is a kind of multiplication of two
vectors, defined in such a way that the outcome is a scalar. Indeed, another term for inner
product is scalar product, a term we shall not use here. The definition of the inner product
is as follows.

DEFINITION

Inner Product (Dot Product) of Vectors

The inner product or dot product a • b (read “a dot b”) of two vectors a and b is
the product of their lengths times the cosine of their angle (see Fig. 178),

(1)

a • b ϭ ƒ a ƒ ƒ b ƒ cos g

if

a

a•bϭ0

if

a ϭ 0 or b ϭ 0.

0, b

0

The angle g, 0 Ϲ g Ϲ p, between a and b is measured when the initial points of
the vectors coincide, as in Fig. 178. In components, a ϭ [a1, a2, a3], b ϭ [b1, b2, b3],
and
(2)

a • b ϭ a1b1 ϩ a2b2 ϩ a3b3.

c09.qxd

10/30/10

3:25 PM

362

Page 362

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

The second line in (1) is needed because g is undefined when a ϭ 0 or b ϭ 0. The
derivation of (2) from (1) is shown below.

a

a

γ

γ

γ

a
b

b

a. b > 0

a. b = 0

b
a. b < 0

(orthogonality)

Fig. 178. Angle between vectors and value of inner product

Orthogonality. Since the cosine in (1) may be positive, 0, or negative, so may be the
inner product (Fig. 178). The case that the inner product is zero is of particular practical
interest and suggests the following concept.
A vector a is called orthogonal to a vector b if a • b ϭ 0. Then b is also orthogonal
to a, and we call a and b orthogonal vectors. Clearly, this happens for nonzero vectors
if and only if cos g ϭ 0; thus g ϭ p>2 (90°). This proves the important
THEOREM 1

Orthogonality Criterion

The inner product of two nonzero vectors is 0 if and only if these vectors are
perpendicular.

Length and Angle.

Equation (1) with b ϭ a gives a • a ϭ ƒ a ƒ 2. Hence
ƒ a ƒ ϭ 1a • a.

(3)

From (3) and (1) we obtain for the angle g between two nonzero vectors
(4)

EXAMPLE 1

cos g ϭ

a•b
ƒaƒ ƒbƒ

ϭ

a•b
.
1a • a1b • b

Inner Product. Angle Between Vectors
Find the inner product and the lengths of a ϭ [1, 2, 0] and b ϭ [3, Ϫ2, 1] as well as the angle between these
vectors.
a • b ϭ 1 # 3 ϩ 2 # 1Ϫ22 ϩ 0 # 1 ϭ Ϫ1, ƒ a ƒ ϭ 1a • a ϭ 15, ƒ b ƒ ϭ 1b • b ϭ 114, and (4)
gives the angle

Solution.

g ϭ arccos

a•b
ƒaƒ ƒbƒ

ϭ arccos (Ϫ0.11952) ϭ 1.69061 ϭ 96.865°.

᭿

c09.qxd

10/30/10

3:25 PM

Page 363

SEC. 9.2 Inner Product (Dot Product)

363

From the definition we see that the inner product has the following properties. For any
vectors a, b, c and scalars q1, q2,
(a)
(5)

(b)
(c)

(q1a ϩ q2b) • c ϭ q1a • c ϩ q1b • c

(Linearity)

a•bϭb•a
a•aм0
a • a ϭ 0 if and only if a ϭ 0

(Symmetry)
r (Positive-definiteness).

Hence dot multiplication is commutative as shown by (5b). Furthermore, it is distributive
with respect to vector addition. This follows from (5a) with q1 ϭ 1 and q2 ϭ 1:
(5a*)

(a ϩ b) • c ϭ a • c ϩ b • c

(Distributivity).

Furthermore, from (1) and ƒ cos g ƒ Ϲ 1 we see that
(6)

ƒa • bƒ Ϲ ƒaƒ ƒbƒ

(Cauchy–Schwarz inequality).

Using this and (3), you may prove (see Prob. 16)
(7)

ƒa ϩ bƒ Ϲ ƒaƒ ϩ ƒbƒ

(Triangle inequality).

Geometrically, (7) with Ͻ says that one side of a triangle must be shorter than the other
two sides together; this motivates the name of (7).
A simple direct calculation with inner products shows that
(8)

ƒ a ϩ b ƒ 2 ϩ ƒ a Ϫ b ƒ 2 ϭ 2( ƒ a ƒ 2 ϩ ƒ b ƒ 2) (Parallelogram equality).

Equations (6)–(8) play a basic role in so-called Hilbert spaces, which are abstract inner
product spaces. Hilbert spaces form the basis of quantum mechanics, for details see
[GenRef7] listed in App. 1.
Derivation of (2) from (1). We write a ϭ a1i ϩ a2 j ϩ a3k and b ϭ b1i ϩ b2 j ϩ b3k,
as in (8) of Sec. 9.1. If we substitute this into a • b and use (5a*), we first have a sum of
3 ϫ 3 ϭ 9 products
a • b ϭ a1b1i • i ϩ a1b2i • j ϩ Á ϩ a3b3k • k.
Now i, j, k are unit vectors, so that i • i ϭ j • j ϭ k • k ϭ 1 by (3). Since the coordinate
axes are perpendicular, so are i, j, k, and Theorem 1 implies that the other six of those
nine products are 0, namely, i • j ϭ j • i ϭ j • k ϭ k • j ϭ k • i ϭ i • k ϭ 0. But this
reduces our sum for a • b to (2).
᭿

c09.qxd

10/30/10

3:25 PM

364

Page 364

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

Applications of Inner Products
Typical applications of inner products are shown in the following examples and in
Problem Set 9.2.
EXAMPLE 2

Work Done by a Force Expressed as an Inner Product
This is a major application. It concerns a body on which a constant force p acts. (For a variable force, see
Sec. 10.1.) Let the body be given a displacement d. Then the work done by p in the displacement is defined as
W ϭ ƒ p ƒ ƒ d ƒ cos a ϭ p • d,

(9)

that is, magnitude ƒ p ƒ of the force times length ƒ d ƒ of the displacement times the cosine of the angle a between
p and d (Fig. 179). If a Ͻ 90°, as in Fig. 179, then W Ͼ 0. If p and d are orthogonal, then the work is zero
(why?). If a Ͼ 90°, then W Ͻ 0, which means that in the displacement one has to do work against the force.
For example, think of swimming across a river at some angle a against the current.

y
y
Rop

–p

e

p

p

x

α

c

25°

d

Fig. 179. Work done by a force

EXAMPLE 3

x

a

Fig. 180. Example 3

Component of a Force in a Given Direction
What force in the rope in Fig. 180 will hold a car of 5000 lb in equilibrium if the ramp makes an angle of 25°
with the horizontal?

Solution. Introducing coordinates as shown, the weight is a ϭ [0, Ϫ5000] because this force points

downward, in the negative y-direction. We have to represent a as a sum (resultant) of two forces, a ϭ c ϩ p,
where c is the force the car exerts on the ramp, which is of no interest to us, and p is parallel to the rope. A
vector in the direction of the rope is (see Fig. 180)
b ϭ [Ϫ1, tan 25°] ϭ [Ϫ1, 0.46631],

thus

ƒ b ƒ ϭ 1.10338,

The direction of the unit vector u is opposite to the direction of the rope so that
uϭϪ

1
ƒbƒ

b ϭ [0.90631, Ϫ0.42262].

Since ƒ u ƒ ϭ 1 and cos g Ͼ 0, we see that we can write our result as
ƒ p ƒ ϭ ( ƒ a ƒ cos g) ƒ u ƒ ϭ a • u ϭ Ϫ

a•b
ƒbƒ

ϭ

5000 # 0.46631
1.10338

ϭ 2113 [1b].

We can also note that g ϭ 90° Ϫ 25° ϭ 65° is the angle between a and p so that
ƒ p ƒ ϭ ƒ a ƒ cos g ϭ 5000 cos 65° ϭ 2113 [1b].
Answer: About 2100 lb.

᭿

c09.qxd

10/30/10

3:25 PM

Page 365

SEC. 9.2 Inner Product (Dot Product)

365

Example 3 is typical of applications that deal with the component or projection of a
vector a in the direction of a vector b ( 0). If we denote by p the length of the orthogonal
projection of a on a straight line l parallel to b as shown in Fig. 181, then
p ϭ ƒ a ƒ cos g.

(10)

Here p is taken with the plus sign if pb has the direction of b and with the minus sign if
pb has the direction opposite to b.

a

a
l

a
γ

γ

l

b

b

p
( p > 0)

( p = 0)

γ

l
p

b

( p < 0)

Fig. 181. Component of a vector a in the direction of a vector b

Multiplying (10) by ƒ b ƒ > ƒ b ƒ ϭ 1, we have a • b in the numerator and thus
pϭ

(11)

a•b
ƒbƒ

(b

0).

If b is a unit vector, as it is often used for fixing a direction, then (11) simply gives
pϭa•b

(12)

( ƒ b ƒ ϭ 1).

Figure 182 shows the projection p of a in the direction of b (as in Fig. 181) and the
projection q ϭ ƒ b ƒ cos g of b in the direction of a.
a
q
b
p

Fig. 182. Projections p of a on b and q of b on a

EXAMPLE 4

Orthonormal Basis
By definition, an orthonormal basis for 3-space is a basis {a, b, c} consisting of orthogonal unit vectors. It has
the great advantage that the determination of the coefficients in representations v ϭ l 1a ϩ l 2b ϩ l 3c of a given
vector v is very simple. We claim that l 1 ϭ a • v, l 2 ϭ b • v, l 3 ϭ c • v. Indeed, this follows simply by taking
the inner products of the representation with a, b, c, respectively, and using the orthonormality of the basis,
a • v ϭ l 1a • a ϩ l 2a • b ϩ l 3a • c ϭ l 1, etc.
For example, the unit vectors i, j, k in (8), Sec. 9.1, associated with a Cartesian coordinate system form an
᭿
orthonormal basis, called the standard basis with respect to the given coordinate system.

c09.qxd

10/30/10

3:25 PM

366
EXAMPLE 5

Page 366

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl
Orthogonal Straight Lines in the Plane
Find the straight line L 1 through the point P: (1, 3) in the xy-plane and perpendicular to the straight line
L 2 : x Ϫ 2y ϩ 2 ϭ 0; see Fig. 183.

Solution. The idea is to write a general straight line L 1 : a1x ϩ a2y ϭ c as a • r ϭ c with a ϭ [a1, a2]

0
and r ϭ [x, y], according to (2). Now the line L*1 through the origin and parallel to L 1 is a • r ϭ 0. Hence, by
Theorem 1, the vector a is perpendicular to r. Hence it is perpendicular to L*1 and also to L 1 because L 1 and
L*1 are parallel. a is called a normal vector of L 1 (and of L*1 ).
Now a normal vector of the given line x Ϫ 2y ϩ 2 ϭ 0 is b ϭ [1, Ϫ2]. Thus L 1 is perpendicular to L 2
if b • a ϭ a1 Ϫ 2a2 ϭ 0, for instance, if a ϭ [2, 1]. Hence L 1 is given by 2x ϩ y ϭ c. It passes through
P: (1, 3) when 2 # 1 ϩ 3 ϭ c ϭ 5. Answer: y ϭ Ϫ2x ϩ 5. Show that the point of intersection is
(x, y) ϭ (1.6, 1.8).
᭿

EXAMPLE 6

Normal Vector to a Plane
Find a unit vector perpendicular to the plane 4x ϩ 2y ϩ 4z ϭ Ϫ7.

Solution. Using (2), we may write any plane in space as
a • r ϭ a1x ϩ a2y ϩ a3z ϭ c

(13)
where a ϭ [a1, a2, a3]

0 and r ϭ [x, y, z]. The unit vector in the direction of a is (Fig. 184)
nϭ

1

a.

ƒaƒ

Dividing by ƒ a ƒ , we obtain from (13)
n•rϭp

(14)

where

pϭ

c

.

ƒaƒ

From (12) we see that p is the projection of r in the direction of n. This projection has the same constant value
c> ƒ a ƒ for the position vector r of any point in the plane. Clearly this holds if and only if n is perpendicular to
the plane. n is called a unit normal vector of the plane (the other being Ϫn).
Furthermore, from this and the definition of projection, it follows that ƒ p ƒ is the distance of the plane from
the origin. Representation (14) is called Hesse’s2 normal form of a plane. In our case, a ϭ [4, 2, 4], c ϭ Ϫ7,
ƒ a ƒ ϭ 6, n ϭ 16 a ϭ [23 , 13 , 23 ], and the plane has the distance 76 from the origin.
᭿

y
n

P

3

L2

2
1

|p|

L1

1

2

3

Fig. 183. Example 5

2

x

r

Fig. 184. Normal vector to a plane

LUDWIG OTTO HESSE (1811–1874), German mathematician who contributed to the theory of curves and
surfaces.

c09.qxd

10/30/10

3:25 PM

Page 367

SEC. 9.2 Inner Product (Dot Product)

367

PROBLEM SET 9.2
1–10

INNER PRODUCT

Let a ϭ [1, Ϫ3, 5], b ϭ [4, 0, 8], c ϭ [Ϫ2, 9, 1].
Find:
1. a • b, b • a, b • c
2. (Ϫ3a ϩ 5c) • b, 15(a Ϫ c) • b
3. ƒ a ƒ , ƒ 2b ƒ , ƒ Ϫc ƒ
4. ƒ a ϩ b ƒ , ƒ a ƒ ϩ ƒ b ƒ
5. ƒ b ϩ c ƒ , ƒ b ƒ ϩ ƒ c ƒ
6. ƒ a ϩ c ƒ 2 ϩ ƒ a Ϫ c ƒ 2 Ϫ 2( ƒ a ƒ 2 ϩ ƒ c ƒ 2)
7. ƒ a • c ƒ , ƒ a ƒ ƒ c ƒ
8. 5a • 13b, 65a • b
9. 15a • b ϩ 15a • c, 15a • (b ϩ c)
10. a • (b Ϫ c), (a Ϫ b) • c
11–16

What laws do Probs. 1 and 4–7 illustrate?
What does u • v ϭ u • w imply if u ϭ 0? If u 0?
Prove the Cauchy–Schwarz inequality.
Verify the Cauchy–Schwarz and triangle inequalities
for the above a and b.
15. Prove the parallelogram equality. Explain its name.
16. Triangle inequality. Prove Eq. (7). Hint. Use Eq. (3)
for ƒ a ϩ b ƒ and Eq. (6) to prove the square of Eq. (7),
then take roots.

WORK

Find the work done by a force p acting on a body if the
body is displaced along the straight segment AB from A to
B. Sketch AB and p. Show the details.
17. p ϭ [2, 5, 0], A: (1, 3, 3), B: (3, 5, 5)
18. p ϭ [Ϫ1, Ϫ2, 4], A: (0, 0, 0), B: (6, 7, 5)
19. p ϭ [0, 4, 3], A: (4, 5, Ϫ1), B: (1, 3, 0)
20. p ϭ [6, Ϫ3, Ϫ3], A: (1, 5, 2), B: (3, 4, 1)
21. Resultant. Is the work done by the resultant of two
forces in a displacement the sum of the work done
by each of the forces separately? Give proof or
counterexample.
22–30

27. Addition law. cos (a Ϫ b) ϭ cos a cos b ϩ sin a
sin b. Obtain this by using a ϭ [cos a, sin a],
b ϭ [cos b, sin b] where 0 Ϲ a Ϲ b Ϲ 2p.
28. Triangle. Find the angles of the triangle with vertices
A: (0, 0, 2), B: (3, 0, 2), and C: (1, 1, 1). Sketch the
triangle.
29. Parallelogram. Find the angles if the vertices are
(0, 0), (6, 0), (8, 3), and (2, 3).
30. Distance. Find the distance of the point A: (1, 0, 2)
from the plane P: 3x ϩ y ϩ z ϭ 9. Make a sketch.

GENERAL PROBLEMS

11.
12.
13.
14.

17–20

25. What will happen to the angle in Prob. 24 if we replace
c by nc with larger and larger n?
26. Cosine law. Deduce the law of cosines by using
vectors a, b, and a Ϫ b.

ANGLE BETWEEN VECTORS

Let a ϭ [1, 1, 0], b ϭ [3, 2, 1], and c ϭ [1, 0, 2]. Find the
angle between:
22. a, b
23. b, c
24. a ϩ c, b ϩ c

31–35
ORTHOGONALITY is particularly important,
mainly because of orthogonal coordinates, such as Cartesian
coordinates, whose natural basis [Eq. (9), Sec. 9.1], consists
of three orthogonal unit vectors.
31. For what values of a1 are [a1, 4, 3] and [3, Ϫ2, 12]
orthogonal?
32. Planes. For what c are 3x ϩ z ϭ 5 and 8x Ϫ y ϩ
cz ϭ 9 orthogonal?
33. Unit vectors. Find all unit vectors a ϭ [a1, a2] in the
plane orthogonal to [4, 3].
34. Corner reflector. Find the angle between a light ray
and its reflection in three orthogonal plane mirrors,
known as corner reflector.
35. Parallelogram. When will the diagonals be orthogonal? Give a proof.
36–40

COMPONENT IN THE DIRECTION
OF A VECTOR

Find the component of a in the direction of b. Make a
sketch.
36. a ϭ [1, 1, 1], b ϭ [2, 1, 3]
37. a ϭ [3, 4, 0], b ϭ [4, Ϫ3, 2]
38. a ϭ [8, 2, 0], b ϭ [Ϫ4, Ϫ1, 0]
39. When will the component (the projection) of a in the
direction of b be equal to the component (the
projection) of b in the direction of a? First guess.
40. What happens to the component of a in the direction
of b if you change the length of b?

c09.qxd

10/30/10

3:25 PM

368

9.3

Page 368

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

Vector Product (Cross Product)
We shall define another form of multiplication of vectors, inspired by applications, whose
result will be a vector. This is in contrast to the dot product of Sec. 9.2 where multiplication
resulted in a scalar. We can construct a vector v that is perpendicular to two vectors a
and b, which are two sides of a parallelogram on a plane in space as indicated in Fig. 185,
such that the length ƒ v ƒ is numerically equal to the area of that parallelogram. Here then
is the new concept.

DEFINITION

Vector Product (Cross Product, Outer Product) of Vectors

The vector product or cross product a ؋ b (read “a cross b”) of two vectors a
and b is the vector v denoted by
vϭa؋b
I. If a ϭ 0 or b ϭ 0, then we define v ϭ a ؋ b ϭ 0.
II. If both vectors are nonzero vectors, then vector v has the length
ƒ v ƒ ϭ ƒ a ؋ b ƒ ϭ ƒ a ƒ ƒ b ƒ sin g,

(1)

where g is the angle between a and b as in Sec. 9.2.
Furthermore, by design, a and b form the sides of a parallelogram on a plane
in space. The parallelogram is shaded in blue in Fig. 185. The area of this blue
parallelogram is precisely given by Eq. (1), so that the length ƒ v ƒ of the vector
v is equal to the area of that parallelogram.
III. If a and b lie in the same straight line, i.e., a and b have the same or opposite
directions, then g is 0° or 180° so that sin g ϭ 0. In that case ƒ v ƒ ϭ 0 so that
v ϭ a ؋ b ϭ 0.
IV. If cases I and III do not occur, then v is a nonzero vector. The direction of
v ϭ a ؋ b is perpendicular to both a and b such that a, b, v—precisely in this
order (!)—form a right-handed triple as shown in Figs. 185–187 and explained
below.
Another term for vector product is outer product.
Remark. Note that I and III completely characterize the exceptional case when the cross
product is equal to the zero vector, and II and IV the regular case where the cross product
is perpendicular to two vectors.
Just as we did with the dot product, we would also like to express the cross product in
components. Let a ϭ [a1, a2, a3] and b ϭ [b1, b2, b3]. Then v ϭ [v1, v2, v3] ϭ a ؋ b has
the components
(2)

v1 ϭ a2b3 Ϫ a3b2,

v2 ϭ a3b1 Ϫ a1b3,

v3 ϭ a1b2 Ϫ a2b1.

Here the Cartesian coordinate system is right-handed, as explained below (see also
Fig. 188). (For a left-handed system, each component of v must be multiplied by Ϫ1.
Derivation of (2) in App. 4.)

c09.qxd

10/30/10

3:25 PM

Page 369

SEC. 9.3 Vector Product (Cross Product)

369

Right-Handed Triple. A triple of vectors a, b, v is right-handed if the vectors in the
given order assume the same sort of orientation as the thumb, index finger, and middle
finger of the right hand when these are held as in Fig. 186. We may also say that if a is
rotated into the direction of b through the angle g (Ͻp), then v advances in the same
direction as a right-handed screw would if turned in the same way (Fig. 187).

v

v

b
v=a×b
b
b

γ
a

a

a

Fig. 185. Vector product

Fig. 186. Right-handed
triple of vectors a, b, v

Fig. 187. Right-handed
screw

Right-Handed Cartesian Coordinate System. The system is called right-handed if
the corresponding unit vectors i, j, k in the positive directions of the axes (see Sec. 9.1)
form a right-handed triple as in Fig. 188a. The system is called left-handed if the sense
of k is reversed, as in Fig. 188b. In applications, we prefer right-handed systems.

z

k

j

i

j

i
k

x

x

y

y

z
(a) Right-handed

(b) Left-handed

Fig. 188. The two types of Cartesian coordinate systems

How to Memorize (2). If you know second- and third-order determinants, you see that
(2) can be written
(2*)

v1 ϭ 2

a2

a3

b2

b3

2,

v2 ϭ Ϫ2

a1

a3

b1

b3

2 ϭ ϩ2

a3

a1

b3

b1

2,

v3 ϭ 2

a1

a2

b1

b2

2

c09.qxd

10/30/10

3:25 PM

Page 370

370

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

and v ϭ [v1, v2, v3] ϭ v1i ϩ v2 j ϩ v3k is the expansion of the following symbolic
determinant by its first row. (We call the determinant “symbolic” because the first row
consists of vectors rather than of numbers.)
i
(2**)

j

k

v ϭ a ؋ b ϭ 3 a1

a2

a3 3 ϭ 2

b1

b2

b3

a2

a3

b2

b3

2iϪ2

a1

a3

b1

b3

2 jϩ2

a1

a2

b1

b2

2 k.

For a left-handed system the determinant has a minus sign in front.
EXAMPLE 1

Vector Product
For the vector product v ϭ a ؋ b of a ϭ [1, 1, 0] and b ϭ [3, 0, 0] in right-handed coordinates we obtain
from (2)
v1 ϭ 0,

v2 ϭ 0,

v3 ϭ 1 # 0 Ϫ 1 # 3 ϭ Ϫ3.

We confirm this by (2**):
i

j

k

v ϭ a ؋ b ϭ 31

1

03ϭ2

3

0

1

0

0

0

2iϪ2

1

0

3

0

2jϩ2

1

1

3

0

2 k ϭ Ϫ3k ϭ [0, 0, Ϫ3].

0

To check the result in this simple case, sketch a, b, and v. Can you see that two vectors in the xy-plane must
always have their vector product parallel to the z-axis (or equal to the zero vector)?
᭿

EXAMPLE 2

Vector Products of the Standard Basis Vectors
i؋jϭ

(3)

k,

j ؋ i ϭ Ϫk,

j؋kϭ

i,

k ؋ j ϭ Ϫi,

k؋iϭ

j

i ؋ k ϭ Ϫj.

᭿

We shall use this in the next proof.

THEOREM 1

General Properties of Vector Products

(a) For every scalar l,
(la) ؋ b ϭ l(a ؋ b) ϭ a ؋ (lb).

(4)

(b) Cross multiplication is distributive with respect to vector addition; that is,
a×b

b

(a)
(5)

b×a

a ؋ (b ϩ c) ϭ (a ؋ b) ϩ (a ؋ c),

( b) (a ϩ b) ؋ c ϭ (a ؋ c) ϩ (b ؋ c).

a

Fig. 189.
Anticommutativity
of cross
multiplication

(c) Cross multiplication is not commutative but anticommutative; that is,
(6)

b ؋ a ϭ Ϫ(a ؋ b)

(Fig. 189).

c09.qxd

10/30/10

3:25 PM

Page 371

SEC. 9.3 Vector Product (Cross Product)

371

(d) Cross multiplication is not associative; that is, in general,
a ؋ (b ؋ c)

(7)

(a ؋ b) ؋ c

so that the parentheses cannot be omitted.
PROOF

Equation (4) follows directly from the definition. In (5a), formula (2*) gives for the first
component on the left

2

a2

a3

b2 ϩ c2

b3 ϩ c3

2 ϭ a2(b3 ϩ c3) Ϫ a3(b2 ϩ c2)
ϭ (a2b3 Ϫ a3b2) ϩ (a2c3 Ϫ a3c2)
ϭ2

a2

a3

b2

b3

2ϩ2

a2

a3

c2

c3

2.

By (2*) the sum of the two determinants is the first component of (a ؋ b) ϩ (a ؋ c), the
right side of (5a). For the other components in (5a) and in 5(b), equality follows by the
same idea.
Anticommutativity (6) follows from (2**) by noting that the interchange of Rows 2
and 3 multiplies the determinant by Ϫ1. We can confirm this geometrically if we set
a ؋ b ϭ v and b ؋ a ϭ w; then ƒ v ƒ ϭ ƒ w ƒ by (1), and for b, a, w to form a right-handed
triple, we must have w ϭ Ϫv.
Finally, i ؋ (i ؋ j) ϭ i ؋ k ϭ Ϫj, whereas (i ؋ i) ؋ j ϭ 0 ؋ j ϭ 0 (see Example 2).
This proves (7).
᭿

Typical Applications of Vector Products
EXAMPLE 3

Moment of a Force
In mechanics the moment m of a force p about a point Q is defined as the product m ϭ ƒ p ƒ d, where d is the
(perpendicular) distance between Q and the line of action L of p (Fig. 190). If r is the vector from Q to any
point A on L, then d ϭ ƒ r ƒ sin g, as shown in Fig. 190, and
m ϭ ƒ r ƒ ƒ p ƒ sin g.
Since g is the angle between r and p, we see from (1) that m ϭ ƒ r ؋ p ƒ . The vector
mϭr؋p

(8)

is called the moment vector or vector moment of p about Q. Its magnitude is m. If m 0, its direction is
that of the axis of the rotation about Q that p has the tendency to produce. This axis is perpendicular to both
r and p.
᭿
L

p

r

Q
d

γ
A

Fig. 190. Moment of a force p

c09.qxd

10/30/10

3:25 PM

372
EXAMPLE 4

Page 372

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl
Moment of a Force
Find the moment of the force p about the center Q of a wheel, as given in Fig. 191.

Solution. Introducing coordinates as shown in Fig. 191, we have
p ϭ [1000 cos 30°, 1000 sin 30°,

0] ϭ [866, 500,

r ϭ [0, 1.5,

0],

0].

(Note that the center of the wheel is at y ϭ Ϫ1.5 on the y-axis.) Hence (8) and (2**) give
i
mϭr؋pϭ3 0
866

j

k
0 3 ϭ 0i Ϫ 0j ϩ 2

1.5
500

0

1.5

866

500

2 k ϭ [0, 0, Ϫ1299].

0

This moment vector m is normal, i.e., perpendicular to the plane of the wheel. Hence it has the direction of the
axis of rotation about the center Q of the wheel that the force p has the tendency to produce. The moment m
points in the negative z-direction, This is, the direction in which a right-handed screw would advance if turned
in that way.
᭿
y

|p| = 1000 lb
30°
x
1.5 ft

Q

Fig. 191. Moment of a force p

EXAMPLE 5

Velocity of a Rotating Body
A rotation of a rigid body B in space can be simply and uniquely described by a vector w as follows. The
direction of w is that of the axis of rotation and such that the rotation appears clockwise if one looks from the
initial point of w to its terminal point. The length of w is equal to the angular speed v (Ͼ0) of the rotation,
that is, the linear (or tangential) speed of a point of B divided by its distance from the axis of rotation.
Let P be any point of B and d its distance from the axis. Then P has the speed vd. Let r be the position vector
of P referred to a coordinate system with origin 0 on the axis of rotation. Then d ϭ ƒ r ƒ sin g, where g is the
angle between w and r. Therefore,
vd ϭ ƒ w ƒ ƒ r ƒ sin g ϭ ƒ w ؋ r ƒ .
From this and the definition of vector product we see that the velocity vector v of P can be represented in the
form (Fig. 192)
v ϭ w ؋ r.

(9)

᭿

This simple formula is useful for determining v at any point of B.
w

d
P

v
w

γ

r
0

Fig. 192. Rotation of a rigid body

r

c09.qxd

10/30/10

3:25 PM

Page 373

SEC. 9.3 Vector Product (Cross Product)

373

Scalar Triple Product
Certain products of vectors, having three or more factors, occur in applications. The most
important of these products is the scalar triple product or mixed product of three vectors
a, b, c.
(a b c) ϭ a • (b ؋ c).

(10*)

The scalar triple product is indeed a scalar since (10*) involves a dot product, which in
turn is a scalar. We want to express the scalar triple product in components and as a thirdorder determinant. To this end, let a ϭ [a1, a2, a3], b ϭ [b1, b2, b3], and c ϭ [c1, c2, c3].
Also set b ؋ c ϭ v ϭ [v1, v2, v3]. Then from the dot product in components [formula
(2) in Sec. 9.2] and from (2*) with b and c instead of a and b we first obtain
a • (b ؋ c) ϭ a • v ϭ a1v1 ϩ a2v2 ϩ a3v3
ϭ a1 2

b2

b3

c2

c3

2 ϩ a2 2

b3

b1

c3

c1

2 ϩ a3 2

b1

b2

c1

c2

2.

The sum on the right is the expansion of a third-order determinant by its first row. Thus
we obtain the desired formula for the scalar triple product, that is,

(10)

a1

a2

a3

(a b c) ϭ a • (b ؋ c) ϭ 3 b1

b2

b3 3 .

c1

c2

c3

The most important properties of the scalar triple product are as follows.

THEOREM 2

Properties and Applications of Scalar Triple Products

(a) In (10) the dot and cross can be interchanged:
(11)

(a b c) ϭ a • (b ؋ c) ϭ (a ؋ b) • c.

(b) Geometric interpretation. The absolute value ƒ (a b c) ƒ of (10) is the
volume of the parallelepiped (oblique box) with a, b, c as edge vectors (Fig. 193).
(c) Linear independence. Three vectors in R 3 are linearly independent if
and only if their scalar triple product is not zero.

PROOF

(a) Dot multiplication is commutative, so that by (10)
c1

c2

c3

(a ؋ b) • c ϭ c • (a ؋ b) ϭ 3 a1

a2

a3 3 .

b1

b2

b3

c09.qxd

10/30/10

3:25 PM

374

Page 374

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

From this we obtain the determinant in (10) by interchanging Rows 1 and 2 and in the
result Rows 2 and 3. But this does not change the value of the determinant because each
interchange produces a factor Ϫ1, and (Ϫ1)(Ϫ1) ϭ 1. This proves (11).
(b) The volume of that box equals the height h ϭ ƒ a ƒ ƒ cos g ƒ (Fig. 193) times the area
of the base, which is the area ƒ b ؋ c ƒ of the parallelogram with sides b and c. Hence the
volume is
ƒ a ƒ ƒ b ؋ c ƒ ƒ cos g ƒ ϭ ƒ a • (b ؋ c) ƒ

(Fig. 193)

as given by the absolute value of (11).
(c) Three nonzero vectors, whose initial points coincide, are linearly independent if and
only if the vectors do not lie in the same plane nor lie on the same straight line.
This happens if and only if the triple product in (b) is not zero, so that the independence
criterion follows. (The case of one of the vectors being the zero vector is trivial.)

b×c
a
h

β

c
b

Fig. 193. Geometric interpretation of a scalar triple product

EXAMPLE 6

Tetrahedron
A tetrahedron is determined by three edge vectors a, b, c, as indicated in Fig. 194. Find the volume of the tetrahedron
in Fig. 194, when a ϭ [2, 0, 3], b ϭ [0, 4, 1], c ϭ [5, 6, 0].

Solution. The volume V of the parallelepiped with these vectors as edge vectors is the absolute value of the
scalar triple product
a

c

b

Fig. 194.
Tetrahedron

2

0

3

(a b c) ϭ 3 0

4

13 ϭ 2 2

5

6

4

1

6

0

2ϩ32

0

4

5

6

2 ϭ Ϫ12 Ϫ 60 ϭ Ϫ72.

0

Hence V ϭ 72. The minus sign indicates that if the coordinates are right-handed, the triple a, b, c is left-handed.
The volume of a tetrahedron is 16 of that of the parallelepiped (can you prove it?), hence 12.
Can you sketch the tetrahedron, choosing the origin as the common initial point of the vectors? What are the
coordinates of the four vertices?
᭿

This is the end of vector algebra (in space R3 and in the plane). Vector calculus
(differentiation) begins in the next section.

PROBLEM SET 9.3
1–10

GENERAL PROBLEMS

1. Give the details of the proofs of Eqs. (4) and (5).
2. What does a ؋ b ϭ a ؋ c with a 0 imply?
3. Give the details of the proofs of Eqs. (6) and (11).

4. Lagrange’s identity for ƒ a ؋ b ƒ . Verify it for
a ϭ [3, 4, 2] and b ϭ [1, 0, 2]. Prove it, using
sin2 g ϭ 1 Ϫ cos2 g. The identity is
(12)

ƒ a ؋ b ƒ ϭ 2(a • a) (b • b) Ϫ (a • b)2.

c09.qxd

10/30/10

3:25 PM

Page 375

SEC. 9.4 Vector and Scalar Functions and Their Fields. Vector Calculus: Derivatives
5. What happens in Example 3 of the text if you replace
p by Ϫp?
6. What happens in Example 5 if you choose a P at
distance 2d from the axis of rotation?
7. Rotation. A wheel is rotating about the y-axis with
angular speed v ϭ 20 sec؊1. The rotation appears
clockwise if one looks from the origin in the positive
y-direction. Find the velocity and speed at the point
[8, 6, 0]. Make a sketch.
8. Rotation. What are the velocity and speed in Prob. 7
at the point (4, 2, Ϫ2) if the wheel rotates about the
line y ϭ x, z ϭ 0 with v ϭ 10 sec؊1?
9. Scalar triple product. What does (a b c) ϭ 0 imply
with respect to these vectors?
10. WRITING REPORT. Summarize the most important
applications discussed in this section. Give examples.
No proofs.
11–23

VECTOR AND SCALAR
TRIPLE PRODUCTS

With respect to right-handed Cartesian coordinates,
let a ϭ [2, 1, 0], b ϭ [Ϫ3, 2, 0], c ϭ [1, 4, Ϫ2], and
d ϭ [5, Ϫ1, 3]. Showing details, find:
11. a ؋ b, b ؋ a, a • b
12. 3c ؋ 5d, 15d ؋ c, 15d • c, 15c • d
13. c ؋ (a ϩ b), a ؋ c ϩ b ؋ c
14. 4b ؋ 3c ϩ 12c ؋ b
15. (a ϩ d) ؋ (d ϩ a)
16. (b ؋ c) • d, b • (c ؋ d)
17. (b ؋ c) ؋ d, b ؋ (c ؋ d)
18. (a ؋ b) ؋ a, a ؋ (b ؋ a)
19. (i j k), (i k j)
20. (a ؋ b) ؋ (c ؋ d), (a b d)c Ϫ (a b c)d
21. 4b ؋ 3c, 12 ƒ b ؋ c ƒ , 12 ƒ c ؋ b ƒ
22. (a Ϫ b c Ϫ b d Ϫ b), (a c d)
23. b ؋ b, (b Ϫ c) ؋ (c Ϫ b), b • b
24. TEAM PROJECT. Useful Formulas for Three and
Four Vectors. Prove (13)–(16), which are often useful
in practical work, and illustrate each formula with two

9.4

375

examples. Hint. For (13) choose Cartesian coordinates
such that d ϭ [d1, 0, 0] and c ϭ [c1, c2, 0]. Show that
each side of (13) then equals [Ϫb2c2d1, b1c2d1, 0], and
give reasons why the two sides are then equal in any
Cartesian coordinate system. For (14) and (15) use (13).
(13) b ؋ (c ؋ d) ϭ (b • d)c Ϫ (b • c)d
(14) (a ؋ b) ؋ (c ؋ d) ϭ (a b d)c Ϫ (a b c)d
(15) (a ؋ b) • (c ؋ d) ϭ (a • c)(b • d) Ϫ (a • d)(b • c)
(16) (a b c) ϭ (b c a) ϭ (c a b)
ϭ Ϫ(c b a) ϭ Ϫ(a c b)
25–35

APPLICATIONS

25. Moment m of a force p. Find the moment vector m
and m of p ϭ [2, 3, 0] about Q: (2, 1, 0) acting on a
line through A: (0, 3, 0). Make a sketch.
26. Moment. Solve Prob. 25 if p ϭ [1, 0, 3], Q: (2, 0, 3),
and A: (4, 3, 5).
27. Parallelogram. Find the area if the vertices are (4, 2,
0), (10, 4, 0), (5, 4, 0), and (11, 6, 0). Make a sketch.
28. A remarkable parallelogram. Find the area of the
quadrangle Q whose vertices are the midpoints of
the sides of the quadrangle P with vertices A: (2, 1, 0),
B: (5, Ϫ1. 0), C: (8, 2, 0), and D: (4, 3, 0). Verify that
Q is a parallelogram.
29. Triangle. Find the area if the vertices are (0, 0, 1),
(2, 0, 5), and (2, 3, 4).
30. Plane. Find the plane through the points A: (1, 2, 14 ),
B: (4, 2, Ϫ2), and C: (0, 8, 4).
31. Plane. Find the plane through (1, 3, 4), (1, Ϫ2, 6), and
(4, 0, 7).
32. Parallelepiped. Find the volume if the edge vectors
are i ϩ j, Ϫ2i ϩ 2k, and Ϫ2i Ϫ 3k. Make a sketch.
33. Tetrahedron. Find the volume if the vertices are
(1, 1, 1), (5, Ϫ7, 3), (7, 4, 8), and (10, 7, 4).
34. Tetrahedron. Find the volume if the vertices are
(1, 3, 6), (3, 7, 12), (8, 8, 9), and (2, 2, 8).
35. WRITING PROJECT. Applications of Cross
Products. Summarize the most important applications
we have discussed in this section and give a few simple
examples. No proofs.

Vector and Scalar Functions and Their Fields.
Vector Calculus: Derivatives
Our discussion of vector calculus begins with identifying the two types of functions on which
it operates. Let P be any point in a domain of definition. Typical domains in applications
are three-dimensional, or a surface or a curve in space. Then we define a vector function
v, whose values are vectors, that is,
v ϭ v(P) ϭ [v1(P), v2(P), v3(P)]

c09.qxd

10/30/10

3:25 PM

376

Page 376

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

that depends on points P in space. We say that a vector function defines a vector field in
a domain of definition. Typical domains were just mentioned. Examples of vector fields
are the field of tangent vectors of a curve (shown in Fig. 195), normal vectors of a surface
(Fig. 196), and velocity field of a rotating body (Fig. 197). Note that vector functions may
also depend on time t or on some other parameters.
Similarly, we define a scalar function f, whose values are scalars, that is,
f ϭ f (P)
that depends on P. We say that a scalar function defines a scalar field in that threedimensional domain or surface or curve in space. Two representative examples of scalar
fields are the temperature field of a body and the pressure field of the air in Earth’s
atmosphere. Note that scalar functions may also depend on some parameter such as
time t.
Notation. If we introduce Cartesian coordinates x, y, z, then, instead of writing v(P) for
the vector function, we can write
v(x, y, z) ϭ [v1(x, y, z), v2(x, y, z), v3(x, y, z)].

Fig. 195. Field of tangent
vectors of a curve

Fig. 196. Field of normal
vectors of a surface

We have to keep in mind that the components depend on our choice of coordinate system,
whereas a vector field that has a physical or geometric meaning should have magnitude
and direction depending only on P, not on the choice of coordinate system.
Similarly, for a scalar function, we write
f (P) ϭ f (x, y, z).
We illustrate our discussion of vector functions, scalar functions, vector fields, and scalar
fields by the following three examples.
EXAMPLE 1

Scalar Function (Euclidean Distance in Space)
The distance f(P) of any point P from a fixed point P0 in space is a scalar function whose domain of definition
is the whole space. f (P) defines a scalar field in space. If we introduce a Cartesian coordinate system and P0
has the coordinates x 0, y0, z 0, then f is given by the well-known formula
f (P) ϭ f (x, y, z) ϭ 2(x Ϫ x 0)2 ϩ (y Ϫ y0)2 ϩ (z Ϫ z 0)2
where x, y, z are the coordinates of P. If we replace the given Cartesian coordinate system with another such
system by translating and rotating the given system, then the values of the coordinates of P and P0 will in general
change, but f (P) will have the same value as before. Hence f (P) is a scalar function. The direction cosines of
the straight line through P and P0 are not scalars because their values depend on the choice of the coordinate
᭿
system.

c09.qxd

10/30/10

3:25 PM

Page 377

SEC. 9.4 Vector and Scalar Functions and Their Fields. Vector Calculus: Derivatives
EXAMPLE 2

377

Vector Field (Velocity Field)
At any instant the velocity vectors v(P) of a rotating body B constitute a vector field, called the velocity field
of the rotation. If we introduce a Cartesian coordinate system having the origin on the axis of rotation, then (see
Example 5 in Sec. 9.3)
(1)

v(x, y, z) ϭ w ؋ r ϭ w ؋ [x, y, z] ϭ w ؋ (xi ϩ yj ϩ zk)

where x, y, z are the coordinates of any point P of B at the instant under consideration. If the coordinates are
such that the z-axis is the axis of rotation and w points in the positive z-direction, then w ϭ vk and
i

j

k

v ϭ 40

0

v 4 ϭ v[Ϫy, x, 0] ϭ v(Ϫyi ϩ xj).

x

y

z

An example of a rotating body and the corresponding velocity field are shown in Fig. 197.

᭿

Fig. 197. Velocity field of a rotating body

EXAMPLE 3

Vector Field (Field of Force, Gravitational Field)
Let a particle A of mass M be fixed at a point P0 and let a particle B of mass m be free to take up various positions
P in space. Then A attracts B. According to Newton’s law of gravitation the corresponding gravitational force p
is directed from P to P0, and its magnitude is proportional to 1>r 2, where r is the distance between P and P0, say,
ƒpƒ ϭ

(2)

c
r2

c ϭ GMm.

,

Here G ϭ 6.67 # 10؊8 cm3>(g # sec 2) is the gravitational constant. Hence p defines a vector field in space. If
we introduce Cartesian coordinates such that P0 has the coordinates x 0, y0, z 0 and P has the coordinates x, y, z,
then by the Pythagorean theorem,
r ϭ 2(x Ϫ x 0)2 ϩ ( y Ϫ y0)2 ϩ (z Ϫ z 0)2

(м0).

Assuming that r Ͼ 0 and introducing the vector
r ϭ [x Ϫ x 0,

y Ϫ y0,

z Ϫ z 0] ϭ (x Ϫ x 0)i ϩ ( y Ϫ y0)j ϩ (z Ϫ z 0)k,

we have ƒ r ƒ ϭ r, and (Ϫ1>r) r is a unit vector in the direction of p; the minus sign indicates that p is directed
from P to P0 (Fig. 198). From this and (2) we obtain

(3)

1
c
x Ϫ x0
p ϭ ƒ p ƒ aϪ rb ϭ Ϫ 3 r ϭ c Ϫc
,
r
r
r3
ϭ Ϫc

x Ϫ x0
r

3

iϪc

y Ϫ y0
r

3

Ϫc

y Ϫ y0
,
r3

jϪc

This vector function describes the gravitational force acting on B.

z Ϫ z0
r3

Ϫc

z Ϫ z0
d
r3

k.

᭿

c09.qxd

10/30/10

378

3:25 PM

Page 378

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

P

P0

Fig. 198. Gravitational field in Example 3

Vector Calculus
The student may be pleased to learn that many of the concepts covered in (regular)
calculus carry over to vector calculus. Indeed, we show how the basic concepts of
convergence, continuity, and differentiability from calculus can be defined for vector
functions in a simple and natural way. Most important of these is the derivative of a
vector function.
Convergence. An infinite sequence of vectors a(n), n ϭ 1, 2, Á , is said to converge if
there is a vector a such that
(4)

lim ƒ a(n) Ϫ a ƒ ϭ 0.

n:ϱ

a is called the limit vector of that sequence, and we write
(5)

lim a(n) ϭ a.

n:ϱ

If the vectors are given in Cartesian coordinates, then this sequence of vectors converges
to a if and only if the three sequences of components of the vectors converge to the
corresponding components of a. We leave the simple proof to the student.
Similarly, a vector function v(t) of a real variable t is said to have the limit l as t
approaches t 0, if v(t) is defined in some neighborhood of t 0 (possibly except at t 0) and
(6)

lim ƒ v(t) Ϫ l ƒ ϭ 0.

t:t0

Then we write
(7)

lim v(t) ϭ l.

t:t0

Here, a neighborhood of t0 is an interval (segment) on the t-axis containing t 0 as an interior
point (not as an endpoint).
Continuity. A vector function v(t) is said to be continuous at t ϭ t 0 if it is defined in
some neighborhood of t 0 (including at t 0 itself!) and
(8)

lim v(t) ϭ v(t 0).

t:t0

c09.qxd

10/30/10

3:25 PM

Page 379

SEC. 9.4 Vector and Scalar Functions and Their Fields. Vector Calculus: Derivatives

379

If we introduce a Cartesian coordinate system, we may write
v(t) ϭ [v1(t), v2(t), v3(t)] ϭ v1(t)i ϩ v2(t)j ϩ v3(t)k.
Then v(t) is continuous at t 0 if and only if its three components are continuous at t 0.
We now state the most important of these definitions.
DEFINITION

Derivative of a Vector Function

A vector function v(t) is said to be differentiable at a point t if the following limit
exists:
v r (t) ϭ lim

(9)

v(t ϩ ¢t) Ϫ v(t)

¢t:0

.

¢t

This vector v r (t) is called the derivative of v(t). See Fig. 199.

v′(t)
v(t + Δt)
v(t)

Fig. 199. Derivative of a vector function

In components with respect to a given Cartesian coordinate system,
v r (t) ϭ [v1r (t), v2r (t), v3r (t)].

(10)

Hence the derivative v r (t) is obtained by differentiating each component separately. For
instance, if v ϭ [t, t 2, 0], then v r ϭ [1, 2t, 0].
Equation (10) follows from (9) and conversely because (9) is a “vector form” of the
usual formula of calculus by which the derivative of a function of a single variable is
defined. [The curve in Fig. 199 is the locus of the terminal points representing v(t) for
values of the independent variable in some interval containing t and t ϩ ¢t in (9)]. It
follows that the familiar differentiation rules continue to hold for differentiating vector
functions, for instance,
(cv) r ϭ cv r

(c constant),

(u ϩ v) r ϭ u r ϩ v r
and in particular
(11)

(u • v) r ϭ u r • v ϩ u • v r

(12)

(u ؋ v) r ϭ u r ؋ v ϩ u ؋ v r

(13)

(u

v

w) r ϭ (u r

v

w) ϩ (u v r

w) ϩ (u v

w r ).

c09.qxd

10/30/10

3:25 PM

380

Page 380

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

The simple proofs are left to the student. In (12), note the order of the vectors carefully
because cross multiplication is not commutative.
EXAMPLE 4

Derivative of a Vector Function of Constant Length
Let v(t) be a vector function whose length is constant, say, ƒ v(t) ƒ ϭ c. Then ƒ v ƒ 2 ϭ v • v ϭ c2, and
(v • v) r ϭ 2v • v r ϭ 0, by differentiation [see (11)]. This yields the following result. The derivative of a vector
᭿
function v(t) of constant length is either the zero vector or is perpendicular to v(t).

Partial Derivatives of a Vector Function
Our present discussion shows that partial differentiation of vector functions of two or more
variables can be introduced as follows. Suppose that the components of a vector function
v ϭ [v1,

v2,

v3] ϭ v1i ϩ v2 j ϩ v3k

are differentiable functions of n variables t 1, Á , t n. Then the partial derivative of v with
respect to t m is denoted by 0v>0t m and is defined as the vector function
0v
0v
0v
0v
ϭ 1 i ϩ 2 j ϩ 3 k.
0t m
0t m
0t m
0t m
Similarly, second partial derivatives are
0 2v
0 2v1
0 2v2
0 2v3
ϭ
iϩ
jϩ
k,
0t l 0t m
0t l 0t m
0t l 0t m
0t l 0t m
and so on.
EXAMPLE 5

Partial Derivatives
Let r(t 1, t 2) ϭ a cos t 1 i ϩ a sin t 1 j ϩ t 2 k. Then

0r
ϭ Ϫa sin t 1 i ϩ a cos t 1 j and
0t 1

0r
ϭ k.
0t 2

᭿

Various physical and geometric applications of derivatives of vector functions will be
discussed in the next sections as well as in Chap. 10.

PROBLEM SET 9.4
1–8

SCALAR FIELDS IN THE PLANE

Let the temperature T in a body be independent of z so that
it is given by a scalar function T ϭ T(x, t). Identify the
isotherms T(x, y) ϭ const. Sketch some of them.
1. T ϭ x 2 Ϫ y 2
2. T ϭ xy
3. T ϭ 3x Ϫ 4y
4. T ϭ arctan (y>x)
2
2
5. T ϭ y>(x ϩ y )
6. T ϭ x>(x 2 ϩ y 2)
2
2
7. T ϭ 9x ϩ 4y
8. CAS PROJECT. Scalar Fields in the Plane. Sketch
or graph isotherms of the following fields and describe
what they look like.

(a)
(c)
(e)
(g)
9–14

x 2 Ϫ 4x Ϫ y 2
cos x sinh y
ex sin y
x 4 Ϫ 6x 2y 2 ϩ y 4

(b)
(d)
(f)
(h)

x 2y Ϫ y 3>3
sin x sinh y
e2x cos 2y
x 2 Ϫ 2x Ϫ y 2

SCALAR FIELDS IN SPACE

What kind of surfaces are the level surfaces f (x, y, z) ϭ
const?
9. f ϭ 4x Ϫ 3y ϩ 2z
10. f ϭ 9(x 2 ϩ y 2) ϩ z 2
2
2
11. f ϭ 5x ϩ 2y
12. f ϭ z Ϫ 2x 2 ϩ y 2
13. f ϭ z Ϫ (x 2 ϩ y 2)
14. f ϭ x Ϫ y 2

c09.qxd

10/30/10

3:25 PM

Page 381

SEC. 9.5 Curves. Arc Length. Curvature. Torsion
15–20

381

VECTOR FIELDS

22–25

9.5

DIFFERENTIATION

22. Find the first and second derivatives of r ϭ [3 cos 2t,
3 sin 2t, 4t].

Sketch figures similar to Fig. 198. Try to interpet the field
of v as a velocity field.
15. v ϭ i ϩ j
16. v ϭ Ϫyi ϩ xj
17. v ϭ xj
18. v ϭ xi ϩ yj
19. v ϭ xi Ϫ yj
20. v ϭ yi Ϫ xj
21. CAS PROJECT. Vector Fields. Plot by arrows:
(a) v ϭ [x, x 2]
(b) v ϭ [1>y, 1>x]
2
2
(c) v ϭ [cos x, sin x]
(d) v ϭ e؊(x ϩy ) [x, Ϫy]

23. Prove (11)–(13). Give two typical examples for each
formula.
24. Find the first partial derivatives of v1 ϭ [ex cos y,
ex sin y] and v2 ϭ [cos x cosh y, Ϫsin x sinh y].
25. WRITING PROJECT. Differentiation of Vector
Functions. Summarize the essential ideas and facts
and give examples of your own.

Curves. Arc Length. Curvature. Torsion
Vector calculus has important applications to curves (Sec. 9.5) and surfaces (to be covered
in Sec. 10.5) in physics and geometry. The application of vector calculus to geometry is
a field known as differential geometry. Differential geometric methods are applied
to problems in mechanics, computer-aided as well as traditional engineering design,
geodesy, geography, space travel, and relativity theory. For details, see [GenRef8] and
[GenRef9] in App. 1.
Bodies that move in space form paths that may be represented by curves C. This and
other applications show the need for parametric representations of C with parameter t,
which may denote time or something else (see Fig. 200). A typical parametric representation
is given by
(1)

r (t) ϭ [x (t), y (t), z (t)] ϭ x (t)i ϩ y (t)j ϩ z (t)k.

C

r (t)

z

x

y

Fig. 200. Parametric representation of a curve

Here t is the parameter and x, y, z are Cartesian coordinates, that is, the usual rectangular
coordinates as shown in Sec. 9.1. To each value t ϭ t 0, there corresponds a point of C
with position vector r (t 0) whose coordinates are x (t 0), y (t 0), z (t 0). This is illustrated
in Figs. 201 and 202.
The use of parametric representations has key advantages over other representations
that involve projections into the xy-plane and xz-plane or involve a pair of equations with
y or with z as independent variable. The projections look like this:
˛

(2)

y ϭ f (x),

z ϭ g (x).

c09.qxd

10/30/10

3:25 PM

382

Page 382

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

The advantages of using (1) instead of (2) are that, in (1), the coordinates x, y, z all
play an equal role, that is, all three coordinates are dependent variables. Moreover, the
parametric representation (1) induces an orientation on C. This means that as we
increase t, we travel along the curve C in a certain direction. The sense of increasing
t is called the positive sense on C. The sense of decreasing t is then called the negative
sense on C, given by (1).
Examples 1–4 give parametric representations of several important curves.
EXAMPLE 1

Circle. Parametric Representation. Positive Sense
The circle x 2 ϩ y 2 ϭ 4, z ϭ 0 in the xy-plane with center 0 and radius 2 can be represented parametrically by
r (t) ϭ [2 cos t, 2 sin t, 0]

or simply by

r (t) ϭ [2 cos t, 2 sin t]

(Fig. 201)

where 0 Ϲ t Ϲ 2p. Indeed, x 2 ϩ y 2 ϭ (2 cos t)2 ϩ (2 sin t)2 ϭ 4 (cos2 t ϩ sin2 t) ϭ 4, For t ϭ 0 we have
r (0) ϭ [2, 0], for t ϭ 12 p we get r (12 p) ϭ [0, 2], and so on. The positive sense induced by this representation
is the counterclockwise sense.
If we replace t with t* ϭ Ϫt, we have t ϭ Ϫt* and get
r* (t*) ϭ [2 cos (Ϫt*), 2 sin (Ϫt*)] ϭ [2 cos t*, Ϫ2 sin t*].

᭿

This has reversed the orientation, and the circle is now oriented clockwise.

EXAMPLE 2

Ellipse
The vector function
r (t) ϭ [a cos t, b sin t,

(3)

0] ϭ a cos t i ϩ b sin t j

(Fig. 202)

represents an ellipse in the xy-plane with center at the origin and principal axes in the direction of the x- and
y-axes. In fact, since cos2 t ϩ sin2 t ϭ 1, we obtain from (3)
x2
a2

ϩ

y2
b2

ϭ 1,

z ϭ 0.

᭿

If b ϭ a, then (3) represents a circle of radius a.
y
1

(t = 2_ π)

(t = π)

y

1

(t = 2_ π)

(t = π)
2

b

x

a

x

(t = 0)
3

(t = 2_ π)

3

(t = 2_ π)

Fig. 201. Circle in Example 1

EXAMPLE 3

(t = 0)

Fig. 202. Ellipse in Example 2

Straight Line
A straight line L through a point A with position vector a in the direction of a constant vector b (see Fig. 203)
can be represented parametrically in the form
(4)

r (t) ϭ a ϩ t b ϭ [a1 ϩ t b1,
˛

a2 ϩ tb2,

a3 ϩ t b3].
˛

c09.qxd

10/30/10

3:25 PM

Page 383

SEC. 9.5 Curves. Arc Length. Curvature. Torsion

383

If b is a unit vector, its components are the direction cosines of L. In this case, ƒ t ƒ measures the distance of the
points of L from A. For instance, the straight line in the xy-plane through A: (3, 2) having slope 1 is (sketch it)
r (t) ϭ [3, 2,

0] ϩ t[1, 1,

᭿

0] ϭ [3 ϩ t, 2 ϩ t, 0].

L
b

z

x

A

a

y

Fig. 203. Parametric representation of a straight line

A plane curve is a curve that lies in a plane in space. A curve that is not plane is called
a twisted curve. A standard example of a twisted curve is the following.
EXAMPLE 4

Circular Helix
The twisted curve C represented by the vector function
r (t) ϭ [a cos t, a sin t,

(5)

ct] ϭ a cos t i ϩ a sin t j ϩ ct k

(c

0)

is called a circular helix. It lies on the cylinder x 2 ϩ y 2 ϭ a 2. If c Ͼ 0, the helix is shaped like a right-handed
screw (Fig. 204). If c Ͻ 0, it looks like a left-handed screw (Fig. 205). If c ϭ 0, then (5) is a circle.
᭿
z
z

y
x
y
x

Fig. 204. Right-handed circular helix

Fig. 205. Left-handed circular helix

A simple curve is a curve without multiple points, that is, without points at which the
curve intersects or touches itself. Circle and helix are simple curves. Figure 206 shows
curves that are not simple. An example is [sin 2t, cos t, 0]. Can you sketch it?
An arc of a curve is the portion between any two points of the curve. For simplicity,
we say “curve” for curves as well as for arcs.

Fig. 206. Curves with multiple points

10/30/10

3:25 PM

384

Page 384

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

Tangent to a Curve
The next idea is the approximation of a curve by straight lines, leading to tangents and
to a definition of length. Tangents are straight lines touching a curve. The tangent to a
simple curve C at a point P of C is the limiting position of a straight line L through P
and a point Q of C as Q approaches P along C. See Fig. 207.
Let us formalize this concept. If C is given by r(t), and P and Q correspond to t and
t ϩ ¢t, then a vector in the direction of L is
1
[r (t ϩ ¢t) Ϫ r (t)].
¢t

(6)

In the limit this vector becomes the derivative
r r (t) ϭ lim

(7)

¢t : 0

1
[r (t ϩ ¢t) Ϫ r (t)],
¢t

provided r(t) is differentiable, as we shall assume from now on. If r r (t) 0, we call r r (t)
a tangent vector of C at P because it has the direction of the tangent. The corresponding
unit vector is the unit tangent vector (see Fig. 207)
uϭ

(8)

1
ƒ rr ƒ

rr.

Note that both r r and u point in the direction of increasing t. Hence their sense depends
on the orientation of C. It is reversed if we reverse the orientation.
It is now easy to see that the tangent to C at P is given by
q (w) ϭ r ϩ wr r

(9)

(Fig. 208).

This is the sum of the position vector r of P and a multiple of the tangent vector r r of C
at P. Both vectors depend on P. The variable w is the parameter in (9).
en

t

L

Ta
ng

c09.qxd

u
Q

T
C

w

r′

q

P
r (t)

P
r

r(t + Δ t)

0
C

0

Fig. 207. Tangent to a curve

EXAMPLE 5

Fig. 208. Formula (9) for the tangent to a curve

Tangent to an Ellipse
Find the tangent to the ellipse 14 x 2 ϩ y 2 ϭ 1 at P: ( 12, 1> 12).

Solution. Equation (3) with semi-axes a ϭ 2 and b ϭ 1 gives r (t) ϭ [2 cos t, sin t]. The derivative is
r r (t) ϭ [Ϫ2 sin t, cos t]. Now P corresponds to t ϭ p>4 because
r (p>4) ϭ [2 cos (p>4), sin (p>4)] ϭ [ 12,

1> 12].

c09.qxd

10/30/10

3:25 PM

Page 385

SEC. 9.5 Curves. Arc Length. Curvature. Torsion
Hence r r (p>4) ϭ [Ϫ 12,
q(w) ϭ [ 12,

385

1> 12]. From (9) we thus get the answer
1> 12] ϩ w [Ϫ 12,

1> 12] ϭ [ 12 (1 Ϫ w), (1> 12) (1 ϩ w)].

᭿

To check the result, sketch or graph the ellipse and the tangent.

Length of a Curve
We are now ready to define the length l of a curve. l will be the limit of the lengths of
broken lines of n chords (see Fig. 209, where n ϭ 5) with larger and larger n. For this,
let r (t), a Ϲ t Ϲ b, represent C. For each n ϭ 1, 2, Á , we subdivide (“partition”) the
interval a Ϲ t Ϲ b by points
t 0 (ϭ a), t 1, Á , t n؊1,

t n (ϭ b),

where

t 0 Ͻ t 1 Ͻ Á Ͻ t n.

This gives a broken line of chords with endpoints r (t 0), Á , r (t n). We do this arbitrarily
but so that the greatest ƒ ¢t m ƒ ϭ ƒ t m Ϫ t m؊1 ƒ approaches 0 as n : ϱ. The lengths
l 1, l 2, Á of these chords can be obtained from the Pythagorean theorem. If r(t) has a
continuous derivative r r (t), it can be shown that the sequence l 1, l 2, Á has a limit, which
is independent of the particular choice of the representation of C and of the choice of
subdivisions. This limit is given by the integral
b

(10)

lϭ

Ύ 2rr • rr dt
a

ar r ϭ

dr
b.
dt

l is called the length of C, and C is called rectifiable. Formula (10) is made plausible
in calculus for plane curves and is proved for curves in space in [GenRef8] listed in App. 1.
The actual evaluation of the integral (10) will, in general, be difficult. However, some
simple cases are given in the problem set.

Arc Length s of a Curve
The length (10) of a curve C is a constant, a positive number. But if we replace the fixed
b in (10) with a variable t, the integral becomes a function of t, denoted by s(t) and called
the arc length function or simply the arc length of C. Thus
t

(11)

s(t) ϭ

Ύ 2rr • rr dෂt
a

dr
ar r ϭ ෂ b .
dt

Here the variable of integration is denoted by ~
t because t is now used in the upper limit.
Geometrically, s (t 0) with some t 0 Ͼ a is the length of the arc of C between the points
with parametric values a and t 0. The choice of a (the point s ϭ 0) is arbitrary; changing
a means changing s by a constant.

Fig. 209. Length of a curve

c09.qxd

10/30/10

3:25 PM

386

Page 386

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

Linear Element ds. If we differentiate (11) and square, we have
2

(12)

2

2

2

ds
dr dr
dx
dy
dz
a b ϭ
•
ϭ ƒ r r (t) ƒ 2 ϭ a b ϩ a b ϩ a b .
dt
dt dt
dt
dt
dt

It is customary to write
(13*)

dr ϭ [dx, dy, dz] ϭ dx i ϩ dy j ϩ dz k

and
(13)

ds 2 ϭ dr • dr ϭ dx 2 ϩ dy 2 ϩ dz 2.

ds is called the linear element of C.
Arc Length as Parameter. The use of s in (1) instead of an arbitrary t simplifies various
formulas. For the unit tangent vector (8) we simply obtain
u (s) ϭ r r (s).

(14)

Indeed, ƒ r r (s) ƒ ϭ (ds>ds) ϭ 1 in (12) shows that r r (s) is a unit vector. Even greater
simplifications due to the use of s will occur in curvature and torsion (below).
EXAMPLE 6

Circular Helix. Circle. Arc Length as Parameter
The helix r(t) ϭ [a cos t, a sin t, ct] in (5) has the derivative r r (t) ϭ [Ϫa sin t, a cos t, c]. Hence
r r • r r ϭ a 2 ϩ c2, a constant, which we denote by K 2. Hence the integrand in (11) is constant, equal to K,
and the integral is s ϭ Kt. Thus t ϭ s>K, so that a representation of the helix with the arc length s as
parameter is
(15)

s
s
r*(s) ϭ r a b ϭ c a cos ,
K
K

a sin

s
K

,

cs
K

d,

K ϭ 2a 2 ϩ c2.

A circle is obtained if we set c ϭ 0. Then K ϭ a, t ϭ s>a, and a representation with arc length s as parameter is
s
s
r*(s) ϭ r a b ϭ c a cos ,
a
a

s
a sin d .
a

᭿

Curves in Mechanics. Velocity. Acceleration
Curves play a basic role in mechanics, where they may serve as paths of moving bodies.
Then such a curve C should be represented by a parametric representation r(t) with time
t as parameter. The tangent vector (7) of C is then called the velocity vector v because,
being tangent, it points in the instantaneous direction of motion and its length gives the
speed ƒ v ƒ ϭ ƒ r r ƒ ϭ 2r r • r r ϭ ds>dt; see (12). The second derivative of r(t) is called
the acceleration vector and is denoted by a. Its length ƒ a ƒ is called the acceleration of
the motion. Thus
(16)

v (t) ϭ r r (t),

a (t) ϭ v r (t) ϭ r s (t).

c09.qxd

10/30/10

3:25 PM

Page 387

SEC. 9.5 Curves. Arc Length. Curvature. Torsion

387

Tangential and Normal Acceleration. Whereas the velocity vector is always tangent
to the path of motion, the acceleration vector will generally have another direction. We
can split the acceleration vector into two directional components, that is,
a ϭ atan ϩ anorm,

(17)

where the tangential acceleration vector atan is tangent to the path (or, sometimes, 0)
and the normal acceleration vector anorm is normal (perpendicular) to the path (or,
sometimes, 0).
Expressions for the vectors in (17) are obtained from (16) by the chain rule. We first have
dr
dr ds
ds
ϭ
ϭ u (s)
dt
ds dt
dt

v (t) ϭ

where u(s) is the unit tangent vector (14). Another differentiation gives
2

dv
d
ds
du ds
d 2s
a (t) ϭ
ϭ au (s) b ϭ
a b ϩ u (s) 2 .
dt
dt
dt
dt
ds dt

(18)

Since the tangent vector u(s) has constant length (length one), its derivative du>ds is
perpendicular to u(s), from the result in Example 4 in Sec. 9.4. Hence the first term on
the right of (18) is the normal acceleration vector, and the second term on the right is the
tangential acceleration vector, so that (18) is of the form (17).
Now the length ƒ atan ƒ is the absolute value of the projection of a in the direction of v,
given by (11) in Sec. 9.2 with b ϭ v; that is, ƒ atan ƒ ϭ ƒ a • v ƒ > ƒ v ƒ . Hence atan is this
expression times the unit vector (1> ƒ v ƒ )v in the direction of v, that is,
a•v
atan ϭ v • v v.

(18*)

Also,

anorm ϭ a Ϫ atan.

We now turn to two examples that are relevant to applications in space travel.
They deal with the centripetal and centrifugal accelerations, as well as the Coriolis
acceleration.
EXAMPLE 7

Centripetal Acceleration. Centrifugal Force
The vector function
r (t) ϭ [R cos vt, R sin vt] ϭ R cos vt i ϩ R sin vt j

(Fig. 210)

(with fixed i and j) represents a circle C of radius R with center at the origin of the xy-plane and describes the
motion of a small body B counterclockwise around the circle. Differentiation gives the velocity vector
v ϭ r r ϭ [ϪRv sin vt, Rv cos vt] ϭ ϪRv sin vt i ϩ Rv cos vt j

(Fig. 210)

v is tangent to C. Its magnitude, the speed, is
ƒ v ƒ ϭ ƒ r r ƒ ϭ 2r r • r r ϭ Rv.
Hence it is constant. The speed divided by the distance R from the center is called the angular speed. It equals
v, so that it is constant, too. Differentiating the velocity vector, we obtain the acceleration vector
(19)

a ϭ v r ϭ [ϪRv2 cos vt,

ϪRv2 sin vt] ϭ ϪRv2 cos vt i Ϫ Rv2 sin vt j.

c09.qxd

10/30/10

3:25 PM

388

Page 388

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl
y

j
b′

b

x

i

Fig. 210. Centripetal acceleration a

This shows that a ϭ Ϫv2r (Fig. 210), so that there is an acceleration toward the center, called the centripetal
acceleration of the motion. It occurs because the velocity vector is changing direction at a constant rate. Its
magnitude is constant, ƒ a ƒ ϭ v2 ƒ r ƒ ϭ v2R. Multiplying a by the mass m of B, we get the centripetal force ma.
The opposite vector Ϫma is called the centrifugal force. At each instant these two forces are in equilibrium.
We see that in this motion the acceleration vector is normal (perpendicular) to C; hence there is no tangential
acceleration.
᭿

EXAMPLE 8

Superposition of Rotations. Coriolis Acceleration
A projectile is moving with constant speed along a meridian of the rotating earth in Fig. 211. Find its acceleration.

z

acor

k
P

x

b
y

Fig. 211. Example 8. Superposition of two rotations

Solution. Let x, y, z be a fixed Cartesian coordinate system in space, with unit vectors i, j, k in the directions
of the axes. Let the Earth, together with a unit vector b, be rotating about the z-axis with angular speed v Ͼ 0
(see Example 7). Since b is rotating together with the Earth, it is of the form
b (t) ϭ cos vt i ϩ sin vt j.
Let the projectile be moving on the meridian whose plane is spanned by b and k (Fig. 211) with constant angular
speed v Ͼ 0. Then its position vector in terms of b and k is
r (t) ϭ R cos gt b (t) ϩ R sin gt k

(R ϭ Radius of the Earth).

c09.qxd

10/30/10

3:25 PM

Page 389

SEC. 9.5 Curves. Arc Length. Curvature. Torsion

389

We have finished setting up the model. Next, we apply vector calculus to obtain the desired acceleration of the
projectile. Our result will be unexpected—and highly relevant for air and space travel. The first and second
derivatives of b with respect to t are
(20)

b r (t) ϭ Ϫv sin vt i ϩ v cos vt j
b s (t) ϭ Ϫv2 cos vt i Ϫ v2 sin vt j ϭ Ϫv2b (t).

The first and second derivatives of r(t) with respect to t are
v ϭ r r (t) ϭ R cos gt b r Ϫ gR sin gt b ϩ gR cos gt k
(21)

a ϭ v r ϭ R cos gt b s Ϫ 2gR sin gt b r Ϫ g2R cos gt b Ϫ g2R sin gt k
ϭ R cos gt b s Ϫ 2gR sin gt b r Ϫ g2r.

By analogy with Example 7 and because of b s ϭ Ϫv2b in (20) we conclude that the first term in a (involving v
in b s !) is the centripetal acceleration due to the rotation of the Earth. Similarly, the third term in the last line (involving
g!) is the centripetal acceleration due to the motion of the projectile on the meridian M of the rotating Earth.
The second, unexpected term Ϫ2gR sin gt b r in a is called the Coriolis acceleration3 (Fig. 211) and is
due to the interaction of the two rotations. On the Northern Hemisphere, sin gt Ͼ 0 (for t Ͼ 0; also g Ͼ 0
by assumption), so that acor has the direction of Ϫb r , that is, opposite to the rotation of the Earth. ƒ acor ƒ
is maximum at the North Pole and zero at the equator. The projectile B of mass m 0 experiences a force
Ϫm 0 acor opposite to m 0 acor, which tends to let B deviate from M to the right (and in the Southern
Hemisphere, where sin gt Ͻ 0, to the left). This deviation has been observed for missiles, rockets, shells,
and atmospheric airflow.
᭿

Curvature and Torsion. Optional
This last topic of Sec. 9.5 is optional but completes our discussion of curves relevant to
vector calculus.
The curvature ␬(s) of a curve C: r (s) (s the arc length) at a point P of C measures the
rate of change ƒ u r (s) ƒ of the unit tangent vector u (s) at P. Hence ␬(s) measures the deviation
of C at P from a straight line (its tangent at P). Since u (s) ϭ r r (s), the definition is
(22)

␬ (s) ϭ ƒ u r (s) ƒ ϭ ƒ r s (s) ƒ

( r ϭ d>ds).

The torsion t (s) of C at P measures the rate of change of the osculating plane O of
curve C at point P. Note that this plane is spanned by u and u r and shown in Fig. 212.
Hence t (s) measures the deviation of C at P from a plane (from O at P). Now the rate
of change is also measured by the derivative b r of a normal vector b at O. By the definition
of vector product, a unit normal vector of O is b ϭ u ؋ (1>␬)u r ϭ u ؋ p. Here p ϭ (1>␬)u r
is called the unit principal normal vector and b is called the unit binormal vector of C
at P. The vectors are labeled in Fig. 212. Here we must assume that ␬ 0; hence ␬ Ͼ 0.
The absolute value of the torsion is now defined by
(23*)

ƒ t (s) ƒ ϭ ƒ b r (s) ƒ .

Whereas ␬(s) is nonnegative, it is practical to give the torsion a sign, motivated by
“right-handed” and “left-handed” (see Figs. 204 and 205). This needs a little further
calculation. Since b is a unit vector, it has constant length. Hence b r is perpendicular

3

GUSTAVE GASPARD CORIOLIS (1792–1843), French engineer who did research in mechanics.

10/30/10

390

3:25 PM

Page 390

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

u

Binormal

c09.qxd

Normal plane

b

Rectifying plane

Curve

p
t u

Princ
ipal
norm
al

gen

Tan

Osculating plane

Fig. 212. Trihedron. Unit vectors u, p, b and planes

to b (see Example 4 in Sec. 9.4). Now b r is also perpendicular to u because, by the
definition of vector product, we have b • u ϭ 0, b • u r ϭ 0. This implies
(b • u) r ϭ 0;

that is,

b r • u ϩ b • u r ϭ b r • u ϩ 0 ϭ 0.

Hence if b r 0 at P, it must have the direction of p or Ϫp, so that it must be of the form
b r ϭ Ϫtp. Taking the dot product of this by p and using p • p ϭ 1 gives
(23)

t (s) ϭ Ϫp (s) • b r (s).

The minus sign is chosen to make the torsion of a right-handed helix positive and that of
a left-handed helix negative (Figs. 204 and 205). The orthonormal vector triple u, p, b is
called the trihedron of C. Figure 212 also shows the names of the three straight lines in
the directions of u, p, b, which are the intersections of the osculating plane, the normal
plane, and the rectifying plane.

PROBLEM SET 9.5
1–10

PARAMETRIC REPRESENTATIONS

What curves are represented by the following?
Sketch them.
1. [3 ϩ 2 cos t, 2 sin t, 0]
2. [a ϩ t, b ϩ 3t, c Ϫ 5t]
3. [0, t, t 3]
4. [Ϫ2, 2 ϩ 5 cos t, Ϫ1 ϩ 5 sin t]
5. [2 ϩ 4 cos t, 1 ϩ sin t, 0]
6. [a ϩ 3 cos pt, b Ϫ 2 sin pt, 0]
7. [4 cos t, 4 sin t, 3t]
8. [cosh t, sinh t, 2]
9. [cos t, sin 2t, 0]
10. [t, 2, 1>t]

11–20

FIND A PARAMETRIC REPRESENTATION

11. Circle in the plane z ϭ 1 with center (3, 2) and passing
through the origin.
12. Circle in the yz-plane with center (4, 0) and passing
through (0, 3). Sketch it.
13. Straight line through (2, 1, 3) in the direction of i ϩ 2j.
14. Straight line through (1, 1, 1) and (4, 0, 2). Sketch it.
15. Straight line y ϭ 4x Ϫ 1, z ϭ 5x.
16. The intersection of the circular cylinder of radius 1
about the z-axis and the plane z ϭ y.
17. Circle 12 x 2 ϩ y 2 ϭ 1, z ϭ y.
18. Helix x 2 ϩ y 2 ϭ 25, z ϭ 2 arctan (y>x).
19. Hyperbola 4x 2 Ϫ 3y 2 ϭ 4, z ϭ Ϫ2.

c09.qxd

10/30/10

3:25 PM

Page 391

SEC. 9.5 Curves. Arc Length. Curvature. Torsion
20. Intersection of 2x Ϫ y ϩ 3z ϭ 2 and x ϩ 2y Ϫ z ϭ 3.
21. Orientation. Explain why setting t ϭ Ϫt* reverses
the orientation of [a cos t, a sin t, 0].
22. CAS PROJECT. Curves. Graph the following more
complicated curves:
(a) r (t) ϭ [2 cos t ϩ cos 2t, 2 sin t Ϫ sin 2t] (Steiner’s
hypocycloid).
(b) r (t) ϭ [cos t ϩ k cos 2t, sin t Ϫ k sin 2t] with k ϭ
10, 2, 1, 12 , 0, Ϫ12 , Ϫ1.
(c) r (t) ϭ [cos t, sin 5t] (a Lissajous curve).
(d) r (t) ϭ [cos t, sin kt]. For what k’s will it be closed?
(e) r (t) ϭ [R sin vt ϩ vRt, R cos vt ϩ R] (cycloid).
23. CAS PROJECT. Famous Curves in Polar Form.
Use your CAS to graph the following curves4 given in
polar form r ϭ r (u), r2 ϭ x 2 ϩ y 2, tan u ϭ y>x, and
investigate their form depending on parameters a and b.
r ϭ au Spiral of Archimedes
r ϭ aebu Logarithmic spiral
rϭ

2a sin2 u
cos u

Cissoid of Diocles

a
rϭ
ϩ b Conchoid of Nicomedes
cos u
r ϭ a>u Hyperbolic spiral
rϭ

3a sin 2u
cos3 u ϩ sin3 u

r ϭ 2a

sin 3u
sin 2u

Folium of Descartes

Maclaurin’s trisectrix

r ϭ 2a cos u ϩ b Pascal’s snail
24–28

TANGENT

Given a curve C: r (t), find a tangent vector r r (t), a unit
tangent vector u r (t), and the tangent of C at P. Sketch curve
and tangent.
24. r (t) ϭ [t, 12 t 2, 1], P: (2, 2, 1)
25. r (t) ϭ [10 cos t, 1, 10 sin t], P: (6, 1, 8)
26. r (t) ϭ [cos t, sin t, 9t], P: (1, 0, 18p)
27. r (t) ϭ [t, 1>t, 0], P: (2, 12, 0)
28. r (t) ϭ [t, t 2, t 3], P: (1, 1, 1)
29–32

LENGTH

Find the length and sketch the curve.
29. Catenary r (t) ϭ [t, cosh t] from t ϭ 0 to t ϭ 1.
30. Circular helix r (t) ϭ [4 cos t, 4 sin t, 5t] from (4, 0, 0)
to (4, 0, 10p).

391
31. Circle r (t) ϭ [a cos t, a sin t] from (a, 0) to (0, a).
32. Hypocycloid r (t) ϭ [a cos3 t, a sin3 t], total length.
33. Plane curve. Show that Eq. (10) implies
/ ϭ ͐ab 21 ϩ y r 2 dx for the length of a plane curve
C: y ϭ f (x), z ϭ 0, and a ϭ x ϭ b.
34. Polar coordinates r ϭ 2x 2 ϩ y 2, u ϭ arctan (y>x)
give
b

/ϭ

Ύ 2r

2

ϩ r r 2 du,

a

where r r ϭ dr>du. Derive this. Use it to find the total
length of the cardioid r ϭ a(1 Ϫ cos u). Sketch this
curve. Hint. Use (10) in App. 3.1.
35–46

CURVES IN MECHANICS

Forces acting on moving objects (cars, airplanes, ships, etc.)
require the engineer to know corresponding tangential and
normal accelerations. In Probs. 35–38 find them, along
with the velocity and speed. Sketch the path.
35. Parabola r (t) ϭ [t, t 2, 0]. Find v and a.
36. Straight line r (t) ϭ [8t, 6t, 0]. Find v and a.
37. Cycloid r (t) ϭ (R sin vt ϩ Rt) i ϩ (R cos vt ϩ R) j.
This is the path of a point on the rim of a wheel of
radius R that rolls without slipping along the x-axis.
Find v and a at the maximum y-values of the curve.
38. Ellipse r ϭ [cos t, 2 sin t, 0].

THE USE OF A CAS may greatly facilitate the
39–42
investigation of more complicated paths, as they occur in
gear transmissions and other constructions. To grasp the
idea, using a CAS, graph the path and find velocity, speed,
and tangential and normal acceleration.
39. r (t) ϭ [cos t ϩ cos 2t, sin t Ϫ sin 2t]
40. r (t) ϭ [2 cos t ϩ cos 2t, 2 sin t Ϫ sin 2t]
41. r (t) ϭ [cos t, sin 2t, cos 2t]
42. r (t) ϭ [ct cos t, ct sin t, ct] (c

0)

43. Sun and Earth. Find the acceleration of the Earth
toward the sun from (19) and the fact that Earth
revolves about the sun in a nearly circular orbit with
an almost constant speed of 30 km>s.
44. Earth and moon. Find the centripetal acceleration
of the moon toward Earth, assuming that the orbit
of the moon is a circle of radius 239,000 miles ϭ
3.85 # 108 m, and the time for one complete revolution
is 27.3 days ϭ 2.36 # 106 s.

4
Named after ARCHIMEDES (c. 287–212 B.C.), DESCARTES (Sec. 9.1), DIOCLES (200 B.C.),
MACLAURIN (Sec. 15.4), NICOMEDES (250? B.C.) ÉTIENNE PASCAL (1588–1651), father of BLAISE
PASCAL (1623–1662).

c09.qxd

10/30/10

3:25 PM

Page 392

392

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

45. Satellite. Find the speed of an artificial Earth satellite
traveling at an altitude of 80 miles above Earth’s
surface, where g ϭ 31 ft>sec2. (The radius of the Earth
is 3960 miles.)
46. Satellite. A satellite moves in a circular orbit
450 miles above Earth’s surface and completes
1 revolution in 100 min. Find the acceleration of gravity
at the orbit from these data and from the radius of Earth
(3960 miles).
47–55

CURVATURE AND TORSION

47. Circle. Show that a circle of radius a has curvature
1>a.
48. Curvature. Using (22), show that if C is represented
by r (t) with arbitrary t, then
(22*) ␬(t) ϭ

2(r r • r r )(r s • r s ) Ϫ (r r • r s )2
(r r • r r )3>2

.

49. Plane curve. Using (22*), show that for a curve
y ϭ f (x),
(22**) ␬ (x) ϭ

9.6

ƒ ys ƒ
(1 ϩ y r 2)3>2

ay r ϭ

dy
dx

, etc.b .

50. Torsion. Using b ϭ u ؋ p and (23), show that (when
␬ Ͼ 0)
(23**) t (s) ϭ (u p p r ) ϭ (r r

rs

r t )> ␬2.

51. Torsion. Show that if C is represented by r (t) with
arbitrary parameter t, then, assuming ␬ Ͼ 0 as before,
(23***) t (t) ϭ

(r r

rs

rt)

(r r • r r )(r s • r s ) Ϫ (r r • r s )2

.

52. Helix. Show that the helix [a cos t, a sin t, ct] can
be represented by [a cos (s>K ), a sin (s>K ), cs>K ],
where K ϭ 2a 2 ϩ c2 and s is the arc length. Show
that it has constant curvature ␬ ϭ a>K 2 and torsion
t ϭ c>K 2.
53. Find the torsion of C: r (t) ϭ [t, t 2, t 3], which looks
similar to the curve in Fig. 212.
54. Frenet5 formulas. Show that
u r ϭ ␬ p,

p r ϭ Ϫ␬u ϩ tb,

b r ϭ Ϫtp.

55. Obtain ␬ and t in Prob. 52 from (22*) and (23***)
and the original representation in Prob. 54 with
parameter t.

Calculus Review:
Functions of Several Variables. Optional
The parametric representations of curves C required vector functions that depended on a
single variable x, s, or t. We now want to systematically cover vector functions of several
variables. This optional section is inserted into the book for your convenience and to make
the book reasonably self-contained. Go onto Sec. 9.7 and consult Sec. 9.6 only when
needed. For partial derivatives, see App. A3.2.

Chain Rules
Figure 213 shows the notations in the following basic theorem.
z

v

D
[x(u, v), y(u, v), z(u, v)]

(u, v)
B
u
x

Fig. 213. Notations in Theorem 1
5

JEAN-FRÉDÉRIC FRENET (1816–1900), French mathematician.

y

c09.qxd

10/30/10

3:25 PM

Page 393

SEC. 9.6 Calculus Review: Functions of Several Variables. Optional

THEOREM 1

393

Chain Rule

Let w ϭ f (x, y, z) be continuous and have continuous first partial derivatives in a
domain D in xyz-space. Let x ϭ x(u, v), y ϭ y(u, v), z ϭ z(u, v) be functions that
are continuous and have first partial derivatives in a domain B in the uv-plane,
where B is such that for every point (u, v) in B, the corresponding point [x(u, v),
y(u, v), z(u, v)] lies in D. See Fig. 213. Then the function
w ϭ f (x(u, v), y(u, v), z(u, v))
is defined in B, has first partial derivatives with respect to u and v in B, and

0w 0x
0w 0y
0w 0z
0w
ϭ
ϩ
ϩ
0u
0x 0u
0y 0u
0z 0u
(1)
0w
0w 0x
0w 0y
0w 0z
ϭ
ϩ
ϩ
.
0v
0x 0v
0y 0v
0z 0v

In this theorem, a domain D is an open connected point set in xyz-space, where
“connected” means that any two points of D can be joined by a broken line of finitely
many linear segments all of whose points belong to D. “Open” means that every point P
of D has a neighborhood (a little ball with center P) all of whose points belong to D. For
example, the interior of a cube or of an ellipsoid (the solid without the boundary surface)
is a domain.
In calculus, x, y, z are often called the intermediate variables, in contrast with the
independent variables u, v and the dependent variable w.

Special Cases of Practical Interest
If w ϭ f (x, y) and x ϭ x(u, v), y ϭ y(u, v) as before, then (1) becomes

0w
0w 0x
0w 0y
ϭ
ϩ
0u
0x 0u
0y 0u
(2)
0w 0x
0w 0y
0w
.
ϭ
ϩ
0v
0x 0v
0y 0v
If w ϭ f (x, y, z) and x ϭ x(t), y ϭ y(t), z ϭ z(t), then (1) gives

(3)

dw
0w dx
0w dy
0w dz
ϭ
ϩ
ϩ
.
dt
0x dt
0y dt
0z dt

c09.qxd

10/30/10

3:25 PM

394

Page 394

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

If w ϭ f (x, y) and x ϭ x(t), y ϭ y(t), then (3) reduces to
dw
0w dx
0w dy
ϭ
ϩ
.
dt
0x dt
0y dt

(4)

Finally, the simplest case w ϭ f (x), x ϭ x(t) gives
dw dx
dw
ϭ
.
dt
dx dt

(5)

EXAMPLE 1

Chain Rule
If w ϭ x 2 Ϫ y 2 and we define polar coordinates r, u by x ϭ r cos u, y ϭ r sin u, then (2) gives
0w
ϭ 2x cos u Ϫ 2y sin u ϭ 2r cos2 u Ϫ 2r sin2 u ϭ 2r cos 2u
0r
0w

ϭ 2x(Ϫr sin u) Ϫ 2y(r cos u) ϭ Ϫ2r 2 cos u sin u Ϫ 2r 2 sin u cos u ϭ Ϫ2r 2 sin 2u.

0u

᭿

Partial Derivatives on a Surface z ϭ g (x, y)
Let w ϭ f (x, y, z) and let z ϭ g (x, y) represent a surface S in space. Then on S the function
becomes
ෂ (x, y) ϭ f (x, y, g (x, y)).
w
Hence, by (1), the partial derivatives are

(6)

ෂ
0w
0f
0f 0g
ϭ
ϩ
,
0x
0x
0z 0x

ෂ
0w
0f
0f 0g
ϭ
ϩ
0y
0y
0z 0y

[z ϭ g (x, y)].

We shall need this formula in Sec. 10.9.
EXAMPLE 2

Partial Derivatives on Surface
Let w ϭ f ϭ x 3 ϩ y 3 ϩ z 3 and let z ϭ g ϭ x 2 ϩ y 2. Then (6) gives
ෂ
0w
ϭ 3x 2 ϩ 3z 2 # 2x ϭ 3x 2 ϩ 3(x 2 ϩ y 2)2 # 2x,
0x
ෂ
0w
ϭ 3y 2 ϩ 3z 2 # 2y ϭ 3y 2 ϩ 3(x 2 ϩ y 2)2 # 2y.
0y
We confirm this by substitution, using w(x, y) ϭ x 3 ϩ y 3 ϩ (x 2 ϩ y 2)3, that is,
ෂ
0w
ϭ 3x 2 ϩ 3(x 2 ϩ y 2)2 # 2x,
0x

ෂ
0w
ϭ 3y 2 ϩ 3(x 2 ϩ y 2)2 # 2y.
0y

᭿

c09.qxd

10/30/10

3:25 PM

Page 395

SEC. 9.7 Gradient of a Scalar Field. Directional Derivative

395

Mean Value Theorems
THEOREM 2

Mean Value Theorem

Let f (x, y, z) be continuous and have continuous first partial derivatives in a
domain D in xyz-space. Let P0: (x 0, y0, z 0) and P: (x 0 ϩ h, y0 ϩ k, z 0 ϩ l) be
points in D such that the straight line segment P0P joining these points lies entirely
in D. Then
(7)

f (x 0 ϩ h, y0 ϩ k, z 0 ϩ l ) Ϫ f (x 0, y0, z 0) ϭ h

0f
0f
0f
ϩk ϩl ,
0x
0y
0z

the partial derivatives being evaluated at a suitable point of that segment.

Special Cases
For a function f (x, y) of two variables (satisfying assumptions as in the theorem), formula
(7) reduces to (Fig. 214)
(8)

f (x 0 ϩ h, y0 ϩ k) Ϫ f (x 0, y0) ϭ h

0f
0f
ϩk ,
0x
0y

and, for a function f (x) of a single variable, (7) becomes
(9)

f (x 0 ϩ h) Ϫ f (x 0) ϭ h

0f
,
0x

where in (9), the domain D is a segment of the x-axis and the derivative is taken at a
suitable point between x 0 and x 0 ϩ h.

(x0 + h, y0 + k)

(x0, y0)

D

Fig. 214. Mean value theorem for a function of two variables [Formula (8)]

9.7

Gradient of a Scalar Field.
Directional Derivative
We shall see that some of the vector fields that occur in applications—not all of them!—
can be obtained from scalar fields. Using scalar fields instead of vector fields is of a
considerable advantage because scalar fields are easier to use than vector fields. It is the

c09.qxd

10/30/10

3:25 PM

396

Page 396

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

“gradient” that allows us to obtain vector fields from scalar fields, and thus the gradient
is of great practical importance to the engineer.

DEFINITION 1

Gradient

The setting is that we are given a scalar function f (x, y, z) that is defined and
differentiable in a domain in 3-space with Cartesian coordinates x, y, z. We denote
the gradient of that function by grad f or ٌf (read nabla f ). Then the qradient of
f ( x, y, z) is defined as the vector function

(1)

grad f ϭ ٌf ϭ c

0f 0f 0f
0f
0f
0f
, , d ϭ
iϩ
jϩ
k.
0x 0y 0z
0x
0y
0z

Remarks. For a definition of the gradient in curvilinear coordinates, see App. 3.4.
As a quick example, if f (x, y, z) ϭ 2y 3 ϩ 4xz ϩ 3x, then grad f ϭ [4z ϩ 3, 6y 2, 4x].
Furthermore, we will show later in this section that (1) actually does define a vector.
The notation ٌf is suggested by the differential operator ٌ (read nabla) defined by
(1*)

ٌϭ

0
0
0
iϩ
jϩ
k.
0x
0y
0z

Gradients are useful in several ways, notably in giving the rate of change of f (x, y, z)
in any direction in space, in obtaining surface normal vectors, and in deriving vector fields
from scalar fields, as we are going to show in this section.

Directional Derivative
From calculus we know that the partial derivatives in (1) give the rates of change of
f (x, y, z) in the directions of the three coordinate axes. It seems natural to extend this and
ask for the rate of change of f in an arbitrary direction in space. This leads to the following
concept.

DEFINITION 2

Directional Derivative

The directional derivative Db f or df>ds of a function f (x, y, z) at a point P in the
direction of a vector b is defined by (see Fig. 215)

(2)

Db f ϭ

df
ds

ϭ lim
s:0

f (Q) Ϫ f (P)
.
s

Here Q is a variable point on the straight line L in the direction of b, and ƒ s ƒ is the
distance between P and Q. Also, s Ͼ 0 if Q lies in the direction of b (as in Fig. 215),
s Ͻ 0 if Q lies in the direction of Ϫb, and s ϭ 0 if Q ϭ P.

c09.qxd

10/30/10

3:25 PM

Page 397

SEC. 9.7 Gradient of a Scalar Field. Directional Derivative

397

L
s

Q
b

P

Fig. 215. Directional derivative

The next idea is to use Cartesian xyz-coordinates and for b a unit vector. Then the line L
is given by
r (s) ϭ x (s) i ϩ y (s) j ϩ z (s) k ϭ p 0 ϩ sb

(3)

( ƒ b ƒ ϭ 1)

where p 0 the position vector of P. Equation (2) now shows that Db f ϭ df>ds is the
derivative of the function f (x (s), y (s), z (s)) with respect to the arc length s of L. Hence,
assuming that f has continuous partial derivatives and applying the chain rule [formula
(3) in the previous section], we obtain

Db f ϭ

(4)

df
ds

ϭ

0f
0x

xr ϩ

0f
0y

yr ϩ

0f
zr
0z

where primes denote derivatives with respect to s (which are taken at s ϭ 0). But here,
differentiating (3) gives r r ϭ x r i ϩ y r j ϩ z r k ϭ b. Hence (4) is simply the inner product
of grad f and b [see (2), Sec. 9.2]; that is,

Db f ϭ

(5)

df
ds

ϭ b • grad f

( ƒ b ƒ ϭ 1).

ATTENTION! If the direction is given by a vector a of any length (

Da f ϭ

(5*)

EXAMPLE 1

df
ds

ϭ

1
ƒaƒ

0), then

a • grad f.

Gradient. Directional Derivative
Find the directional derivative of f (x, y, z) ϭ 2x 2 ϩ 3y 2 ϩ z 2 at P: (2, 1, 3) in the direction of a ϭ [1, 0, Ϫ2].

Solution. grad f ϭ [4x, 6y, 2z] gives at P the vector grad f (P) ϭ [8, 6, 6]. From this and (5*) we obtain,
since ƒ a ƒ ϭ 15,

Da f (P) ϭ

1
15

[1, 0, Ϫ2] • [8, 6, 6] ϭ

1
15

(8 ϩ 0 Ϫ 12) ϭ Ϫ

4
15

ϭ Ϫ1.789.

The minus sign indicates that at P the function f is decreasing in the direction of a.

᭿

c09.qxd

10/30/10

3:25 PM

398

Page 398

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

Gradient Is a Vector. Maximum Increase
Here is a finer point of mathematics that concerns the consistency of our theory: grad f
in (1) looks like a vector—after all, it has three components! But to prove that it actually
is a vector, since it is defined in terms of components depending on the Cartesian
coordinates, we must show that grad f has a length and direction independent of the choice
of those coordinates. See proof of Theorem 1. In contrast, [0f>0x, 20f>0y, 0f>0z] also looks
like a vector but does not have a length and direction independent of the choice of Cartesian
coordinates.
Incidentally, the direction makes the gradient eminently useful: grad f points in the
direction of maximum increase of f.

THEOREM 1

Use of Gradient: Direction of Maximum Increase

Let f (P) ϭ f (x, y, z) be a scalar function having continuous first partial derivatives
in some domain B in space. Then grad f exists in B and is a vector, that is, its length
and direction are independent of the particular choice of Cartesian coordinates. If
grad f (P) 0 at some point P, it has the direction of maximum increase of f at P.

PROOF

From (5) and the definition of inner product [(1) in Sec. 9.2] we have
(6)

Db f ϭ ƒ b ƒ ƒ grad f ƒ cos g ϭ ƒ grad f ƒ cos g

where g is the angle between b and grad f. Now f is a scalar function. Hence its value at
a point P depends on P but not on the particular choice of coordinates. The same holds
for the arc length s of the line L in Fig. 215, hence also for Db f. Now (6) shows that Db f
is maximum when cos g ϭ 1, g ϭ 0, and then Db f ϭ ƒ grad f ƒ . It follows that the length
and direction of grad f are independent of the choice of coordinates. Since g ϭ 0 if and
only if b has the direction of grad f, the latter is the direction of maximum increase of f
at P, provided grad f 0 at P. Make sure that you understood the proof to get a good
feel for mathematics.

Gradient as Surface Normal Vector
Gradients have an important application in connection with surfaces, namely, as surface
normal vectors, as follows. Let S be a surface represented by f (x, y, z) ϭ c ϭ const, where
f is differentiable. Such a surface is called a level surface of f, and for different c we get
different level surfaces. Now let C be a curve on S through a point P of S. As a curve in
space, C has a representation r (t) ϭ [x (t), y (t), z (t)]. For C to lie on the surface S, the
components of r (t) must satisfy f (x, y, z) ϭ c, that is,
(7)

f (x (t), y (t), z (t) ϭ c.

Now a tangent vector of C is r r (t) ϭ [x r (t), y r (t), z r (t)]. And the tangent vectors of all
curves on S passing through P will generally form a plane, called the tangent plane of S
at P. (Exceptions occur at edges or cusps of S, for instance, at the apex of the cone in
Fig. 217.) The normal of this plane (the straight line through P perpendicular to the tangent
plane) is called the surface normal to S at P. A vector in the direction of the surface

c09.qxd

10/30/10

3:25 PM

Page 399

SEC. 9.7 Gradient of a Scalar Field. Directional Derivative

399

normal is called a surface normal vector of S at P. We can obtain such a vector quite
simply by differentiating (7) with respect to t. By the chain rule,
0f
0x

xr ϩ

0f
0y

0f

yr ϩ

0z

z r ϭ (grad f ) • r r ϭ 0.

Hence grad f is orthogonal to all the vectors r r in the tangent plane, so that it is a normal
vector of S at P. Our result is as follows (see Fig. 216).
Tangent plane

f = const

grad f
C
P

Fig. 216. Gradient as surface normal vector

THEOREM 2

Gradient as Surface Normal Vector

Let f be a differentiable scalar function in space. Let f (x, y, z) ϭ c ϭ const represent
a surface S. Then if the gradient of f at a point P of S is not the zero vector, it is a
normal vector of S at P.

EXAMPLE 2

Gradient as Surface Normal Vector. Cone
Find a unit normal vector n of the cone of revolution z 2 ϭ 4(x 2 ϩ y 2) at the point P: (1, 0, 2).

Solution. The cone is the level surface f ϭ 0 of f (x, y, z) ϭ 4(x 2 ϩ y 2) Ϫ z 2. Thus (Fig. 217)
grad f ϭ [8x, 8y,
nϭ

Ϫ2z],

grad f (P) ϭ [8,

1
2
grad f (P) ϭ c
,
15
ƒ grad f (P) ƒ

0,

Ϫ

0,

Ϫ4]

1
d.
15

n points downward since it has a negative z-component. The other unit normal vector of the cone at P is Ϫn.
z

P
n

1
x

y

Fig. 217. Cone and unit normal vector n

᭿

c09.qxd

10/30/10

3:25 PM

400

Page 400

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

Vector Fields That Are Gradients
of Scalar Fields (“Potentials”)
At the beginning of this section we mentioned that some vector fields have the advantage
that they can be obtained from scalar fields, which can be worked with more easily. Such
a vector field is given by a vector function v (P), which is obtained as the gradient of a
scalar function, say, v(P) ϭ grad f (P). The function f (P) is called a potential function or
a potential of v (P). Such a v (P) and the corresponding vector field are called conservative
because in such a vector field, energy is conserved; that is, no energy is lost (or gained)
in displacing a body (or a charge in the case of an electrical field) from a point P to another
point in the field and back to P. We show this in Sec. 10.2.
Conservative fields play a central role in physics and engineering. A basic application
concerns the gravitational force (see Example 3 in Sec. 9.4) and we show that it has a
potential which satisfies Laplace’s equation, the most important partial differential
equation in physics and its applications.
THEOREM 3

Gravitational Field. Laplace’s Equation

The force of attraction
pϭϪ

(8)

c
r

3

r ϭ Ϫc c

x Ϫ x 0 y Ϫ y0 z Ϫ z 0
,
,
d
r3
r3
r3

between two particles at points P0: (x 0, y0, z 0) and P: (x, y, z) (as given by Newton’s
law of gravitation) has the potential f (x, y, z) ϭ c>r, where r (Ͼ 0) is the distance
between P0 and P.
Thus p ϭ grad f ϭ grad (c>r). This potential f is a solution of Laplace’s equation
ٌ2f ϭ

(9)

0 2f
0x 2

ϩ

0 2f
0y 2

ϩ

0 2f
0z 2

ϭ 0.

[ٌ2f (read nabla squared f ) is called the Laplacian of f.]

PROOF

That distance is r ϭ ((x Ϫ x 0)2 ϩ ( y Ϫ y0)2 ϩ (z Ϫ z 2)2)1>2. The key observation now
is that for the components of p ϭ [p1, p2, p3] we obtain by partial differentiation
(10a)

Ϫ2(x Ϫ x 0)
x Ϫ x0
0 1
a bϭ
ϭϪ
0x r
2[(x Ϫ x 0)2 ϩ ( y Ϫ y0)2 ϩ (z Ϫ z 0)2]3>2
r3

and similarly
y Ϫ y0
0 1
a bϭ Ϫ
,
0y r
r3
(10b)

0 1
z Ϫ z0
a bϭ Ϫ
.
0z r
r3

c09.qxd

10/30/10

3:25 PM

Page 401

SEC. 9.7 Gradient of a Scalar Field. Directional Derivative

401

From this we see that, indeed, p is the gradient of the scalar function f ϭ c>r. The second
statement of the theorem follows by partially differentiating (10), that is,
02 1
1
3(x Ϫ x 0)2
a
ϩ
,
b
ϭ
Ϫ
0x 2 r
r3
r5
02 1
1
3( y Ϫ y0)2
a
ϩ
,
b
ϭ
Ϫ
0y 2 r
r3
r5
02 1
1
3(z Ϫ z 0)2
,
2 a b ϭ Ϫ 3 ϩ
0z r
r
r5
and then adding these three expressions. Their common denominator is r 5. Hence the
three terms Ϫ1>r 3 contribute Ϫ3r 2 to the numerator, and the three other terms give
the sum
3(x Ϫ x 0)2 ϩ 3( y Ϫ y0)2 ϩ 3(z Ϫ z 0)2 ϭ 3r 2,
᭿

so that the numerator is 0, and we obtain (9).
ٌ2f is also denoted by ¢ f. The differential operator

(11)

ٌ2 ϭ ¢ ϭ

02
0x 2

ϩ

02
0y 2

ϩ

02
0z 2

(read “nabla squared” or “delta”) is called the Laplace operator. It can be shown that the
field of force produced by any distribution of masses is given by a vector function that is
the gradient of a scalar function f, and f satisfies (9) in any region that is free of matter.
The great importance of the Laplace equation also results from the fact that there are
other laws in physics that are of the same form as Newton’s law of gravitation. For instance,
in electrostatics the force of attraction (or repulsion) between two particles of opposite (or
like) charge Q 1 and Q 2 is

(12)

pϭ

k
r3

r

(Coulomb’s law6).

Laplace’s equation will be discussed in detail in Chaps. 12 and 18.
A method for finding out whether a given vector field has a potential will be explained
in Sec. 9.9.

6

CHARLES AUGUSTIN DE COULOMB (1736–1806), French physicist and engineer. Coulomb’s law was
derived by him from his own very precise measurements.

c09.qxd

10/30/10

402

3:25 PM

Page 402

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

PROBLEM SET 9.7
1–6
CALCULATION OF GRADIENTS
Find grad f. Graph some level curves f ϭ const. Indicate
ٌf by arrows at some points of these curves.
1. f ϭ (x ϩ 1)(2y Ϫ 1)
2. f ϭ 9x 2 ϩ 4y 2
3. f ϭ y>x
4. ( y ϩ 6)2 ϩ (x Ϫ 4)2
5. f ϭ x 4 ϩ y 4
6. f ϭ (x 2 Ϫ y 2)>(x 2 ϩ y 2)
7–10
USEFUL FORMULAS FOR GRADIENT
AND LAPLACIAN
Prove and illustrate by an example.
7. ٌ( f n) ϭ nf n؊1ٌf
8. ٌ( fg) ϭ f ٌg ϩ gٌf
9. ٌ( f>g) ϭ (1>g2)(gٌf Ϫ f ٌg)
10. ٌ2( fg) ϭ gٌ2f ϩ 2ٌf ؒ ٌg ϩ f ٌ2g
11–15
USE OF GRADIENTS. ELECTRIC FORCE
The force in an electrostatic field given by f (x, y, z) has the
direction of the gradient. Find ٌf and its value at P.
11. f ϭ xy, P: (Ϫ4, 5)
12. f ϭ x>(x 2 ϩ y 2), P: (1, 1)
13. f ϭ ln (x 2 ϩ y 2), P: (8, 6)
14. f ϭ (x 2 ϩ y 2 ϩ z 2)؊1>2 P: (12, 0, 16)
15. f ϭ 4x 2 ϩ 9y 2 ϩ z 2, P: (5, Ϫ1, Ϫ11)
16. For what points P: (x, y, z) does ٌf with
f ϭ 25x 2 ϩ 9y 2 ϩ 16z 2 have the direction from P to
the origin?
17. Same question as in Prob. 16 when f ϭ 25x 2 ϩ 4y 2.

9.8

VELOCITY FIELDS
18–23
Given the velocity potential f of a flow, find the velocity
v ϭ ٌf of the field and its value v( P) at P. Sketch v( P)
and the curve f ϭ const passing through P.
18. f ϭ x 2 Ϫ 6x Ϫ y 2, P: (Ϫ1, 5)
19. f ϭ cos x cosh y, P: (12 p, ln 2)
20. f ϭ x(1 ϩ (x 2 ϩ y 2)؊1), P: (1, 1)
21. f ϭ ex cos y, P: (1, 12 p)
22. At what points is the flow in Prob. 21 directed vertically
upward?
23. At what points is the flow in Prob. 21 horizontal?
24–27
HEAT FLOW
Experiments show that in a temperature field, heat flows in
the direction of maximum decrease of temperature T. Find
this direction in general and at the given point P. Sketch
that direction at P as an arrow.
24. T ϭ 3x 2 Ϫ 2y 2, P: (2.5, 1.8)
25. T ϭ z>(x 2 ϩ y 2), P: (0, 1, 2)
26. T ϭ x 2 ϩ y 2 ϩ 4z 2, P: (2, Ϫ1, 2)
27. CAS PROJECT. Isotherms. Graph some curves of
constant temperature (“isotherms”) and indicate
directions of heat flow by arrows when the temperature
equals (a) x 3 Ϫ 3xy 2, (b) sin x sinh y, and (c) excos y.
28. Steepest ascent. If z (x, y) ϭ 3000 Ϫ x 2 Ϫ 9y 2
[meters] gives the elevation of a mountain at sea level,
what is the direction of steepest ascent at P: (4, 1)?
29. Gradient. What does it mean if ƒ ٌf (P) ƒ Ͼ ƒ ٌf (Q) ƒ at
two points P and Q in a scalar field?

Divergence of a Vector Field
Vector calculus owes much of its importance in engineering and physics to the gradient,
divergence, and curl. From a scalar field we can obtain a vector field by the gradient
(Sec. 9.7). Conversely, from a vector field we can obtain a scalar field by the divergence
or another vector field by the curl (to be discussed in Sec. 9.9). These concepts were
suggested by basic physical applications. This will be evident from our examples.
To begin, let v (x, y, z) be a differentiable vector function, where x, y, z are Cartesian
coordinates, and let v1, v2, v3 be the components of v. Then the function

(1)

div v ϭ

0v1
0x

ϩ

0v2
0y

ϩ

0v3
0z

c09.qxd

10/30/10

3:25 PM

Page 403

SEC. 9.8 Divergence of a Vector Field

403

is called the divergence of v or the divergence of the vector field defined by v. For
example, if
v ϭ [3xz, 2xy, Ϫyz 2] ϭ 3xzi ϩ 2xy j Ϫ yz 2k,

div v ϭ 3z ϩ 2x Ϫ 2yz.

then

Another common notation for the divergence is
div v ϭ ٌ • v ϭ c
ϭa
ϭ

0 0 0
, ,
d • [v1, v2, v3]
0x 0y 0z

0
0
0
i ϩ j ϩ kb • (v1i ϩ v2 j ϩ v3k)
0x
0y
0z

0v1
0x

ϩ

0v2
0y

ϩ

0v3

,

0z

with the understanding that the “product” (0>0x ) v1 in the dot product means the partial
derivative 0v1>0x, etc. This is a convenient notation, but nothing more. Note that ١ • v
means the scalar div v, whereas ٌf means the vector grad f defined in Sec. 9.7.
In Example 2 we shall see that the divergence has an important physical meaning.
Clearly, the values of a function that characterizes a physical or geometric property must
be independent of the particular choice of coordinates. In other words, these values must
be invariant with respect to coordinate transformations. Accordingly, the following
theorem should hold.
˛

THEOREM 1

Invariance of the Divergence

The divergence div v is a scalar function, that is, its values depend only on the
points in space (and, of course, on v) but not on the choice of the coordinates in
(1), so that with respect to other Cartesian coordinates x*, y*, z* and corresponding
components v1*, v2*, v3* of v,
(2)

div v ϭ

0v*1
0x*

ϩ

0v*2
0y*

ϩ

0v*3

.

0z*

We shall prove this theorem in Sec. 10.7, using integrals.
Presently, let us turn to the more immediate practical task of gaining a feel for the
significance of the divergence. Let f (x, y, z) be a twice differentiable scalar function. Then
its gradient exists,
v ϭ grad f ϭ c

0f 0f 0f
0f
0f
0f
, , d ϭ
iϩ jϩ k
0x 0y 0z
0x
0y
0z

and we can differentiate once more, the first component with respect to x, the second with
respect to y, the third with respect to z, and then form the divergence,
div v ϭ div (grad f ) ϭ

0 2f
0x 2

ϩ

0 2f
0y 2

ϩ

0 2f
0z 2

.

c09.qxd

10/30/10

3:25 PM

404

Page 404

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

Hence we have the basic result that the divergence of the gradient is the Laplacian
(Sec. 9.7),
div (grad f ) ϭ ٌ2f.

(3)
EXAMPLE 1

Gravitational Force. Laplace’s Equation
The gravitational force p in Theorem 3 of the last section is the gradient of the scalar function f (x, y, z) ϭ c>r,
which satisfies Laplaces equation ٌ2f ϭ 0. According to (3) this implies that div p ϭ 0 (r Ͼ 0).
᭿

The following example from hydrodynamics shows the physical significance of the
divergence of a vector field. We shall get back to this topic in Sec. 10.8 and add further
physical details.
EXAMPLE 2

Flow of a Compressible Fluid. Physical Meaning of the Divergence
We consider the motion of a fluid in a region R having no sources or sinks in R, that is, no points at which
fluid is produced or disappears. The concept of fluid state is meant to cover also gases and vapors. Fluids in
the restricted sense, or liquids, such as water or oil, have very small compressibility, which can be neglected in
many problems. In contrast, gases and vapors have high compressibility. Their density r (ϭ mass per unit volume)
depends on the coordinates x, y, z in space and may also depend on time t. We assume that our fluid is
compressible. We consider the flow through a rectangular box B of small edges ¢x, ¢y, ¢z parallel to the
coordinate axes as shown in Fig. 218. (Here ¢ is a standard notation for small quantities and, of course, has
nothing to do with the notation for the Laplacian in (11) of Sec. 9.7.) The box B has the volume ¢V ϭ ¢x ¢y ¢z.
Let v ϭ [v1, v2, v3] ϭ v1i ϩ v2j ϩ v3k be the velocity vector of the motion. We set
(4)

u ϭ rv ϭ [u 1, u 2, u 3] ϭ u 1i ϩ u 2 j ϩ u 3 k

and assume that u and v are continuously differentiable vector functions of x, y, z, and t, that is, they have first
partial derivatives which are continuous. Let us calculate the change in the mass included in B by considering
the flux across the boundary, that is, the total loss of mass leaving B per unit time. Consider the flow through
the left of the three faces of B that are visible in Fig. 218, whose area is ¢x ¢z. Since the vectors v1i and v3 k
are parallel to that face, the components v1 and v3 of v contribute nothing to this flow. Hence the mass of fluid
entering through that face during a short time interval ¢t is given approximately by
(rv2)y ¢x ¢z ¢t ϭ (u2)y ¢x ¢z ¢t,
where the subscript y indicates that this expression refers to the left face. The mass of fluid leaving the box B
through the opposite face during the same time interval is approximately (u 2)yϩ¢y ¢x ¢z ¢t, where the subscript
y ϩ ¢y indicates that this expression refers to the right face (which is not visible in Fig. 218). The difference
¢u 2 ¢x ¢z ¢t ϭ

¢u 2

[¢u 2 ϭ (u 2)yϩ¢y Ϫ (u 2)y]

¢V ¢t
¢y

is the approximate loss of mass. Two similar expressions are obtained by considering the other two pairs of
parallel faces of B. If we add these three expressions, we find that the total loss of mass in B during the time
interval ¢t is approximately
a

¢u 1

ϩ

¢x

¢u 2

ϩ

¢y

¢u 3
¢z

b ¢V ¢t,

where
¢u 1 ϭ (u 1)xϩ¢x Ϫ (u 1)x

and

¢u 3 ϭ (u 3)zϩ¢z Ϫ (u 3)z.

This loss of mass in B is caused by the time rate of change of the density and is thus equal to
0r
Ϫ

¢V ¢t.
0t

c09.qxd

10/30/10

3:25 PM

Page 405

SEC. 9.8 Divergence of a Vector Field

405

Box B
Δz
(x, y, z)

z

Δx

k
j
i

Δy

y

x

Fig. 218. Physical interpretation of the divergence
If we equate both expressions, divide the resulting equation by ¢V ¢t, and let ¢x, ¢y, ¢z, and ¢t approach
zero, then we obtain
0r
div u ϭ div (rv) ϭ Ϫ

0t

or
0r
(5)
0t

ϩ div (rv) ϭ 0.

This important relation is called the condition for the conservation of mass or the continuity equation of a
compressible fluid flow.
If the flow is steady, that is, independent of time, then 0r> 0t ϭ 0 and the continuity equation is
div (rv) ϭ 0.

(6)

If the density r is constant, so that the fluid is incompressible, then equation (6) becomes
div v ϭ 0.

(7)

This relation is known as the condition of incompressibility. It expresses the fact that the balance of outflow
and inflow for a given volume element is zero at any time. Clearly, the assumption that the flow has no sources
or sinks in R is essential to our argument. v is also referred to as solenoidal.
From this discussion you should conclude and remember that, roughly speaking, the divergence measures
᭿
outflow minus inflow.

Comment. The divergence theorem of Gauss, an integral theorem involving the
divergence, follows in the next chapter (Sec. 10.7).

PROBLEM SET 9.8
1–6
CALCULATION OF THE DIVERGENCE
Find div v and its value at P.
1. v ϭ [x 2, 4y 2, 9z 2], P: (Ϫ1, 0, 12 ]
2. v ϭ [0, cos xyz, sin xyz], P: (2, 12 p, 0]
3. v ϭ (x 2 ϩ y 2)؊1[x, y]
4. v ϭ [v1( y, z), v2(z, x), v3(x, y)], P: (3, 1, Ϫ1)]

5.
6.
7.
8.

v ϭ x 2y 2z 2[x, y, z], P: (3, Ϫ1, 4)
v ϭ (x 2 ϩ y 2 ϩ z 2)؊3>2[x, y, z]
For what v3 is v ϭ [ex cos y, ex sin y, v3] solenoidal?
Let v ϭ [x, y, v3]. Find a v3 such that (a) div v Ͼ 0
everywhere, (b) div v Ͼ 0 if ƒ z ƒ Ͻ 1 and div v Ͻ 0 if
ƒ z ƒ Ͼ 1.

c09.qxd

10/30/10

406

3:25 PM

Page 406

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl
that at time t ϭ 0 are in the cube whose faces are
portions of the planes x ϭ 0, x ϭ 1, y ϭ 0, y ϭ 1,
z ϭ 0, z ϭ 1 occupy at t ϭ 1 the volume 1.
12. Compressible flow. Consider the flow with velocity
vector v ϭ xi. Show that the individual particles have
the position vectors r (t) ϭ c1eti ϩ c2 j ϩ c3k with
constant c1, c2, c3. Show that the particles that at
t ϭ 0 are in the cube of Prob. 11 at t ϭ 1 occupy the
volume e.
13. Rotational flow. The velocity vector v (x, y, z) of an
incompressible fluid rotating in a cylindrical vessel is
of the form v ϭ w ؋ r, where w is the (constant)
rotation vector; see Example 5 in Sec. 9.3. Show that
div v ϭ 0. Is this plausible because of our present
Example 2?
14. Does div u ϭ div v imply u ϭ v or u ϭ v ϩ k
(k constant)? Give reason.

9. PROJECT. Useful Formulas for the Divergence.
Prove
(a) div (kv) ϭ k div v (k constant)
(b) div ( fv) ϭ f div v ϩ v • ٌf
(c) div ( f ٌg) ϭ f ٌ2g ϩ ٌf • ٌg
(d) div ( f ٌg) Ϫ div (gٌf ) ϭ f ٌ2g Ϫ gٌ2f
Verify (b) for f ϭ exyz and v ϭ axi ϩ byj ϩ czk.
Obtain the answer to Prob. 6 from (b). Verify (c) for
f ϭ x 2 Ϫ y 2 and g ϭ exϩy. Give examples of your
own for which (a)–(d) are advantageous.
10. CAS EXPERIMENT. Visualizing the Divergence.
Graph the given velocity field v of a fluid flow in a
square centered at the origin with sides parallel to the
coordinate axes. Recall that the divergence measures
outflow minus inflow. By looking at the flow near the
sides of the square, can you see whether div v must
be positive or negative or may perhaps be zero? Then
calculate div v. First do the given flows and then do
some of your own. Enjoy it.
(a) v ϭ i
(b) v ϭ xi
(c) v ϭ xi Ϫ yj
(d) v ϭ xi ϩ yj
(e) v ϭ Ϫxi Ϫ yj
(f) v ϭ (x 2 ϩ y 2)؊1(Ϫyi ϩ xj)
11. Incompressible flow. Show that the flow with velocity
vector v ϭ yi is incompressible. Show that the particles

9.9

LAPLACIAN

15–20

Calculate ٌ2f by Eq. (3). Check by direct differentiation.
Indicate when (3) is simpler. Show the details of your work.
15. f ϭ cos2 x ϩ sin2 y
16. f ϭ exyz
17. f ϭ ln (x 2 ϩ y 2)
18. f ϭ z Ϫ 2x 2 ϩ y 2
19. f ϭ 1>(x 2 ϩ y 2 ϩ z 2)
20. f ϭ e2x cosh 2y

Curl of a Vector Field
The concepts of gradient (Sec. 9.7), divergence (Sec. 9.8), and curl are of fundamental
importance in vector calculus and frequently applied in vector fields. In this section
we define and discuss the concept of the curl and apply it to several engineering
problems.
Let v(x, y, z) ϭ [v1, v2, v3] ϭ v1i ϩ v2 j ϩ v3k be a differentiable vector function of
the Cartesian coordinates x, y, z. Then the curl of the vector function v or of the vector
field given by v is defined by the “symbolic” determinant

(1)

i
0
curl v ϭ ٌ ؋ v ϭ 4
0x
v1
ϭa

0v3
0y

Ϫ

0v2
0z

j
0
0y
v2

bi ϩ a

k
0
4
0z
v3
0v1
0z

Ϫ

0v3
0x

bj ϩ a

0v2
0x

Ϫ

0v1
0y

b k.

c09.qxd

10/30/10

3:25 PM

Page 407

SEC. 9.9 Curl of a Vector Field

407

This is the formula when x, y, z are right-handed. If they are left-handed, the determinant
has a minus sign in front ( just as in (2**) in Sec. 9.3).
Instead of curl v one also uses the notation rot v. This is suggested by “rotation,”
an application explored in Example 2. Note that curl v is a vector, as shown in
Theorem 3.
EXAMPLE 1

Curl of a Vector Function
Let v ϭ [yz, 3zx,

z] ϭ yzi ϩ 3zxj ϩ zk with right-handed x, y, z. Then (1) gives

curl v ϭ 5

i

j

k

0
0x

0
0y

0
5 ϭ Ϫ3xi ϩ yj ϩ (3z Ϫ z)k ϭ Ϫ3xi ϩ yj ϩ 2zk.
0z

yz

3zx

z

᭿

The curl has many applications. A typical example follows. More about the nature and
significance of the curl will be considered in Sec. 10.9.
EXAMPLE 2

Rotation of a Rigid Body. Relation to the Curl
We have seen in Example 5, Sec. 9.3, that a rotation of a rigid body B about a fixed axis in space can be
described by a vector w of magnitude v in the direction of the axis of rotation, where v (Ͼ0) is the angular
speed of the rotation, and w is directed so that the rotation appears clockwise if we look in the direction of w.
According to (9), Sec. 9.3, the velocity field of the rotation can be represented in the form
vϭw؋r
where r is the position vector of a moving point with respect to a Cartesian coordinate system having the origin
on the axis of rotation. Let us choose right-handed Cartesian coordinates such that the axis of rotation is the
z-axis. Then (see Example 2 in Sec. 9.4)
w ϭ [0, 0,

v] ϭ vk,

v ϭ w ؋ r ϭ [Ϫvy, vx, 0] ϭ Ϫvyi ϩ vxj.

Hence
i

j

k

0
0x

0
0y

0
6 ϭ [0, 0,
0z

Ϫvy

vx

0

curl v ϭ 6

2v] ϭ 2vk ϭ 2w.

This proves the following theorem.

THEOREM 1

᭿

Rotating Body and Curl

The curl of the velocity field of a rotating rigid body has the direction of
the axis of the rotation, and its magnitude equals twice the angular speed of the
rotation.

Next we show how the grad, div, and curl are interrelated, thereby shedding further light
on the nature of the curl.

c09.qxd

10/30/10

3:25 PM

408

Page 408

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

THEOREM 2

Grad, Div, Curl

Gradient fields are irrotational. That is, if a continuously differentiable vector
function is the gradient of a scalar function f, then its curl is the zero vector,
(2)

curl (grad f ) ϭ 0.

Furthermore, the divergence of the curl of a twice continuously differentiable vector
function v is zero,
(3)

PROOF
EXAMPLE 3

div (curl v) ϭ 0.

Both (2) and (3) follow directly from the definitions by straightforward calculation. In the
proof of (3) the six terms cancel in pairs.
᭿
Rotational and Irrotational Fields
The field in Example 2 is not irrotational. A similar velocity field is obtained by stirring tea or coffee in a cup.
The gravitational field in Theorem 3 of Sec. 9.7 has curl p ϭ 0. It is an irrotational gradient field.
᭿

The term “irrotational” for curl v ϭ 0 is suggested by the use of the curl for characterizing
the rotation in a field. If a gradient field occurs elsewhere, not as a velocity field, it is
usually called conservative (see Sec. 9.7). Relation (3) is plausible because of the
interpretation of the curl as a rotation and of the divergence as a flux (see Example 2 in
Sec. 9.8).
Finally, since the curl is defined in terms of coordinates, we should do what we did for
the gradient in Sec. 9.7, namely, to find out whether the curl is a vector. This is true, as
follows.
THEOREM 3

Invariance of the Curl

curl v is a vector. It has a length and a direction that are independent of the particular
choice of a Cartesian coordinate system in space.

PROOF

The proof is quite involved and shown in App. 4.
We have completed our discussion of vector differential calculus. The companion
Chap. 10 on vector integral calculus follows and makes use of many concepts covered
in this chapter, including dot and cross products, parametric representation of curves C,
along with grad, div, and curl.

PROBLEM SET 9.9
1. WRITING REPORT. Grad, div, curl. List the
definitions and most important facts and formulas for
grad, div, curl, and ٌ2. Use your list to write a
corresponding report of 3–4 pages, with examples of
your own. No proofs.

2. (a) What direction does curl v have if v is parallel
to the yz-plane? (b) If, moreover, v is independent
of x?
3. Prove Theorem 2. Give two examples for (2) and (3)
each.

c09.qxd

10/30/10

3:25 PM

Page 409

Chapter 9 Review Questions and Problems
4–8

CALCULUTION OF CURL

Find curl v for v given with respect to right-handed
Cartesian coordinates. Show the details of your work.
4. v ϭ [2y 2, 5x, 0]
5. v ϭ xyz [x, y, z]
6. v ϭ (x 2 ϩ y 2 ϩ z 2)؊3>2 [x, y, z]
7. v ϭ [0, 0, e؊x sin y]
2
2
2
8. v ϭ [e؊z , e؊x , e؊y ]
9–13

FLUID FLOW

Let v be the velocity vector of a steady fluid flow. Is the
flow irrotational? Incompressible? Find the streamlines (the
paths of the particles). Hint. See the answers to Probs. 9
and 11 for a determination of a path.
9. v ϭ [0, 3z 2, 0]
10. v ϭ [sec x, csc x, 0]
11. v ϭ [ y, Ϫ2x, 0]
12. v ϭ [Ϫy, x, p]
13. v ϭ [x, y, Ϫz]

409
14. PROJECT. Useful Formulas for the Curl. Assuming
sufficient differentiability, show that
(a) curl (u ϩ v) ϭ curl u ϩ curl v
(b) div (curl v) ϭ 0
(c) curl ( f v) ϭ (grad f ) ؋ v ϩ f curl v
(d) curl (grad f ) ϭ 0
(e) div (u ؋ v) ϭ v • curl u Ϫ u • curl v
15–20

DIV AND CURL

With respect to right-handed coordinates, let u ϭ [y, z, x],
v ϭ [ yz, zx, xy], f ϭ xyz, and g ϭ x ϩ y ϩ z. Find the given
expressions. Check your result by a formula in Proj. 14
if applicable.
15. curl (u ϩ v), curl v
16. curl (gv)
17. v • curl u, u • curl v, u • curl u
18. div (u ؋ v)
19. curl (gu ϩ v), curl (gu)
20. div (grad ( fg))

CHAPTER 9 REVIEW QUESTIONS AND PROBLEMS
1. What is a vector? A vector function? A vector field? A
scalar? A scalar function? A scalar field? Give examples.
2. What is an inner product, a vector product, a scalar triple
product? What applications motivate these products?
3. What are right-handed and left-handed coordinates?
When is this distinction important?
4. When is a vector product the zero vector? What is
orthogonality?
5. How is the derivative of a vector function defined?
What is its significance in geometry and mechanics?
6. If r(t) represents a motion, what are r r (t), ƒ r r (t) ƒ , r s (t),
and ƒ r s (t) ƒ ?
7. Can a moving body have constant speed but variable
velocity? Nonzero acceleration?
8. What do you know about directional derivatives? Their
relation to the gradient?
9. Write down the definitions and explain the significance
of grad, div, and curl.
10. Granted sufficient differentiability, which of the
following expressions make sense? f curl v, v curl f,
u ؋ v, u ؋ v ؋ w, f • v, f • (v ؋ w), u • (v ؋ w),
v ؋ curl v, div ( f v), curl ( f v), and curl ( f • v).
11–19

ALGEBRAIC OPERATIONS FOR VECTORS

Let a ϭ [4, 7, 0], b ϭ [3, Ϫ1, 5], c ϭ [Ϫ6, 2, 0], and d ϭ
[1, Ϫ2, 8]. Calculate the following expressions. Try to
make a sketch.
11. a • c, 3b • 8d, 24d • b, a • a

12. a ؋ c, b ؋ d, d ؋ b, a ؋ a
13. b ؋ c, c ؋ b, c ؋ c, c • c
14. 5(a ؋ b) • c, a • (5b ؋ c), (5a b c), 5(a • b) ؋ c
15. 6(a ؋ b) ؋ d, a ؋ 6(b ؋ d), 2a ؋ 3b ؋ d
16. (1> ƒ a ƒ )a, (1> ƒ b ƒ )b, a • b> ƒ b ƒ , a • b> ƒ a ƒ
17. (a b d), (b a d), (b d a)
18. ƒ a ϩ b ƒ ,

ƒaƒ ϩ ƒbƒ

19. a ؋ b Ϫ b ؋ a, (a ؋ c) • c,

ƒa ؋ bƒ

20. Commutativity. When is u ؋ v ϭ v ؋ u? When is
u • v ϭ v • u?
21. Resultant, equilibrium. Find u such that u and a, b,
c, d above and u are in equilibrium.
22. Resultant. Find the most general v such that the resultant
of v, a, b, c (see above) is parallel to the yz-plane.
23. Angle. Find the angle between a and c. Between b and
d. Sketch a and c.
24. Planes. Find the angle between the two planes
P1: 4x Ϫ y ϩ 3z ϭ 12 and P2: x ϩ 2y ϩ 4z ϭ 4. Make
a sketch.
25. Work. Find the work done by q ϭ [5, 2, 0] in the
displacement from (1, 1, 0) to (4, 3, 0).
26. Component. When is the component of a vector v in
the direction of a vector w equal to the component of
w in the direction of v?
27. Component. Find the component of v ϭ [4, 7, 0] in
the direction of w ϭ [2, 2, 0]. Sketch it.

c09.qxd

10/30/10

410

3:25 PM

Page 410

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

28. Moment. When is the moment of a force equal to zero?
29. Moment. A force p ϭ [4, 2, 0] is acting in a line
through (2, 3, 0). Find its moment vector about the
center (5, 1, 0) of a wheel.
30. Velocity, acceleration. Find the velocity, speed,
and acceleration of the motion given by r (t) ϭ [3
cos t, 3 sin t, 4t] (t ϭ time) at the point P : (3> 12,
3> 12, p).
31. Tetrahedron. Find the volume if the vertices are
(0, 0, 0), (3, 1, 2), (2, 4, 0), (5, 4, 0).

SUMMARY OF CHAPTER

32–40

GRAD, DIV, CURL, ٌ2, Dvf

Let f ϭ xy Ϫ yz, v ϭ [2y, 2z, 4x ϩ z], and w ϭ [3z 2,
x 2 Ϫ y 2, y 2]. Find:
32. grad f and f grad f at P: (2, 7, 0)
33. div v, div w
34. curl v, curl w
35. div (grad f ), ٌ2f, ٌ2(xyf )
36. (curl w) • v at (4, 0, 2)
37. grad (div w)
38. Dv f at P: (1, 1, 2)
39. Dw f at P: (3, 0, 2)
40. v • ((curl w) ؋ v)

9

Vector Differential Calculus. Grad, Div, Curl
All vectors of the form a ϭ [a1, a2, a3] ϭ a1i ϩ a2 j ϩ a3k constitute the real
vector space R 3 with componentwise vector addition
(1)

[a1, a2, a3] ϩ [b1, b2, b3] ϭ [a1 ϩ b1, a2 ϩ b2, a3 ϩ b3]

and componentwise scalar multiplication (c a scalar, a real number)
(2)

c[a1, a2, a3] ϭ [ca1, ca2, ca3]

(Sec. 9.1).

For instance, the resultant of forces a and b is the sum a ϩ b.
The inner product or dot product of two vectors is defined by
(3)

a • b ϭ ƒ a ƒ ƒ b ƒ cos g ϭ a1b1 ϩ a2b2 ϩ a3b3

(Sec. 9.2)

where g is the angle between a and b. This gives for the norm or length ƒ a ƒ of a
(4)

ƒ a ƒ ϭ 1a • a ϭ 2a 21 ϩ a 22 ϩ a 23

as well as a formula for g. If a • b ϭ 0, we call a and b orthogonal. The dot product
is suggested by the work W ϭ p • d done by a force p in a displacement d.
The vector product or cross product v ϭ a ؋ b is a vector of length
(5)

ƒ a ؋ b ƒ ϭ ƒ a ƒ ƒ b ƒ sin g

(Sec. 9.3)

and perpendicular to both a and b such that a, b, v form a right-handed triple. In
terms of components with respect to right-handed coordinates,
i
(6)

j

k

a ؋ b ϭ 4 a1

a2

a3 4

b1

b2

b3

(Sec. 9.3).

c09.qxd

10/30/10

3:25 PM

Page 411

Summary of Chapter 9

411

The vector product is suggested, for instance, by moments of forces or by rotations.
CAUTION! This multiplication is anticommutative, a ؋ b ϭ Ϫb ؋ a, and is not
associative.
An (oblique) box with edges a, b, c has volume equal to the absolute value of
the scalar triple product
(a b c) ϭ a • (b ؋ c) ϭ (a ؋ b) • c.

(7)

Sections 9.4–9.9 extend differential calculus to vector functions
v (t) ϭ [v1(t), v2(t), v3(t)] ϭ v1(t)i ϩ v2(t)j ϩ v3(t)k
and to vector functions of more than one variable (see below). The derivative of
v(t) is
(8)

vr ϭ

v(t ϩ ¢t) Ϫ v(t)
dv
ϭ lim
ϭ [v1r , v2r , v3r ] ϭ v1r i ϩ v2r j ϩ v3r k.
¢t : 0
dt
¢t

Differentiation rules are as in calculus. They imply (Sec. 9.4)
(u • v) r ϭ u r • v ϩ u • v r ,

(u ؋ v) r ϭ u r ؋ v ϩ u ؋ v r .

Curves C in space represented by the position vector r(t) have r r (t) as a tangent
vector (the velocity in mechanics when t is time), r r (s) (s arc length, Sec. 9.5) as
the unit tangent vector, and ƒ r s (s) ƒ ϭ ␬ as the curvature (the acceleration in
mechanics).
Vector functions v (x, y, z) ϭ [v1 (x, y, z), v2 (x, y, z), v3 (x, y, z)] represent vector
fields in space. Partial derivatives with respect to the Cartesian coordinates x, y, z
are obtained componentwise, for instance,
0v1
0v1 0v2 0v3
0v2
0v3
0v
ϭB
,
,
Rϭ
iϩ
jϩ
k
0x
0x 0x 0x
0x
0x
0x

(Sec. 9.6).

The gradient of a scalar function f is
(9)

grad f ϭ ٌf ϭ B

0f 0f 0f
, , R
0x 0y 0z

(Sec. 9.7).

The directional derivative of f in the direction of a vector a is
(10)

Da f ϭ

df
ds

ϭ

1
ƒaƒ

a • ٌf

(Sec. 9.7).

The divergence of a vector function v is
(11)

div v ϭ ٌ • v ϭ

0v1
0x

ϩ

0v2
0y

ϩ

0v3
0z

.

(Sec. 9.8).

c09.qxd

10/30/10

412

3:25 PM

Page 412

CHAP. 9 Vector Differential Calculus. Grad, Div, Curl

The curl of v is

(12)

curl v ϭ ٌ ؋ v ϭ 5

i

j

k

0
0x

0
0y

0
5
0z

v1

v2

v3

or minus the determinant if the coordinates are left-handed.
Some basic formulas for grad, div, curl are (Secs. 9.7–9.9)
ٌ( fg) ϭ f ٌg ϩ gٌf

(13)

ٌ( f>g) ϭ (1>g2)(gٌf Ϫ f ٌg)
div ( f v) ϭ f div v ϩ v • ٌf

(14)

(15)

div ( f ٌg) ϭ f ٌ2g ϩ ٌf • ٌg
ٌ2f ϭ div (ٌf )
ٌ2( fg) ϭ gٌ2f ϩ 2ٌf • ٌg ϩ f ٌ2g
curl ( f v) ϭ ٌf ؋ v ϩ f curl v

(16)

div (u ؋ v) ϭ v • curl u Ϫ u • curl v
curl (ٌf ) ϭ 0

(17)

div (curl v) ϭ 0.

For grad, div, curl, and ٌ in curvilinear coordinates see App. A3.4.
2

(Sec. 9.9)

c10-a.qxd

10/30/10

12:18 PM

Page 413

CHAPTER

10

Vector Integral Calculus.
Integral Theorems
Vector integral calculus can be seen as a generalization of regular integral calculus. You
may wish to review integration. (To refresh your memory, there is an optional review
section on double integrals; see Sec. 10.3.)
Indeed, vector integral calculus extends integrals as known from regular calculus to
integrals over curves, called line integrals (Secs. 10.1, 10.2), surfaces, called surface integrals
(Sec. 10.6), and solids, called triple integrals (Sec. 10.7). The beauty of vector integral
calculus is that we can transform these different integrals into one another. You do this
to simplify evaluations, that is, one type of integral might be easier to solve than another,
such as in potential theory (Sec. 10.8). More specifically, Green’s theorem in the plane
allows you to transform line integrals into double integrals, or conversely, double integrals
into line integrals, as shown in Sec. 10.4. Gauss’s convergence theorem (Sec. 10.7) converts
surface integrals into triple integrals, and vice-versa, and Stokes’s theorem deals with
converting line integrals into surface integrals, and vice-versa.
This chapter is a companion to Chapter 9 on vector differential calculus. From Chapter 9,
you will need to know inner product, curl, and divergence and how to parameterize curves.
The root of the transformation of the integrals was largely physical intuition. Since the
corresponding formulas involve the divergence and the curl, the study of this material will
lead to a deeper physical understanding of these two operations.
Vector integral calculus is very important to the engineer and physicist and has many
applications in solid mechanics, in fluid flow, in heat problems, and others.
Prerequisite: Elementary integral calculus, Secs. 9.7–9.9
Sections that may be omitted in a shorter course: 10.3, 10.5, 10.8
References and Answers to Problems: App. 1 Part B, App. 2

10.1

Line Integrals
The concept of a line integral is a simple and natural generalization of a definite integral
b

(1)

Ύ f (x) dx.
a

Recall that, in (1), we integrate the function f (x), also known as the integrand, from x ϭ a
along the x-axis to x ϭ b. Now, in a line integral, we shall integrate a given function, also
413

c10-a.qxd

10/30/10

414

12:18 PM

Page 414

CHAP. 10 Vector Integral Calculus. Integral Theorems
B

A
B
C

C

A
(a)

(b)

Fig. 219. Oriented curve

called the integrand, along a curve C in space or in the plane. (Hence curve integral
would be a better name but line integral is standard).
This requires that we represent the curve C by a parametric representation (as in Sec. 9.5)
(2)

r(t) ϭ [x(t), y(t), z(t)] ϭ x(t) i ϩ y(t) j ϩ z(t) k

(a Ϲ t Ϲ b).

The curve C is called the path of integration. Look at Fig. 219a. The path of integration
goes from A to B. Thus A: r(a) is its initial point and B: r(b) is its terminal point. C is
now oriented. The direction from A to B, in which t increases is called the positive
direction on C. We mark it by an arrow. The points A and B may coincide, as it happens
in Fig. 219b. Then C is called a closed path.
C is called a smooth curve if it has at each point a unique tangent whose direction varies
continuously as we move along C. We note that r(t) in (2) is differentiable. Its derivative
r r (t) ϭ dr>dt is continuous and different from the zero vector at every point of C.

General Assumption
In this book, every path of integration of a line integral is assumed to be piecewise smooth,
that is, it consists of finitely many smooth curves.
For example, the boundary curve of a square is piecewise smooth. It consists of four
smooth curves or, in this case, line segments which are the four sides of the square.

Definition and Evaluation of Line Integrals
A line integral of a vector function F(r) over a curve C: r(t) is defined by
b

Ύ F(r) • dr ϭ Ύ F(r(t)) • rr(t) dt

(3)

C

a

rr ϭ

dr
dt

where r(t) is the parametric representation of C as given in (2). (The dot product was defined
in Sec. 9.2.) Writing (3) in terms of components, with dr ϭ [dx, dy, dz] as in Sec. 9.5
and r ϭ d>dt, we get

Ύ F(r) • dr ϭ Ύ (F

1

(3 r )

C

dx ϩ F2 dy ϩ F3 dz)

C

b

ϭ

Ύ (F xr ϩ F yr ϩ F zr) dt.
1

a

2

3

c10-a.qxd

10/30/10

3:32 PM

Page 415

SEC. 10.1 Line Integrals

415

If the path of integration C in (3) is a closed curve, then instead of

Ύ

Ώ.

we also write

C

C

Note that the integrand in (3) is a scalar, not a vector, because we take the dot product. Indeed,
F • r r > ƒ r r ƒ is the tangential component of F. (For “component” see (11) in Sec. 9.2.)
We see that the integral in (3) on the right is a definite integral of a function of t taken
over the interval a Ϲ t Ϲ b on the t-axis in the positive direction: The direction of
increasing t. This definite integral exists for continuous F and piecewise smooth C, because
this makes F • r r piecewise continuous.
Line integrals (3) arise naturally in mechanics, where they give the work done by a
force F in a displacement along C. This will be explained in detail below. We may thus
call the line integral (3) the work integral. Other forms of the line integral will be discussed
later in this section.
EXAMPLE 1

Evaluation of a Line Integral in the Plane
Find the value of the line integral (3) when F(r) ϭ [Ϫy, Ϫxy] ϭ Ϫyi Ϫ xyj and C is the circular arc in Fig. 220
from A to B.

Solution. We may represent C by r(t) ϭ [cos t, sin t] ϭ cos t i ϩ sin t j, where 0 Ϲ t Ϲ p>2. Then
x(t) ϭ cos t, y(t) ϭ sin t, and

F(r(t)) ϭ Ϫy(t) i Ϫ x(t)y(t) j ϭ [Ϫsin t, Ϫcos t sin t] ϭ Ϫsin t i Ϫ cos t sin t j.

y
B

By differentiation, r r (t) ϭ [Ϫsin t,
in the second term]

C

Ύ F(r) • dr ϭ Ύ
C

A
1

p>2

[Ϫsin t, Ϫcos t sin t] • [Ϫsin t, cos t] dt ϭ

0

x

ϭ

Fig. 220. Example 1

EXAMPLE 2

cos t] ϭ Ϫsin t i ϩ cos t j, so that by (3) [use (10) in App. 3.1; set cos t ϭ u

Ύ

p>2

(sin2 t Ϫ cos2 t sin t) dt

0

p>2

0

Ύ

1
2

0

(1 Ϫ cos 2t) dt Ϫ

Ύ u (Ϫdu) ϭ 4 Ϫ 0 Ϫ 3 Ϸ 0.4521.
p

2

1

᭿

1

Line Integral in Space
The evaluation of line integrals in space is practically the same as it is in the plane. To see this, find the value
of (3) when F(r) ϭ [z, x, y] ϭ z i ϩ x j ϩ y k and C is the helix (Fig. 221)
r(t) ϭ [cos t, sin t, 3t] ϭ cos t i ϩ sin t j ϩ 3t k

(4)

(0 Ϲ t Ϲ 2p).

Solution. From (4) we have x(t) ϭ cos t, y(t) ϭ sin t, z(t) ϭ 3t. Thus

z
C

F(r(t)) • r r (t) ϭ (3t i ϩ cos t j ϩ sin t k) • (Ϫsin t i ϩ cos t j ϩ 3k).

B

The dot product is 3t(Ϫsin t) ϩ cos2 t ϩ 3 sin t. Hence (3) gives
x

A

Ύ F(r) • dr ϭ Ύ

y

Fig. 221. Example 2

C

2p

(Ϫ3t sin t ϩ cos2 t ϩ 3 sin t) dt ϭ 6p ϩ p ϩ 0 ϭ 7p Ϸ 21.99.

᭿

0

Simple general properties of the line integral (3) follow directly from corresponding
properties of the definite integral in calculus, namely,
(5a)

Ύ kF • dr ϭ k Ύ F • dr
C

C

(k constant)

c10-a.qxd

10/30/10

12:18 PM

416

Page 416

CHAP. 10 Vector Integral Calculus. Integral Theorems

Ύ (F ϩ G) • dr ϭ Ύ F • dr ϩ Ύ G • dr

B
C2

(5b)

C

C1

C

Ύ F • dr ϭ Ύ F • dr ϩ Ύ F • dr

(5c)

C

A

Fig. 222.
Formula (5c)

THEOREM 1

C

C1

(Fig. 222)

C2

where in (5c) the path C is subdivided into two arcs C1 and C2 that have the same
orientation as C (Fig. 222). In (5b) the orientation of C is the same in all three integrals.
If the sense of integration along C is reversed, the value of the integral is multiplied by Ϫ1.
However, we note the following independence if the sense is preserved.
Direction-Preserving Parametric Transformations

Any representations of C that give the same positive direction on C also yield the
same value of the line integral (3).

PROOF

The proof follows by the chain rule. Let r(t) be the given representation with a Ϲ t Ϲ b
as in (3). Consider the transformation t ϭ ␾(t*) which transforms the t interval to
a* Ϲ t* Ϲ b* and has a positive derivative dt>dt*. We write r(t) ϭ r(␾(t*)) ϭ r*(t*).
Then dt ϭ (dt>dt*) dt* and

Ύ

F(r*) • dr* ϭ

C

Ύ

b*

F(r(␾(t*))) •

a*

dr dt
dt*
dt dt*

b

ϭ

Ύ F(r(t)) • drdt dt ϭ Ύ F(r) • dr.
a

᭿

C

Motivation of the Line Integral (3):
Work Done by a Force
The work W done by a constant force F in the displacement along a straight segment d
is W ϭ F • d; see Example 2 in Sec. 9.2. This suggests that we define the work W done
by a variable force F in the displacement along a curve C: r(t) as the limit of sums of
works done in displacements along small chords of C. We show that this definition amounts
to defining W by the line integral (3).
For this we choose points t 0 (ϭa) Ͻ t 1 Ͻ Á Ͻ t n (ϭb). Then the work ¢Wm done
by F(r(t m)) in the straight displacement from r(t m) to r(t mϩ1) is
¢Wm ϭ F(r(t m)) • [r(t mϩ1) Ϫ r(t m)] Ϸ F(r(t m)) • r r (t m)¢t m

(¢t m ϭ ¢t mϩ1 Ϫ t m).

The sum of these n works is Wn ϭ ¢W0 ϩ Á ϩ ¢Wn؊1. If we choose points and
consider Wn for every n arbitrarily but so that the greatest ¢t m approaches zero as
n : ϱ, then the limit of Wn as n : ϱ is the line integral (3). This integral exists
because of our general assumption that F is continuous and C is piecewise smooth;
this makes r r (t) continuous, except at finitely many points where C may have corners
or cusps.
᭿

c10-a.qxd

10/30/10

12:18 PM

Page 417

SEC. 10.1 Line Integrals
EXAMPLE 3

417

Work Done by a Variable Force
If F in Example 1 is a force, the work done by F in the displacement along the quarter-circle is 0.4521, measured
in suitable units, say, newton-meters (nt # m, also called joules, abbreviation J; see also inside front cover).
Similarly in Example 2.
᭿

EXAMPLE 4

Work Done Equals the Gain in Kinetic Energy
Let F be a force, so that (3) is work. Let t be time, so that dr>dt ϭ v, velocity. Then we can write (3) as
b

Wϭ

(6)

Ύ F • dr ϭ Ύ F(r(t)) • v(t) dt.
C

a

Now by Newton’s second law, that is, force ϭ mass ϫ acceleration, we get
F ϭ mr s (t) ϭ mv r (t),
where m is the mass of the body displaced. Substitution into (5) gives [see (11), Sec. 9.4]
b

b

Ύ mvr • v dt ϭ Ύ m a

Wϭ

a

a

tϭb
v•v r
m
b dt ϭ ƒ v ƒ 2 `
.
2
2
tϭa

On the right, m ƒ v ƒ >2 is the kinetic energy. Hence the work done equals the gain in kinetic energy. This is a
basic law in mechanics.
᭿
2

Other Forms of Line Integrals
The line integrals

ΎF

(7)

1

dx,

C

ΎF

2

ΎF

dy,

3

C

dz

C

are special cases of (3) when F ϭ F1 i or F2 j or F3 k, respectively.
Furthermore, without taking a dot product as in (3) we can obtain a line integral whose
value is a vector rather than a scalar, namely,

(8)

Ύ

F(r) dt ϭ

C

Ύ

b

b

F(r(t)) dt ϭ

a

Ύ [F (r(t)),
1

F2(r(t)), F3(r(t))] dt.

a

Obviously, a special case of (7) is obtained by taking F1 ϭ f, F2 ϭ F3 ϭ 0. Then
b

Ύ f (r) dt ϭ Ύ f (r(t)) dt

(8*)

C

a

with C as in (2). The evaluation is similar to that before.
EXAMPLE 5

A Line Integral of the Form (8)
Integrate F(r) ϭ [xy, yz, z] along the helix in Example 2.

Solution. F(r(t)) ϭ [cos t sin t, 3t sin t, 3t] integrated with respect to t from 0 to 2p gives

Ύ

2p

0

1
F(r(t) dt ϭ c Ϫ cos2 t,
2

3 sin t Ϫ 3t cos t,

3
2

t2d `

2p

ϭ [0, Ϫ6p,
0

6p2].

᭿

c10-a.qxd

10/30/10

12:18 PM

418

Page 418

CHAP. 10 Vector Integral Calculus. Integral Theorems

Path Dependence
Path dependence of line integrals is practically and theoretically so important that we
formulate it as a theorem. And a whole section (Sec. 10.2) will be devoted to conditions
under which path dependence does not occur.
THEOREM 2

Path Dependence

The line integral (3) generally depends not only on F and on the endpoints A and
B of the path, but also on the path itself along which the integral is taken.
PROOF

Almost any example will show this. Take, for instance, the straight segment C1: r1(t) ϭ
[t, t, 0] and the parabola C2: r2(t) ϭ [t, t 2, 0] with 0 Ϲ t Ϲ 1 (Fig. 223) and integrate
F ϭ [0, xy, 0]. Then F(r1(t)) • r1r (t) ϭ t 2, F(r2(t)) • r2r (t) ϭ 2t 4, so that integration gives
1>3 and 2>5, respectively.
᭿
B

1
C1
C2
A

1

Fig. 223. Proof of Theorem 2

PROBLEM SET 10.1
1. WRITING PROJECT. From Definite Integrals to
Line Integrals. Write a short report (1–2 pages) with
examples on line integrals as generalizations of definite
integrals. The latter give the area under a curve. Explain
the corresponding geometric interpretation of a line
integral.
2–11
Calculate

LINE INTEGRAL. WORK

Ύ F(r) • dr for the given data. If F is a force, this
C

gives the work done by the force in the displacement along
C. Show the details.
2. F ϭ [ y 2, Ϫx 2], C: y ϭ 4x 2 from (0, 0) to (1, 4)
3. F as in Prob. 2, C from (0, 0) straight to (1, 4). Compare.
4. F ϭ [xy, x 2y 2], C from (2, 0) straight to (0, 2)
5. F as in Prob. 4, C the quarter-circle from (2, 0) to
(0, 2) with center (0, 0)
6. F ϭ [x Ϫ y, y Ϫ z, z Ϫ x], C: r ϭ [2 cos t, t, 2 sin t]
from (2, 0, 0) to (2, 2p, 0)
7. F ϭ [x 2, y 2, z 2], C: r ϭ [cos t, sin t, et] from (1, 0, 1)
to (1, 0, e2p). Sketch C.

8. F ϭ [ex, cosh y, sinh z], C: r ϭ [t, t 2, t 3] from (0, 0, 0)
to (12, 14, 18). Sketch C.
9. F ϭ [x ϩ y, y ϩ z, z ϩx], C: r ϭ [2t, 5t, t] from t ϭ 0
to 1. Also from t ϭ Ϫ1 to 1.
10. F ϭ [x, Ϫz, 2y] from (0, 0, 0) straight to (1, 1, 0), then
to (1, 1, 1), back to (0, 0, 0)
11. F ϭ [e؊x, e؊y, e؊z], C: r ϭ [t, t 2, t] from (0, 0, 0) to
(2, 4, 2). Sketch C.
12. PROJECT. Change of Parameter. Path Dependence.
Consider the integral

Ύ F(r) • dr, where F ϭ [xy, Ϫy ].
2

C

(a) One path, several representations. Find the value
of the integral when r ϭ [cos t, sin t], 0 Ϲ t Ϲ p>2.
Show that the value remains the same if you set t ϭ Ϫp
or t ϭ p 2 or apply two other parametric transformations
of your own choice.
(b) Several paths. Evaluate the integral when C: y ϭ
x n, thus r ϭ [t, t n], 0 Ϲ t Ϲ 1, where n ϭ 1, 2, 3, Á .
Note that these infinitely many paths have the same
endpoints.

c10-a.qxd

10/30/10

12:18 PM

Page 419

SEC. 10.2 Path Independence of Line Integrals

419

(c) Limit. What is the limit in (b) as n : ϱ ? Can you
confirm your result by direct integration without referring
to (b)?
(d) Show path dependence with a simple example of
your choice involving two paths.
13. ML-Inequality, Estimation of Line Integrals. Let F
be a vector function defined on a curve C. Let ƒ F ƒ be
bounded, say, ƒ F ƒ Ϲ M on C, where M is some positive
number. Show that
(9)

Ύ

2 F • dr 2 Ϲ ML

15–20

15. F ϭ [ y 2, z 2, x 2], C: r ϭ [3 cos t, 3 sin t, 2t],
0 Ϲ t Ϲ 4p
16. f ϭ 3x ϩ y ϩ 5z, C: r ϭ [t, cosh t, sinh t],
0 Ϲ t Ϲ 1. Sketch C.
17. F ϭ [x ϩ y, y ϩ z, z ϩ x],
0ϹtϹp

C: r ϭ [4 cos t, sin t, 0],

18. F ϭ [ y 1>3, x 1>3, 0], C the hypocycloid r ϭ [cos3 t,
sin3 t, 0], 0 Ϲ t Ϲ p>4

(L ϭ Length of C).

C

19. f ϭ xyz, C: r ϭ [4t, 3t 2, 12t],
Sketch C.

14. Using (9), find a bound for the absolute value of the
work W done by the force F ϭ [x 2, y] in the displacement from (0, 0) straight to (3, 4). Integrate exactly
and compare.

10.2

INTEGRALS (8) AND (8*)

Evaluate them with F or f and C as follows.

Ϫ2 Ϲ t Ϲ 2.

20. F ϭ [xz, yz, x 2y 2], C: r ϭ [t, t, et], 0 Ϲ t Ϲ 5.
Sketch C.

Path Independence of Line Integrals
We want to find out under what conditions, in some domain, a line integral takes on the
same value no matter what path of integration is taken (in that domain). As before we
consider line integrals

(1)
B

D
A

Fig. 224. Path
independence

Ύ F(r) • dr ϭ Ύ (F

1

C

dx ϩ F2 dy ϩ F3 dz)

(dr ϭ [dx, dy, dz])

C

The line integral (1) is said to be path independent in a domain D in space if for every
pair of endpoints A, B in domain D, (1) has the same value for all paths in D that begin at
A and end at B. This is illustrated in Fig. 224. (See Sec. 9.6 for “domain.”)
Path independence is important. For instance, in mechanics it may mean that we have
to do the same amount of work regardless of the path to the mountaintop, be it short and
steep or long and gentle. Or it may mean that in releasing an elastic spring we get back
the work done in expanding it. Not all forces are of this type—think of swimming in a
big round pool in which the water is rotating as in a whirlpool.
We shall follow up with three ideas about path independence. We shall see that path
independence of (1) in a domain D holds if and only if:
(Theorem 1) F ϭ grad f, where grad f is the gradient of f as explained in Sec. 9.7.
(Theorem 2) Integration around closed curves C in D always gives 0.
(Theorem 3) curl F ϭ 0, provided D is simply connected, as defined below.
Do you see that these theorems can help in understanding the examples and counterexample
just mentioned?
Let us begin our discussion with the following very practical criterion for path
independence.

c10-a.qxd

10/30/10

12:18 PM

420

Page 420

CHAP. 10 Vector Integral Calculus. Integral Theorems

THEOREM 1

Path Independence

A line integral (1) with continuous F1, F2, F3 in a domain D in space is path
independent in D if and only if F ϭ [F1, F2, F3] is the gradient of some function
f in D,
(2)

PROOF

F ϭ grad f,

F1 ϭ

thus,

0f

F2 ϭ

,
0x

0f

F3 ϭ

,
0y

0f
.
0z

(a) We assume that (2) holds for some function f in D and show that this implies path
independence. Let C be any path in D from any point A to any point B in D, given by
r(t) ϭ [x(t), y(t), z(t)], where a Ϲ t Ϲ b. Then from (2), the chain rule in Sec. 9.6, and
(3 r ) in the last section we obtain

Ύ (F

1

dx ϩ F2 dy ϩ F3 dz) ϭ

C

Ύ a 0x dx ϩ 0y dy ϩ 0z dzb
0f

0f

0f

C

b

ϭ

Ύ a 0x

0f dx

a

ϭ

Ύ

b

a

df
dt

dt

ϩ

0f dy
0y dt

ϩ

0f dz
0z dt

b dt

tϭb

dt ϭ f [x(t), y(t), z(t)] 2
tϭa

ϭ f (x(b), y(b), z(b)) Ϫ f (x(a), y(a), z(a))
ϭ f (B) Ϫ f (A).
(b) The more complicated proof of the converse, that path independence implies (2)
for some f, is given in App. 4.
᭿
The last formula in part (a) of the proof,

(3)

Ύ

B

(F1 dx ϩ F2 dy ϩ F3 dz) ϭ f (B) Ϫ f (A)

[F ϭ grad f ]

A

is the analog of the usual formula for definite integrals in calculus,

Ύ

b

a

b

g(x) dx ϭ G(x) 2 ϭ G(b) Ϫ G(a)

[G r (x) ϭ g(x)].

a

Formula (3) should be applied whenever a line integral is independent of path.
Potential theory relates to our present discussion if we remember from Sec. 9.7 that when
F ϭ grad f, then f is called a potential of F. Thus the integral (1) is independent of path
in D if and only if F is the gradient of a potential in D.

c10-a.qxd

10/30/10

12:18 PM

Page 421

SEC. 10.2 Path Independence of Line Integrals
EXAMPLE 1

421

Path Independence
Show that the integral

Ύ F • dr ϭ Ύ (2x dx ϩ 2y dy ϩ 4z dz) is path independent in any domain in space and
C

C

find its value in the integration from A: (0, 0, 0) to B: (2, 2, 2).

Solution. F ϭ [2x, 2y, 4z] ϭ grad f, where f ϭ x 2 ϩ y 2 ϩ 2z 2 because 0f> 0x ϭ 2x ϭ F1, 0f>0y ϭ 2y ϭ F2,

0f> 0z ϭ 4z ϭ F3. Hence the integral is independent of path according to Theorem 1, and (3) gives
f (B) Ϫ f (A) ϭ f (2, 2, 2) Ϫ f (0, 0, 0) ϭ 4 ϩ 4 ϩ 8 ϭ 16.
If you want to check this, use the most convenient path C: r(t) ϭ [t, t, t], 0 Ϲ t Ϲ 2, on which
F(r(t) ϭ [2t, 2t, 4t], so that F(r(t)) • r r (t) ϭ 2t ϩ 2t ϩ 4t ϭ 8t, and integration from 0 to 2 gives 8 # 22>2 ϭ 16.
If you did not see the potential by inspection, use the method in the next example.
᭿

EXAMPLE 2

Path Independence. Determination of a Potential
Evaluate the integral I ϭ

Ύ (3x

2

dx ϩ 2yz dy ϩ y 2 dz) from A: (0, 1, 2) to B: (1, Ϫ1, 7) by showing that F has a

C

potential and applying (3).

Solution. If F has a potential f, we should have
fx ϭ F1 ϭ 3x 2,

fy ϭ F2 ϭ 2yz,

fz ϭ F3 ϭ y 2.

We show that we can satisfy these conditions. By integration of fx and differentiation,
f ϭ x 3 ϩ g( y, z),

fy ϭ gy ϭ 2yz,

g ϭ y 2z ϩ h(z),

fz ϭ y 2 ϩ h r ϭ y 2,

hr ϭ 0

h ϭ 0, say.

f ϭ x 3 ϩ y 2z ϩ h(z)

This gives f (x, y, z) ϭ x 3 ϩ y 3z and by (3),
I ϭ f (1, Ϫ1, 7) Ϫ f (0, 1, 2) ϭ 1 ϩ 7 Ϫ (0 ϩ 2) ϭ 6.

᭿

Path Independence and Integration
Around Closed Curves
The simple idea is that two paths with common endpoints (Fig. 225) make up a single
closed curve. This gives almost immediately
THEOREM 2

Path Independence

The integral (1) is path independent in a domain D if and only if its value around
every closed path in D is zero.
PROOF

If we have path independence, then integration from A to B along C1 and along C2 in
Fig. 225 gives the same value. Now C1 and C2 together make up a closed curve C, and
C1
if we integrate from A along C1 to B as before, but then in the opposite sense along C2
B
back to A (so that this second integral is multiplied by Ϫ1), the sum of the two integrals
is zero, but this is the integral around the closed curve C.
Conversely, assume that the integral around any closed path C in D is zero. Given any
A
points A and B and any two curves C1 and C2 from A to B in D, we see that C1 with the
C2
orientation reversed and C2 together form a closed path C. By assumption, the integral
Fig. 225. Proof of over C is zero. Hence the integrals over C1 and C2, both taken from A to B, must be equal.
This proves the theorem.
᭿
Theorem 2

c10-a.qxd

10/30/10

422

12:18 PM

Page 422

CHAP. 10 Vector Integral Calculus. Integral Theorems

Work. Conservative and Nonconservative (Dissipative) Physical Systems
Recall from the last section that in mechanics, the integral (1) gives the work done by a
force F in the displacement of a body along the curve C. Then Theorem 2 states that work
is path independent in D if and only if its value is zero for displacement around every
closed path in D. Furthermore, Theorem 1 tells us that this happens if and only if F is the
gradient of a potential in D. In this case, F and the vector field defined by F are called
conservative in D because in this case mechanical energy is conserved; that is, no work
is done in the displacement from a point A and back to A. Similarly for the displacement
of an electrical charge (an electron, for instance) in a conservative electrostatic field.
Physically, the kinetic energy of a body can be interpreted as the ability of the body to
do work by virtue of its motion, and if the body moves in a conservative field of force,
after the completion of a round trip the body will return to its initial position with the
same kinetic energy it had originally. For instance, the gravitational force is conservative;
if we throw a ball vertically up, it will (if we assume air resistance to be negligible) return
to our hand with the same kinetic energy it had when it left our hand.
Friction, air resistance, and water resistance always act against the direction of motion.
They tend to diminish the total mechanical energy of a system, usually converting it into
heat or mechanical energy of the surrounding medium (possibly both). Furthermore,
if during the motion of a body, these forces become so large that they can no longer
be neglected, then the resultant force F of the forces acting on the body is no longer
conservative. This leads to the following terms. A physical system is called conservative
if all the forces acting in it are conservative. If this does not hold, then the physical system
is called nonconservative or dissipative.

Path Independence and Exactness
of Differential Forms
Theorem 1 relates path independence of the line integral (1) to the gradient and Theorem 2
to integration around closed curves. A third idea (leading to Theorems 3* and 3, below)
relates path independence to the exactness of the differential form or Pfaffian form1
F • dr ϭ F1 dx ϩ F2 dy ϩ F3 dz

(4)

under the integral sign in (1). This form (4) is called exact in a domain D in space if it
is the differential
df ϭ

0f
0x

dx ϩ

0f
0y

dy ϩ

0f
0z

dz ϭ (grad f ) • dr

of a differentiable function f(x, y, z) everywhere in D, that is, if we have
F • dr ϭ df.
Comparing these two formulas, we see that the form (4) is exact if and only if there is a
differentiable function f (x, y, z) in D such that everywhere in D,
(5)
1

F ϭ grad f,

thus,

F1 ϭ

0f
,
0x

F2 ϭ

JOHANN FRIEDRICH PFAFF (1765–1825). German mathematician.

0f
,
0y

F3 ϭ

0f
.
0z

c10-a.qxd

10/30/10

12:18 PM

Page 423

SEC. 10.2 Path Independence of Line Integrals

423

Hence Theorem 1 implies
THEOREM 3*

Path Independence

The integral (1) is path independent in a domain D in space if and only if the differential
form (4) has continuous coefficient functions F1, F2, F3 and is exact in D.
This theorem is of practical importance because it leads to a useful exactness criterion.
First we need the following concept, which is of general interest.
A domain D is called simply connected if every closed curve in D can be continuously
shrunk to any point in D without leaving D.
For example, the interior of a sphere or a cube, the interior of a sphere with finitely many
points removed, and the domain between two concentric spheres are simply connected. On
the other hand, the interior of a torus, which is a doughnut as shown in Fig. 249 in Sec. 10.6
is not simply connected. Neither is the interior of a cube with one space diagonal removed.
The criterion for exactness (and path independence by Theorem 3*) is now as follows.
THEOREM 3

Criterion for Exactness and Path Independence

Let F1, F2, F3 in the line integral (1),

Ύ F(r) • dr ϭ Ύ (F

1

C

dx ϩ F2 dy ϩ F3 dz),

C

be continuous and have continuous first partial derivatives in a domain D in space. Then:
(a) If the differential form (4) is exact in D—and thus (1) is path independent
by Theorem 3*—, then in D,
curl F ϭ 0;

(6)
in components (see Sec. 9.9)
(6 r )

0F3
0y

ϭ

0F2

0F1

,

0z

0z

ϭ

0F3

0F2

,

0x

0x

ϭ

0F1

.

0y

(b) If (6) holds in D and D is simply connected, then (4) is exact in D—and
thus (1) is path independent by Theorem 3*.
PROOF

(a) If (4) is exact in D, then F ϭ grad f in D by Theorem 3*, and, furthermore,
curl F ϭ curl (grad f ) ϭ 0 by (2) in Sec. 9.9, so that (6) holds.
᭿
(b) The proof needs “Stokes’s theorem” and will be given in Sec. 10.9.
Line Integral in the Plane. For

Ύ F(r) • dr ϭ Ύ (F

1

C

dx ϩ F2 dy) the curl has only one

C

component (the z-component), so that (6 r ) reduces to the single relation
(6 s )

0F2
0x

ϭ

0F1
0y

(which also occurs in (5) of Sec. 1.4 on exact ODEs).

c10-a.qxd

10/30/10

3:32 PM

424
EXAMPLE 3

Page 424

CHAP. 10 Vector Integral Calculus. Integral Theorems
Exactness and Independence of Path. Determination of a Potential
Using (6 r ), show that the differential form under the integral sign of
Iϭ

Ύ [2xyz

2

dx ϩ (x 2z 2 ϩ z cos yz) dy ϩ (2x 2yz ϩ y cos yz) dz]

C

is exact, so that we have independence of path in any domain, and find the value of I from A: (0, 0, 1) to
B: (1, p>4, 2).

Solution. Exactness follows from (6 r ), which gives
(F3)y ϭ 2x 2z ϩ cos yz Ϫ yz sin yz ϭ (F2)z
(F1)z ϭ 4xyz ϭ (F3)x
(F2)x ϭ 2xz 2 ϭ (F1)y.
To find f, we integrate F2 (which is “long,” so that we save work) and then differentiate to compare with F1 and F3,
fϭ

ΎF

2

Ύ (x z

dy ϭ

2 2

ϩ z cos yz) dy ϭ x 2z 2y ϩ sin yz ϩ g(x, z)

fx ϭ 2xz 2y ϩ gx ϭ F1 ϭ 2xyz 2,

gx ϭ 0,

g ϭ h(z)

fz ϭ 2x 2zy ϩ y cos yz ϩ h r ϭ F3 ϭ 2x 2zy ϩ y cos yz,

h r ϭ 0.

h r ϭ 0 implies h ϭ const and we can take h ϭ 0, so that g ϭ 0 in the first line. This gives, by (3),

p #
p
f (B) Ϫ f (A) ϭ 1 #
4 ϩ sin
Ϫ 0 ϭ p ϩ 1.
4
2

f (x, y, z) ϭ x 2yz 2 ϩ sin yz,

᭿

The assumption in Theorem 3 that D is simply connected is essential and cannot be omitted.
Perhaps the simplest example to see this is the following.
EXAMPLE 4

On the Assumption of Simple Connectedness in Theorem 3
Let
(7)

F1 ϭ Ϫ

y
x 2 ϩ y2

F2 ϭ

,

x
x 2 ϩ y2

,

F3 ϭ 0.

Differentiation shows that (6 r ) is satisfied in any domain of the xy-plane not containing the origin, for example,
in the domain D: 12 Ͻ 2x 2 ϩ y 2 Ͻ 32 shown in Fig. 226. Indeed, F1 and F2 do not depend on z, and F3 ϭ 0,
so that the first two relations in (6 r ) are trivially true, and the third is verified by differentiation:
0F2
0x
0F1
0y

ϭ

x 2 ϩ y 2 Ϫ x # 2x
(x ϩ y )
2

ϭϪ

2 2

x 2 ϩ y 2 Ϫ y # 2y

(x 2 ϩ y 2)2
ϭ

(x ϩ y )
2

y2 Ϫ x 2

ϭ

2 2

,

y2 Ϫ x 2
(x 2 ϩ y 2)2

.

Clearly, D in Fig. 226 is not simply connected. If the integral
Iϭ

Ύ (F

1

dx ϩ F2 dy) ϭ

C

Ύ

C

Ϫy dx ϩ x dy
x 2 ϩ y2

were independent of path in D, then I ϭ 0 on any closed curve in D, for example, on the circle x 2 ϩ y 2 ϭ 1.
But setting x ϭ r cos u, y ϭ r sin u and noting that the circle is represented by r ϭ 1, we have
x ϭ cos u,

dx ϭ Ϫsin u du,

y ϭ sin u,

dy ϭ cos u du,

c10-a.qxd

10/30/10

12:18 PM

Page 425

SEC. 10.2 Path Independence of Line Integrals

425

so that Ϫy dx ϩ x dy ϭ sin2 u du ϩ cos2 u du ϭ du and counterclockwise integration gives
Iϭ

Ύ

2p

du

ϭ 2p.

1

0

Since D is not simply connected, we cannot apply Theorem 3 and cannot conclude that I is independent of path
in D.
Although F ϭ grad f, where f ϭ arctan ( y>x) (verify!), we cannot apply Theorem 1 either because the polar
angle f ϭ u ϭ arctan ( y>x) is not single-valued, as it is required for a function in calculus.
᭿
y

C

3
_
2

x

Fig. 226. Example 4

PROBLEM SET 10.2
1. WRITING PROJECT. Report on Path Independence.
Make a list of the main ideas and facts on path
independence and dependence in this section. Then
work this list into a report. Explain the definitions and
the practical usefulness of the theorems, with illustrative
examples of your own. No proofs.
2. On Example 4. Does the situation in Example 4 of the
text change if you take the domain 0 Ͻ 2x 2 ϩ y 2 Ͻ
3>2?

PATH INDEPENDENT INTEGRALS

3–9

Show that the form under the integral sign is exact in the
plane (Probs. 3–4) or in space (Probs. 5–9) and evaluate the
integral. Show the details of your work.
3.

4.

Ύ

(p, 0)

Ύ

(6, 1)

(12 cos 12 x cos 2y dx Ϫ 2 sin 12 x sin 2y dy)

(p>2, p)

e4y(2x dx ϩ 4x 2 dy)

(4, 0)

5.

Ύ

(2, 1>2, p>2)

8.

Ύ

(3, p, 3)

Ύ

(1, 0, 1)

(cos yz dx Ϫ xz sin yz dy Ϫ xy sin yz dz)

(5, 3, p)

9.

z

(ex cosh y dx ϩ (ex sinh y ϩ e cosh y) dy

(0, 1, 0)

ϩ ez sinh y dz)
10. PROJECT. Path
Iϭ

Ύ (x y dx ϩ 2xy
2

2

(a) Show

dy) is path dependent in the

xy-plane.
(b) Integrate from (0, 0) along the straight-line
segment to (1, b), 0 Ϲ b Ϲ 1, and then vertically up to
(1, 1); see the figure. For which b is I maximum? What
is its maximum value?
(c) Integrate I from (0, 0) along the straight-line segment
to (c, 1), 0 Ϲ c Ϲ 1, and then horizontally to (1, 1). For
c ϭ 1, do you get the same value as for b ϭ 1 in (b)?
For which c is I maximum? What is its maximum value?
(c, 1)

(1, 1)

1

(0, 0, p)

6.

Ύ

(1, 1, 0)

Ύ

(1, 1, 1)

2

ex

ϩ y2 ϩ z2

(x dx ϩ y dy ϩ z dz)

(0, 0, 0)

7.

(0, 2, 3)

(1, b)

( yz sinh xz dx ϩ cosh xz dy ϩ xy sinh xz dz)

that

C

y

exy( y sin z dx ϩ x sin z dy ϩ cos z dz)

Dependence.

(0, 0)

1

x

Project 10. Path Dependence

c10-a.qxd

10/30/10

12:18 PM

426

Page 426

CHAP. 10 Vector Integral Calculus. Integral Theorems

11. On Example 4. Show that in Example 4 of the text,
F ϭ grad (arctan ( y>x)). Give examples of domains in
which the integral is path independent.
12. CAS EXPERIMENT. Extension of Project 10. Integrate x 2y dx ϩ 2xy 2 dy over various circles through the
points (0, 0) and (1, 1). Find experimentally the smallest
value of the integral and the approximate location of
the center of the circle.
13–19

PATH INDEPENDENCE?

Check, and if independent, integrate from (0, 0, 0) to (a, b, c).
2
13. 2ex (x cos 2y dx Ϫ sin 2y dy)

10.3

14. (sinh xy) (z dx Ϫ x dz)
15. x 2y dx Ϫ 4xy 2 dy ϩ 8z 2x dz
16. ey dx ϩ (xey Ϫ ez) dy Ϫ yez dz
17. 4y dx ϩ z dy ϩ ( y Ϫ 2z) dz
18. (cos xy)( yz dx ϩ xz dy) Ϫ 2 sin xy dz
19. (cos (x 2 ϩ 2y 2 ϩ z 2)) (2x dx ϩ 4y dy ϩ 2z dz)
20. Path Dependence. Construct three simple examples
in each of which two equations (6 r ) are satisfied, but
the third is not.

Calculus Review: Double Integrals.
Optional
This section is optional. Students familiar with double integrals from calculus should
skip this review and go on to Sec. 10.4. This section is included in the book to make it
reasonably self-contained.
In a definite integral (1), Sec. 10.1, we integrate a function f (x) over an interval
(a segment) of the x-axis. In a double integral we integrate a function f (x, y), called the
integrand, over a closed bounded region2 R in the xy-plane, whose boundary curve has a
unique tangent at almost every point, but may perhaps have finitely many cusps (such as
the vertices of a triangle or rectangle).
The definition of the double integral is quite similar to that of the definite integral. We
subdivide the region R by drawing parallels to the x- and y-axes (Fig. 227). We number the
rectangles that are entirely within R from 1 to n. In each such rectangle we choose a point,
say, (x k, yk) in the kth rectangle, whose area we denote by ¢Ak. Then we form the sum
n

Jn ϭ a f (x k, yk) ¢Ak.
kϭ1

y

x

Fig. 227. Subdivision of a region R

2
A region R is a domain (Sec. 9.6) plus, perhaps, some or all of its boundary points. R is closed if its boundary
(all its boundary points) are regarded as belonging to R; and R is bounded if it can be enclosed in a circle of
sufficiently large radius. A boundary point P of R is a point (of R or not) such that every disk with center P
contains points of R and also points not of R.

c10-a.qxd

10/30/10

12:18 PM

Page 427

SEC. 10.3 Calculus Review: Double Integrals. Optional

427

This we do for larger and larger positive integers n in a completely independent manner,
but so that the length of the maximum diagonal of the rectangles approaches zero as n
approaches infinity. In this fashion we obtain a sequence of real numbers Jn1, Jn2, Á .
Assuming that f (x, y) is continuous in R and R is bounded by finitely many smooth curves
(see Sec. 10.1), one can show (see Ref. [GenRef4] in App. 1) that this sequence converges
and its limit is independent of the choice of subdivisions and corresponding points
(x k, yk). This limit is called the double integral of f (x, y) over the region R, and is
denoted by

Ύ Ύ f (x, y) dx dy

Ύ Ύ f (x, y) dA.

or

R

R

Double integrals have properties quite similar to those of definite integrals. Indeed, for
any functions f and g of (x, y), defined and continuous in a region R,

Ύ Ύ kf dx dy ϭ k Ύ Ύ f dx dy
R

(1)

(k constant)

R

Ύ Ύ ( f ϩ g) dx dy ϭ Ύ Ύ f dx dy ϩ Ύ Ύ g dx dy
R

R

R

Ύ Ύ f dx dy ϭ Ύ Ύ f dx dy ϩ Ύ Ύ f dx dy
R

R1

(Fig. 228).

R2

Furthermore, if R is simply connected (see Sec. 10.2), then there exists at least one point
(x 0, y0) in R such that we have

Ύ Ύ f (x, y) dx dy ϭ f (x , y )A,

(2)

0

0

R

where A is the area of R. This is called the mean value theorem for double integrals.

R2
R1

Fig. 228. Formula (1)

Evaluation of Double Integrals
by Two Successive Integrations
Double integrals over a region R may be evaluated by two successive integrations. We
may integrate first over y and then over x. Then the formula is
b

(3)

Ύ Ύ f (x, y) dx dy ϭ Ύ c Ύ
R

a

h(x)

g(x)

f (x, y) dy d dx

(Fig. 229).

c10-a.qxd

10/30/10

428

12:18 PM

Page 428

CHAP. 10 Vector Integral Calculus. Integral Theorems

Here y ϭ g(x) and y ϭ h(x) represent the boundary curve of R (see Fig. 229) and, keeping
x constant, we integrate f (x, y) over y from g(x) to h(x). The result is a function of x, and
we integrate it from x ϭ a to x ϭ b (Fig. 229).
Similarly, for integrating first over x and then over y the formula is
d

Ύ Ύ f (x, y) dx dy ϭ Ύ c Ύ

(4)

R

c

y

q(y)

f (x, y) dx d dy

p(y)

(Fig. 230).

y
d

h(x)

p( y)
R

R
c

g(x)
a

b

x

q( y)

x

Fig. 229. Evaluation of a double integral

Fig. 230. Evaluation of a double integral

The boundary curve of R is now represented by x ϭ p( y) and x ϭ q( y). Treating y as a
constant, we first integrate f (x, y) over x from p( y) to q( y) (see Fig. 230) and then the
resulting function of y from y ϭ c to y ϭ d.
In (3) we assumed that R can be given by inequalities a Ϲ x Ϲ b and g(x) Ϲ y Ϲ h(x).
Similarly in (4) by c Ϲ y Ϲ d and p( y) Ϲ x Ϲ q( y). If a region R has no such representation,
then, in any practical case, it will at least be possible to subdivide R into finitely many
portions each of which can be given by those inequalities. Then we integrate f (x, y) over
each portion and take the sum of the results. This will give the value of the integral of
f (x, y) over the entire region R.

Applications of Double Integrals
Double integrals have various physical and geometric applications. For instance, the area
A of a region R in the xy-plane is given by the double integral
Aϭ

Ύ Ύ dx dy.
R

The volume V beneath the surface z ϭ f (x, y) (Ͼ0) and above a region R in the xy-plane
is (Fig. 231)
Vϭ

Ύ Ύ f (x, y) dx dy
R

because the term f (x k, yk)¢Ak in Jn at the beginning of this section represents the volume
of a rectangular box with base of area ¢Ak and altitude f (x k, yk).

c10-a.qxd

10/30/10

12:18 PM

Page 429

SEC. 10.3 Calculus Review: Double Integrals. Optional

429

z
f(x, y)

y

x

R

Fig. 231. Double integral as volume

As another application, let f (x, y) be the density (ϭ mass per unit area) of a distribution
of mass in the xy-plane. Then the total mass M in R is
Mϭ

Ύ Ύ f (x, y) dx dy;
R

the center of gravity of the mass in R has the coordinates x, y, where
1
M

xϭ

Ύ Ύ xf (x, y) dx dy

and

yϭ

R

1
M

Ύ Ύ yf (x, y) dx dy;
R

the moments of inertia Ix and Iy of the mass in R about the x- and y-axes, respectively, are
Ix ϭ

Ύ Ύ y f (x, y) dx dy,

Iy ϭ

2

R

Ύ Ύ x f (x, y) dx dy;
2

R

and the polar moment of inertia I0 about the origin of the mass in R is
I0 ϭ Ix ϩ Iy ϭ

Ύ Ύ (x

2

ϩ y 2) f (x, y) dx dy.

R

An example is given below.

Change of Variables in Double Integrals. Jacobian
Practical problems often require a change of the variables of integration in double integrals.
Recall from calculus that for a definite integral the formula for the change from x to u is

Ύ

(5)

b

b

f (x) dx ϭ

a

Ύ f (x(u)) dudx du.
a

Here we assume that x ϭ x(u) is continuous and has a continuous derivative in some
interval a Ϲ u Ϲ b such that x(a) ϭ a, x(b) ϭ b [or x(a) ϭ b, x(b) ϭ a] and x(u) varies
between a and b when u varies between a and b.
The formula for a change of variables in double integrals from x, y to u, v is
(6)

Ύ Ύ f (x, y) dx dy ϭ Ύ Ύ f (x(u, v), y(u, v)) 2 0(u, v) 2 du dv;
0(x, y)

R

R*

10/30/10

12:18 PM

430

Page 430

CHAP. 10 Vector Integral Calculus. Integral Theorems

that is, the integrand is expressed in terms of u and v, and dx dy is replaced by du dv times
the absolute value of the Jacobian3

(7)

Jϭ

0(x, y)
0(u, v)

ϭ4

0x
0u

0x
0v

0y
0u

0y
0v

4ϭ

0x 0y
0x 0y
Ϫ
.
0u 0v
0v 0u

Here we assume the following. The functions
x ϭ x(u, v),

y ϭ y(u, v)

effecting the change are continuous and have continuous partial derivatives in some region
R* in the uv-plane such that for every (u, v) in R* the corresponding point (x, y) lies in
R and, conversely, to every (x, y) in R there corresponds one and only one (u, v) in R*;
furthermore, the Jacobian J is either positive throughout R* or negative throughout R*.
For a proof, see Ref. [GenRef4] in App. 1.
Change of Variables in a Double Integral
Evaluate the following double integral over the square R in Fig. 232.

Ύ Ύ (x

ϩ y 2) dx dy

2

R

Solution. The shape of R suggests the transformation x ϩ y ϭ u, x Ϫ y ϭ v. Then x ϭ 12 (u ϩ v),
y ϭ 12 (u Ϫ v). The Jacobian is

Jϭ

0(x, y)
0(u, v)

ϭ †

1
2

1
2

1
2

Ϫ 12

1
† ϭϪ .
2

R corresponds to the square 0 Ϲ u Ϲ 2, 0 Ϲ v Ϲ 2. Therefore,

Ύ Ύ (x

2

2

ϩ y 2) dx dy ϭ

R

2

Ύ Ύ 2 (u
0

1

2

0

ϩ v2)

1
2

du dv ϭ

8
3

.

᭿

y
0

u
=

=

2

x

1
=

=

2

u
0

v

EXAMPLE 1

v

c10-a.qxd

Fig. 232. Region R in Example 1

3
Named after the German mathematician CARL GUSTAV JACOB JACOBI (1804–1851), known for his
contributions to elliptic functions, partial differential equations, and mechanics.

c10-a.qxd

10/30/10

12:18 PM

Page 431

SEC. 10.3 Calculus Review: Double Integrals. Optional

431

Of particular practical interest are polar coordinates r and u, which can be introduced
by setting x ϭ r cos u, y ϭ r sin u. Then
Jϭ

0(x, y)
0(r, u)

ϭ2

cos u

Ϫr sin u

sin u

r cos u

2ϭr

and

Ύ Ύ f (x, y) dx dy ϭ Ύ Ύ f (r cos u, r sin u) r dr du

(8)

R*

R

where R* is the region in the ru-plane corresponding to R in the xy-plane.

EXAMPLE 2

Double Integrals in Polar Coordinates. Center of Gravity. Moments of Inertia
Let f (x, y) ϭ 1 be the mass density in the region in Fig. 233. Find the total mass, the center of gravity, and the
moments of inertia Ix, Iy, I0.

y

Solution. We use the polar coordinates just defined and formula (8). This gives the total mass
1

Fig. 233.
Example 2

Mϭ

x

p>2

p>2

1

Ύ Ύ dx dy ϭ Ύ Ύ r dr du ϭ Ύ
R

0

0

0

1
2

du ϭ

p
4

.

The center of gravity has the coordinates

xϭ

p>2

0

yϭ

1

p Ύ Ύ r cos u r dr du ϭ p Ύ
4

4
3p

4

0

p>2

0

1
3

cos u du ϭ

4
3p

ϭ 0.4244

for reasons of symmetry.

The moments of inertia are

Ix ϭ

Ύ Ύy

2

dx dy ϭ

R

p>2

0

ϭ

Ύ

p>2

0

Iy ϭ

p
16

Why are x and y less than 12 ?

1

Ύ Ύr

2

sin2 u r dr du ϭ

0

1
8

p>2

0

(1 Ϫ cos 2u) du ϭ

for reasons of symmetry,

Ύ

1
4

sin2 u du

1 p
p
a Ϫ 0b ϭ
ϭ 0.1963
2
16

8

I0 ϭ Ix ϩ Iy ϭ

p
8

ϭ 0.3927.

᭿

This is the end of our review on double integrals. These integrals will be needed in this
chapter, beginning in the next section.

c10-a.qxd

10/30/10

12:18 PM

432

Page 432

CHAP. 10 Vector Integral Calculus. Integral Theorems

PROBLEM SET 10.3
1. Mean value theorem. Illustrate (2) with an example.

14.

y

DOUBLE INTEGRALS

2–8

R

Describe the region of integration and evaluate.
2

2.

ΎΎ

2x

3

y

0

3.

x

ΎΎ
0

(x ϩ y)2 dy dx
r2 x

r1

(x 2 ϩ y 2) dx dy

15.

Ϫy

y

4. Prob. 3, order reversed.
1

5.

x

Ύ Ύ (1 Ϫ 2xy) dy dx
0

2

6.

R

x2

y

Ύ Ύ sinh (x ϩ y) dx dy
0

0

r

7. Prob. 6, order reversed.
p>4

8.

Ύ Ύ
0

9–11

cos y

x 2 sin y dx dy

0

R

VOLUME

Find the volume of the given region in space.
9. The region beneath z ϭ 4x 2 ϩ 9y 2 and above the
rectangle with vertices (0, 0), (3, 0), (3, 2), (0, 2) in the
xy-plane.
10. The first octant region bounded by the coordinate planes
and the surfaces y ϭ 1 Ϫ x 2, z ϭ 1 Ϫ x 2. Sketch it.
11. The region above the xy-plane and below the paraboloid z ϭ 1 Ϫ (x 2 ϩ y 2).
12–16

CENTER OF GRAVITY

x

17–20

MOMENTS OF INERTIA

Find Ix, Iy, I0 of a mass of density f (x, y) ϭ 1 in the region
R in the figures, which the engineer is likely to need, along
with other profiles listed in engineering handbooks.
17. R as in Prob. 13.
18. R as in Prob. 12.
19.
y

Find the center of gravity ( x, y ) of a mass of density
f (x, y) ϭ 1 in the given region R.
12.

h
2

–

y

a
2

a
2

–b

h

b
2

2

–

R
1
b
2

b

x

20.

x

h
2

y
h

13.

x

16. y

y
h
R
b

x

–

a
2

–

b
2

0

b
2

a
2

x

c10-a.qxd

10/30/10

12:18 PM

Page 433

SEC. 10.4 Green’s Theorem in the Plane

10.4

433

Green’s Theorem in the Plane
Double integrals over a plane region may be transformed into line integrals over the
boundary of the region and conversely. This is of practical interest because it may simplify
the evaluation of an integral. It also helps in theoretical work when we want to switch from
one kind of integral to the other. The transformation can be done by the following theorem.
Green’s Theorem in the Plane4
(Transformation between Double Integrals and Line Integrals)

THEOREM 1

Let R be a closed bounded region (see Sec. 10.3) in the xy-plane whose boundary
C consists of finitely many smooth curves (see Sec. 10.1). Let F1(x, y) and F2(x, y)
be functions that are continuous and have continuous partial derivatives 0F1>0y
and 0F2>0x everywhere in some domain containing R. Then

Ύ Ύ a 0x

0F2

(1)

Ϫ

R

0F1
0y

b dx dy ϭ

Ώ (F

1

dx ϩ F2 dy).

C

Here we integrate along the entire boundary C of R in such a sense that R is on
the left as we advance in the direction of integration (see Fig. 234).

y
C1
R
C2

x

Fig. 234. Region R whose boundary C consists of two parts:
C1 is traversed counterclockwise, while C2 is traversed clockwise
in such a way that R is on the left for both curves

Setting F ϭ [F1,
form,
(1 r )

F2] ϭ F1 i ϩ F2 j and using (1) in Sec. 9.9, we obtain (1) in vectorial

Ύ Ύ (curl F) • k dx dy ϭ Ώ F • dr.
R

C

The proof follows after the first example. For ͛ see Sec. 10.1.
4

GEORGE GREEN (1793–1841), English mathematician who was self-educated, started out as a baker, and
at his death was fellow of Caius College, Cambridge. His work concerned potential theory in connection with
electricity and magnetism, vibrations, waves, and elasticity theory. It remained almost unknown, even in England,
until after his death.
A “domain containing R” in the theorem guarantees that the assumptions about F1 and F2 at boundary points
of R are the same as at other points of R.

c10-a.qxd

10/30/10

12:18 PM

434

Page 434

CHAP. 10 Vector Integral Calculus. Integral Theorems

EXAMPLE 1

Verification of Green’s Theorem in the Plane
Green’s theorem in the plane will be quite important in our further work. Before proving it, let us get used to
it by verifying it for F1 ϭ y 2 Ϫ 7y, F2 ϭ 2xy ϩ 2x and C the circle x 2 ϩ y 2 ϭ 1.

Solution. In (1) on the left we get

Ύ Ύ a 0x

0F2

Ϫ

R

0F1
0y

b dx dy ϭ

Ύ Ύ [(2y ϩ 2) Ϫ (2y Ϫ 7)] dx dy ϭ 9 Ύ Ύdx dy ϭ 9p
R

R

since the circular disk R has area p.
We now show that the line integral in (1) on the right gives the same value, 9p. We must orient C
counterclockwise, say, r(t) ϭ [cos t, sin t]. Then r r (t) ϭ [Ϫsin t, cos t], and on C,
F1 ϭ y 2 Ϫ 7y ϭ sin2 t Ϫ 7 sin t,

F2 ϭ 2xy ϩ 2x ϭ 2 cos t sin t ϩ 2 cos t.

Hence the line integral in (1) becomes, verifying Green’s theorem,

Ώ

(F1x r ϩ F2 y r ) dt ϭ

C

Ύ

2p

Ύ

2p

[(sin2 t Ϫ 7 sin t)(Ϫsin t) ϩ 2(cos t sin t ϩ cos t)(cos t)] dt

0

ϭ

(Ϫsin3 t ϩ 7 sin2 t ϩ 2 cos2 t sin t ϩ 2 cos2 t) dt

0

᭿

ϭ 0 ϩ 7p Ϫ 0 ϩ 2p ϭ 9p.

PROOF

We prove Green’s theorem in the plane, first for a special region R that can be represented
in both forms
a Ϲ x Ϲ b,

u(x) Ϲ y Ϲ v(x)

(Fig. 235)

c Ϲ y Ϲ d,

p( y) Ϲ x Ϲ q(y)

(Fig. 236)

and

y

y

d

C**
v(x)

p(y)
R

R
C*

q(y)

c
u(x)
x

x

Fig. 235. Example of a special region

Fig. 236. Example of a special region

a

b

Using (3) in the last section, we obtain for the second term on the left side of (1) taken
without the minus sign
(2)

b

Ύ Ύ 0y dx dy ϭ Ύ c Ύ
0F1

R

a

v(x)

u(x)

0F1
0y

dy d dx

(see Fig. 235).

c10-a.qxd

10/30/10

12:18 PM

Page 435

SEC. 10.4 Green’s Theorem in the Plane

435

(The first term will be considered later.) We integrate the inner integral:

Ύ

v(x)

yϭv(x)

0F1
0y

u(x)

dy ϭ F1(x, y) 2

ϭ F1 [x, v(x)] Ϫ F1 [x, u(x)].
yϭu(x)

By inserting this into (2) we find (changing a direction of integration)

Ύ Ύ 0y

0F1

dx dy ϭ

R

Ύ

b

b

F1 [x, v(x)] dx Ϫ

a

ϭϪ

Ύ F [x, u(x)] dx
1

a

Ύ

a

b

Ύ F [x, u(x)] dx.

F1 [x, v(x)] dx Ϫ

1

b

a

Since y ϭ v(x) represents the curve C** (Fig. 235) and y ϭ u(x) represents C*, the last
two integrals may be written as line integrals over C** and C* (oriented as in Fig. 235);
therefore,
(3)

Ύ Ύ 0y dx dy ϭ Ϫ Ύ
0F1

R

F1(x, y) dx Ϫ

C**

Ύ F (x, y) dx
1

C*

Ώ

ϭ Ϫ F1(x, y) dx.
C

This proves (1) in Green’s theorem if F2 ϭ 0.
~
~
The result remains valid if C has portions parallel to the y-axis (such as C and C in
Fig. 237). Indeed, the integrals over these portions are zero because in (3) on the right we
integrate with respect to x. Hence we may add these integrals to the integrals over C* and
C** to obtain the integral over the whole boundary C in (3).
We now treat the first term in (1) on the left in the same way. Instead of (3) in the last
section we use (4), and the second representation of the special region (see Fig. 236).
Then (again changing a direction of integration)

Ύ Ύ 0x

0F2

d

dx dy ϭ

R

Ύ Ύ
c

ϭ

Ύ

c

q(y)

0F2
0x

p(y)

d

c

F2(q(y), y) dy ϩ

c

ϭ

dx d dy

Ύ F ( p(y), y) dy
2

d

Ώ F (x, y) dy.
2

C

Together with (3) this gives (1) and proves Green’s theorem for special regions.
We now prove the theorem for a region R that itself is not a special region but can be
subdivided into finitely many special regions as shown in Fig. 238. In this case we apply
the theorem to each subregion and then add the results; the left-hand members add up to
the integral over R while the right-hand members add up to the line integral over C plus

c10-a.qxd

10/30/10

12:18 PM

436

Page 436

CHAP. 10 Vector Integral Calculus. Integral Theorems
y

y

C**
C
C
C*

x

x

Fig. 237. Proof of Green’s theorem

Fig. 238. Proof of Green’s theorem

integrals over the curves introduced for subdividing R. The simple key observation now
is that each of the latter integrals occurs twice, taken once in each direction. Hence they
cancel each other, leaving us with the line integral over C.
The proof thus far covers all regions that are of interest in practical problems. To prove
the theorem for a most general region R satisfying the conditions in the theorem, we must
approximate R by a region of the type just considered and then use a limiting process.
For details of this see Ref. [GenRef4] in App. 1.

Some Applications of Green’s Theorem
EXAMPLE 2

Area of a Plane Region as a Line Integral Over the Boundary
In (1) we first choose F1 ϭ 0, F2 ϭ x and then F1 ϭ Ϫy, F2 ϭ 0. This gives

Ύ Ύ dx dy ϭ Ώ x dy

and

C

R

Ύ Ύ dx dy ϭ Ϫ Ώ y dx
R

C

respectively. The double integral is the area A of R. By addition we have

Aϭ

(4)

1
2

Ώ (x dy Ϫ y dx)
C

where we integrate as indicated in Green’s theorem. This interesting formula expresses the area of R in terms
of a line integral over the boundary. It is used, for instance, in the theory of certain planimeters (mechanical
instruments for measuring area). See also Prob. 11.
For an ellipse x 2>a 2 ϩ y 2>b 2 ϭ 1 or x ϭ a cos t, y ϭ b sin t we get x r ϭ Ϫa sin t, y r ϭ b cos t; thus from
(4) we obtain the familiar formula for the area of the region bounded by an ellipse,
Aϭ

EXAMPLE 3

1
2

Ύ

2p

0

(xy r Ϫ yx r ) dt ϭ

1
2

Ύ

2p

[ab cos2 t Ϫ (Ϫab sin2 t)] dt ϭ pab.

0

Area of a Plane Region in Polar Coordinates
Let r and u be polar coordinates defined by x ϭ r cos u, y ϭ r sin u. Then
dx ϭ cos u dr Ϫ r sin u du,

dy ϭ sin u dr ϩ r cos u du,

᭿

c10-a.qxd

10/30/10

12:18 PM

Page 437

SEC. 10.4 Green’s Theorem in the Plane

437

and (4) becomes a formula that is well known from calculus, namely,

Aϭ

(5)

1
2

Ώr

2

du.

C

As an application of (5), we consider the cardioid r ϭ a(1 Ϫ cos u), where 0 Ϲ u Ϲ 2p (Fig. 239). We find
Aϭ

a2

2 Ύ

2p

3p

(1 Ϫ cos u)2 du ϭ

2

0

EXAMPLE 4

᭿

a 2.

Transformation of a Double Integral of the Laplacian of a Function
into a Line Integral of Its Normal Derivative
The Laplacian plays an important role in physics and engineering. A first impression of this was obtained in
Sec. 9.7, and we shall discuss this further in Chap. 12. At present, let us use Green’s theorem for deriving a
basic integral formula involving the Laplacian.
We take a function w(x, y) that is continuous and has continuous first and second partial derivatives in a
domain of the xy-plane containing a region R of the type indicated in Green’s theorem. We set F1 ϭ Ϫ0w> 0y
and F2 ϭ 0w>0x. Then 0F1> 0y and 0F2> 0x are continuous in R, and in (1) on the left we obtain
0F2

(6)

0x

0F1

Ϫ

0y

ϭ

0 2w
0x

2

ϩ

0 2w
0y 2

ϭ ٌ2w,

the Laplacian of w (see Sec. 9.7). Furthermore, using those expressions for F1 and F2, we get in (1) on the right
(7)

Ώ (F

1

dx ϩ F2 dy) ϭ

C

Ώ aF

1

C

dx
ds

ϩ F2

dy
b ds ϭ
ds

Ώ aϪ 0y ds ϩ 0x dyds b ds
0w dx

0w

C

where s is the arc length of C, and C is oriented as shown in Fig. 240. The integrand of the last integral may
be written as the dot product
(8)

(grad w) • n ϭ c

0w 0w
dy dx
0w dy
0w dx
,
Ϫ
.
d • c ,Ϫ d ϭ
0x ds
0y ds
0x 0y
ds ds

The vector n is a unit normal vector to C, because the vector r r (s) ϭ dr>ds ϭ [dx>ds, dy>ds] is the unit tangent
vector of C, and r r • n ϭ 0, so that n is perpendicular to r r . Also, n is directed to the exterior of C because in
Fig. 240 the positive x-component dx>ds of r r is the negative y-component of n, and similarly at other points. From
this and (4) in Sec. 9.7 we see that the left side of (8) is the derivative of w in the direction of the outward normal
of C. This derivative is called the normal derivative of w and is denoted by 0w/0n; that is, 0w> 0n ϭ (grad w) • n.
Because of (6), (7), and (8), Green’s theorem gives the desired formula relating the Laplacian to the normal derivative,

Ύ Ύٌ

2

(9)

R

w dx dy ϭ

Ώ

C

0w
ds.
0n

For instance, w ϭ x 2 Ϫ y 2 satisfies Laplace’s equation ٌ2w ϭ 0. Hence its normal derivative integrated over a closed
᭿
curve must give 0. Can you verify this directly by integration, say, for the square 0 Ϲ x Ϲ 1, 0 Ϲ y Ϲ 1?
y
y

r'

R

n

x
C

x

Fig. 239. Cardioid

Fig. 240. Example 4

c10-a.qxd

10/30/10

12:18 PM

438

Page 438

CHAP. 10 Vector Integral Calculus. Integral Theorems

Green’s theorem in the plane can be used in both directions, and thus may aid in the
evaluation of a given integral by transforming the given integral into another integral that
is easier to solve. This is illustrated further in the problem set. Moreover, and perhaps
more fundamentally, Green’s theorem will be the essential tool in the proof of a very
important integral theorem, namely, Stokes’s theorem in Sec. 10.9.

PROBLEM SET 10.4
1–10
Evaluate

LINE INTEGRALS: EVALUATION
BY GREEN’S THEOREM

Ύ F(r) • dr counterclockwise around the boundary

where k is a unit vector perpendicular to the xy-plane.
Verify (10) and (11) for F ϭ [7x, Ϫ3y] and C the circle
x 2 ϩ y 2 ϭ 4 as well as for an example of your own
choice.

C

C of the region R by Green’s theorem, where
13–17

1. F ϭ [ y, Ϫx], C the circle x 2 ϩ y 2 ϭ 1>4
2. F ϭ [6y 2, 2x Ϫ 2y 4],
Ϯ(2, 2), Ϯ(2, Ϫ2)

R the square with vertices

ds taken counterclockwise
Ύ 0w
0n

Using (9), find the value of

3. F ϭ [x e , y e ], R the rectangle with vertices (0, 0),
(2, 0), (2, 3), (0, 3)
2 y

INTEGRAL
OF THE NORMAL DERIVATIVE

2 x

4. F ϭ [x cosh 2y, 2x 2 sinh 2y],

R: x 2 Ϲ y Ϲ x

5. F ϭ [x ϩ y , x Ϫ y ], R: 1 Ϲ y Ϲ 2 Ϫ x
2

2

2

2

2

C

over the boundary C of the region R.
13. w ϭ cosh x, R the triangle with vertices (0, 0), (4, 2),
(0, 2).

6. F ϭ [cosh y, Ϫsinh x], R: 1 Ϲ x Ϲ 3, x Ϲ y Ϲ 3x

14. w ϭ x 2y ϩ xy 2,

7. F ϭ grad (x 3 cos2 (xy)),

15. w ϭ ex cos y ϩ xy 3,

؊x

8. F ϭ [Ϫe cos y, Ϫe
x 2 ϩ y 2 Ϲ 16, x м 0

؊x

R as in Prob. 5
sin y],

R the semidisk

9. F ϭ [ey>x, ey ln x ϩ 2x], R: 1 ϩ x 4 Ϲ y Ϲ 2
10. F ϭ [x 2y 2, Ϫx>y 2], R: 1 Ϲ x 2 ϩ y 2 Ϲ 4, x м 0,
y м x. Sketch R.
11. CAS EXPERIMENT. Apply (4) to figures of your
choice whose area can also be obtained by another
method and compare the results.
12. PROJECT. Other Forms of Green’s Theorem in
the Plane. Let R and C be as in Green’s theorem, r r
a unit tangent vector, and n the outer unit normal vector
of C (Fig. 240 in Example 4). Show that (1) may be
written

R: x 2 ϩ y 2 Ϲ 1, x м 0, y м 0
R: 1 Ϲ y Ϲ 10 Ϫ x 2, x м 0

16. W ϭ x 2 ϩ y 2, C: x 2 ϩ y 2 ϭ 4. Confirm the answer
by direct integration.
17. w ϭ x 3 Ϫ y 3, 0 Ϲ y Ϲ x 2,

ƒxƒ Ϲ 2

18. Laplace’s equation. Show that for a solution w(x, y)
of Laplace’s equation ٌ2w ϭ 0 in a region R with
boundary curve C and outer unit normal vector n,

(12)

ΎΎ
R

ca

2

2

0w
0w
b ϩ a b d dx dy
0x
0y
ϭ

Ώ w 0n ds.
0w

C

Ύ Ύ div F dx dy ϭ Ώ F • n ds

(10)

R

C

or

(11)

Ύ Ύ (curl F) • k dx dy ϭ Ώ F • rr ds
R

C

19. Show that w ϭ ex sin y satisfies Laplace’s equation
ٌ2w ϭ 0 and, using (12), integrate w(0w>0n) counterclockwise around the boundary curve C of the rectangle
0 Ϲ x Ϲ 2, 0 Ϲ y Ϲ 5.
20. Same task as in Prob. 19 when w ϭ x 2 ϩ y 2 and C
the boundary curve of the triangle with vertices (0, 0),
(1, 0), (0, 1).

c10-a.qxd

10/30/10

12:18 PM

Page 439

SEC. 10.5 Surfaces for Surface Integrals

10.5

439

Surfaces for Surface Integrals
Whereas, with line integrals, we integrate over curves in space (Secs. 10.1, 10.2), with
surface integrals we integrate over surfaces in space. Each curve in space is represented
by a parametric equation (Secs. 9.5, 10.1). This suggests that we should also find
parametric representations for the surfaces in space. This is indeed one of the goals of
this section. The surfaces considered are cylinders, spheres, cones, and others. The
second goal is to learn about surface normals. Both goals prepare us for Sec. 10.6 on
surface integrals. Note that for simplicity, we shall say “surface” also for a portion of
a surface.

Representation of Surfaces
Representations of a surface S in xyz-space are
z ϭ f (x, y)

(1)

or

g(x, y, z) ϭ 0.

For example, z ϭ ϩ 2a 2 Ϫ x 2 Ϫ y 2 or x 2 ϩ y 2 ϩ z 2 Ϫ a 2 ϭ 0 (z м 0) represents a
hemisphere of radius a and center 0.
Now for curves C in line integrals, it was more practical and gave greater flexibility to
use a parametric representation r ϭ r(t), where a Ϲ t Ϲ b. This is a mapping of the
interval a Ϲ t Ϲ b, located on the t-axis, onto the curve C (actually a portion of it) in
xyz-space. It maps every t in that interval onto the point of C with position vector r(t).
See Fig. 241A.
Similarly, for surfaces S in surface integrals, it will often be more practical to use a
parametric representation. Surfaces are two-dimensional. Hence we need two parameters,

Curve C
in space
z

x

r(t)

z

y

x

Surface S
in space

r (u,v)
y

v
t
a

b
(t-axis)

R
u
(uv-plane)

(A) Curve

(B) Surface

Fig. 241. Parametric representations of a curve and a surface

c10-a.qxd

10/30/10

12:18 PM

440

Page 440

CHAP. 10 Vector Integral Calculus. Integral Theorems

which we call u and v. Thus a parametric representation of a surface S in space is of
the form
r(u, v) ϭ [x(u, v), y(u, v), z(u, v)] ϭ x(u, v) i ϩ y(u, v) j ϩ z(u, v) k

(2)

where (u, v) varies in some region R of the uv-plane. This mapping (2) maps every point
(u, v) in R onto the point of S with position vector r(u, v). See Fig. 241B.
EXAMPLE 1

Parametric Representation of a Cylinder
The circular cylinder x 2 ϩ y 2 ϭ a 2, Ϫ1 Ϲ z Ϲ 1, has radius a, height 2, and the z-axis as axis. A parametric
representation is
r(u, v) ϭ [a cos u, a sin u, v] ϭ a cos u i ϩ a sin u j ϩ vk

(Fig. 242).

The components of r are x ϭ a cos u, y ϭ a sin u, z ϭ v. The parameters u, v vary in the rectangle R: 0 Ϲ u Ϲ
2p, Ϫ1 Ϲ v Ϲ 1in the uv-plane. The curves u ϭ const are vertical straight lines. The curves v ϭ const are
᭿
parallel circles. The point P in Fig. 242 corresponds to u ϭ p>3 ϭ 60°, v ϭ 0.7.
z
z
P
(v = 1)

v

P
v

(v = 0)

u

u

y

x
y

x

(v = –1)

Fig. 242. Parametric representation
of a cylinder

EXAMPLE 2

Fig. 243. Parametric representation
of a sphere

Parametric Representation of a Sphere
A sphere x 2 ϩ y 2 ϩ z 2 ϭ a 2 can be represented in the form
(3)

r(u, v) ϭ a cos v cos u i ϩ a cos v sin u j ϩ a sin vk

where the parameters u, v vary in the rectangle R in the uv-plane given by the inequalities 0 Ϲ u Ϲ 2p,
Ϫp>2 Ϲ v Ϲ p>2. The components of r are
x ϭ a cos v cos u,

y ϭ a cos v sin u,

z ϭ a sin v.

The curves u ϭ const and v ϭ const are the “meridians” and “parallels” on S (see Fig. 243). This representation
is used in geography for measuring the latitude and longitude of points on the globe.
Another parametric representation of the sphere also used in mathematics is
(3*)

r(u, v) ϭ a cos u sin v i ϩ a sin u sin v j ϩ a cos v k

where 0 Ϲ u Ϲ 2p, 0 Ϲ v Ϲ p.

᭿

c10-a.qxd

10/30/10

3:32 PM

Page 441

SEC. 10.5 Surfaces for Surface Integrals
EXAMPLE 3

441

Parametric Representation of a Cone
A circular cone z ϭ 2x 2 ϩ y 2, 0 Ϲ t Ϲ H can be represented by
r(u, v) ϭ [u cos v, u sin v, u] ϭ u cos v i ϩ u sin v j ϩ u k,
in components x ϭ u cos v, y ϭ u sin v, z ϭ u. The parameters vary in the rectangle R: 0 Ϲ u Ϲ H, 0 Ϲ v Ϲ 2p.
Check that x 2 ϩ y 2 ϭ z 2, as it should be. What are the curves u ϭ const and v ϭ const?
᭿

Tangent Plane and Surface Normal
Recall from Sec. 9.7 that the tangent vectors of all the curves on a surface S through a point
P of S form a plane, called the tangent plane of S at P (Fig. 244). Exceptions are points where
S has an edge or a cusp (like a cone), so that S cannot have a tangent plane at such a point.
Furthermore, a vector perpendicular to the tangent plane is called a normal vector of S at P.
Now since S can be given by r ϭ r(u, v) in (2), the new idea is that we get a curve C
on S by taking a pair of differentiable functions
u ϭ u(t),

v ϭ v(t)

whose derivatives u r ϭ du>dt and v r ϭ dv>dt are continuous. Then C has the position
vector ~r (t) ϭ r(u(t), v(t)) . By differentiation and the use of the chain rule (Sec. 9.6) we
obtain a tangent vector of C on S
~
~r r (t) ϭ d r ϭ 0r u r ϩ 0r v r .
dt
0u
0v
Hence the partial derivatives ru and rv at P are tangential to S at P. We assume that they
are linearly independent, which geometrically means that the curves u ϭ const and
v ϭ const on S intersect at P at a nonzero angle. Then ru and rv span the tangent plane
of S at P. Hence their cross product gives a normal vector N of S at P.
N ϭ ru ؋ rv

(4)

0.

The corresponding unit normal vector n of S at P is (Fig. 244)
(5)

nϭ

1
1
Nϭ
ru ؋ rv.
ƒNƒ
ƒ ru ؋ rv ƒ
n

rv
P
ru

S

Fig. 244. Tangent plane and normal vector

c10-a.qxd

10/30/10

12:18 PM

442

Page 442

CHAP. 10 Vector Integral Calculus. Integral Theorems

Also, if S is represented by g(x, y, z) ϭ 0, then, by Theorem 2 in Sec. 9.7,
nϭ

(5*)

1
ƒ grad g ƒ

grad g.

A surface S is called a smooth surface if its surface normal depends continuously on
the points of S.
S is called piecewise smooth if it consists of finitely many smooth portions.
For instance, a sphere is smooth, and the surface of a cube is piecewise smooth
(explain!). We can now summarize our discussion as follows.
THEOREM 1

Tangent Plane and Surface Normal

If a surface S is given by (2) with continuous ru ϭ 0r>0u and rv ϭ 0r>0v satisfying
(4) at every point of S, then S has, at every point P, a unique tangent plane passing
through P and spanned by ru and rv, and a unique normal whose direction depends
continuously on the points of S. A normal vector is given by (4) and the corresponding
unit normal vector by (5). (See Fig. 244.)
EXAMPLE 4

Unit Normal Vector of a Sphere
From (5*) we find that the sphere g(x, y, z) ϭ x 2 ϩ y 2 ϩ z 2 Ϫ a 2 ϭ 0 has the unit normal vector
y
z
x y z
x
n(x, y, z) ϭ c , , d ϭ i ϩ j ϩ k.
a a a
a
a
a
We see that n has the direction of the position vector [x, y, z] of the corresponding point. Is it obvious that this
must be the case?
᭿

EXAMPLE 5

Unit Normal Vector of a Cone
At the apex of the cone g(x, y, z) ϭ Ϫz ϩ 2x 2 ϩ y 2 ϭ 0 in Example 3, the unit normal vector n becomes
undetermined because from (5*) we get
nϭ c

y

x

,

,

Ϫ1

22(x ϩ y ) 22(x ϩ y ) 12
2

2

2

2

d ϭ

y
1
x
a
iϩ
j Ϫ kb .
2
12 2x 2 ϩ y 2
2x ϩ y 2

᭿

We are now ready to discuss surface integrals and their applications, beginning in the next
section.

PROBLEM SET 10.5
1–8

PARAMETRIC SURFACE REPRESENTATION

Familiarize yourself with parametric representations of
important surfaces by deriving a representation (1), by
finding the parameter curves (curves u ϭ const and
v ϭ const) of the surface and a normal vector N ϭ ru ؋ rv
of the surface. Show the details of your work.
1. xy-plane r(u, v) ϭ (u, v) (thus u i ϩ vj; similarly in
Probs. 2–8).
2. xy-plane in polar coordinates r(u, v) ϭ [u cos v, u sin v]
(thus u ϭ r, v ϭ u)

3. Cone r(u, v) ϭ [u cos v, u sin v, cu]
4. Elliptic cylinder r(u, v) ϭ [a cos v, b sin v, u]
5. Paraboloid of revolution r(u, v) ϭ [u cos v, u sin v,
u 2]
6. Helicoid r(u, v) ϭ [u cos v, u sin v, v]. Explain the
name.
7. Ellipsoid r(u, v) ϭ [a cos v cos u, b cos v sin u,
c sin v]
8. Hyperbolic paraboloid r(u, v) ϭ [au cosh v,
bu sinh v, u 2]

c10-a.qxd

10/30/10

12:18 PM

Page 443

SEC. 10.6 Surface Integrals

443

9. CAS EXPERIMENT. Graphing Surfaces, Dependence on a, b, c. Graph the surfaces in Probs. 3–8. In
Prob. 6 generalize the surface by introducing parameters a, b. Then find out in Probs. 4 and 6–8 how the
shape of the surfaces depends on a, b, c.
10. Orthogonal parameter curves u ϭ const and
v ϭ const on r(u, v) occur if and only if ru • rv ϭ 0.
Give examples. Prove it.
11. Satisfying (4). Represent the paraboloid in Prob. 5 so
~
~
that N(0, 0) 0 and show N.
12. Condition (4). Find the points in Probs. 1–8 at which
(4) N 0 does not hold. Indicate whether this results
from the shape of the surface or from the choice of the
representation.
13. Representation z ϭ f (x, y). Show that z ϭ f (x, y) or
g ϭ z Ϫ f (x, y) ϭ 0 can be written ( fu ϭ 0f> 0u, etc.)

(6)

r(u, v) ϭ [u, v, f (u, v)]

10.6

N ϭ grad g ϭ [Ϫfu, Ϫfv,

and
1].

14–19

DERIVE A PARAMETRIC
REPRESENTATION

Find a normal vector. The answer gives one representation;
there are many. Sketch the surface and parameter curves.
14. Plane 4x ϩ 3y ϩ 2z ϭ 12
15. Cylinder of revolution (x Ϫ 2)2 ϩ ( y ϩ 1)2 ϭ 25
16. Ellipsoid x 2 ϩ y 2 ϩ 19 z 2 ϭ 1
17. Sphere x 2 ϩ ( y ϩ 2.8)2 ϩ (z Ϫ 3.2)2 ϭ 2.25
18. Elliptic cone z ϭ 2x 2 ϩ 4y 2
19. Hyperbolic cylinder x 2 Ϫ y 2 ϭ 1
20. PROJECT. Tangent Planes T(P) will be less
important in our work, but you should know how to
represent them.
(a) If S: r(u, v), then T(P): (r* Ϫ r
ru rv) ϭ 0
(a scalar triple product) or
r*( p, q) ϭ r( P) ϩ pru( P) ϩ qrv( P).
(b) If S: g(x, y, z) ϭ 0, then
T( P): (r* Ϫ r( P)) ؒ ٌg ϭ 0.
(c) If S: z ϭ f (x, y), then
T( P): z* Ϫ z ϭ (x* Ϫ x) fx( P) ϩ ( y* Ϫ y) fy( P).
Interpret (a)Ϫ(c) geometrically. Give two examples for
(a), two for (b), and two for (c).

Surface Integrals
To define a surface integral, we take a surface S, given by a parametric representation as
just discussed,
(1)

r (u, v) ϭ [x (u, v), y (u, v), z (u, v)] ϭ x (u, v)i ϩ y (u, v)j ϩ z (u, v)k

where (u, v) varies over a region R in the uv-plane. We assume S to be piecewise smooth
(Sec. 10.5), so that S has a normal vector
(2)

N ϭ ru ؋ rv

and unit normal vector

nϭ

1
ƒNƒ

N

at every point (except perhaps for some edges or cusps, as for a cube or cone). For a given
vector function F we can now define the surface integral over S by
(3)

Ύ Ύ F • n dA ϭ Ύ Ύ F (r (u, v)) • N (u, v) du dv.
S

R

Here N ϭ ƒ N ƒ n by (2), and ƒ N ƒ ϭ ƒ ru ؋ rv ƒ is the area of the parallelogram with sides
ru and rv, by the definition of cross product. Hence
(3*)

n dA ϭ n ƒ N ƒ du dv ϭ N du dv.

And we see that dA ϭ ƒ N ƒ du dv is the element of area of S.

c10-b.qxd

10/30/10

12:31 PM

444

Page 444

CHAP. 10 Vector Integral Calculus. Integral Theorems

Also F • n is the normal component of F. This integral arises naturally in flow problems,
where it gives the flux across S when F ϭ rv. Recall, from Sec. 9.8, that the flux across
S is the mass of fluid crossing S per unit time. Furthermore, r is the density of the fluid
and v the velocity vector of the flow, as illustrated by Example 1 below. We may thus
call the surface integral (3) the flux integral.
We can write (3) in components, using F ϭ [F1, F2, F3], N ϭ [N1, N2, N3],
and n ϭ [cos a, cos b, cos g]. Here, a, b, g are the angles between n and the coordinate
axes; indeed, for the angle between n and i, formula (4) in Sec. 9.2 gives cos a ϭ
n • i> ƒ n ƒ ƒ i ƒ ϭ n • i, and so on. We thus obtain from (3)

Ύ Ύ F • n dA ϭ Ύ Ύ (F

1

(4)

S

cos a ϩ F2 cos b ϩ F3 cos g) dA

S

ϭ

Ύ Ύ (F N
1

1

ϩ F2N2 ϩ F3N3) du dv.

R

In (4) we can write cos a dA ϭ dy dz, cos b dA ϭ dz dx, cos g dA ϭ dx dy. Then (4)
becomes the following integral for the flux:

(5)

Ύ Ύ F • n dA ϭ Ύ Ύ (F

1

S

dy dz ϩ F2 dz dx ϩ F3 dx dy).

S

We can use this formula to evaluate surface integrals by converting them to double integrals
over regions in the coordinate planes of the xyz-coordinate system. But we must carefully
take into account the orientation of S (the choice of n). We explain this for the integrals
of the F3-terms,

Ύ ΎF

(5 r )

3

cos g dA ϭ

S

Ύ ΎF

3

dx dy.

S

If the surface S is given by z ϭ h(x, y) with (x, y) varying in a region R in the xy-plane,
and if S is oriented so that cos g Ͼ 0, then (5 r ) gives
(5 s )

Ύ ΎF

3

S

cos g dA ϭ ϩ

ΎΎF (x, y, h (x, y)) dx dy.
3

R

But if cos g Ͻ 0, the integral on the right of (5 s ) gets a minus sign in front. This follows
if we note that the element of area dx dy in the xy-plane is the projection ƒ cos g ƒ dA
of the element of area dA of S; and we have cos g ϭ ϩ ƒ cos g ƒ when cos g Ͼ 0, but
cos g ϭ Ϫ ƒ cos g ƒ when cos g Ͻ 0. Similarly for the other two terms in (5). At the same
time, this justifies the notations in (5).
Other forms of surface integrals will be discussed later in this section.
EXAMPLE 1

Flux Through a Surface
Compute the flux of water through the parabolic cylinder S: y ϭ x 2, 0 Ϲ x Ϲ 2, 0 Ϲ z Ϲ 3 (Fig. 245) if the
velocity vector is v ϭ F ϭ [3z 2, 6, 6xz], speed being measured in meters>sec. (Generally, F ϭ rv, but water
has the density r ϭ 1 g>cm3 ϭ 1 ton>m3.)

c10-b.qxd

10/30/10

12:31 PM

Page 445

SEC. 10.6 Surface Integrals

445
z
3

2

x

4

y

Fig. 245. Surface S in Example 1

Solution. Writing x ϭ u and z ϭ v, we have y ϭ x 2 ϭ u 2. Hence a representation of S is
S:

r ϭ [u, u 2, v]

(0 Ϲ u Ϲ 2, 0 Ϲ v Ϲ 3).

By differentiation and by the definition of the cross product,
N ϭ ru ؋ rv ϭ [1, 2u,

0] ؋ [0, 0,

1] ϭ [2u, Ϫ1, 0].

On S, writing simply F (S) for F[r (u, v)], we have F (S) ϭ [3v2, 6, 6uv]. Hence F(S) • N ϭ 6uv2 Ϫ 6. By
integration we thus get from (3) the flux
3

2

Ύ Ύ F • n dA ϭ Ύ Ύ (6uv
0

S

Ϫ 6) du dv ϭ

0

3

ϭ

3

2

Ύ (12v

2

0

Ύ (3u v

2 2

0

Ϫ 12) dv ϭ (4v3 Ϫ 12v) `

Ϫ 6u) `

3
vϭ0

2

dv
uϭ0

ϭ 108 Ϫ 36 ϭ 72 [m3>sec]

or 72,000 liters>sec. Note that the y-component of F is positive (equal to 6), so that in Fig. 245 the flow goes
from left to right.
Let us confirm this result by (5). Since
N ϭ ƒ N ƒ n ϭ ƒ N ƒ [cos a, cos b,

cos g] ϭ [2u,

Ϫ1, 0] ϭ [2x, Ϫ1, 0]

we see that cos a Ͼ 0, cos b Ͻ 0, and cos g ϭ 0. Hence the second term of (5) on the right gets a minus sign,
and the last term is absent. This gives, in agreement with the previous result,
3

4

Ύ Ύ F • n dA ϭ Ύ Ύ 3z
S

EXAMPLE 2

0

0

2

2

dy dz Ϫ

3

3

2

Ύ Ύ 6 dz dx ϭ Ύ 4 (3z ) dz Ϫ Ύ 6 # 3 dx ϭ 4 # 3
2

0

0

3

0

Ϫ 6 # 3 # 2 ϭ 72.

᭿

0

Surface Integral
Evaluate (3) when F ϭ [x 2, 0, 3y 2] and S is the portion of the plane x ϩ y ϩ z ϭ 1 in the first octant (Fig. 246).

Solution. Writing x ϭ u and y ϭ v, we have z ϭ 1 Ϫ x Ϫ y ϭ 1 Ϫ u Ϫ v. Hence we can represent the

plane x ϩ y ϩ z ϭ 1 in the form r(u, v) ϭ [u, v, 1 Ϫ u Ϫ v]. We obtain the first-octant portion S of this plane
by restricting x ϭ u and y ϭ v to the projection R of S in the xy-plane. R is the triangle bounded by the two
coordinate axes and the straight line x ϩ y ϭ 1, obtained from x ϩ y ϩ z ϭ 1 by setting z ϭ 0. Thus
0 Ϲ x Ϲ 1 Ϫ y, 0 Ϲ y Ϲ 1.
z
1

R
x

1

1

y

Fig. 246. Portion of a plane in Example 2

c10-b.qxd

10/30/10

12:31 PM

446

Page 446

CHAP. 10 Vector Integral Calculus. Integral Theorems
By inspection or by differentiation,
N ϭ ru ؋ rv ϭ [1, 0, Ϫ1] ؋ [0, 1, Ϫ1] ϭ [1, 1, 1].
Hence F (S) • N ϭ [u 2, 0, 3v2] • [1, 1, 1] ϭ u 2 ϩ 3v2. By (3),

Ύ Ύ F • n dA ϭ Ύ Ύ (u
S

1

2

ϩ 3v2) du dv ϭ

ΎΎ
0

R

ϭ

Ύ

1

0

1؊v

(u 2 ϩ 3v2) du dv

0

1
1
3
2
c (1 Ϫ v) ϩ 3v (1 Ϫ v) d dv ϭ .
3
3

᭿

Orientation of Surfaces
From (3) or (4) we see that the value of the integral depends on the choice of the unit
normal vector n. (Instead of n we could choose Ϫn.) We express this by saying that such
an integral is an integral over an oriented surface S, that is, over a surface S on which
we have chosen one of the two possible unit normal vectors in a continuous fashion. (For
a piecewise smooth surface, this needs some further discussion, which we give below.)
If we change the orientation of S, this means that we replace n with Ϫn. Then each
component of n in (4) is multiplied by Ϫ1, so that we have

THEOREM 1

Change of Orientation in a Surface Integral

The replacement of n by Ϫn (hence of N by ϪN) corresponds to the multiplication
of the integral in (3) or (4) by Ϫ1.

In practice, how do we make such a change of N happen, if S is given in the form (1)?
The easiest way is to interchange u and v, because then ru becomes rv and conversely,
so that N ϭ ru ؋ rv becomes rv ؋ ru ϭ Ϫru ؋ rv ϭ ϪN, as wanted. Let us illustrate
this.
EXAMPLE 3

Change of Orientation in a Surface Integral
In Example 1 we now represent S by r~ ϭ [v, v2, u], 0 Ϲ v Ϲ 2, 0 Ϲ u Ϲ 3. Then
~
N ϭ r~u ؋ r~v ϭ [0, 0, 1] ؋ [1, 2v, 0] ϭ [Ϫ2v, 1, 0].
~
~
~
For F ϭ [3z 2, 6, 6xz] we now get F (S) ϭ [3u 2, 6, 6uv]. Hence F (S) • N ϭ Ϫ6u 2v ϩ 6 and integration gives the
old result times Ϫ1,
3

2

3

Ύ Ύ ~F(S) • N~ dv du ϭ Ύ Ύ (Ϫ6u v ϩ 6) dv du ϭ Ύ (Ϫ12u
2

R

0

0

2

ϩ 12) du ϭ Ϫ72.

᭿

0

Orientation of Smooth Surfaces
A smooth surface S (see Sec. 10.5) is called orientable if the positive normal direction,
when given at an arbitrary point P0 of S, can be continued in a unique and continuous
way to the entire surface. In many practical applications, the surfaces are smooth and thus
orientable.

c10-b.qxd

10/30/10

12:31 PM

Page 447

SEC. 10.6 Surface Integrals

447
n

S

S

C

C
n
(a) Smooth surface
n
S2
n

C*
S1
(b) Piecewise smooth surface

Fig. 247. Orientation of a surface

Orientation of Piecewise Smooth Surfaces
Here the following idea will do it. For a smooth orientable surface S with boundary curve
C we may associate with each of the two possible orientations of S an orientation of C,
as shown in Fig. 247a. Then a piecewise smooth surface is called orientable if we can
orient each smooth piece of S so that along each curve C* which is a common boundary
of two pieces S1 and S2 the positive direction of C* relative to S1 is opposite to the
direction of C* relative to S2. See Fig. 247b for two adjacent pieces; note the arrows
along C*.

Theory: Nonorientable Surfaces
A sufficiently small piece of a smooth surface is always orientable. This may not hold for
entire surfaces. A well-known example is the Möbius strip,5 shown in Fig. 248. To make
a model, take the rectangular paper in Fig. 248, make a half-twist, and join the short sides
together so that A goes onto A, and B onto B. At P0 take a normal vector pointing, say,
to the left. Displace it along C to the right (in the lower part of the figure) around the strip
until you return to P0 and see that you get a normal vector pointing to the right, opposite
to the given one. See also Prob. 17.

B

A
P0

C
A

B

C

Fig. 248. Möbius strip

5
AUGUST FERDINAND MÖBIUS (1790–1868), German mathematician, student of Gauss, known for his
work in surface theory, geometry, and complex analysis (see Sec. 17.2).

c10-b.qxd

10/30/10

12:31 PM

448

Page 448

CHAP. 10 Vector Integral Calculus. Integral Theorems

Surface Integrals Without Regard to Orientation
Another type of surface integral is

Ύ Ύ G (r) dA ϭ Ύ Ύ G (r (u, v)) ƒ N (u, v) ƒ du dv.

(6)

S

R

Here dA ϭ ƒ N ƒ du dv ϭ ƒ ru ؋ rv ƒ du dv is the element of area of the surface S represented
by (1) and we disregard the orientation.
We shall need later (in Sec. 10.9) the mean value theorem for surface integrals, which
states that if R in (6) is simply connected (see Sec. 10.2) and G(r) is continuous in a
domain containing R, then there is a point (u 0, v0) in R such that

Ύ Ύ G (r) dA ϭ G (r (u , v )) A

(7)

0

(A ϭ Area of S ).

0

S

As for applications, if G(r) is the mass density of S, then (6) is the total mass of S. If
G ϭ 1, then (6) gives the area A(S) of S,

Ύ Ύ dA ϭ Ύ Ύ ƒ r

A (S ) ϭ

(8)

u

S

؋ rv ƒ du dv.

R

Examples 4 and 5 show how to apply (8) to a sphere and a torus. The final example,
Example 6, explains how to calculate moments of inertia for a surface.
EXAMPLE 4

Area of a Sphere
For a sphere r (u, v) ϭ [a cos v cos u, a cos v sin u, a sin v], 0 Ϲ u Ϲ 2p,
in Sec. 10.5], we obtain by direct calculation (verify!)
ru ؋ rv ϭ [a 2 cos2 v cos u,

a 2 cos2 v sin u,

Ϫp>2 Ϲ v Ϲ p>2 [see (3)

a 2 cos v sin v].

Using cos2 u ϩ sin2 u ϭ 1 and then cos2 v ϩ sin2 v ϭ 1, we obtain
ƒ ru ؋ rv ƒ ϭ a 2(cos4 v cos2 u ϩ cos4 v sin2 u ϩ cos2 v sin2 v)1>2 ϭ a 2 ƒ cos v ƒ .
With this, (8) gives the familiar formula (note that ƒ cos v ƒ ϭ cos v when Ϫp>2 Ϲ v Ϲ p>2)
A (S) ϭ a 2

EXAMPLE 5

p>2

Ύ Ύ
Ϫp>2

2p

0

ƒ cos v ƒ du dv ϭ 2pa 2

Ύ

p>2

Ϫp>2

cos v dv ϭ 4pa 2.

᭿

Torus Surface (Doughnut Surface): Representation and Area
A torus surface S is obtained by rotating a circle C about a straight line L in space so that C does not intersect
or touch L but its plane always passes through L. If L is the z-axis and C has radius b and its center has distance
a (Ͼ b) from L, as in Fig. 249, then S can be represented by
r (u, v) ϭ (a ϩ b cos v) cos u i ϩ (a ϩ b cos v) sin u j ϩ b sin v k
where 0 Ϲ u Ϲ 2p, 0 Ϲ v Ϲ 2p. Thus
ru ϭ Ϫ(a ϩ b cos v) sin u i ϩ (a ϩ b cos v) cos u j
rv ϭ Ϫb sin v cos u i Ϫ b sin v sin u j ϩ b cos v k
ru ؋ rv ϭ b (a ϩ b cos v)(cos u cos v i ϩ sin u cos v j ϩ sin v k).

c10-b.qxd

10/30/10

12:31 PM

Page 449

SEC. 10.6 Surface Integrals

449

Hence ƒ ru ؋ rv ƒ ϭ b (a ϩ b cos v), and (8) gives the total area of the torus,
2p

Ύ Ύ

A(S) ϭ

(9)

0

2p

᭿

b (a ϩ b cos v) du dv ϭ 4p2ab.

0

z
C
v

A

y

b

a

y
u

x

Fig. 249. Torus in Example 5

EXAMPLE 6

Moment of Inertia of a Surface
Find the moment of inertia I of a spherical lamina S: ϭ x 2 ϩ y 2 ϩ z 2 ϭ a 2 of constant mass density and total
mass M about the z-axis.

Solution. If a mass is distributed over a surface S and ␮(x, y, z) is the density of the mass (ϭ mass per unit
area), then the moment of inertia I of the mass with respect to a given axis L is defined by the surface integral
Iϭ

(10)

Ύ Ύ␮D

2

dA

S

where D(x, y, z) is the distance of the point (x, y, z) from L. Since, in the present example, ␮ is constant and S
has the area A ϭ 4pa 2, we have ␮ ϭ M>A ϭ M>(4pa 2).
For S we use the same representation as in Example 4. Then D 2 ϭ x 2 ϩ y 2 ϭ a 2 cos2 v. Also, as in that example,
dA ϭ a 2 cos v du dv. This gives the following result. [In the integration, use cos3 v ϭ cos v (1 Ϫ sin2 v).]
Iϭ

Ύ Ύ ␮D

2

dA ϭ

S

p>2

M
4pa

2

Ύ Ύ

Representations z ϭ f (x, y).
v ϭ y, r ϭ [u, v, f ] gives

2p

Ϫp>2 0

a 4 cos3 v du dv ϭ

Ma 2
2

Ύ

p>2

Ϫp>2

cos3 v dv ϭ

2Ma 2
3

.

If a surface S is given by z ϭ f (x, y), then setting u ϭ x,

ƒ N ƒ ϭ ƒ ru ؋ rv ƒ ϭ ƒ [1, 0, fu] ؋ [0, 1, fv] ƒ ϭ ƒ [Ϫfu, Ϫfv, 1] ƒ ϭ 21 ϩ f 2u ϩ f 2v
and, since fu ϭ fx, fv ϭ fy, formula (6) becomes

(11)

ΎΎ
S

G (r) dA ϭ

ΎΎ
R*

᭿

1ϩa

G (x, y, f (x, y))
G

2

0f
0x

b ϩa

2

0f
0y

b dx dy.

c10-b.qxd

10/30/10

450

12:31 PM

Page 450

CHAP. 10 Vector Integral Calculus. Integral Theorems
N

z
S

R*

y

x

Fig. 250. Formula (11)

Here R* is the projection of S into the xy-plane (Fig. 250) and the normal vector N on S
points up. If it points down, the integral on the right is preceded by a minus sign.
From (11) with G ϭ 1 we obtain for the area A(S) of S: z ϭ f (x, y) the formula
2

Ύ Ύ G1 ϩ a 0x b
0f

A (S) ϭ

(12)

ϩa

2

0f
0y

b dx dy

R*

where R* is the projection of S into the xy-plane, as before.

PROBLEM SET 10.6
1–10

FLUX INTEGRALS (3)

Ύ F • n dA
S

Evaluate the integral for the given data. Describe the kind
of surface. Show the details of your work.
1. F ϭ [Ϫx 2, y 2, 0], S: r ϭ [u, v, 3u Ϫ 2v],
0 Ϲ u Ϲ 1.5, Ϫ2 Ϲ v Ϲ 2
2. F ϭ [ey, ex, 1], S: x ϩ y ϩ z ϭ 1, x м 0, y м 0,
zм0
3. F ϭ [0, x, 0], S: x 2 ϩ y 2 ϩ z 2 ϭ 1, x м 0,
y м 0, z м 0
4. F ϭ [ey, Ϫez, ex], S: x 2 ϩ y 2 ϭ 25, x м 0,
y м 0, 0 Ϲ z Ϲ 2
5. F ϭ [x, y, z], S: r ϭ [u cos v, u sin v, u 2],
0 Ϲ u Ϲ 4, Ϫp Ϲ v Ϲ p
6. F ϭ [cosh y, 0, sinh x], S: z ϭ x ϩ y 2, 0 Ϲ y Ϲ x,
0ϹxϹ1
7. F ϭ [0, sin y, cos z], S the cylinder x ϭ y 2, where
0 Ϲ y Ϲ p>4 and 0 Ϲ z Ϲ y
8. F ϭ [tan xy, x, y], S: y 2 ϩ z 2 ϭ 1, 2 Ϲ x Ϲ 5,
y м 0, z м 0
9. F ϭ [0, sinh z, cosh x], S: x 2 ϩ z 2 ϭ 4,
0 Ϲ x Ϲ 1> 12, 0 Ϲ y Ϲ 5, z м 0
10. F ϭ [ y 2, x 2, z 4], S: z ϭ 4 2x 2 ϩ y 2, 0 Ϲ z Ϲ 8,
yм0
11. CAS EXPERIMENT. Flux Integral. Write a program for evaluating surface integrals (3) that prints
intermediate results (F, F • N, the integral over one of

the two variables). Can you obtain experimentally some
rules on functions and surfaces giving integrals that can
be evaluated by the usual methods of calculus? Make
a list of positive and negative results.
12–16

SURFACE INTEGRALS (6)

Ύ Ύ G (r) dA
S

Evaluate these integrals for the following data. Indicate the
kind of surface. Show the details.
12. G ϭ cos x ϩ sin x, S the portion of x ϩ y ϩ z ϭ 1
in the first octant
13. G ϭ x ϩ y ϩ z, z ϭ x ϩ 2y, 0 Ϲ x Ϲ p,
0ϹyϹx
14. G ϭ ax ϩ by ϩ cz, S: x 2 ϩ y 2 ϩ z 2 ϭ 1, y ϭ 0,
zϭ0
15. G ϭ (1 ϩ 9xz)3>2, S: r ϭ [u, v, u 3], 0 Ϲ u Ϲ 1,
Ϫ2 Ϲ v Ϲ 2
16. G ϭ arctan ( y>x), S: z ϭ x 2 ϩ y 2, 1 Ϲ z Ϲ 9,
x м 0, y м 0
17. Fun with Möbius. Make Möbius strips from long slim
rectangles R of grid paper (graph paper) by pasting the
short sides together after giving the paper a half-twist.
In each case count the number of parts obtained by
cutting along lines parallel to the edge. (a) Make R three
squares wide and cut until you reach the beginning.
(b) Make R four squares wide. Begin cutting one square
away from the edge until you reach the beginning. Then
cut the portion that is still two squares wide. (c) Make

c10-b.qxd

10/30/10

12:31 PM

Page 451

SEC. 10.6 Surface Integrals

451

R five squares wide and cut similarly. (d) Make R six
squares wide and cut. Formulate a conjecture about the
number of parts obtained.
18. Gauss “Double Ring” (See Möbius, Works 2, 518–
559). Make a paper cross (Fig. 251) into a “double ring”
by joining opposite arms along their outer edges (without
twist), one ring below the plane of the cross and the other
above. Show experimentally that one can choose any four
boundary points A, B, C, D and join A and C as well as
B and D by two nonintersecting curves. What happens if
you cut along the two curves? If you make a half-twist
in each of the two rings and then cut? (Cf. E. Kreyszig,
Proc. CSHPM 13 (2000), 23–43.)

21. Find a formula for the moment of inertia of the lamina
in Prob. 20 about the line y ϭ x, z ϭ 0.
22–23 Find the moment of inertia of a lamina S of density
1 about an axis B, where
22. S: x 2 ϩ y 2 ϭ 1, 0 Ϲ z Ϲ h, B: the line z ϭ h>2 in
the xz-plane
23. S: x 2 ϩ y 2 ϭ z 2, 0 Ϲ z Ϲ h, B: the z-axis
24. Steiner’s theorem.6 If IB is the moment of inertia of
a mass distribution of total mass M with respect to a line
B through the center of gravity, show that its moment
of inertia IK with respect to a line K, which is parallel
to B and has the distance k from it is
IK ϭ IB ϩ k 2M.

b

A

B

C

D

a

a

25. Using Steiner’s theorem, find the moment of inertia of
a mass of density 1 on the sphere S : x 2 ϩ y 2 ϩ z 2 ϭ 1
about the line K: x ϭ 1, y ϭ 0 from the moment of
inertia of the mass about a suitable line B, which you
must first calculate.
26. TEAM PROJECT. First Fundamental Form of S.
Given a surface S: r (u, v), the differential form
(13)

b

Fig. 251. Problem 18. Gauss “Double Ring”

with coefficients (in standard notation, unrelated to F,
G elsewhere in this chapter)
(14)

APPLICATIONS
19. Center of gravity. Justify the following formulas for
the mass M and the center of gravity ( x, y, z) of a lamina
S of density (mass per unit area) s (x, y, z) in space:

Ύ Ύ s dA,

Mϭ

xϭ

1
M

S

1
yϭ
M

ΎΎ

Ύ Ύ xs dA,

S

ΎΎ

Ύ Ύ(y

2

S

Iy ϭ

Ύ Ύ (x
S

Iz ϭ

b

lϭ
(15)

Ύ Ύ (x
S

6

2

ϩ y 2)s dA.

Ύ 2rr (t) • rr (t) dt
b

ϭ

S

ϩ z 2)s dA,

F ϭ ru • rv, G ϭ rv • rv

a

zs dA.

20. Moments of inertia. Justify the following formulas
for the moments of inertia of the lamina in Prob. 19
about the x-, y-, and z-axes, respectively:
Ix ϭ

E ϭ ru • ru,

is called the first fundamental form of S. This form
is basic because it permits us to calculate lengths,
angles, and areas on S. To show this prove (a)–(c):
(a) For a curve C: u ϭ u (t), v ϭ v (t), a Ϲ t Ϲ b, on
S, formulas (10), Sec. 9.5, and (14) give the length

S

1
ys dA, z ϭ
M

ds 2 ϭ E du 2 ϩ 2F du dv ϩ G dv2

2

ϩ z 2)s dA,

Ύ 2Eur
a

2

ϩ 2Fu r v r ϩ Gv r 2 dt.

(b) The angle g between two intersecting curves
C1: u ϭ g (t), v ϭ h (t) and C2: u ϭ p (t), v ϭ q (t) on
S: r (u, v) is obtained from
(16)

cos g ϭ

a•b
ƒaƒ ƒbƒ

where a ϭ ru g r ϩ rv h r and b ϭ ru p r ϩ rv q r are tangent vectors of C1 and C2.

JACOB STEINER (1796–1863), Swiss geometer, born in a small village, learned to write only at age 14,
became a pupil of Pestalozzi at 18, later studied at Heidelberg and Berlin and, finally, because of his outstanding
research, was appointed professor at Berlin University.

c10-b.qxd

10/30/10

12:31 PM

452

Page 452

CHAP. 10 Vector Integral Calculus. Integral Theorems
(c) The square of the length of the normal vector N
can be written
(17)

(e) Find the first fundamental form of the torus in
Example 5. Use it to calculate the area A of the torus.
Show that A can also be obtained by the theorem of
Pappus,7 which states that the area of a surface of
revolution equals the product of the length of a
meridian C and the length of the path of the center of
gravity of C when C is rotated through the angle 2p.
(f) Calculate the first fundamental form for the usual
representations of important surfaces of your own
choice (cylinder, cone, etc.) and apply them to the
calculation of lengths and areas on these surfaces.

ƒ N ƒ 2 ϭ ƒ ru ؋ rv ƒ 2 ϭ EG Ϫ F 2,

so that formula (8) for the area A (S) of S becomes
A (S) ϭ

ΎΎ dA ϭ ΎΎ ƒ N ƒ du dv
S

(18)
ϭ

R

ΎΎ 2EG Ϫ F

2

du dv.

R

(d) For polar coordinates u (ϭ r) and v (ϭ u) defined
by x ϭ u cos v, y ϭ u sin v we have E ϭ 1, F ϭ 0,
G ϭ u 2, so that
ds 2 ϭ du 2 ϩ u 2 dv2 ϭ dr 2 ϩ r 2 du2.
Calculate from this and (18) the area of a disk of
radius a.

10.7

Triple Integrals.
Divergence Theorem of Gauss
In this section we discuss another “big” integral theorem, the divergence theorem, which
transforms surface integrals into triple integrals. So let us begin with a review of the
latter.
A triple integral is an integral of a function f (x, y, z) taken over a closed bounded,
three-dimensional region T in space. (Note that “closed” and “bounded” are defined in
the same way as in footnote 2 of Sec. 10.3, with “sphere” substituted for “circle”). We
subdivide T by planes parallel to the coordinate planes. Then we consider those boxes of
the subdivision that lie entirely inside T, and number them from 1 to n. Here each box
consists of a rectangular parallelepiped. In each such box we choose an arbitrary point,
say, (x k, yk, z k) in box k. The volume of box k we denote by ¢Vk. We now form the sum
n

Jn ϭ a f (x k, yk, z k) ¢Vk.
kϭ1

This we do for larger and larger positive integers n arbitrarily but so that the maximum
length of all the edges of those n boxes approaches zero as n approaches infinity. This
gives a sequence of real numbers Jn1, Jn2, Á . We assume that f (x, y, z) is continuous in a
domain containing T, and T is bounded by finitely many smooth surfaces (see Sec. 10.5).
Then it can be shown (see Ref. [GenRef4] in App. 1) that the sequence converges to
a limit that is independent of the choice of subdivisions and corresponding points

7
PAPPUS OF ALEXANDRIA (about A.D. 300), Greek mathematician. The theorem is also called Guldin’s
theorem. HABAKUK GULDIN (1577–1643) was born in St. Gallen, Switzerland, and later became professor
in Graz and Vienna.

c10-b.qxd

10/30/10

12:31 PM

Page 453

SEC. 10.7 Triple Integrals. Divergence Theorem of Gauss

453

(x k, yk, z k). This limit is called the triple integral of f (x, y, z) over the region T and is
denoted by

ΎΎΎ f (x, y, z) dx dy dz

ΎΎΎ f (x, y, z) dV.

or by

T

T

Triple integrals can be evaluated by three successive integrations. This is similar to the
evaluation of double integrals by two successive integrations, as discussed in Sec. 10.3.
Example 1 below explains this.

Divergence Theorem of Gauss
Triple integrals can be transformed into surface integrals over the boundary surface of a
region in space and conversely. Such a transformation is of practical interest because one
of the two kinds of integral is often simpler than the other. It also helps in establishing
fundamental equations in fluid flow, heat conduction, etc., as we shall see. The transformation
is done by the divergence theorem, which involves the divergence of a vector function
F ϭ [F1, F2, F3] ϭ F1i ϩ F2 j ϩ F3k, namely,
div F ϭ

(1)

THEOREM 1

0F1
0x

ϩ

0F2
0y

ϩ

0F3

(Sec. 9.8).

0z

Divergence Theorem of Gauss
(Transformation Between Triple and Surface Integrals)

Let T be a closed bounded region in space whose boundary is a piecewise smooth
orientable surface S. Let F (x, y, z) be a vector function that is continuous and has
continuous first partial derivatives in some domain containing T. Then

ΎΎΎ div F dV ϭ ΎΎ F • n dA.

(2)

T

S

In components of F ϭ [F1, F2, F3] and of the outer unit normal vector
n ϭ [cos a, cos b, cos g] of S (as in Fig. 253), formula (2) becomes

ΎΎΎ a 0x

0F1

ϩ

0F2
0y

ϩ

0F3
0z

b dx dy dz

T

(2*)

ϭ

ΎΎ (F

1

cos a ϩ F2 cos b ϩ F3 cos g) dA

S

ϭ

ΎΎ (F

1 dy

S

dz ϩ F2 dz dx ϩ F3 dx dy).

c10-b.qxd

10/30/10

12:31 PM

454

Page 454

CHAP. 10 Vector Integral Calculus. Integral Theorems

“Closed bounded region” is explained above, “piecewise smooth orientable” in Sec. 10.5,
and “domain containing T ” in footnote 4, Sec. 10.4, for the two-dimensional case.
Before we prove the theorem, let us show a standard application.
EXAMPLE 1

Evaluation of a Surface Integral by the Divergence Theorem
Before we prove the theorem, let us show a typical application. Evaluate

z
b

ΎΎ (x

Iϭ

3

dy dz ϩ x 2y dz dx ϩ x 2z dx dy)

S

where S is the closed surface in Fig. 252 consisting of the cylinder x 2 ϩ y 2 ϭ a 2 (0 Ϲ z Ϲ b) and the circular
disks z ϭ 0 and z ϭ b (x 2 ϩ y 2 Ϲ a 2).
x

a

a y

Fig. 252. Surface S
in Example 1

Solution. F1 ϭ x3, F2 ϭ x2y, F3 ϭ x2z. Hence div F ϭ 3x 2 ϩ x 2 ϩ x 2 ϭ 5x 2. The form of the surface

suggests that we introduce polar coordinates r, u defined by x ϭ r cos u, y ϭ r sin u (thus cylindrical coordinates
r, u, z). Then the volume element is dx dy dz ϭ r dr du dz, and we obtain
Iϭ

ΎΎΎ 5x

b

2

dx dy dz ϭ

b

Ύ Ύ

2p

a4

zϭ0 uϭ0

PROOF

a
2

2

(5r cos u) r dr du dz

zϭ0 uϭ0 rϭ0

T

ϭ5

2p

Ύ Ύ Ύ

4

cos2 u du dz ϭ 5

Ύ

b

a 4p

zϭ0

4

dz ϭ

5p
4

a 4b.

᭿

We prove the divergence theorem, beginning with the first equation in (2*). This
equation is true if and only if the integrals of each component on both sides are equal;
that is,
(3)

ΎΎΎ

0F1
0x

dx dy dz ϭ

T

(4)

ΎΎΎ
ΎΎΎ
T

1 cos

a dA,

S

0F2
0y

dx dy dz ϭ

T

(5)

ΎΎ F

ΎΎ F

2 cos

b dA,

3 cos

g dA.

S

0F3
0z

dx dy dz ϭ

ΎΎ F
S

We first prove (5) for a special region T that is bounded by a piecewise smooth
orientable surface S and has the property that any straight line parallel to any one of the
coordinate axes and intersecting T has at most one segment (or a single point) in common
with T. This implies that T can be represented in the form
(6)

g (x, y) Ϲ z Ϲ h(x, y)

where (x, y) varies in the orthogonal projection R of T in the xy-plane. Clearly, z ϭ g (x, y)
represents the “bottom” S2 of S (Fig. 253), whereas z ϭ h (x, y) represents the “top” S1 of
S, and there may be a remaining vertical portion S3 of S. (The portion S3 may degenerate
into a curve, as for a sphere.)

c10-b.qxd

10/30/10

3:41 PM

Page 455

SEC. 10.7 Triple Integrals. Divergence Theorem of Gauss

455

To prove (5), we use (6). Since F is continuously differentiable in some domain containing
T, we have

ΎΎΎ

(7)

0F3
0z

dx dy dz ϭ

T

ΎΎ c Ύ

h (x, y)

0F3
0z

g (x, y)

R

dz d dx dy.

Integration of the inner integral [ Á ] gives F3[x, y, h (x, y)] Ϫ F3[x, y, g (x, y)]. Hence the
triple integral in (7) equals

ΎΎ F [x, y, h (x, y)] dx dy Ϫ ΎΎ F [x, y, g (x, y)] dx dy.

(8)

3

3

R

R

n γ

S1

z

γ

n

S3
S2

n
x
y

R

Fig. 253. Example of a special region

But the same result is also obtained by evaluating the right side of (5); that is [see also
the last line of (2*)],

ΎΎ F

3 cos

S

g dA ϭ

ΎΎ F

3 dx

dy

S

ϭ ϩ

ΎΎ F [x, y, h (x, y)] dx dy Ϫ ΎΎ F [x, y, g(x, y)] dx dy,
3

3

R

R

where the first integral over R gets a plus sign because cos g Ͼ 0 on S1 in Fig. 253 [as
in (5 s ), Sec. 10.6], and the second integral gets a minus sign because cos g Ͻ 0 on S2.
This proves (5).
The relations (3) and (4) now follow by merely relabeling the variables and using the
fact that, by assumption, T has representations similar to (6), namely,
ෂ
ෂ
g( y, z) Ϲ x Ϲ h ( y, z)

and

ෂ
g(z, x) Ϲ y Ϲ ෂ
h (z, x) .

This proves the first equation in (2*) for special regions. It implies (2) because the left side
of (2*) is just the definition of the divergence, and the right sides of (2) and of the first
equation in (2*) are equal, as was shown in the first line of (4) in the last section. Finally,
equality of the right sides of (2) and (2*), last line, is seen from (5) in the last section.
This establishes the divergence theorem for special regions.

c10-b.qxd

10/30/10

12:31 PM

456

Page 456

CHAP. 10 Vector Integral Calculus. Integral Theorems

For any region T that can be subdivided into finitely many special regions by means of
auxiliary surfaces, the theorem follows by adding the result for each part separately. This
procedure is analogous to that in the proof of Green’s theorem in Sec. 10.4. The surface
integrals over the auxiliary surfaces cancel in pairs, and the sum of the remaining surface
integrals is the surface integral over the whole boundary surface S of T; the triple integrals
over the parts of T add up to the triple integral over T.
The divergence theorem is now proved for any bounded region that is of interest in
practical problems. The extension to a most general region T of the type indicated in the
theorem would require a certain limit process; this is similar to the situation in the case
of Green’s theorem in Sec. 10.4.
᭿
EXAMPLE 2

Verification of the Divergence Theorem
Evaluate

ΎΎ (7x i Ϫ zk) • n dA over the sphere S : x

2

ϩ y 2 ϩ z 2 ϭ 4 (a) by (2), (b) directly.

S

Solution. (a) div F ϭ div [7x, 0, Ϫz] ϭ div [7xi Ϫ zk] ϭ 7 Ϫ 1 ϭ 6. Answer: 6 ؒ ( 43 )p ؒ 23 ϭ 64p.

(b) We can represent S by (3), Sec. 10.5 (with a ϭ 2), and we shall use n dA ϭ N du dv [see (3*), Sec. 10.6].
Accordingly,
S:
Then

r ϭ [2 cos v cos u, 2 cos v sin u,

.

2 sin u]

ru ϭ [Ϫ2 cos v sin u, 2 cos v cos u,

0]

rv ϭ [Ϫ2 sin v cos u, Ϫ2 sin v sin u, 2 cos v]
N ϭ ru ؋ rv ϭ [4 cos2 v cos u, 4 cos2 v sin u, 4 cos v sin v].
Now on S we have x ϭ 2 cos v cos u, z ϭ 2 sin v, so that F ϭ [7x, 0, Ϫz] becomes on S
F(S) ϭ [14 cos v cos u, 0,
and

Ϫ2 sin v]

F(S) • N ϭ (14 cos v cos u) # 4 cos2 v cos u ϩ (Ϫ2 sin v) # 4 cos v sin v
ϭ 56 cos3 v cos2 u Ϫ 8 cos v sin2 v.

On S we have to integrate over u from 0 to 2p. This gives

p ؒ 56 cos3 v Ϫ 2p ؒ 8 cos v sin2 v.
The integral of cos v sin2 v equals (sin3 v)>3, and that of cos3 v ϭ cos v (1 Ϫ sin2 v) equals sin v Ϫ (sin3 v)>3.
On S we have Ϫp>2 Ϲ v Ϲ p>2, so that by substituting these limits we get
56p(2 Ϫ 23 ) Ϫ 16p ؒ 23 ϭ 64p
as hoped for. To see the point of Gauss’s theorem, compare the amounts of work.

᭿

Coordinate Invariance of the Divergence. The divergence (1) is defined in terms of
coordinates, but we can use the divergence theorem to show that div F has a meaning
independent of coordinates.
For this purpose we first note that triple integrals have properties quite similar to those
of double integrals in Sec. 10.3. In particular, the mean value theorem for triple integrals
asserts that for any continuous function f (x, y, z) in a bounded and simply connected region
T there is a point Q : (x 0, y0, z 0) in T such that
(9)

ΎΎΎ f (x, y, z) dV ϭ f (x , y , z ) V(T)
0

T

0

0

(V(T) ϭ volume of T ).

c10-b.qxd

10/30/10

12:31 PM

Page 457

SEC. 10.7 Triple Integrals. Divergence Theorem of Gauss

457

In this formula we interchange the two sides, divide by V(T), and set f ϭ div F. Then by
the divergence theorem we obtain for the divergence an integral over the boundary surface
S(T) of T,
(10)

div F(x 0, y0, z 0) ϭ

1
V(T )

ΎΎΎ div F dV ϭ V(T1 ) ΎΎ F • n dA.
T

S(T)

We now choose a point P: (x 1, y1, z 1) in T and let T shrink down onto P so that the
maximum distance d(T ) of the points of T from P goes to zero. Then Q: (x 0, y0, z 0) must
approach P. Hence (10) becomes
(11)

div F(P) ϭ

lim

d(T ) : 0

1
V(T )

ΎΎ F • n dA.

S(T)

This proves
THEOREM 2

Invariance of the Divergence

The divergence of a vector function F with continuous first partial derivatives in a
region T is independent of the particular choice of Cartesian coordinates. For any
P in T it is given by (11).

Equation (11) is sometimes used as a definition of the divergence. Then the representation (1)
in Cartesian coordinates can be derived from (11).
Further applications of the divergence theorem follow in the problem set and in the
next section. The examples in the next section will also shed further light on the nature
of the divergence.

PROBLEM SET 10.7
1–8

APPLICATION: MASS DISTRIBUTION

Find the total mass of a mass distribution of density s in
a region T in space.
1. s ϭ x 2 ϩ y 2 ϩ z 2, T the box ƒ x ƒ Ϲ 4, ƒ y ƒ Ϲ 1,
0ϹzϹ2
2. s ϭ xyz, T the box 0 Ϲ x Ϲ a, 0 Ϲ y Ϲ b,
0ϹzϹc
3. s ϭ e؊x؊y؊z, T : 0 Ϲ x Ϲ 1 Ϫ y, 0 Ϲ y Ϲ 1,
0ϹzϹ2
4. s as in Prob. 3, T the tetrahedron with vertices (0, 0, 0),
(3, 0, 0), (0, 3, 0), (0, 0, 3)
5. s ϭ sin 2x cos 2y, T : 0 Ϲ x Ϲ 14 p,
1
1
0ϹzϹ6
4 p Ϫ x Ϲ y Ϲ 4 p,
2 2 2
6. s ϭ x y z , T the cylindrical region x 2 ϩ z 2 Ϲ 16,
ƒyƒ Ϲ 4
7. s ϭ arctan ( y>x), T: x 2 ϩ y 2 ϩ z 2 Ϲ a 2, z м 0
8. s ϭ x 2 ϩ y 2, T as in Prob. 7

9–18

APPLICATION
OF THE DIVERGENCE THEOREM

Evaluate the surface integral

ΎΎ F • n dA by the divergence
S

theorem. Show the details.
9. F ϭ [x 2, 0, z 2], S the surface of the box ƒ x ƒ Ϲ 1,
ƒ y ƒ Ϲ 3, 0 Ϲ z Ϲ 2
10. Solve Prob. 9 by direct integration.
11. F ϭ [ex, ey, ez], S the surface of the cube ƒ x ƒ Ϲ 1,
ƒ y ƒ Ϲ 1, ƒ z ƒ Ϲ 1
12. F ϭ [x 3 Ϫ y 3, y 3 Ϫ z 3, z 3 Ϫ x 3], S the surface of
x 2 ϩ y 2 ϩ z 2 Ϲ 25, z м 0
13. F ϭ [sin y, cos x, cos z], S, the surface of
x 2 ϩ y 2 Ϲ 4, ƒ z ƒ Ϲ 2 (a cylinder and two disks!)
14. F as in Prob. 13, S the surface of x 2 ϩ y 2 Ϲ 9,
0ϹzϹ2

c10-b.qxd

10/30/10

12:31 PM

458

Page 458

CHAP. 10 Vector Integral Calculus. Integral Theorems

15. F ϭ [2x 2, 12 y 2, sin pz], S the surface of the tetrahedron with vertices (0, 0, 0), (1, 0, 0), (0, 1, 0), (0, 0, 1)
16. F ϭ [cosh x, z, y], S as in Prob. 15
17. F ϭ [x 2, y 2, z 2], S the surface of the cone x 2 ϩ y 2 Ϲ z 2,
0ϹzϹh
18. F ϭ [xy, yz, zx], S the surface of the cone x 2 ϩ y 2
Ϲ 4z 2, 0 Ϲ z Ϲ 2
19–23

APPLICATION: MOMENT OF INERTIA

Given a mass of density 1 in a region T of space, find the
moment of intertia about the x-axis
Ix ϭ

The box Ϫa Ϲ x Ϲ a, Ϫb Ϲ y Ϲ b, Ϫc Ϲ z Ϲ c
The ball x 2 ϩ y 2 ϩ z 2 Ϲ a 2
The cylinder y 2 ϩ z 2 Ϲ a 2, 0 Ϲ x Ϲ h
The paraboloid y 2 ϩ z 2 Ϲ x, 0 Ϲ x Ϲ h
The cone y 2 ϩ z 2 Ϲ x 2, 0 Ϲ x Ϲ h
Why is Ix in Prob. 23 for large h larger than Ix in Prob.
22 (and the same h)? Why is it smaller for h ϭ 1? Give
physical reason.

19.
20.
21.
22.
23.
24.

ΎΎΎ (y

25. Show that for a solid of revolution, Ix ϭ

p

h

r
2 Ύ

4

(x) dx.

0

Solve Probs. 20–23 by this formula.
2

ϩ z 2) dx dy dz.

T

10.8

Further Applications
of the Divergence Theorem
The divergence theorem has many important applications: In fluid flow, it helps characterize
sources and sinks of fluids. In heat flow, it leads to the heat equation. In potential theory,
it gives properties of the solutions of Laplace’s equation. In this section, we assume that
the region T and its boundary surface S are such that the divergence theorem applies.

EXAMPLE 1

Fluid Flow. Physical Interpretation of the Divergence
From the divergence theorem we may obtain an intuitive interpretation of the divergence of a vector. For this
purpose we consider the flow of an incompressible fluid (see Sec. 9.8) of constant density r ϭ 1 which is steady,
that is, does not vary with time. Such a flow is determined by the field of its velocity vector v (P) at any point P.
Let S be the boundary surface of a region T in space, and let n be the outer unit normal vector of S. Then
v • n is the normal component of v in the direction of n, and ƒ v • n dA ƒ is the mass of fluid leaving T (if v • n Ͼ 0
at some P) or entering T (if v • n Ͻ 0 at P) per unit time at some point P of S through a small portion ¢S of
S of area ¢A. Hence the total mass of fluid that flows across S from T to the outside per unit time is given by
the surface integral

Ύ Ύ v • n dA.
S

Division by the volume V of T gives the average flow out of T:
(1)

1
V

Ύ Ύ v • n dA.
S

Since the flow is steady and the fluid is incompressible, the amount of fluid flowing outward must be continuously
supplied. Hence, if the value of the integral (1) is different from zero, there must be sources (positive sources
and negative sources, called sinks) in T, that is, points where fluid is produced or disappears.
If we let T shrink down to a fixed point P in T, we obtain from (1) the source intensity at P given by the
right side of (11) in the last section with F • n replaced by v • n, that is,

(2)

div v (P) ϭ lim

d (T ) : 0

1
V (T )

ΎΎ v • n dA.

S (T)

c10-b.qxd

10/30/10

12:31 PM

Page 459

SEC. 10.8 Further Applications of the Divergence Theorem

459

Hence the divergence of the velocity vector v of a steady incompressible flow is the source intensity of the flow
at the corresponding point.
There are no sources in T if and only if div v is zero everywhere in T. Then for any closed surface S in T we have

Ύ Ύ v • n dA ϭ 0.

᭿

S

EXAMPLE 2

Modeling of Heat Flow. Heat or Diffusion Equation
Physical experiments show that in a body, heat flows in the direction of decreasing temperature, and the rate of
flow is proportional to the gradient of the temperature. This means that the velocity v of the heat flow in a body
is of the form
v ϭ ϪK grad U

(3)

where U (x, y, z, t) is temperature, t is time, and K is called the thermal conductivity of the body; in ordinary
physical circumstances K is a constant. Using this information, set up the mathematical model of heat flow, the
so-called heat equation or diffusion equation.

Solution. Let T be a region in the body bounded by a surface S with outer unit normal vector n such that
the divergence theorem applies. Then v ؒ n is the component of v in the direction of n, and the amount of heat
leaving T per unit time is

Ύ Ύ v • n dA.
S

This expression is obtained similarly to the corresponding surface integral in the last example. Using
div (grad U) ϭ ٌ2U ϭ Uxx ϩ Uyy ϩ Uzz
(the Laplacian; see (3) in Sec. 9.8), we have by the divergence theorem and (3)

Ύ Ύ v • n dA ϭ ϪK Ύ Ύ Ύ div (grad U) dx dy dz
S

T

(4)
ϭ ϪK

Ύ Ύ Ύٌ

2

U dx dy dz.

T

On the other hand, the total amount of heat H in T is
Hϭ

Ύ Ύ Ύ srU dx dy dz
T

where the constant s is the specific heat of the material of the body and r is the density (ϭ mass per unit volume)
of the material. Hence the time rate of decrease of H is
Ϫ

0H
ϭϪ
0t

Ύ Ύ Ύ s r 0t dx dy dz
0U

T

and this must be equal to the above amount of heat leaving T. From (4) we thus have
Ϫ
or

Ύ Ύ Ύ s r 0t dx dy dz ϭ ϪK Ύ Ύ Ύ ٌ
0U

2

T

U dx dy dz

T

Ύ Ύ Ύ as r 0t Ϫ K ٌ Ub dx dy dz ϭ 0.
0U

T

2

c10-b.qxd

10/30/10

12:31 PM

460

Page 460

CHAP. 10 Vector Integral Calculus. Integral Theorems
Since this holds for any region T in the body, the integrand (if continuous) must be zero everywhere; that is,
0U
ϭ c2ٌ2U
0t

(5)

c2 ϭ

K
sr

where c2 is called the thermal diffusivity of the material. This partial differential equation is called the heat
equation. It is the fundamental equation for heat conduction. And our derivation is another impressive
demonstration of the great importance of the divergence theorem. Methods for solving heat problems will be
shown in Chap. 12.
The heat equation is also called the diffusion equation because it also models diffusion processes of motions
of molecules tending to level off differences in density or pressure in gases or liquids.
If heat flow does not depend on time, it is called steady-state heat flow. Then 0U> 0t ϭ 0, so that (5) reduces
to Laplace’s equation ٌ2U ϭ 0. We met this equation in Secs. 9.7 and 9.8, and we shall now see that the
᭿
divergence theorem adds basic insights into the nature of solutions of this equation.

Potential Theory. Harmonic Functions
The theory of solutions of Laplace’s equation

(6)

ٌ2f ϭ

0 2f
0x 2

ϩ

0 2f
0y 2

ϩ

0 2f
0z 2

ϭ0

is called potential theory. A solution of (6) with continuous second-order partial derivatives
is called a harmonic function. That continuity is needed for application of the divergence
theorem in potential theory, where the theorem plays a key role that we want to explore.
Further details of potential theory follow in Chaps. 12 and 18.
EXAMPLE 3

A Basic Property of Solutions of Laplace’s Equation
The integrands in the divergence theorem are div F and F • n (Sec. 10.7). If F is the gradient of a scalar function,
say, F ϭ grad f, then div F ϭ div (grad f ) ϭ ٌ2f; see (3), Sec. 9.8. Also, F • n ϭ n • F ϭ n • grad f. This is the
directional derivative of f in the outer normal direction of S, the boundary surface of the region T in the theorem.
This derivative is called the (outer) normal derivative of f and is denoted by 0f> 0n. Thus the formula in the
divergence theorem becomes

(7)

Ύ Ύ Ύٌ

Ύ Ύ 0n dA.
0f

2

f dV ϭ

T

S

This is the three-dimensional analog of (9) in Sec. 10.4. Because of the assumptions in the divergence theorem
this gives the following result.
᭿

THEOREM 1

A Basic Property of Harmonic Functions

Let f (x, y, z) be a harmonic function in some domain D is space. Let S be any
piecewise smooth closed orientable surface in D whose entire region it encloses
belongs to D. Then the integral of the normal derivative of f taken over S is zero.
(For “piecewise smooth” see Sec. 10.5.)

c10-b.qxd

10/30/10

12:31 PM

Page 461

SEC. 10.8 Further Applications of the Divergence Theorem
EXAMPLE 4

461

Green’s Theorems
Let f and g be scalar functions such that F ϭ f grad g satisfies the assumptions of the divergence theorem in
some region T. Then
div F ϭ div ( f grad g)
ϭ div a Bf
ϭa

0f 0g
0x 0x

ϩf

0 2g
0x

2b

ϩa

0f 0g
0y 0y

ϩf

0g

0g
,f

0g
,f

Rb

0x

0y

0z

0 2g

ϩa

0f 0g

0y

2b

0z 0z

ϩf

0 2g
0z 2

b

ϭ f ٌ2g ϩ grad f ؒ grad g.
Also, since f is a scalar function,
F•nϭn•F
ϭ n • ( f grad g)
ϭ (n • grad g) f.
Now n • grad g is the directional derivative 0g>0n of g in the outer normal direction of S. Hence the formula in
the divergence theorem becomes “Green’s first formula”

Ύ Ύ Ύ ( f ٌ g ϩ grad f • grad g) dV ϭ Ύ Ύ f 0n dA.
0g

(8)

2

T

S

Formula (8) together with the assumptions is known as the first form of Green’s theorem.
Interchanging f and g we obtain a similar formula. Subtracting this formula from (8) we find

(9)

Ύ Ύ Ύ ( f ٌ g Ϫ gٌ
2

2

Ύ Ύ af 0n Ϫ g 0n b dA.
0f

0g

f ) dV ϭ

T

S

This formula is called Green’s second formula or (together with the assumptions) the second form of Green’s
theorem.
᭿

EXAMPLE 5

Uniqueness of Solutions of Laplace’s Equation
Let f be harmonic in a domain D and let f be zero everywhere on a piecewise smooth closed orientable surface
S in D whose entire region T it encloses belongs to D. Then ٌ2g is zero in T, and the surface integral in (8) is
zero, so that (8) with g ϭ f gives

Ύ Ύ Ύ grad f • grad f dV ϭ Ύ Ύ Ύ ƒ grad f ƒ
T

2

dV ϭ 0.

T

Since f is harmonic, grad f and thus ƒ grad f ƒ are continuous in T and on S, and since ƒ grad f ƒ is nonnegative,
to make the integral over T zero, grad f must be the zero vector everywhere in T. Hence fx ϭ fy ϭ fz ϭ 0,
and f is constant in T and, because of continuity, it is equal to its value 0 on S. This proves the following
theorem.

c10-b.qxd

10/30/10

12:31 PM

462

Page 462

CHAP. 10 Vector Integral Calculus. Integral Theorems

THEOREM 2

Harmonic Functions

Let f (x, y, z) be harmonic in some domain D and zero at every point of a piecewise
smooth closed orientable surface S in D whose entire region T it encloses belongs
to D. Then f is identically zero in T.

This theorem has an important consequence. Let f1 and f2 be functions that satisfy the assumptions of Theorem
1 and take on the same values on S. Then their difference f1 Ϫ f2 satisfies those assumptions and has the value
0 everywhere on S. Hence, Theorem 2 implies that
f1 Ϫ f2 ϭ 0

throughout

T,

and we have the following fundamental result.

THEOREM 3

Uniqueness Theorem for Laplace’s Equation

Let T be a region that satisfies the assumptions of the divergence theorem, and let
f (x, y, z) be a harmonic function in a domain D that contains T and its boundary
surface S. Then f is uniquely determined in T by its values on S.

The problem of determining a solution u of a partial differential equation in a region T such that u assumes
given values on the boundary surface S of T is called the Dirichlet problem.8 We may thus reformulate Theorem
3 as follows.

THEOREM 3*

Uniqueness Theorem for the Dirichlet Problem

If the assumptions in Theorem 3 are satisfied and the Dirichlet problem for the
Laplace equation has a solution in T, then this solution is unique.

These theorems demonstrate the extreme importance of the divergence theorem in potential theory.

᭿

PROBLEM SET 10.8
1–6

VERIFICATIONS

1. Harmonic functions. Verify Theorem 1 for f ϭ 2z Ϫ
x 2 Ϫ y 2 and S the surface of the box 0 Ϲ x Ϲ a,
0 Ϲ y Ϲ b, 0 Ϲ z Ϲ c.
2. Harmonic functions. Verify Theorem 1 for f ϭ
x 2 Ϫ y 2 and the surface of the cylinder x2 ϩ y2 ϭ 4,
0 Ϲ z Ϲ h.
2

8

3. Green’s first identity. Verify (8) for f ϭ 4y 2, g ϭ x 2,
S the surface of the “unit cube” 0 Ϲ x Ϲ 1,
0 Ϲ y Ϲ 1, 0 Ϲ z Ϲ 1. What are the assumptions on
f and g in (8)? Must f and g be harmonic?
4. Green’s first identity. Verify (8) for f ϭ x,
g ϭ y 2 ϩ z 2, S the surface of the box 0 Ϲ x Ϲ 1,
0 Ϲ y Ϲ 2, 0 Ϲ z Ϲ 3.

PETER GUSTAV LEJEUNE DIRICHLET (1805–1859), German mathematician, studied in Paris under
Cauchy and others and succeeded Gauss at Göttingen in 1855. He became known by his important research on
Fourier series (he knew Fourier personally) and in number theory.

c10-b.qxd

10/30/10

12:31 PM

Page 463

SEC. 10.8 Stokes’s Theorem
5. Green’s
g ϭ 2x2,
6. Green’s
g ϭ y4,
7–11

463

second identity. Verify (9) for f ϭ 6y2,
S the unit cube in Prob. 3.
second identity. Verify (9) for f ϭ x2,
S the unit cube in Prob. 3.

VOLUME

Use the divergence theorem, assuming that the assumptions
on T and S are satisfied.
7. Show that a region T with boundary surface S has the
volume
Vϭ

Ύ Ύ x dy dz ϭ Ύ Ύ y dz dx ϭ Ύ Ύ z dx dy
S

ϭ

1
3

S

S

8. Cone. Using the third expression for v in Prob. 7,
verify V ϭ pa 2 h>3 for the volume of a circular cone
of height h and radius of base a.
9. Ball. Find the volume under a hemisphere of radius a
from in Prob. 7.
10. Volume. Show that a region T with boundary surface
S has the volume

dV.

T

Ύ Ύ af 0n Ϫ g 0n b dA ϭ 0.
0f

0g

(c)

S

(d) If 0f>0n ϭ 0g> 0n on S, then f ϭ g ϩ c in T, where
c is a constant.
(e) The Laplacian can be represented independently
of coordinate systems in the form

Ύ Ύ r cos ␾ dA

ٌ2f ϭ lim

d (T ): 0

1
V (T )

ΎΎ 0n0f dA

S (T)

S

where r is the distance of a variable point P: (x, y, z)
on S from the origin O and ␾ is the angle between
the directed line OP and the outer normal of S at P.

10.9

2

(b) If 0g>0n ϭ 0 on S, then g is constant in T.

S

1
3

Ύ Ύ g 0n dA ϭ Ύ Ύ Ύ ƒ grad g ƒ
0g

(a)

S

Ύ Ύ (x dy dz ϩ y dz dx ϩ z dx dy).

Vϭ

Make a sketch. Hint. Use (2) in Sec. 10.7 with
F ϭ [x, y, z].
11. Ball. Find the volume of a ball of radius a from
Prob. 10.
12. TEAM PROJECT. Divergence Theorem and Potential Theory. The importance of the divergence theorem in potential theory is obvious from (7)–(9) and
Theorems 1–3. To emphasize it further, consider
functions f and g that are harmonic in some domain D
containing a region T with boundary surface S such that
T satisfies the assumptions in the divergence theorem.
Prove, and illustrate by examples, that then:

where d (T ) is the maximum distance of the points of a
region T bounded by S (T ) from the point at which the
Laplacian is evaluated and V (T ) is the volume of T.

Stokes’s Theorem
Let us review some of the material covered so far. Double integrals over a region in the plane
can be transformed into line integrals over the boundary curve of that region and conversely,
line integrals into double integrals. This important result is known as Green’s theorem in the
plane and was explained in Sec. 10.4. We also learned that we can transform triple integrals
into surface integrals and vice versa, that is, surface integrals into triple integrals. This “big”
theorem is called Gauss’s divergence theorem and was shown in Sec. 10.7.
To complete our discussion on transforming integrals, we now introduce another “big”
theorem that allows us to transform surface integrals into line integrals and conversely,
line integrals into surface integrals. It is called Stokes’s Theorem, and it generalizes
Green’s theorem in the plane (see Example 2 below for this immediate observation). Recall
from Sec. 9.9 that
i
(1)

curl F ϭ 4 0>0x
F1

which we will need immediately.

j

k

0>0y

0>0z 4

F2

F3

c10-b.qxd

10/30/10

12:31 PM

464

THEOREM 1

Page 464

CHAP. 10 Vector Integral Calculus. Integral Theorems

Stokes’s Theorem9
(Transformation Between Surface and Line Integrals)

Let S be a piecewise smooth9 oriented surface in space and let the boundary of S
be a piecewise smooth simple closed curve C. Let F (x, y, z) be a continuous vector
function that has continuous first partial derivatives in a domain in space containing
S. Then

ΎΎ (curl F) • n dA ϭ Ώ F • rr (s) ds.

(2)

C

S

Here n is a unit normal vector of S and, depending on n, the integration around C
is taken in the sense shown in Fig. 254. Furthermore, r r ϭ dr>ds is the unit tangent
vector and s the arc length of C.
In components, formula (2) becomes

ΎΎ c a 0y

0F3

Ϫ

0F2
0z

b N1 ϩ a

0F1
0z

Ϫ

0F3
0x

b N2 ϩ a

0F2
0x

Ϫ

R

(2*)

ϭ

Ώ (F

1

0F1
0y

b N3 d du dv

dx ϩ F2 dy ϩ F3 dz).

C

Here, F ϭ [F1, F2, F3], N ϭ [N1, N2, N3], n dA ϭ N du dv, r r ds ϭ
[dx, dy, dz], and R is the region with boundary curve C in the uv-plane
corresponding to S represented by r (u, v).

The proof follows after Example 1.
z

n
N

S
C

C

S

r'

Fig. 254. Stokes’s theorem

EXAMPLE 1

x

n

r'

y

Fig. 255. Surface S in Example 1

Verification of Stokes’s Theorem
Before we prove Stokes’s theorem, let us first get used to it by verifying it for F ϭ [ y, z, x] and S the paraboloid
(Fig. 255)
z ϭ f (x, y) ϭ 1 Ϫ (x 2 ϩ y 2),

z м 0.

9
Sir GEORGE GABRIEL STOKES (1819–1903), Irish mathematician and physicist who became a professor
in Cambridge in 1849. He is also known for his important contribution to the theory of infinite series and to
viscous flow (Navier–Stokes equations), geodesy, and optics.
“Piecewise smooth” curves and surfaces are defined in Secs. 10.1 and 10.5.

c10-b.qxd

10/30/10

12:31 PM

Page 465

SEC. 10.9 Stokes’s Theorem

465

Solution. The curve C, oriented as in Fig. 255, is the circle r (s) ϭ [cos s, sin s, 0]. Its unit tangent vector
is r r (s) ϭ [Ϫsin s, cos s, 0]. The function F ϭ [y, z, x] on C is F (r (s)) ϭ [sin s, 0, cos s]. Hence

Ώ F • dr ϭ Ύ
C

2p

F (r (s)) • r r (s) ds ϭ

0

Ύ

2p

[(sin s)(Ϫsin s) ϩ 0 ϩ 0] ds ϭ Ϫp.

0

We now consider the surface integral. We have F1 ϭ y, F2 ϭ z, F3 ϭ x, so that in (2*) we obtain
curl F ϭ curl [F1,

F2,

F3] ϭ curl [ y,

z, x] ϭ [Ϫ1, Ϫ1, Ϫ1].

A normal vector of S is N ϭ grad (z Ϫ f (x, y)) ϭ [2x, 2y, 1]. Hence (curl F) • N ϭ Ϫ2x Ϫ 2y Ϫ 1. Now
n dA ϭ N dx dy (see (3*) in Sec. 10.6 with x, y instead of u, v). Using polar coordinates r, u defined by
x ϭ r cos u, y ϭ r sin u and denoting the projection of S into the xy-plane by R, we thus obtain

ΎΎ (curl F) • n dA ϭ ΎΎ (curl F) • N dx dy ϭ ΎΎ (Ϫ2x Ϫ 2y Ϫ 1) dx dy
S

R

R

2p

ϭ

Ύ Ύ

1

(Ϫ2r (cos u ϩ sin u) Ϫ 1)r dr du

uϭ0 rϭ0

ϭ

Ύ

2p

uϭ0

PROOF

aϪ

2
3

(cos u ϩ sin u) Ϫ

1
2

b du ϭ 0 ϩ 0 Ϫ

1
2

(2p) ϭ Ϫp.

᭿

We prove Stokes’s theorem. Obviously, (2) holds if the integrals of each component on
both sides of (2*) are equal; that is,

(3)

ΎΎ a 0z

0F1

0F1

N2 Ϫ

0y

Ώ F dx

N3 b du dv ϭ

1

C

R

(4)

ΎΎ aϪ 0z

0F2

N1 ϩ

0F2
0x

N3 b du dv ϭ

R

(5)

ΎΎ a 0y N
0F3

1

Ϫ

0F3
0x

N2 b du dv ϭ

R

Ώ F dy
2

C

Ώ F dz.
3

C

We prove this first for a surface S that can be represented simultaneously in the forms
(6)

(a) z ϭ f (x, y),

(b)

y ϭ g (x, z),

(c)

x ϭ h ( y, z).

We prove (3), using (6a). Setting u ϭ x, v ϭ y, we have from (6a)
r (u, v) ϭ r (x, y) ϭ [x, y, f (x, y)] ϭ xi ϩ yj ϩ f k
and in (2), Sec. 10.6, by direct calculation
N ϭ ru ؋ rv ϭ rx ؋ ry ϭ [Ϫfx,

Ϫfy,

1] ϭ Ϫfx i Ϫ fy j ϩ k.

c10-b.qxd

10/30/10

12:31 PM

466

Page 466

CHAP. 10 Vector Integral Calculus. Integral Theorems

Note that N is an upper normal vector of S, since it has a positive z-component. Also,
R ϭ S*, the projection of S into the xy-plane, with boundary curve C ϭ C* (Fig. 256).
Hence the left side of (3) is

ΎΎ c

(7)

0F1
0z

(Ϫfy) Ϫ

d dx dy.

0F1
0y

S*

We now consider the right side of (3). We transform this line integral over C ϭ C* into
a double integral over S* by applying Green’s theorem [formula (1) in Sec. 10.4 with
F2 ϭ 0]. This gives

ΏF

1

dx ϭ

C*

ΎΎϪ 0y dx dy.
0F1

S*

γ

z

n

S

y
S*

x

C*

Fig. 256. Proof of Stokes’s theorem

Here, F1 ϭ F1 (x, y, f (x, y)). Hence by the chain rule (see also Prob. 10 in Problem Set 9.6),
Ϫ

0F1(x, y, f (x, y))
0y

ϭϪ

0F1(x, y, z)
0y

Ϫ

0F1(x, y, z) 0f
0z

0y

[z ϭ f (x, y)].

We see that the right side of this equals the integrand in (7). This proves (3). Relations
(4) and (5) follow in the same way if we use (6b) and (6c), respectively. By addition we
obtain (2*). This proves Stokes’s theorem for a surface S that can be represented
simultaneously in the forms (6a), (6b), (6c).
As in the proof of the divergence theorem, our result may be immediately extended
to a surface S that can be decomposed into finitely many pieces, each of which is of
the kind just considered. This covers most of the cases of practical interest. The proof
in the case of a most general surface S satisfying the assumptions of the theorem would
require a limit process; this is similar to the situation in the case of Green’s theorem
in Sec. 10.4.
᭿
EXAMPLE 2

Green’s Theorem in the Plane as a Special Case of Stokes’s Theorem
Let F ϭ [F1, F2] ϭ F1 i ϩ F2 j be a vector function that is continuously differentiable in a domain in the
xy-plane containing a simply connected bounded closed region S whose boundary C is a piecewise smooth
simple closed curve. Then, according to (1),
(curl F) • n ϭ (curl F) • k ϭ

0F2
0x

Ϫ

0F1
0y

.

c10-b.qxd

10/30/10

12:31 PM

Page 467

SEC. 10.9 Stokes’s Theorem

467

Hence the formula in Stokes’s theorem now takes the form

ΎΎa 0x

0F2

Ϫ

0F1
0y

Ώ (F

b dA ϭ

1

dx ϩ F2 dy).

C

S

This shows that Green’s theorem in the plane (Sec. 10.4) is a special case of Stokes’s theorem (which we needed
in the proof of the latter!).
᭿

EXAMPLE 3

Evaluation of a Line Integral by Stokes’s Theorem
Evaluate ͐C F • r r ds, where C is the circle x 2 ϩ y 2 ϭ 4, z ϭ Ϫ3, oriented counterclockwise as seen by a person
standing at the origin, and, with respect to right-handed Cartesian coordinates,
F ϭ [ y,

xz 3,

Ϫzy 3] ϭ yi ϩ xz 3j Ϫ zy 3k.

Solution. As a surface S bounded by C we can take the plane circular disk x 2 ϩ y 2 Ϲ 4 in the plane z ϭ Ϫ3.
Then n in Stokes’s theorem points in the positive z-direction; thus n ϭ k. Hence (curl F) • n is simply the component
of curl F in the positive z-direction. Since F with z ϭ Ϫ3 has the components F1 ϭ y, F2 ϭ Ϫ27x, F3 ϭ 3y 3, we
thus obtain
(curl F) • n ϭ

0F2
0x

0F1

Ϫ

ϭ Ϫ27 Ϫ 1 ϭ Ϫ28.

0y

Hence the integral over S in Stokes’s theorem equals Ϫ28 times the area 4p of the disk S. This yields the answer
Ϫ28 ؒ 4p ϭ Ϫ112p Ϸ Ϫ352. Confirm this by direct calculation, which involves somewhat more work. ᭿

EXAMPLE 4

Physical Meaning of the Curl in Fluid Motion. Circulation
Let Sr0 be a circular disk of radius r0 and center P bounded by the circle Cr0 (Fig. 257), and let F (Q) ϵ F (x, y, z)
be a continuously differentiable vector function in a domain containing Sr0. Then by Stokes’s theorem and the
mean value theorem for surface integrals (see Sec. 10.6),

n
P

Ώ F • rrds ϭ ΎΎ(curl F) • n dA ϭ (curl F) • n(P*)A

Cr

r0

0

Fig. 257.
Example 4

Cr0

Sr0

where Ar0 is the area of Sr0 and P* is a suitable point of Sr0. This may be written in the form
(curl F) • n(P*) ϭ

1
Ar0

Ώ F • rrds.
Cr0

In the case of a fluid motion with velocity vector F ϭ v, the integral

Ώ v • rrds
Cr0

is called the circulation of the flow around Cr0. It measures the extent to which the corresponding fluid motion
is a rotation around the circle Cr0. If we now let r0 approach zero, we find
(8)

(curl v) • n(P) ϭ lim

r0 : 0

1
Ar0

Ώ v • rr ds;
Cr0

that is, the component of the curl in the positive normal direction can be regarded as the specific circulation
(circulation per unit area) of the flow in the surface at the corresponding point.
᭿

EXAMPLE 5

Work Done in the Displacement around a Closed Curve
Find the work done by the force F ϭ 2xy 3 sin z i ϩ 3x 2y 2 sin z j ϩ x 2y 3 cos z k in the displacement around the
curve of intersection of the paraboloid z ϭ x 2 ϩ y 2 and the cylinder (x Ϫ 1)2 ϩ y 2 ϭ 1.
This work is given by the line integral in Stokes’s theorem. Now F ϭ grad f, where f ϭ x 2y 3 sin z
and curl (grad f ) ϭ 0 (see (2) in Sec. 9.9), so that (curl F) • n ϭ 0 and the work is 0 by Stokes’s theorem. This
agrees with the fact that the present field is conservative (definition in Sec. 9.7).
᭿

Solution.

c10-b.qxd

10/30/10

12:31 PM

468

Page 468

CHAP. 10 Vector Integral Calculus. Integral Theorems

Stokes’s Theorem Applied to Path Independence
We emphasized in Sec. 10.2 that the value of a line integral generally depends not only
on the function to be integrated and on the two endpoints A and B of the path of integration
C, but also on the particular choice of a path from A to B. In Theorem 3 of Sec. 10.2 we
proved that if a line integral

Ύ F (r) • dr ϭ Ύ (F

(9)

1

C

dx ϩ F2 dy ϩ F3 dz)

C

(involving continuous F1, F2, F3 that have continuous first partial derivatives) is path
independent in a domain D, then curl F ϭ 0 in D. And we claimed in Sec. 10.2 that,
conversely, curl F ϭ 0 everywhere in D implies path independence of (9) in D provided D
is simply connected. A proof of this needs Stokes’s theorem and can now be given as follows.
Let C be any closed path in D. Since D is simply connected, we can find a surface S
in D bounded by C. Stokes’s theorem applies and gives

Ώ (F

1

dx ϩ F2 dy ϩ F3 dz) ϭ

C

Ώ F • rrds ϭ ΎΎ(curl F) • n dA
C

S

for proper direction on C and normal vector n on S. Since curl F ϭ 0 in D, the surface
integral and hence the line integral are zero. This and Theorem 2 of Sec. 10.2 imply that
the integral (9) is path independent in D. This completes the proof.
᭿

PROBLEM SET 10.9
1–10

DIRECT INTEGRATION OF SURFACE
INTEGRALS

Evaluate the surface integral
the given F and S.

ΎΎ (curl F) • n dA directly for
S

1. F ϭ [z , Ϫx , 0], S the rectangle with vertices (0, 0, 0),
(1, 0, 0), (0, 4, 4), (1, 4, 4)
2. F ϭ [Ϫ13 sin y, 3 sinh z, x], S the rectangle with vertices
(0, 0, 2), (4, 0, 2), (4, p>2, 2), (0, p>2, 2)
3. F ϭ [e؊z, e؊z cos y, e؊z sin y], S: z ϭ y 2>2,
Ϫ1 Ϲ x Ϲ 1, 0 Ϲ y Ϲ 1
4. F as in Prob. 1, z ϭ xy (0 Ϲ x Ϲ 1, 0 Ϲ y Ϲ 4).
Compare with Prob. 1.
2

2

5. F ϭ [z 2, 32 x, 0],
zϭ1

S: 0 Ϲ x Ϲ a, 0 Ϲ y Ϲ a,

6. F ϭ [y 3, Ϫx 3, 0], S: x 2 ϩ y 2 Ϲ 1, z ϭ 0
7. F ϭ [ey, ez, ex], S: z ϭ x 2 (0 Ϲ x Ϲ 2,
0 Ϲ y Ϲ 1)
8. F ϭ [z 2, x 2, y 2], S: z ϭ 2x 2 ϩ y 2,
y м 0, 0 Ϲ z Ϲ h
9. Verify Stokes’s theorem for F and S in Prob. 5.
10. Verify Stokes’s theorem for F and S in Prob. 6.

11. Stokes’s theorem not applicable. Evaluate

Ώ F • rrds,
C

F ϭ (x 2 ϩ y 2)؊1[Ϫy, x], C: x 2 ϩ y 2 ϭ 1, z ϭ 0, oriented clockwise. Why can Stokes’s theorem not be
applied? What (false) result would it give?
12. WRITING PROJECT. Grad, Div, Curl in
Connection with Integrals. Make a list of ideas and
results on this topic in this chapter. See whether you
can rearrange or combine parts of your material. Then
subdivide the material into 3–5 portions and work out
the details of each portion. Include no proofs but simple
typical examples of your own that lead to a better
understanding of the material.

13–20

EVALUATION OF

Ώ F • rr ds
C

Calculate this line integral by Stokes’s theorem for the
given F and C. Assume the Cartesian coordinates to be
right-handed and the z-component of the surface normal to
be nonnegative.
13. F ϭ [Ϫ5y, 4x, z], C the circle x 2 ϩ y 2 ϭ 16, z ϭ 4

c10-b.qxd

10/30/10

12:31 PM

Page 469

Chapter 10 Review Questions and Problems
14. F ϭ [z 3, x 3, y 3], C the circle x ϭ 2, y 2 ϩ z 2 ϭ 9
15. F ϭ [ y 2, x 2, z ϩ x] around the triangle with vertices
(0, 0, 0), (1, 0, 0), (1, 1, 0)
16. F ϭ [ey, 0, ex], C as in Prob. 15
17. F ϭ [0, z 3, 0], C the boundary curve of the cylinder
x 2 ϩ y 2 ϭ 1, x м 0, y м 0, 0 Ϲ z Ϲ 1

469
18. F ϭ [Ϫy, 2z, 0], C the boundary curve of y 2 ϩ z 2 ϭ 4,
z м 0, 0 Ϲ x Ϲ h
19. F ϭ [z, ez, 0], C the boundary curve of the portion of
the cone z ϭ 2x 2 ϩ y 2, x м 0, y м 0, 0 Ϲ z Ϲ 1
20. F ϭ [0, cos x, 0], C the boundary curve of y 2 ϩ z 2 ϭ 4,
y м 0, z м 0, 0 Ϲ x Ϲ p

CHAPTER 10 REVIEW QUESTIONS AND PROBLEMS
1. State from memory how to evaluate a line integral.
A surface integral.
2. What is path independence of a line integral? What is
its physical meaning and importance?
3. What line integrals can be converted to surface
integrals and conversely? How?
4. What surface integrals can be converted to volume
integrals? How?
5. What role did the gradient play in this chapter? The
curl? State the definitions from memory.
6. What are typical applications of line integrals? Of
surface integrals?
7. Where did orientation of a surface play a role? Explain.
8. State the divergence theorem from memory. Give
applications.
9. In some line and surface integrals we started from
vector functions, but integrands were scalar functions.
How and why did we proceed in this way?
10. State Laplace’s equation. Explain its physical importance. Summarize our discussion on harmonic functions.

11–20

LINE INTEGRALS (WORK INTEGRALS)

Evaluate

Ύ F (r) • dr for given F and C by the method that

17. F ϭ [9z, 5x, 3y], C the ellipse x 2 ϩ y 2 ϭ 9,
zϭxϩ2
18. F ϭ [sin py, cos px, sin px], C the boundary curve of
0 Ϲ x Ϲ 1, 0 Ϲ y Ϲ 2, z ϭ x
19. F ϭ [z, 2y, x], C the helix r ϭ [cos t, sin t, t] from
(1, 0, 0) to (1, 0, 2p)
20. F ϭ [zexz, 2 sinh 2y, xexz], C the parabola y ϭ x,
z ϭ x 2, Ϫ1 Ϲ x Ϲ 1

21–25

DOUBLE INTEGRALS,
CENTER OF GRAVITY

Find the coordinates x, y of the center of gravity of a
mass of density f (x, y) in the region R. Sketch R, show
details.
21. f ϭ xy, R the triangle with vertices (0, 0), (2, 0), (2, 2)
22. f ϭ x 2 ϩ y 2, R: x 2 ϩ y 2 Ϲ a 2, y м 0
23. f ϭ x 2, R: Ϫ1 Ϲ x Ϲ 2, x 2 Ϲ y Ϲ x ϩ 2. Why is
x Ͼ 0?
24. f ϭ 1, R: 0 Ϲ y Ϲ 1 Ϫ x 4
25. f ϭ ky, k Ͼ 0, arbitrary, 0 Ϲ y Ϲ 1 Ϫ x 2,
0ϹxϹ1
26. Why are x and y in Prob. 25 independent of k?

C

seems most suitable. Remember that if F is a force, the
integral gives the work done in the displacement along C.
Show details.
11. F ϭ [2x 2, Ϫ4y 2], C the straight-line segment from
(4, 2) to (Ϫ6, 10)
12. F ϭ [ y cos xy, x cos xy, ez], C the straight-line segment
from (p, 1, 0) to (12 , p, 1)
13. F ϭ [ y 2, 2xy ϩ 5 sin x, 0], C the boundary of
0 Ϲ x Ϲ p>2, 0 Ϲ y Ϲ 2, z ϭ 0
14. F ϭ [Ϫy 3, x 3 ϩ e؊y, 0], C the circle x 2 ϩ y 2 ϭ 25,
zϭ2
15. F ϭ [x 3, e2y, e؊4z], C: x 2 ϩ 9y 2 ϭ 9, z ϭ x 2
16. F ϭ [x 2, y 2, y 2x], C the helix r ϭ [2 cos t, 2 sin t, 3t]
from (2, 0, 0) to (Ϫ2, 0, 3p)

27–35

SURFACE INTEGRALS

ΎΎF • n dA.
S

DIVERGENCE THEOREM
Evaluate the integral diectly or, if possible, by the divergence
theorem. Show details.
27. F ϭ [ax, by, cz], S the sphere x 2 ϩ y 2 ϩ z 2 ϭ 36
28. F ϭ [x ϩ y 2, y ϩ z 2, z ϩ x 2], S the ellipsoid with
semi-axes of lengths a, b, c
29. F ϭ [y ϩ z, 20y, 2z 3], S the surface of 0 Ϲ x Ϲ 2,
0 Ϲ y Ϲ 1, 0 Ϲ z Ϲ y
30. F ϭ [1, 1, 1], S: x 2 ϩ y 2 ϩ 4z 2 ϭ 4, z м 0
31. F ϭ [ex, ey, ez], S the surface of the box ƒ x ƒ Ϲ 1,
ƒ y ƒ Ϲ 1, ƒ z ƒ Ϲ 1

c10-b.qxd

10/30/10

470

12:31 PM

Page 470

CHAP. 10 Vector Integral Calculus. Integral Theorems
34. F ϭ [x, xy, z], S the boundary of x 2 ϩ y 2 Ϲ 1,
0ϹzϹ5
35. F ϭ [x ϩ z, y ϩ z, x ϩ y], S the sphere of radius 3
with center 0

32. F ϭ [y 2, x 2, z 2], S the portion of the paraboloid
z ϭ x 2 ϩ y 2, z Ϲ 9
33. F ϭ [y 2, x 2, z 2], S: r ϭ [u, u 2, v], 0 Ϲ u Ϲ 2,
Ϫ2 Ϲ v Ϲ 2

SUMMARY OF CHAPTER

10

Vector Integral Calculus. Integral Theorems
Chapter 9 extended differential calculus to vectors, that is, to vector functions v (x, y, z)
or v (t). Similarly, Chapter 10 extends integral calculus to vector functions. This
involves line integrals (Sec. 10.1), double integrals (Sec. 10.3), surface integrals (Sec.
10.6), and triple integrals (Sec. 10.7) and the three “big” theorems for transforming
these integrals into one another, the theorems of Green (Sec. 10.4), Gauss (Sec. 10.7),
and Stokes (Sec. 10.9).
The analog of the definite integral of calculus is the line integral (Sec. 10.1)
(1)

Ύ F (r) • dr ϭ Ύ (F

1

C

b

dx ϩ F2 dy ϩ F3 dz) ϭ

C

Ύ F (r (t)) • drdt dt
a

where C : r (t) ϭ [x (t), y (t), z (t)] ϭ x (t)i ϩ y (t)j ϩ z (t)k (a Ϲ t Ϲ b) is a curve in
space (or in the plane). Physically, (1) may represent the work done by a (variable)
force in a displacement. Other kinds of line integrals and their applications are also
discussed in Sec. 10.1.
Independence of path of a line integral in a domain D means that the integral
of a given function over any path C with endpoints P and Q has the same value for
all paths from P to Q that lie in D; here P and Q are fixed. An integral (1) is
independent of path in D if and only if the differential form F1 dx ϩ F2 dy ϩ F3 dz
with continuous F1, F2, F3 is exact in D (Sec. 10.2). Also, if curl F ϭ 0, where
F ϭ [F1, F2, F3], has continuous first partial derivatives in a simply connected
domain D, then the integral (1) is independent of path in D (Sec. 10.2).
Integral Theorems. The formula of Green’s theorem in the plane (Sec. 10.4)
(2)

ΎΎ a 0x

0F2

Ϫ

0F1
0y

b dx dy ϭ

R

Ώ (F

1 dx

ϩ F2 dy)

C

transforms double integrals over a region R in the xy-plane into line integrals over
the boundary curve C of R and conversely. For other forms of (2) see Sec. 10.4.
Similarly, the formula of the divergence theorem of Gauss (Sec. 10.7)
(3)

ΎΎΎdiv F dV ϭ ΎΎF • n dA
T

S

c10-b.qxd

10/30/10

12:31 PM

Page 471

Summary of Chapter 10

471

transforms triple integrals over a region T in space into surface integrals over the
boundary surface S of T, and conversely. Formula (3) implies Green’s formulas

ΎΎΎ( f ٌ g ϩ ٌ f • ٌg) dV ϭ ΎΎ f 0n dA,
0g

2

(4)

T

(5)

S

ΎΎΎ( f ٌ g Ϫ gٌ
2

T

2

f) dV ϭ

ΎΎ af 0n Ϫ g 0n b dA.
0g

0f

S

Finally, the formula of Stokes’s theorem (Sec. 10.9)
(6)

ΎΎ(curl F) • n dA ϭ Ώ F • rr (s) ds
S

C

transforms surface integrals over a surface S into line integrals over the boundary
curve C of S and conversely.

c10-b.qxd

10/30/10

12:31 PM

Page 472

c11-a.qxd

10/30/10

1:24 PM

Page 473

PART

C

Fourier Analysis.
Partial
Differential
Equations (PDEs)
C H A P T E R 1 1 Fourier Analysis
C H A P T E R 1 2 Partial Differential Equations (PDEs)
Chapter 11 and Chapter 12 are directly related to each other in that Fourier analysis has
its most important applications in modeling and solving partial differential equations
(PDEs) related to boundary and initial value problems of mechanics, heat flow,
electrostatics, and other fields. However, the study of PDEs is a study in its own right.
Indeed, PDEs are the subject of much ongoing research.
Fourier analysis allows us to model periodic phenomena which appear frequently in
engineering and elsewhere—think of rotating parts of machines, alternating electric currents
or the motion of planets. Related period functions may be complicated. Now, the ingeneous
idea of Fourier analysis is to represent complicated functions in terms of simple periodic
functions, namely cosines and sines. The representations will be infinite series called
Fourier series.1 This idea can be generalized to more general series (see Sec. 11.5) and
to integral representations (see Sec. 11.7).
The discovery of Fourier series had a huge impetus on applied mathematics as well as on
mathematics as a whole. Indeed, its influence on the concept of a function, on integration
theory, on convergence theory, and other theories of mathematics has been substantial
(see [GenRef7] in App. 1).
Chapter 12 deals with the most important partial differential equations (PDEs) of physics
and engineering, such as the wave equation, the heat equation, and the Laplace equation.
These equations can model a vibrating string/membrane, temperatures on a bar, and
electrostatic potentials, respectively. PDEs are very important in many areas of physics
and engineering and have many more applications than ODEs.
1

JEAN-BAPTISTE JOSEPH FOURIER (1768–1830), French physicist and mathematician, lived and taught
in Paris, accompanied Napoléon in the Egyptian War, and was later made prefect of Grenoble. The beginnings
on Fourier series can be found in works by Euler and by Daniel Bernoulli, but it was Fourier who employed
them in a systematic and general manner in his main work, Théorie analytique de la chaleur (Analytic Theory
of Heat, Paris, 1822), in which he developed the theory of heat conduction (heat equation; see Sec. 12.5), making
these series a most important tool in applied mathematics.

473

c11-a.qxd

10/30/10

1:24 PM

Page 474

CHAPTER

11

Fourier Analysis
This chapter on Fourier analysis covers three broad areas: Fourier series in Secs. 11.1–11.4,
more general orthonormal series called Sturm–Liouville expansions in Secs. 11.5 and 11.6
and Fourier integrals and transforms in Secs. 11.7–11.9.
The central starting point of Fourier analysis is Fourier series. They are infinite series
designed to represent general periodic functions in terms of simple ones, namely, cosines
and sines. This trigonometric system is orthogonal, allowing the computation of the
coefficients of the Fourier series by use of the well-known Euler formulas, as shown in
Sec. 11.1. Fourier series are very important to the engineer and physicist because they
allow the solution of ODEs in connection with forced oscillations (Sec. 11.3) and the
approximation of periodic functions (Sec. 11.4). Moreover, applications of Fourier analysis
to PDEs are given in Chap. 12. Fourier series are, in a certain sense, more universal than
the familiar Taylor series in calculus because many discontinuous periodic functions that
come up in applications can be developed in Fourier series but do not have Taylor series
expansions.
The underlying idea of the Fourier series can be extended in two important ways. We
can replace the trigonometric system by other families of orthogonal functions, e.g., Bessel
functions and obtain the Sturm–Liouville expansions. Note that related Secs. 11.5 and
11.6 used to be part of Chap. 5 but, for greater readability and logical coherence, are now
part of Chap. 11. The second expansion is applying Fourier series to nonperiodic
phenomena and obtaining Fourier integrals and Fourier transforms. Both extensions have
important applications to solving PDEs as will be shown in Chap. 12.
In a digital age, the discrete Fourier transform plays an important role. Signals, such
as voice or music, are sampled and analyzed for frequencies. An important algorithm, in
this context, is the fast Fourier transform. This is discussed in Sec. 11.9.
Note that the two extensions of Fourier series are independent of each other and may
be studied in the order suggested in this chapter or by studying Fourier integrals and
transforms first and then Sturm–Liouville expansions.
Prerequisite: Elementary integral calculus (needed for Fourier coefficients).
Sections that may be omitted in a shorter course: 11.4–11.9.
References and Answers to Problems: App. 1 Part C, App. 2.

11.1

Fourier Series
Fourier series are infinite series that represent periodic functions in terms of cosines and
sines. As such, Fourier series are of greatest importance to the engineer and applied
mathematician. To define Fourier series, we first need some background material.
A function f (x) is called a periodic function if f ( x) is defined for all real x, except

474

c11-a.qxd

10/30/10

1:24 PM

Page 475

SEC. 11.1 Fourier Series

475
f (x)

x

p

Fig. 258. Periodic function of period p

possibly at some points, and if there is some positive number p, called a period of f (x),
such that
f (x ϩ p) ϭ f (x)

(1)

for all x.

(The function f (x) ϭ tan x is a periodic function that is not defined for all real x but
undefined for some points (more precisely, countably many points), that is x ϭ Ϯp>2,
Ϯ3p>2, Á .)
The graph of a periodic function has the characteristic that it can be obtained by periodic
repetition of its graph in any interval of length p (Fig. 258).
The smallest positive period is often called the fundamental period. (See Probs. 2–4.)
Familiar periodic functions are the cosine, sine, tangent, and cotangent. Examples of
functions that are not periodic are x, x 2, x 3, ex, cosh x, and ln x, to mention just a few.
If f (x) has period p, it also has the period 2p because (1) implies f (x ϩ 2p) ϭ
f ([x ϩ p] ϩ p) ϭ f (x ϩ p) ϭ f (x), etc.; thus for any integer n ϭ 1, 2, 3, Á ,
f (x ϩ np) ϭ f (x)

(2)

for all x.

Furthermore if f (x) and g (x) have period p, then af (x) ϩ bg (x) with any constants a and
b also has the period p.
Our problem in the first few sections of this chapter will be the representation of various
functions f (x) of period 2p in terms of the simple functions
(3)

1,

cos x,

sin x,

cos 2x,

sin 2x, Á ,

cos nx,

sin nx, Á .

All these functions have the period 2p. They form the so-called trigonometric system.
Figure 259 shows the first few of them (except for the constant 1, which is periodic with
any period).

0

π

2π
π

0

cos x

0

π

sin x

π

2π
π

0

cos 2x

2π
π

0

π

sin 2x

π

2π
π

cos 3x

2π
π

0

π

2π
π

sin 3x

Fig. 259. Cosine and sine functions having the period 2p (the first few members of the
trigonometric system (3), except for the constant 1)

c11-a.qxd

10/30/10

476

1:24 PM

Page 476

CHAP. 11 Fourier Analysis

The series to be obtained will be a trigonometric series, that is, a series of the form
a0 ϩ a1 cos x ϩ b1 sin x ϩ a2 cos 2x ϩ b2 sin 2x ϩ Á
ؕ

(4)

ϭ a0 ϩ a (an cos nx ϩ bn sin nx).
nϭ1

a0, a1, b1, a2, b2, Á are constants, called the coefficients of the series. We see that each
term has the period 2p. Hence if the coefficients are such that the series converges, its
sum will be a function of period 2p.
Expressions such as (4) will occur frequently in Fourier analysis. To compare the
expression on the right with that on the left, simply write the terms in the summation.
Convergence of one side implies convergence of the other and the sums will be the
same.
Now suppose that f (x) is a given function of period 2p and is such that it can be
represented by a series (4), that is, (4) converges and, moreover, has the sum f (x). Then,
using the equality sign, we write
ؕ

(5)

f (x) ϭ a0 ϩ a (an cos nx ϩ bn sin nx)
nϭ1

and call (5) the Fourier series of f (x). We shall prove that in this case the coefficients
of (5) are the so-called Fourier coefficients of f (x), given by the Euler formulas

(0)

(6)

(a)

(b)

a0 ϭ

1
2p

1
an ϭ p
1
bn ϭ p

Ύ

p

f (x) dx

؊p

Ύ

p

Ύ

p

f (x) cos nx dx

n ϭ 1, 2, Á

f (x) sin nx dx

n ϭ 1, 2, Á .

؊p

؊p

The name “Fourier series” is sometimes also used in the exceptional case that (5) with
coefficients (6) does not converge or does not have the sum f (x)—this may happen but
is merely of theoretical interest. (For Euler see footnote 4 in Sec. 2.5.)

A Basic Example
Before we derive the Euler formulas (6), let us consider how (5) and (6) are applied in
this important basic example. Be fully alert, as the way we approach and solve this
example will be the technique you will use for other functions. Note that the integration
is a little bit different from what you are familiar with in calculus because of the n. Do
not just routinely use your software but try to get a good understanding and make
observations: How are continuous functions (cosines and sines) able to represent a given
discontinuous function? How does the quality of the approximation increase if you take
more and more terms of the series? Why are the approximating functions, called the

c11-a.qxd

10/30/10

1:24 PM

Page 477

SEC. 11.1 Fourier Series

477

partial sums of the series, in this example always zero at 0 and p? Why is the factor
1>n (obtained in the integration) important?
EXAMPLE 1

Periodic Rectangular Wave (Fig. 260)
Find the Fourier coefficients of the periodic function f (x) in Fig. 260. The formula is
(7)

Ϫk

if

Ϫp Ͻ x Ͻ 0

k

if

0ϽxϽp

f (x) ϭ b

and

f (x ϩ 2p) ϭ f (x).

Functions of this kind occur as external forces acting on mechanical systems, electromotive forces in electric
circuits, etc. (The value of f (x) at a single point does not affect the integral; hence we can leave f (x) undefined
at x ϭ 0 and x ϭ Ϯp.)

Solution. From (6.0) we obtain a0 ϭ 0. This can also be seen without integration, since the area under the

curve of f (x) between Ϫp and p (taken with a minus sign where f (x) is negative) is zero. From (6a) we obtain
the coefficients a1, a2, Á of the cosine terms. Since f ( x) is given by two expressions, the integrals from Ϫp
to p split into two integrals:
an ϭ

pΎ
1

p

f (x) cos nx dx ϭ

؊p

ϭ

1

p c

Ύ

0

(Ϫk) cos nx dx ϩ

؊p

1

p c Ϫk

Ύ

p

k cos nx dx d

0

p

sin nx 0
sin nx
` ϩk
` d ϭ0
n
n
؊p
0

because sin nx ϭ 0 at Ϫp, 0, and p for all n ϭ 1, 2, Á . We see that all these cosine coefficients are zero. That
is, the Fourier series of (7) has no cosine terms, just sine terms, it is a Fourier sine series with coefficients
b1, b2, Á obtained from (6b);
bn ϭ

1

p

Ύ

p

f (x) sin nx dx ϭ

؊p

ϭ

1

pc
1

Ύ

0

(Ϫk) sin nx dx ϩ

؊p

p ck

Ύ

p

0

k sin nx dx d

cos nx 0
cos nx p
` Ϫk
` d.
n
n
؊p
0

Since cos (Ϫa) ϭ cos a and cos 0 ϭ 1, this yields
bn ϭ

k
2k
[cos 0 Ϫ cos (Ϫnp) Ϫ cos np ϩ cos 0] ϭ
(1 Ϫ cos np).
np
np

Now, cos p ϭ Ϫ1, cos 2p ϭ 1, cos 3p ϭ Ϫ1, etc.; in general,
Ϫ1
cos np ϭ b
1

for odd n,
and thus
for even n,

1 Ϫ cos np ϭ b

2

for odd n,

0

for even n.

Hence the Fourier coefficients bn of our function are
b1 ϭ

4k

p

,

b2 ϭ 0,

b3 ϭ

4k
3p

,

b4 ϭ 0,

b5 ϭ

4k Á
, .

5p

Fig. 260. Given function f (x) (Periodic reactangular wave)

c11-a.qxd

10/30/10

478

1:24 PM

Page 478

CHAP. 11 Fourier Analysis
Since the an are zero, the Fourier series of f (x) is
4k

(8)

p

(sin x ϩ 13 sin 3x ϩ 15 sin 5x ϩ Á ).

The partial sums are
S1 ϭ

4k

p

S2 ϭ

sin x,

4k

p

asin x ϩ

1
sin 3xb .
3

etc.

Their graphs in Fig. 261 seem to indicate that the series is convergent and has the sum f (x), the given function.
We notice that at x ϭ 0 and x ϭ p, the points of discontinuity of f (x), all partial sums have the value zero, the
arithmetic mean of the limits Ϫk and k of our function, at these points. This is typical.
Furthermore, assuming that f (x) is the sum of the series and setting x ϭ p>2, we have

p
4k
1
1
Á
fa bϭkϭ
2
p a1 Ϫ 3 ϩ 5 Ϫ ϩ b .
Thus
1Ϫ

1
3

ϩ

1
5

Ϫ

1
7

p
ϩϪ Á ϭ .
4

This is a famous result obtained by Leibniz in 1673 from geometric considerations. It illustrates that the values
of various series with constant terms can be obtained by evaluating Fourier series at specific points.
᭿

Fig. 261. First three partial sums of the corresponding Fourier series

c11-a.qxd

10/30/10

1:24 PM

Page 479

SEC. 11.1 Fourier Series

479

Derivation of the Euler Formulas (6)
The key to the Euler formulas (6) is the orthogonality of (3), a concept of basic importance,
as follows. Here we generalize the concept of inner product (Sec. 9.3) to functions.
THEOREM 1

Orthogonality of the Trigonometric System (3)

The trigonometric system (3) is orthogonal on the interval Ϫp Ϲ x Ϲ p (hence
also on 0 Ϲ x Ϲ 2p or any other interval of length 2p because of periodicity); that
is, the integral of the product of any two functions in (3) over that interval is 0, so
that for any integers n and m,
(a)

Ύ

p

Ύ

p

cos nx cos mx dx ϭ 0

(n

m)

(n

m)

(n

m or n ϭ m).

؊p

(9)

(b)

sin nx sin mx dx ϭ 0

؊p

(c)

Ύ

p

sin nx cos mx dx ϭ 0

؊p

PROOF

This follows simply by transforming the integrands trigonometrically from products into
sums. In (9a) and (9b), by (11) in App. A3.1,

Ύ

p

Ύ

p

1
2

cos nx cos mx dx ϭ

؊p

sin nx sin mx dx ϭ

؊p

1
2

Ύ

p

1
2

cos (n ϩ m)x dx ϩ

؊p

Ύ

p

cos (n Ϫ m)x dx Ϫ

؊p

1
2

Ύ

p

cos (n Ϫ m)x dx

؊p

Ύ

p

cos (n ϩ m)x dx.

؊p

Since m n (integer!), the integrals on the right are all 0. Similarly, in (9c), for all integer
m and n (without exception; do you see why?)

Ύ

p

1
sin nx cos mx dx ϭ
2

؊p

Ύ

p

1
sin (n ϩ m)x dx ϩ
2

؊p

Ύ

p

sin (n Ϫ m)x dx ϭ 0 ϩ 0.

᭿

؊p

Application of Theorem 1 to the Fourier Series (5)
We prove (6.0). Integrating on both sides of (5) from Ϫp to p, we get

Ύ

p

f (x) dx ϭ

؊p

Ύ

p

؊p

ؕ

c a0 ϩ a (an cos nx ϩ bn sin nx) d dx.
nϭ1

We now assume that termwise integration is allowed. (We shall say in the proof of
Theorem 2 when this is true.) Then we obtain

Ύ

p

؊p

f (x) dx ϭ a0

Ύ

p

ؕ

dx ϩ a aan

؊p

nϭ1

Ύ

p

cos nx dx ϩ bn

؊p

Ύ

p

sin nx dxb .

؊p

c11-a.qxd

10/30/10

1:24 PM

480

Page 480

CHAP. 11 Fourier Analysis

The first term on the right equals 2pa0. Integration shows that all the other integrals are 0.
Hence division by 2p gives (6.0).
We prove (6a). Multiplying (5) on both sides by cos mx with any fixed positive integer
m and integrating from Ϫp to p, we have
(10)

Ύ

p

f (x) cos mx dx ϭ

؊p

Ύ

p

؊p

ؕ

c a0 ϩ a (an cos nx ϩ bn sin nx) d cos mx dx.
nϭ1

We now integrate term by term. Then on the right we obtain an integral of a0 cos mx,
which is 0; an integral of an cos nx cos mx , which is amp for n ϭ m and 0 for n m by
(9a); and an integral of bn sin nx cos mx, which is 0 for all n and m by (9c). Hence the
right side of (10) equals amp. Division by p gives (6a) (with m instead of n).
We finally prove (6b). Multiplying (5) on both sides by sin mx with any fixed positive
integer m and integrating from Ϫp to p, we get
(11)

Ύ

p

؊p

f (x) sin mx dx ϭ

Ύ

p

؊p

ؕ

c a0 ϩ a (an cos nx ϩ bn sin nx) d sin mx dx.
nϭ1

Integrating term by term, we obtain on the right an integral of a0 sin mx, which is 0; an
integral of an cos nx sin mx, which is 0 by (9c); and an integral of bn sin nx sin mx, which
is bmp if n ϭ m and 0 if n m, by (9b). This implies (6b) (with n denoted by m). This
completes the proof of the Euler formulas (6) for the Fourier coefficients.
᭿

Convergence and Sum of a Fourier Series
The class of functions that can be represented by Fourier series is surprisingly large and
general. Sufficient conditions valid in most applications are as follows.
THEOREM 2

Representation by a Fourier Series

Let f (x) be periodic with period 2p and piecewise continuous (see Sec. 6.1) in the
interval Ϫp Ϲ x Ϲ p. Furthermore, let f (x) have a left-hand derivative and a righthand derivative at each point of that interval. Then the Fourier series (5) of f (x)
[with coefficients (6)] converges. Its sum is f (x), except at points x0 where f (x) is
discontinuous. There the sum of the series is the average of the left- and right-hand
limits2 of f (x) at x 0.
f (x)
f (1 – 0)
2
The left-hand limit of f (x) at x 0 is defined as the limit of f (x) as x approaches x0 from the left
and is commonly denoted by f (x 0 Ϫ 0). Thus

1
f (1 + 0)
0

x

1

Fig. 262. Left- and
right-hand limits
ƒ(1 Ϫ 0) ϭ 1,
ƒ(1 ϩ 0) ϭ _1
2

of the function
f (x) ϭ b

x

2

x>2

if x Ͻ 1
if x м 1

ƒ(x0 Ϫ 0) ϭ lim ƒ( x0 Ϫ h) as h * 0 through positive values.
h*0

The right-hand limit is denoted by ƒ(x0 ϩ 0) and
ƒ(x0 ϩ 0) ϭ lim ƒ( x0 ϩ h) as h * 0 through positive values.
h*0

The left- and right-hand derivatives of ƒ(x) at x0 are defined as the limits of
f (x 0 Ϫ h) Ϫ f (x 0 Ϫ 0)
Ϫh

and

f (x 0 ϩ h) Ϫ f (x 0 ϩ 0)
Ϫh

,

respectively, as h * 0 through positive values. Of course if ƒ(x) is continuous at x0, the last term in
both numerators is simply ƒ(x0).

c11-a.qxd

10/30/10

1:24 PM

Page 481

SEC. 11.1 Fourier Series

PROOF

481

We prove convergence, but only for a continuous function f (x) having continuous first
and second derivatives. And we do not prove that the sum of the series is f (x) because
these proofs are much more advanced; see, for instance, Ref. 3C124 listed in App. 1.
Integrating (6a) by parts, we obtain
1
an ϭ p

Ύ

p

f (x) sin nx
2
np

f (x) cos nx dx ϭ

؊p

p

؊p

Ύ

1
Ϫ np

p

f r (x) sin nx dx.

؊p

The first term on the right is zero. Another integration by parts gives

an ϭ

f r (x) cos nx
n 2p

p

2

Ϫ
؊p

1
n 2p

Ύ

p

f s (x) cos nx dx.

؊p

The first term on the right is zero because of the periodicity and continuity of f r (x). Since
f s is continuous in the interval of integration, we have
ƒ f s (x) ƒ Ͻ M
for an appropriate constant M. Furthermore, ƒ cos nx ƒ Ϲ 1. It follows that
1

Ύ

p

1
ƒ an ƒ ϭ 2
f s (x) cos nx dx 2 Ͻ 2
n p ؊p
n p

2

Ύ

p

M dx ϭ

؊p

2M
n2

.

Similarly, ƒ bn ƒ Ͻ 2 M>n 2 for all n. Hence the absolute value of each term of the Fourier
series of f (x) is at most equal to the corresponding term of the series
ƒ a0 ƒ ϩ 2M a1 ϩ 1 ϩ

1
2

2

ϩ

1
2

2

ϩ

1
3

2

ϩ

1
32

ϩ Áb

which is convergent. Hence that Fourier series converges and the proof is complete.
(Readers already familiar with uniform convergence will see that, by the Weierstrass
test in Sec. 15.5, under our present assumptions the Fourier series converges uniformly,
and our derivation of (6) by integrating term by term is then justified by Theorem 3 of
᭿
Sec. 15.5.)
EXAMPLE 2

Convergence at a Jump as Indicated in Theorem 2
The rectangular wave in Example 1 has a jump at x ϭ 0. Its left-hand limit there is Ϫk and its right-hand limit
is k (Fig. 261). Hence the average of these limits is 0. The Fourier series (8) of the wave does indeed converge
to this value when x ϭ 0 because then all its terms are 0. Similarly for the other jumps. This is in agreement
᭿
with Theorem 2.

Summary. A Fourier series of a given function f (x) of period 2p is a series of the form
(5) with coefficients given by the Euler formulas (6). Theorem 2 gives conditions that are
sufficient for this series to converge and at each x to have the value f (x), except at
discontinuities of f (x), where the series equals the arithmetic mean of the left-hand and
right-hand limits of f (x) at that point.

c11-a.qxd

10/30/10

1:24 PM

482

Page 482

CHAP. 11 Fourier Analysis

PROBLEM SET 11.1
1–5

PERIOD, FUNDAMENTAL PERIOD

The fundamental period is the smallest positive period. Find
it for
1. cos x, sin x, cos 2x, sin 2x, cos px, sin px,
cos 2px, sin 2px
2px
2px
2pnx
2. cos nx, sin nx, cos
, sin
, cos
,
k
k
k
2pnx
sin
k
3. If f ( x) and g (x) have period p, show that h (x) ϭ
af (x) ϩ bg(x) (a, b, constant) has the period p. Thus
all functions of period p form a vector space.
4. Change of scale. If f (x) has period p, show that
f (ax), a 0, and f (x>b), b 0, are periodic functions
of x of periods p>a and bp, respectively. Give examples.
5. Show that f ϭ const is periodic with any period but has
no fundamental period.

17.

–π

10. f (x) ϭ b

FOURIER SERIES

Find the Fourier series of the given function f (x), which is
assumed to have the period 2p. Show the details of your
work. Sketch or graph the partial sums up to that including
cos 5x and sin 5x.
12. f (x) in Prob. 6
13. f (x) in Prob. 9
14. f (x) ϭ x 2 (Ϫp Ͻ x Ͻ p)
15. f (x) ϭ x 2 (0 Ͻ x Ͻ 2p)
16.
1
π

2
–π

0

1
π
2

π

π

0

19.

π

–π

π

0

20.

1
π
2
–π

– 1π
2

21.

1
π
2

0
– 1π
2

π

π

–π

π

Ϫcos2 x if Ϫp Ͻ x Ͻ 0

cos2 x if
0ϽxϽp
11. Calculus review. Review integration techniques for
integrals as they are likely to arise from the Euler
formulas, for instance, definite integrals of x cos nx,
x 2 sin nx, e؊2x cos nx, etc.
12–21

1

–π

GRAPHS OF 2p–PERIODIC FUNCTIONS
Sketch or graph f (x) which for Ϫp Ͻ x Ͻ p is given as

π

0

18.

6–10

follows.
6. f (x) ϭ ƒ x ƒ
7. f (x) ϭ ƒ sin x ƒ , f (x) ϭ sin ƒ x ƒ
8. f (x) ϭ e؊ƒ x ƒ, f (x) ϭ ƒ e؊x ƒ
x
if Ϫp Ͻ x Ͻ 0
9. f (x) ϭ b
p Ϫ x if
0ϽxϽp

π

–π

22. CAS EXPERIMENT. Graphing. Write a program for
graphing partial sums of the following series. Guess
from the graph what f (x) the series may represent.
Confirm or disprove your guess by using the Euler
formulas.
(a) 2(sin x ϩ 13 sin 3x ϩ 15 sin 5x ϩ Á)
Ϫ 2( 12 sin 2x ϩ 14 sin 4x ϩ 16 sin 6x Á)
(b)

1
4
1
1
ϩ 2 acos x ϩ cos 3x ϩ
cos 5x ϩ Á b
p
2
9
25

(c)

2
3

1
p2 ϩ 4(cos x Ϫ 14 cos 2x ϩ 19 cos 3x Ϫ 16
cos 4x

ϩ Ϫ Á)
23. Discontinuities. Verify the last statement in Theorem
2 for the discontinuities of f (x) in Prob. 21.
24. CAS EXPERIMENT. Orthogonality. Integrate and
graph the integral of the product cos mx cos nx (with
various integer m and n of your choice) from Ϫa to a
as a function of a and conclude orthogonality of cos mx

c11-a.qxd

10/30/10

1:24 PM

Page 483

SEC. 11.2 Arbitrary Period. Even and Odd Functions. Half-Range Expansions
and cos nx (m n) for a ϭ p from the graph. For what
m and n will you get orthogonality for a ϭ p>2, p>3,
p>4? Other a? Extend the experiment to cos mx sin nx
and sin mx sin nx.
25. CAS EXPERIMENT. Order of Fourier Coefficients.
The order seems to be 1>n if f is discontinous, and 1>n 2

11.2

483

if f is continuous but f r ϭ df>dx is discontinuous, 1>n 3
if f and f r are continuous but f s is discontinuous, etc.
Try to verify this for examples. Try to prove it by
integrating the Euler formulas by parts. What is the
practical significance of this?

Arbitrary Period. Even and Odd Functions.
Half-Range Expansions
We now expand our initial basic discussion of Fourier series.
Orientation. This section concerns three topics:
1. Transition from period 2p to any period 2L, for the function f, simply by a
transformation of scale on the x-axis.
2. Simplifications. Only cosine terms if f is even (“Fourier cosine series”). Only sine
terms if f is odd (“Fourier sine series”).
3. Expansion of f given for 0 Ϲ x Ϲ L in two Fourier series, one having only cosine
terms and the other only sine terms (“half-range expansions”).

1. From Period 2p to Any Period p ϭ 2L
Clearly, periodic functions in applications may have any period, not just 2p as in the last
section (chosen to have simple formulas). The notation p ϭ 2L for the period is practical
because L will be a length of a violin string in Sec. 12.2, of a rod in heat conduction in
Sec. 12.5, and so on.
The transition from period 2p to be period p ϭ 2L is effected by a suitable change of
scale, as follows. Let f (x) have period p ϭ 2L. Then we can introduce a new variable v
such that f (x), as a function of v, has period 2p. If we set
(1)

(a) x ϭ

p
2p

v,

2p
p
(b) v ϭ p x ϭ x
L

so that

then v ϭ Ϯp corresponds to x ϭ ϮL. This means that f, as a function of v, has period
2p and, therefore, a Fourier series of the form
(2)

ؕ
L
f (x) ϭ f a p vb ϭ a0 ϩ a (an cos nv ϩ bn sin nv)
nϭ1

with coefficients obtained from (6) in the last section
1
a0 ϭ 2p
(3)

Ύ

p

؊p

fa

L

p

vb dv,

1
bn ϭ p

Ύ

p

؊p

1
an ϭ p

Ύ

p

fa

؊p

L

p

L
f a p vb sin nv dv.

vb cos nv dv,

c11-a.qxd

10/30/10

1:24 PM

484

Page 484

CHAP. 11 Fourier Analysis

We could use these formulas directly, but the change to x simplifies calculations. Since
vϭ

(4)

p
L

x,

dv ϭ

we have

p

dx

L

and we integrate over x from ϪL to L. Consequently, we obtain for a function f (x) of
period 2L the Fourier series
(5)

ؕ
np
np
f (x) ϭ a0 ϩ a aan cos
x ϩ bn sin
xb
L
L
nϭ1

with the Fourier coefficients of f (x) given by the Euler formulas (p>L in dx cancels
1> p in (3))
(0)

(6)

(a)

(b)

Ύ

1
2L

a0 ϭ

1
L

an ϭ

1
bn ϭ
L

L

f (x) dx

؊L

Ύ

L

Ύ

L

f (x) cos

؊L

f (x) sin

؊L

npx
dx
L

n ϭ 1, 2, Á

n px
dx
L

n ϭ 1, 2, Á .

Just as in Sec. 11.1, we continue to call (5) with any coefficients a trigonometric series.
And we can integrate from 0 to 2L or over any other interval of length p ϭ 2L.
EXAMPLE 1

Periodic Rectangular Wave
Find the Fourier series of the function (Fig. 263)

0

if

Ϫ2 Ͻ x Ͻ Ϫ1

f (x) ϭ d k

if

Ϫ1 Ͻ x Ͻ

1

0

if

1ϽxϽ

2

p ϭ 2L ϭ 4, L ϭ 2.

Solution. From (6.0) we obtain a0 ϭ k>2 (verify!). From (6a) we obtain
an ϭ

1
2

Ύ

2

f (x) cos

؊2

npx
2

dx ϭ

1
2

Ύ

1

k cos

npx

؊1

2

dx ϭ

2k
np

sin

np
2

.

Thus an ϭ 0 if n is even and
an ϭ 2k>np if

n ϭ 1, 5, 9, Á ,

an ϭ Ϫ2k>np if n ϭ 3, 7, 11, Á .

From (6b) we find that bn ϭ 0 for n ϭ 1, 2, Á . Hence the Fourier series is a Fourier cosine series (that is, it
has no sine terms)
f (x) ϭ

k
2

ϩ

2k

p

acos

p
2

xϪ

1
3

cos

3p
2

xϩ

1
5

cos

5p
2

x Ϫ ϩ Áb .

᭿

c11-a.qxd

10/30/10

1:24 PM

Page 485

SEC. 11.2 Arbitrary Period. Even and Odd Functions. Half-Range Expansions

485
f(x)
k

f(x)
–2

k

2

x

–k
–2

–1

0

1

x

2

Fig. 263. Example 1

EXAMPLE 2

Fig. 264. Example 2

Periodic Rectangular Wave. Change of Scale
Find the Fourier series of the function (Fig. 264)
Ϫk

if

Ϫ2 Ͻ x Ͻ 0

k

if

0ϽxϽ2

f (x) ϭ c

p ϭ 2L ϭ 4,

L ϭ 2.

Solution. Since L ϭ 2, we have in (3) v ϭ px>2 and obtain from (8) in Sec. 11.1 with v instead of x, that is,
g(v) ϭ

4k

p

asin v ϩ

1
3

sin 3v ϩ

1
5

sin 5v ϩ Á b

the present Fourier series
f (x) ϭ

4k

p

asin

p
2

xϩ

1
3

sin

3p
2

1

xϩ

5

sin

x ϩ Áb .

5p
2

᭿

Confirm this by using (6) and integrating.

EXAMPLE 3

Half-Wave Rectifier
A sinusoidal voltage E sin vt, where t is time, is passed through a half-wave rectifier that clips the negative
portion of the wave (Fig. 265). Find the Fourier series of the resulting periodic function
0
u(t) ϭ c
E sin vt

if

ϪL Ͻ t Ͻ 0,

if

0ϽtϽL

p ϭ 2L ϭ

2p
,
v

Lϭ

p
v

.

Solution. Since u ϭ 0 when ϪL Ͻ t Ͻ 0, we obtain from (6.0), with t instead of x,
a0 ϭ

2p Ύ
v

p>v

E sin vt dt ϭ

0

E

p

and from (6a), by using formula (11) in App. A3.1 with x ϭ vt and y ϭ nvt,
an ϭ

pΎ
v

p>v

E sin vt cos nvt dt ϭ

0

2p Ύ
vE

p>v

[sin (1 ϩ n) vt ϩ sin (1 Ϫ n) vt] dt.

0

If n ϭ 1, the integral on the right is zero, and if n ϭ 2, 3, Á , we readily obtain
an ϭ

ϭ

vE
2p
E
2p

cϪ
a

cos (1 ϩ n) vt
(1 ϩ n) v

Ϫ

Ϫcos (1 ϩ n)p ϩ 1
1ϩn

cos (1 Ϫ n) vt
(1 Ϫ n) v

ϩ

d

p>v
0

Ϫcos (1 Ϫ n)p ϩ 1
1Ϫn

b.

If n is odd, this is equal to zero, and for even n we have
an ϭ

E
2
2
2E
a
ϩ
bϭϪ
2p 1 ϩ n
1Ϫn
(n Ϫ 1)(n ϩ 1)p

(n ϭ 2, 4, Á ).

c11-a.qxd

10/30/10

1:24 PM

486

Page 486

CHAP. 11 Fourier Analysis
In a similar fashion we find from (6b) that b1 ϭ E>2 and bn ϭ 0 for n ϭ 2, 3, Á . Consequently,
u(t) ϭ

E

p

E

ϩ

2

sin vt Ϫ

2E

p

a

1
1
cos 2vt ϩ
cos 4vt ϩ Á b .
1#3
3#5

᭿

u(t)

– π /ω

π /ω

0

t

Fig. 265. Half-wave rectifier

2. Simplifications: Even and Odd Functions
If f (x) is an even function, that is, f (Ϫx) ϭ f (x) (see Fig. 266), its Fourier series (5)
reduces to a Fourier cosine series
x

ؕ
np
f (x) ϭ a0 ϩ a an cos
x
L
nϭ1

(5*)
Fig. 266.
Even function

( f even)

with coefficients (note: integration from 0 to L only!)
x

Fig. 267.
Odd function

(6*)

a0 ϭ

1
L

Ύ

L

f (x) dx,

an ϭ

0

Ύ

2
L

L

f (x) cos

0

npx
dx,
L

n ϭ 1, 2, Á .

If f (x) is an odd function, that is, f (Ϫx) ϭ Ϫf (x) (see Fig. 267), its Fourier series (5)
reduces to a Fourier sine series
(5**)

ؕ
np
f (x) ϭ a bn sin
x
L
nϭ1

( f odd)

with coefficients
bn ϭ

(6**)

2
L

Ύ

L

f (x) sin

0

npx
dx.
L

These formulas follow from (5) and (6) by remembering from calculus that the definite
integral gives the net area (ϭ area above the axis minus area below the axis) under the
curve of a function between the limits of integration. This implies
(a)

Ύ

L

Ύ

L

g (x) dx ϭ 2

؊L

(7)
(b)

Ύ

L

g (x) dx

for even g

0

h (x) dx ϭ 0

for odd h

؊L

Formula (7b) implies the reduction to the cosine series (even f makes f (x) sin (npx>L) odd
since sin is odd) and to the sine series (odd f makes f (x) cos (npx>L) odd since cos is even).
Similarly, (7a) reduces the integrals in (6*) and (6**) to integrals from 0 to L. These reductions
are obvious from the graphs of an even and an odd function. (Give a formal proof.)

c11-a.qxd

10/30/10

1:25 PM

Page 487

SEC. 11.2 Arbitrary Period. Even and Odd Functions. Half-Range Expansions

487

Summary
Even Function of Period 2␲. If f is even and L ϭ p, then
ؕ

f (x) ϭ a0 ϩ a an cos nx
nϭ1

with coefficients
1
a0 ϭ p

Ύ

p

2
an ϭ p

f (x) dx,

0

Ύ

p

f (x) cos nx dx, n ϭ 1, 2, Á

0

Odd Function of Period 2p. If f is odd and L ϭ p, then
ؕ

f (x) ϭ a bn sin nx
nϭ1

with coefficients
2
bn ϭ p

Ύ

p

n ϭ 1, 2, Á .

f (x) sin nx dx,

0

EXAMPLE 4

Fourier Cosine and Sine Series
The rectangular wave in Example 1 is even. Hence it follows without calculation that its Fourier series is a
Fourier cosine series, the bn are all zero. Similarly, it follows that the Fourier series of the odd function in
Example 2 is a Fourier sine series.
In Example 3 you can see that the Fourier cosine series represents u(t) Ϫ E> p Ϫ 12 E sin vt. Can you prove
that this is an even function?
᭿

Further simplifications result from the following property, whose very simple proof is left
to the student.
THEOREM 1

Sum and Scalar Multiple

The Fourier coefficients of a sum f1 ϩ f2 are the sums of the corresponding Fourier
coefficients of f1 and f2.
The Fourier coefficients of cf are c times the corresponding Fourier coefficients of f.

EXAMPLE 5

Sawtooth Wave
Find the Fourier series of the function (Fig. 268)
f (x) ϭ x ϩ p if Ϫp Ͻ x Ͻ p

and

f (x ϩ 2p) ϭ f (x).

f (x)

–π

π

x

Fig. 268. The function f(x). Sawtooth wave

c11-a.qxd

10/30/10

488

1:25 PM

Page 488

CHAP. 11 Fourier Analysis
y

S20
S3

5

S2
S1

y

–π

π

0

x

Fig. 269. Partial sums S1, S2, S3, S20 in Example 5

Solution. We have f ϭ f1 ϩ f2, where f1 ϭ x and f2 ϭ p. The Fourier coefficients of f2 are zero, except for
the first one (the constant term), which is p. Hence, by Theorem 1, the Fourier coefficients an, bn are those of
f1, except for a0, which is p. Since f1 is odd, an ϭ 0 for n ϭ 1, 2, Á , and
bn ϭ

pΎ
2

p

f1 (x) sin nx dx ϭ

0

pΎ
2

p

x sin nx dx.

0

Integrating by parts, we obtain
bn ϭ

2 Ϫx cos nx
2
pc
n

p

ϩ
0

1
n

Ύ

p

0

2
cos nx dx d ϭ Ϫ cos np.
n

Hence b1 ϭ 2, b2 ϭ Ϫ 22 , b3 ϭ 23 , b4 ϭ Ϫ 24 , Á , and the Fourier series of f (x) is
f (x) ϭ p ϩ 2 asin x Ϫ

1
2

sin 2x ϩ

1
3

sin 3x Ϫ ϩ Á b .

(Fig. 269)

᭿

3. Half-Range Expansions
Half-range expansions are Fourier series. The idea is simple and useful. Figure 270
explains it. We want to represent f (x) in Fig. 270.0 by a Fourier series, where f (x)
may be the shape of a distorted violin string or the temperature in a metal bar of length
L, for example. (Corresponding problems will be discussed in Chap. 12.) Now comes
the idea.
We could extend f (x) as a function of period L and develop the extended function into
a Fourier series. But this series would, in general, contain both cosine and sine terms. We
can do better and get simpler series. Indeed, for our given f we can calculate Fourier
coefficients from (6*) or from (6**). And we have a choice and can take what seems
more practical. If we use (6*), we get (5*). This is the even periodic extension f1 of f
in Fig. 270a. If we choose (6**) instead, we get (5**), the odd periodic extension f2 of
f in Fig. 270b.
Both extensions have period 2L. This motivates the name half-range expansions: f is
given (and of physical interest) only on half the range, that is, on half the interval of
periodicity of length 2L.
Let us illustrate these ideas with an example that we shall also need in Chap. 12.

c11-a.qxd

10/30/10

1:25 PM

Page 489

SEC. 11.2 Arbitrary Period. Even and Odd Functions. Half-Range Expansions

489

f (x)

x

L

(0) The given function f (x)

f1(x)

–L

x

L

(a) f (x) continued as an even periodic function of period 2L

f2(x)

–L

x

L

(b) f (x) continued as an odd periodic function of period 2L

Fig. 270. Even and odd extensions of period 2L

EXAMPLE 6

“Triangle” and Its Half-Range Expansions
Find the two half-range expansions of the function (Fig. 271)

k
0

L/2

L

2k

x

L

x

if

(L Ϫ x)

if

L

0ϽxϽ

2

f(x) ϭ e

Fig. 271. The given
function in Example 6

2k
L

L

Ͻ x Ͻ L.

2

Solution. (a) Even periodic extension. From (6*) we obtain
a0 ϭ

an ϭ

1 2k
c
L L

Ύ

L>2

2 2k
c
L L

Ύ

L>2

x dx ϩ

L Ύ

2k

0

L

L>2

x cos

L

np
L

k

(L Ϫ x) dx d ϭ
L Ύ

2k

x dx ϩ

2

,

L

(L Ϫ x) cos

np
L

L>2

x dx d .

We consider an. For the first integral we obtain by integration by parts

Ύ

L>2

x cos

np
L

x dx ϭ

Lx

sin

np

np
L

L>2

x2
0

0

ϭ

L2
2np

sin

Ϫ

np
2

Ύ
np

L>2

L

sin

np

ϩ

L2
2

n p2

acos

np
2

x dx

L

0

Ϫ 1b .

Similarly, for the second integral we obtain

Ύ

L

(L Ϫ x) cos

L>2

np
L

x dx ϭ

L
np

(L Ϫ x) sin

ϭ a0 Ϫ

L
np

aL Ϫ

np
L
L
2

L

x2

ϩ
L>2

b sin

np
2

np Ύ
L

L

sin

L>2

bϪ

L2
2

n p2

np
L

x dx

acos np Ϫ cos

np
2

b.

c11-a.qxd

10/30/10

490

1:25 PM

Page 490

CHAP. 11 Fourier Analysis
We insert these two results into the formula for an. The sine terms cancel and so does a factor L2. This gives
an ϭ

a2 cos

4k
n 2p2

np
2

Ϫ cos np Ϫ 1b .

Thus,
a2 ϭ Ϫ16k>(22p2),
and an ϭ 0 if n

a6 ϭ Ϫ16k>(62p2),

a10 ϭ Ϫ16k>(102p2), Á

2, 6, 10, 14, Á . Hence the first half-range expansion of f (x) is (Fig. 272a)
k

f (x) ϭ

2

Ϫ

16k

a

1

p2 22

cos

2p

xϩ

L

1

cos

62

6p
L

x ϩ Áb .

This Fourier cosine series represents the even periodic extension of the given function f (x), of period 2L.
(b) Odd periodic extension. Similarly, from (6**) we obtain
8k

bn ϭ

(5)

2

sin

2

n p

np
2

.

Hence the other half-range expansion of f (x) is (Fig. 272b)
f (x) ϭ

8k
2

p

a

1
2

sin

1

p

1

xϪ

L

2

sin

3

3p

1

xϩ

2

L

5

sin

5p
L

x Ϫ ϩ Á b.

The series represents the odd periodic extension of f (x), of period 2L.
Basic applications of these results will be shown in Secs. 12.3 and 12.5.

–L

᭿

x

L

0
(a) Even extension

–L

x

L

0

(b) Odd extension

Fig. 272. Periodic extensions of f(x) in Example 6

PROBLEM SET 11.2
1–7

EVEN AND ODD FUNCTIONS

Are the following functions even or odd or neither even nor
odd?
1. ex, e؊ƒ x ƒ, x 3 cos nx, x 2 tan px, sinh x Ϫ cosh x
2. sin2 x, sin (x 2), ln x, x>(x 2 ϩ 1), x cot x
3. Sums and products of even functions
4. Sums and products of odd functions
5. Absolute values of odd functions
6. Product of an odd times an even function
7. Find all functions that are both even and odd.

8–17

FOURIER SERIES FOR PERIOD p = 2L

Is the given function even or odd or neither even nor
odd? Find its Fourier series. Show details of your
work.

8.

1

–1

0

1

c11-a.qxd

10/30/10

1:25 PM

Page 491

SEC. 11.2 Arbitrary Period. Even and Odd Functions. Half-Range Expansions
9.

(b) Apply the program to Probs. 8–11, graphing the first
few partial sums of each of the four series on common
axes. Choose the first five or more partial sums until
they approximate the given function reasonably well.
Compare and comment.

1

2

–2
–1

10.

22. Obtain the Fourier series in Prob. 8 from that in
Prob. 17.

4

23–29
–4

4

11. f (x) ϭ x

HALF-RANGE EXPANSIONS

Find (a) the Fourier cosine series, (b) the Fourier sine series.
Sketch f (x) and its two periodic extensions. Show the
details.
23.

–4
2

491

1

(Ϫ1 Ͻ x Ͻ 1), p ϭ 2

12. f (x) ϭ 1 Ϫ x 2>4 (Ϫ2 Ͻ x Ͻ 2), p ϭ 4
13.

4

1
2

24. 1

1
2

–1
2

14. f ( x) ϭ cos px (Ϫ 12 Ͻ x Ͻ 12), p ϭ 1
15.
π

2

4

25. π

–
2

–π

π

π

– π–
2

16. f ( x) ϭ x ƒ x ƒ
17.

(Ϫ1 Ͻ x Ͻ 1), p ϭ 2

26.

π–
2

1

–1

1

27.

20. Numeric Values. Using Prob. 11, show that 1 ϩ 14 ϩ
1
1
Á ϭ 16 p2.
9 ϩ 16 ϩ
21. CAS PROJECT. Fourier Series of 2L-Periodic
Functions. (a) Write a program for obtaining partial
sums of a Fourier series (5).

28.

π

π–
2

π

π–
2

18. Rectifier. Find the Fourier series of the function
obtained by passing the voltage v(t) ϭ V0 cos 100 pt
through a half-wave rectifier that clips the negative
half-waves.
19. Trigonometric Identities. Show that the familiar
identities cos3 x ϭ 34 cos x ϩ 14 cos 3x and sin3 x ϭ 34
sin x Ϫ 14 sin 3x can be interpreted as Fourier series
expansions. Develop cos 4 x.

π–
2

L

L

29. f (x) ϭ sin x (0 Ͻ x Ͻ p)
30. Obtain the solution to Prob. 26 from that of
Prob. 27.

c11-a.qxd

10/30/10

1:25 PM

492

11.3

Page 492

CHAP. 11 Fourier Analysis

Forced Oscillations
Fourier series have important applications for both ODEs and PDEs. In this section we
shall focus on ODEs and cover similar applications for PDEs in Chap. 12. All these
applications will show our indebtedness to Euler’s and Fourier’s ingenious idea of splitting
up periodic functions into the simplest ones possible.
From Sec. 2.8 we know that forced oscillations of a body of mass m on a spring of
modulus k are governed by the ODE
my s ϩ cy r ϩ ky ϭ r (t)

(1)

where y ϭ y (t) is the displacement from rest, c the damping constant, k the spring constant
(spring modulus), and r (t) the external force depending on time t. Figure 274 shows the
model and Fig. 275 its electrical analog, an RLC-circuit governed by
LI s ϩ RI r ϩ

(1*)

1
I ϭ E r (t)
C

(Sec. 2.9).

We consider (1). If r (t) is a sine or cosine function and if there is damping (c Ͼ 0),
then the steady-state solution is a harmonic oscillation with frequency equal to that of r (t).
However, if r (t) is not a pure sine or cosine function but is any other periodic function,
then the steady-state solution will be a superposition of harmonic oscillations with
frequencies equal to that of r(t) and integer multiples of these frequencies. And if one of
these frequencies is close to the (practical) resonant frequency of the vibrating system (see
Sec. 2.8), then the corresponding oscillation may be the dominant part of the response of
the system to the external force. This is what the use of Fourier series will show us. Of
course, this is quite surprising to an observer unfamiliar with Fourier series, which are
highly important in the study of vibrating systems and resonance. Let us discuss the entire
situation in terms of a typical example.
C

Spring
R

External
force r (t)

L

Mass m
Dashpot
E(t)

Fig. 274. Vibrating system
under consideration

EXAMPLE 1

Fig. 275. Electrical analog of the system
in Fig. 274 (RLC-circuit)

Forced Oscillations under a Nonsinusoidal Periodic Driving Force
In (1), let m ϭ 1 (g), c ϭ 0.05 (g>sec), and k ϭ 25 (g>sec2), so that (1) becomes
(2)

y s ϩ 0.05y r ϩ 25y ϭ r (t)

c11-a.qxd

10/30/10

1:25 PM

Page 493

SEC. 11.3 Forced Oscillations

493
r(t)

ππ/2
–π
π

π

–π
ππ/2

t

Fig. 276. Force in Example 1
where r (t) is measured in g ؒ cm>sec2. Let (Fig. 276)

p

tϩ

2

Ϫp Ͻ t Ͻ 0,

if

r (t) ϭ e

r (t ϩ 2p) ϭ r (t).

p

Ϫt ϩ

2

0 Ͻ t Ͻ p,

if

Find the steady-state solution y(t).

Solution. We represent r (t) by a Fourier series, finding
r (t) ϭ

(3)

4

p

acos t ϩ

1
32

cos 3t ϩ

1
52

cos 5t ϩ Á b .

Then we consider the ODE
y s ϩ 0.05y r ϩ 25y ϭ

(4)

4
2

n p

(n ϭ 1, 3, Á )

cos nt

whose right side is a single term of the series (3). From Sec. 2.8 we know that the steady-state solution yn (t)
of (4) is of the form
yn ϭ An cos nt ϩ Bn sin nt.

(5)
By substituting this into (4) we find that
(6)

An ϭ

4(25 Ϫ n 2)
n 2pDn

,

Bn ϭ

0.2
npDn

,

Dn ϭ (25 Ϫ n 2)2 ϩ (0.05n)2.

where

Since the ODE (2) is linear, we may expect the steady-state solution to be
y ϭ y1 ϩ y3 ϩ y5 ϩ Á

(7)

where yn is given by (5) and (6). In fact, this follows readily by substituting (7) into (2) and using the Fourier
series of r (t), provided that termwise differentiation of (7) is permissible. (Readers already familiar with the
notion of uniform convergence [Sec. 15.5] may prove that (7) may be differentiated term by term.)
From (6) we find that the amplitude of (5) is (a factor 1Dn cancels out)
Cn ϭ 2A2n ϩ B 2n ϭ

4
n p 2Dn
2

.

Values of the first few amplitudes are
C1 ϭ 0.0531 C3 ϭ 0.0088 C5 ϭ 0.2037 C7 ϭ 0.0011 C9 ϭ 0.0003.
Figure 277 shows the input (multiplied by 0.1) and the output. For n ϭ 5 the quantity Dn is very small, the
denominator of C5 is small, and C5 is so large that y5 is the dominating term in (7). Hence the output is almost
a harmonic oscillation of five times the frequency of the driving force, a little distorted due to the term y1, whose
amplitude is about 25% of that of y5. You could make the situation still more extreme by decreasing the damping
constant c. Try it.
᭿

c11-a.qxd

10/30/10

494

1:25 PM

Page 494

CHAP. 11 Fourier Analysis
y
0.3
Output

0.2
0.1

–3

–2

–1

0

1

2

t

3

–0.1

Input

–0.2

Fig. 277. Input and steady-state output in Example 1

PROBLEM SET 11.3
1. Coefficients Cn. Derive the formula for Cn from An
and Bn.
2. Change of spring and damping. In Example 1, what
happens to the amplitudes Cn if we take a stiffer spring,
say, of k ϭ 49? If we increase the damping?
3. Phase shift. Explain the role of the Bn’s. What happens
if we let c : 0?
4. Differentiation of input. In Example 1, what happens
if we replace r (t) with its derivative, the rectangular wave?
What is the ratio of the new Cn to the old ones?
5. Sign of coefficients. Some of the An in Example 1 are
positive, some negative. All Bn are positive. Is this
physically understandable?

6–11

GENERAL SOLUTION

Find a general solution of the ODE y s ϩ v2y ϭ r (t) with
r (t) as given. Show the details of your work.
6. r (t) ϭ sin at ϩ sin bt, v2 a2, b2
7. r (t) ϭ sin t, v ϭ 0.5, 0.9, 1.1, 1.5, 10
8. Rectifier. r (t) ϭ p/4 ƒ cos t ƒ if Ϫp Ͻ t Ͻ p and
r (t ϩ 2p) ϭ r (t), ƒ v ƒ
0, 2, 4, Á
9. What kind of solution is excluded in Prob. 8 by
ƒvƒ
0, 2, 4, Á ?
10. Rectifier. r (t) ϭ p/4 ƒ sin t ƒ if 0 Ͻ t Ͻ 2p and
0, 2, 4, Á
r (t ϩ 2p) ϭ r (t), ƒ v ƒ
Ϫ1 if Ϫp Ͻ t Ͻ 0
11. r (t) ϭ b
ƒ v ƒ 1, 3, 5, Á
1 if
0 Ͻ t Ͻ p,
12. CAS Program. Write a program for solving the ODE
just considered and for jointly graphing input and output
of an initial value problem involving that ODE. Apply

the program to Probs. 7 and 11 with initial values of your
choice.

13–16

STEADY-STATE DAMPED OSCILLATIONS

Find the steady-state oscillations of y s ϩ cy r ϩ y ϭ r (t)
with c Ͼ 0 and r (t) as given. Note that the spring constant
is k ϭ 1. Show the details. In Probs. 14–16 sketch r (t).
N

13. r (t) ϭ a (an cos nt ϩ bn sin nt)
nϭ1

14. r (t) ϭ b

Ϫ1 if Ϫp Ͻ t Ͻ 0

1 if
15. r (t) ϭ t (p2 Ϫ t 2)
r (t ϩ 2p) ϭ r (t)
16. r (t) ϭ
t if Ϫp>2
e
p Ϫ t if p>2

17–19

and r (t ϩ 2p) ϭ r (t)
0ϽtϽp
if Ϫp Ͻ t Ͻ p and

Ͻ t Ͻ p>2

Ͻ t Ͻ 3 p>2

and r (t ϩ 2 p) ϭ r (t)

RLC-CIRCUIT

Find the steady-state current I (t) in the RLC-circuit in
Fig. 275, where R ϭ 10 ⍀, L ϭ 1 H, C ϭ 10 ؊1 F and with
E (t) V as follows and periodic with period 2p. Graph or
sketch the first four partial sums. Note that the coefficients
of the solution decrease rapidly. Hint. Remember that the
ODE contains E r(t), not E (t), cf. Sec. 2.9.
17. E (t) ϭ b

Ϫ50t 2

if

Ϫp Ͻ t Ͻ 0

2

if

0ϽtϽp

50t

c11-a.qxd

10/30/10

1:25 PM

Page 495

SEC. 11.4 Approximation by Trigonometric Polynomials

18. E (t) ϭ b

100 (t Ϫ t 2)

if

Ϫp Ͻ t Ͻ 0

100 (t ϩ t 2)

if

0ϽtϽp

19. E (t) ϭ 200t (p2 Ϫ t 2) (Ϫp Ͻ t Ͻ p)

11.4

495
20. CAS EXPERIMENT. Maximum Output Term.
Graph and discuss outputs of y s ϩ cy r ϩ ky ϭ r (t) with
r (t) as in Example 1 for various c and k with emphasis on
the maximum Cn and its ratio to the second largest ƒ Cn ƒ .

Approximation
by Trigonometric Polynomials
Fourier series play a prominent role not only in differential equations but also in
approximation theory, an area that is concerned with approximating functions by
other functions—usually simpler functions. Here is how Fourier series come into the
picture.
Let f (x) be a function on the interval Ϫp Ϲ x Ϲ p that can be represented on this
interval by a Fourier series. Then the Nth partial sum of the Fourier series
N

(1)

f (x) Ϸ a0 ϩ a (an cos nx ϩ bn sin nx)
nϭ1

is an approximation of the given f (x). In (1) we choose an arbitrary N and keep it fixed.
Then we ask whether (1) is the “best” approximation of f by a trigonometric polynomial
of the same degree N, that is, by a function of the form
N

(2)

F (x) ϭ A0 ϩ a (An cos nx ϩ Bn sin nx)

(N fixed).

nϭ1

Here, “best” means that the “error” of the approximation is as small as possible.
Of course we must first define what we mean by the error of such an approximation.
We could choose the maximum of ƒ f (x) Ϫ F (x) ƒ . But in connection with Fourier series
it is better to choose a definition of error that measures the goodness of agreement between
f and F on the whole interval Ϫp Ϲ x Ϲ p. This is preferable since the sum f of a Fourier
series may have jumps: F in Fig. 278 is a good overall approximation of f, but the maximum
of ƒ f (x) Ϫ F (x) ƒ (more precisely, the supremum) is large. We choose
(3)

Eϭ

Ύ

p

( f Ϫ F)2 dx.

؊p

f
F
x0

Fig. 278. Error of approximation

x

c11-a.qxd

10/30/10

496

1:25 PM

Page 496

CHAP. 11 Fourier Analysis

This is called the square error of F relative to the function f on the interval Ϫp Ϲ x Ϲ p.
Clearly, E м 0.
N being fixed, we want to determine the coefficients in (2) such that E is minimum.
Since ( f Ϫ F)2 ϭ f 2 Ϫ 2fF ϩ F 2, we have
Eϭ

(4)

Ύ

p

f dx Ϫ 2
2

؊p

Ύ

p

f F dx ϩ

؊p

Ύ

p

F 2 dx.

؊p

We square (2), insert it into the last integral in (4), and evaluate the occurring integrals.
This gives integrals of cos2 nx and sin2 nx (n м 1), which equal p, and integrals of
cos nx, sin nx, and (cos nx)(sin mx), which are zero (just as in Sec. 11.1). Thus

Ύ

p

F dx ϭ
2

؊p

Ύ

p

N

؊p

2

c A0 ϩ a (An cos nx ϩ Bn sin nx) d dx
nϭ1

2
2
ϭ p(2A02 ϩ A12 ϩ Á ϩ AN
ϩ B12 ϩ Á ϩ BN
).

We now insert (2) into the integral of f F in (4). This gives integrals of f cos nx as well
as f sin nx, just as in Euler’s formulas, Sec. 11.1, for an and bn (each multiplied by An or
Bn). Hence

Ύ

p

f F dx ϭ p(2A0a0 ϩ A1a1 ϩ Á ϩ ANaN ϩ B1b1 ϩ Á ϩ BNbN).

؊p

With these expressions, (4) becomes
Eϭ
(5)

Ύ

p

؊p

N

f 2 dx Ϫ 2p c 2A0 a0 ϩ a (An an ϩ Bn bn) d
nϭ1

N

ϩ p c 2A02 ϩ a (An2 ϩ Bn2) d .
nϭ1

We now take An ϭ an and Bn ϭ bn in (2). Then in (5) the second line cancels half of the
integral-free expression in the first line. Hence for this choice of the coefficients of F the
square error, call it E*, is

(6)

E* ϭ

Ύ

p

؊p

N

f 2 dx Ϫ p c 2a02 ϩ a (an2 ϩ bn2) d .
nϭ1

We finally subtract (6) from (5). Then the integrals drop out and we get terms
A2n Ϫ 2Anan ϩ a 2n ϭ (An Ϫ an)2 and similar terms (Bn Ϫ bn)2:
N

E Ϫ E* ϭ p e 2(A0 Ϫ a0)2 ϩ a [(An Ϫ an)2 ϩ (Bn Ϫ bn)2] f .
nϭ1

Since the sum of squares of real numbers on the right cannot be negative,
E Ϫ E* м 0,

thus

E м E*,

and E ϭ E* if and only if A0 ϭ a0, Á , BN ϭ bN. This proves the following fundamental
minimum property of the partial sums of Fourier series.

c11-a.qxd

10/30/10

1:25 PM

Page 497

SEC. 11.4 Approximation by Trigonometric Polynomials

THEOREM 1

497

Minimum Square Error

The square error of F in (2) (with fixed N) relative to f on the interval Ϫp Ϲ x Ϲ p
is minimum if and only if the coefficients of F in (2) are the Fourier coefficients of f.
This minimum value E* is given by (6).

From (6) we see that E* cannot increase as N increases, but may decrease. Hence with
increasing N the partial sums of the Fourier series of f yield better and better approximations to f, considered from the viewpoint of the square error.
Since E* м 0 and (6) holds for every N, we obtain from (6) the important Bessel’s
inequality
2a02

(7)

ؕ

ϩ a

(an2

ϩ

1

Ϲp

bn2)

nϭ1

Ύ

p

f (x)2 dx

؊p

for the Fourier coefficients of any function f for which integral on the right exists. (For
F. W. Bessel see Sec. 5.5.)
It can be shown (see [C12] in App. 1) that for such a function f, Parseval’s theorem holds;
that is, formula (7) holds with the equality sign, so that it becomes Parseval’s identity3
2a02

(8)

ؕ

ϩ a

(an2

ϩ

nϭ1

EXAMPLE 1

bn2)

1

ϭp

Ύ

p

f (x)2 dx.

؊p

Minimum Square Error for the Sawtooth Wave
Compute the minimum square error E* of F (x) with N ϭ 1, 2, Á , 10, 20, Á , 100 and 1000 relative to
f (x) ϭ x ϩ p

(Ϫp Ͻ x Ͻ p)

on the interval Ϫp Ϲ x Ϲ p.
1

1

(Ϫ1)Nϩ1

2

3

N

Solution. F (x) ϭ p ϩ 2 (sin x Ϫ sin 2x ϩ sin 3x Ϫ ϩ Á ϩ
Sec. 11.3. From this and (6),
E* ϭ

Ύ

p

؊p

N

(x ϩ p)2 dx Ϫ p a2p2 ϩ 4 a

nϭ1

1
n2

sin Nx) by Example 3 in

b.

Numeric values are:
2π

π

–π

0

π x

Fig. 279. F with
N ϭ 20 in Example 1

N

E*

N

E*

N

E*

N

E*

1
2
3
4
5

8.1045
4.9629
3.5666
2.7812
2.2786

6
7
8
9
10

1.9295
1.6730
1.4767
1.3216
1.1959

20
30
40
50
60

0.6129
0.4120
0.3103
0.2488
0.2077

70
80
90
100
1000

0.1782
0.1561
0.1389
0.1250
0.0126

3
MARC ANTOINE PARSEVAL (1755–1836), French mathematician. A physical interpretation of the identity
follows in the next section.

c11-a.qxd

10/30/10

498

1:25 PM

Page 498

CHAP. 11 Fourier Analysis
F ϭ S1, S2, S3 are shown in Fig. 269 in Sec. 11.2, and F ϭ S20 is shown in Fig. 279. Although ƒ f (x) Ϫ F (x) ƒ
is large at Ϯp (how large?), where f is discontinuous, F approximates f quite well on the whole interval, except
near Ϯp, where “waves” remain owing to the “Gibbs phenomenon,” which we shall discuss in the next section.
Can you think of functions f for which E* decreases more quickly with increasing N?
᭿

PROBLEM SET 11.4
factors on which the decrease of E* with N depends.
For each function considered find the smallest N such
that E* Ͻ 0.1.

1. CAS Problem. Do the numeric and graphic work in
Example 1 in the text.

2–5

MINIMUM SQUARE ERROR

Find the trigonometric polynomial F (x) of the form (2) for
which the square error with respect to the given f (x) on the
interval Ϫp Ͻ x Ͻ p is minimum. Compute the minimum
value for N ϭ 1, 2, Á , 5 (or also for larger values if you
have a CAS).
2. f (x) ϭ x (Ϫp Ͻ x Ͻ p)
3. f (x) ϭ ƒ x ƒ (Ϫp Ͻ x Ͻ p)
4. f (x) ϭ x 2 (Ϫp Ͻ x Ͻ p)
Ϫ1 if Ϫp Ͻ x Ͻ 0
5. f (x) ϭ b
1 if
0ϽxϽp
6. Why are the square errors in Prob. 5 substantially larger
than in Prob. 3?
7. f (x) ϭ x 3 (Ϫp Ͻ x Ͻ p)
8. f (x) ϭ ƒ sin x ƒ (Ϫp Ͻ x Ͻ p), full-wave rectifier
9. Monotonicity. Show that the minimum square error
(6) is a monotone decreasing function of N. How can
you use this in practice?
10. CAS EXPERIMENT. Size and Decrease of E*.
Compare the size of the minimum square error E* for
functions of your choice. Find experimentally the

11.5

PARSEVALS’S IDENTITY

11–15

Using (8), prove that the series has the indicated sum.
Compute the first few partial sums to see that the convergence
is rapid.
2
Á ϭ p ϭ 1.233700550
ϩ
32
52
8
Use Example 1 in Sec. 11.1.

11. 1 ϩ

1

ϩ

1

p4
ϩÁϭ
ϭ 1.082323234
2
3
90
Use Prob. 14 in Sec. 11.1.

12. 1 ϩ

1

4

ϩ

1

4

4
Á ϭ p ϭ 1.014678032
ϩ
96
34
54
74
Use Prob. 17 in Sec. 11.1.

13. 1 ϩ

14.

Ύ

p

Ύ

p

1

ϩ

1

ϩ

cos4 x dx ϭ

3p
4

cos6 x dx ϭ

5p
8

؊p

15.

1

؊p

Sturm–Liouville Problems.
Orthogonal Functions
The idea of the Fourier series was to represent general periodic functions in terms of
cosines and sines. The latter formed a trigonometric system. This trigonometric system
has the desirable property of orthogonality which allows us to compute the coefficient of
the Fourier series by the Euler formulas.
The question then arises, can this approach be generalized? That is, can we replace the
trigonometric system of Sec. 11.1 by other orthogonal systems (sets of other orthogonal
functions)? The answer is “yes” and will lead to generalized Fourier series, including the
Fourier–Legendre series and the Fourier–Bessel series in Sec. 11.6.
To prepare for this generalization, we first have to introduce the concept of a Sturm–
Liouville Problem. (The motivation for this approach will become clear as you read on.)
Consider a second-order ODE of the form

c11-a.qxd

10/30/10

1:25 PM

Page 499

SEC. 11.5 Sturm–Liouville Problems. Orthogonal Functions

(1)

499

[ p (x)y r ] r ϩ [ q (x) ϩ lr (x)]y ϭ 0

on some interval a Ϲ x Ϲ b, satisfying conditions of the form
(2)

(a)

k 1 y ϩ k 2 y r ϭ 0 at x ϭ a

(b)

l 1 y ϩ l 2 y r ϭ 0 at x ϭ b.

Here l is a parameter, and k 1, k 2, l 1, l 2 are given real constants. Furthermore, at least one
of each constant in each condition (2) must be different from zero. (We will see in Example
1 that, if p(x) ϭ r(x) ϭ 1 and q(x) ϭ 0, then sin 1lx and cos 1lx satisfy (1) and constants
can be found to satisfy (2).) Equation (1) is known as a Sturm–Liouville equation.4
Together with conditions 2(a), 2(b) it is know as the Sturm–Liouville problem. It is an
example of a boundary value problem.
A boundary value problem consists of an ODE and given boundary conditions
referring to the two boundary points (endpoints) x ϭ a and x ϭ b of a given interval
a Ϲ x Ϲ b.
The goal is to solve these type of problems. To do so, we have to consider

Eigenvalues, Eigenfunctions
Clearly, y ϵ 0 is a solution—the “trivial solution”—of the problem (1), (2) for any l
because (1) is homogeneous and (2) has zeros on the right. This is of no interest. We want
to find eigenfunctions y (x), that is, solutions of (1) satisfying (2) without being identically
zero. We call a number l for which an eigenfunction exists an eigenvalue of the Sturm–
Liouville problem (1), (2).
Many important ODEs in engineering can be written as Sturm–Liouville equations. The
following example serves as a case in point.
EXAMPLE 1

Trigonometric Functions as Eigenfunctions. Vibrating String
Find the eigenvalues and eigenfunctions of the Sturm–Liouville problem
(3)

y s ϩ ly ϭ 0,

y (0) ϭ 0, y(p) ϭ 0.

This problem arises, for instance, if an elastic string (a violin string, for example) is stretched a little and fixed
at its ends x ϭ 0 and x ϭ p and then allowed to vibrate. Then y (x) is the “space function” of the deflection
u (x, t) of the string, assumed in the form u (x, t) ϭ y (x)w (t), where t is time. (This model will be discussed in
great detail in Secs, 12.2–12.4.)

Solution. From (1) nad (2) we see that p ϭ 1, q ϭ 0, r ϭ 1 in (1), and a ϭ 0, b ϭ p, k 1 ϭ l 1 ϭ 1,
k 2 ϭ l 2 ϭ 0 in (2). For negative l ϭ Ϫ␯2 a general solution of the ODE in (3) is y (x) ϭ c1e␯x ϩ c2e؊␯x. From
the boundary conditions we obtain c1 ϭ c2 ϭ 0, so that y ϵ 0, which is not an eigenfunction. For l ϭ 0 the
situation is similar. For positive l ϭ ␯2 a general solution is
y(x) ϭ A cos ␯x ϩ B sin ␯x.

4
JACQUES CHARLES FRANÇOIS STURM (1803–1855) was born and studied in Switzerland and then
moved to Paris, where he later became the successor of Poisson in the chair of mechanics at the Sorbonne (the
University of Paris).
JOSEPH LIOUVILLE (1809–1882), French mathematician and professor in Paris, contributed to various
fields in mathematics and is particularly known by his important work in complex analysis (Liouville’s theorem;
Sec. 14.4), special functions, differential geometry, and number theory.

c11-a.qxd

10/30/10

500

1:25 PM

Page 500

CHAP. 11 Fourier Analysis
From the first boundary condition we obtain y (0) ϭ A ϭ 0. The second boundary condition then yields
y (p) ϭ B sin ␯p ϭ 0,

␯ ϭ 0, Ϯ 1, Ϯ 2, Á .

thus

For ␯ ϭ 0 we have y ϵ 0. For l ϭ ␯2 ϭ 1, 4, 9, 16, Á , taking B ϭ 1, we obtain
y (x) ϭ sin ␯x

(␯ ϭ 2l ϭ 1, 2, Á ).

Hence the eigenvalues of the problem are l ϭ ␯2, where ␯ ϭ 1, 2, Á , and corresponding eigenfunctions are
y(x) ϭ sin ␯x, where ␯ ϭ 1, 2 Á .
᭿

Note that the solution to this problem is precisely the trigonometric system of the Fourier
series considered earlier. It can be shown that, under rather general conditions on the
functions p, q, r in (1), the Sturm–Liouville problem (1), (2) has infinitely many eigenvalues.
The corresponding rather complicated theory can be found in Ref. [All] listed in App. 1.
Furthermore, if p, q, r, and p r in (1) are real-valued and continuous on the interval
a Ϲ x Ϲ b and r is positive throughout that interval (or negative throughout that interval),
then all the eigenvalues of the Sturm–Liouville problem (1), (2) are real. (Proof in App. 4.)
This is what the engineer would expect since eigenvalues are often related to frequencies,
energies, or other physical quantities that must be real.
The most remarkable and important property of eigenfunctions of Sturm–Liouville
problems is their orthogonality, which will be crucial in series developments in terms of
eigenfunctions, as we shall see in the next section. This suggests that we should next
consider orthogonal functions.

Orthogonal Functions
Functions y1(x), y2 (x), Á defined on some interval a Ϲ x Ϲ b are called orthogonal on this
interval with respect to the weight function r (x) Ͼ 0 if for all m and all n different from m,
b

(4)

(ym, yn) ϭ

Ύ r (x) y

m (x) yn (x)

dx ϭ 0

(m

n).

a

(ym, yn) is a standard notation for this integral. The norm ʈymʈ of ym is defined by
b

(5)

ʈ ym ʈ ϭ 2(ym, ym) ϭ

Ύ r (x) y
G

2
m (x)

dx.

a

Note that this is the square root of the integral in (4) with n ϭ m.
The functions y1, y2, Á are called orthonormal on a Ϲ x Ϲ b if they are orthogonal
on this interval and all have norm 1. Then we can write (4), (5) jointly by using the
Kronecker symbol5 dmn, namely,
b

( ym , yn ) ϭ

Ύ r (x) y

m (x) yn (x)

a

5

dx ϭ dmn ϭ e

0

if

m

n

1

if

m ϭ n.

LEOPOLD KRONECKER (1823–1891). German mathematician at Berlin University, who made important
contributions to algebra, group theory, and number theory.

c11-a.qxd

10/30/10

1:25 PM

Page 501

SEC. 11.5 Sturm–Liouville Problems. Orthogonal Functions

501

If r (x) ϭ 1, we more briefly call the functions orthogonal instead of orthogonal with
respect to r (x) ϭ 1; similarly for orthognormality. Then
b

b

(ym, yn) ϭ

Ύy

m (x) yn (x)

dx ϭ 0 (m

ʈymʈ ϭ 2(ym, yn) ϭ

n),

a

Ύy
G

2
m(x)

dx.

a

The next example serves as an illustration of the material on orthogonal functions just
discussed.
EXAMPLE 2

Orthogonal Functions. Orthonormal Functions. Notation
The functions ym (x) ϭ sin mx, m ϭ 1, 2, Á form an orthogonal set on the interval Ϫp Ϲ x Ϲ p, because for
m n we obtain by integration [see (11) in App. A3.1]
( ym, yn ) ϭ

Ύ

p

؊p

sin mx sin nx dx ϭ

2Ύ
1

p

cos (m Ϫ n)x dx Ϫ

؊p

2Ύ
1

p

؊p

cos (m ϩ n)x dx ϭ 0, (m

n).

The norm ʈ ym ʈ ϭ 1( ym, ym) equals 1p because

Ύ

ʈ ym ʈ2 ϭ ( ym, ym ) ϭ

p

sin2 mx dx ϭ p

(m ϭ 1, 2, Á )

؊p

Hence the corresponding orthonormal set, obtained by division by the norm, is
sin x
1p

,

sin 2x
1p

sin 3x

,

1p

,

᭿

Á.

Theorem 1 shows that for any Sturm–Liouville problem, the eigenfunctions associated with
these problems are orthogonal. This means, in practice, if we can formulate a problem as a
Sturm–Liouville problem, then by this theorem we are guaranteed orthogonality.
THEOREM 1

Orthogonality of Eigenfunctions of Sturm–Liouville Problems

Suppose that the functions p, q, r, and p r in the Sturm–Liouville equation (1) are
real-valued and continuous and r (x) Ͼ 0 on the interval a Ϲ x Ϲ b. Let ym (x) and
yn (x) be eigenfunctions of the Sturm–Liouville problem (1), (2) that correspond to
different eigenvalues lm and ln , respectively. Then ym, yn are orthogonal on that
interval with respect to the weight function r, that is,
b

(6)

(ym, yn) ϭ

Ύ r (x)y

m (x)yn (x)

dx ϭ 0

(m

n).

a

If p (a) ϭ 0, then (2a) can be dropped from the problem. If p(b) ϭ 0, then (2b)
can be dropped. [It is then required that y and y r remain bounded at such a point,
and the problem is called singular, as opposed to a regular problem in which (2)
is used.]
If p(a) ϭ p(b), then (2) can be replaced by the “periodic boundary conditions”
(7)

y(a) ϭ y(b),

y r (a) ϭ y r (b).

The boundary value problem consisting of the Sturm–Liouville equation (1) and the periodic
boundary conditions (7) is called a periodic Sturm–Liouville problem.

c11-a.qxd

10/30/10

1:25 PM

502

Page 502

CHAP. 11 Fourier Analysis

PROOF

By assumption, ym and yn satisfy the Sturm–Liouville equations

r ) r ϩ (q ϩ lmr) ym ϭ 0
( pym
(py nr ) r ϩ (q ϩ lnr)yn ϭ 0
respectively. We multiply the first equation by yn, the second by Ϫym, and add,
(lm Ϫ ln)rym yn ϭ ym( pynr ) r Ϫ yn( py rm) r ϭ [( py nr ) ym Ϫ [( py rm) yn] r
where the last equality can be readily verified by performing the indicated differentiation
of the last expression in brackets. This expression is continuous on a Ϲ x Ϲ b since p and
p r are continuous by assumption and ym, yn are solutions of (1). Integrating over x from
a to b, we thus obtain
b

(8)

(lm Ϫ ln)

Ύ ry

m yn

a

b

dx ϭ [ p(y rn ym Ϫ y rm yn)]a

(a Ͻ b).

The expression on the right equals the sum of the subsequent Lines 1 and 2,
(9)

p(b)[ynr (b) ym(b) Ϫ y rm (b) yn(b)]

(Line 1)

Ϫp (a)[ y nr (a)ym (a) Ϫ y m
r (a)yn (a)]

(Line 2).

Hence if (9) is zero, (8) with lm Ϫ ln 0 implies the orthogonality (6). Accordingly,
we have to show that (9) is zero, using the boundary conditions (2) as needed.
Case 1. p (a) ‫ ؍‬p (b) ‫ ؍‬0. Clearly, (9) is zero, and (2) is not needed.
Case 2. p (a) 0, p (b) ‫ ؍‬0. Line 1 of (9) is zero. Consider Line 2. From (2a) we have
k1 yn(a) ϩ k 2 ynr (a) ϭ 0,
k1 ym(a) ϩ k 2 y m
r (a) ϭ 0.
Let k 2

0. We multiply the first equation by ym (a), the last by Ϫyn (a) and add,
k 2[ynr (a)ym(a) Ϫ y m
r (a)yn(a)] ϭ 0.

This is k 2 times Line 2 of (9), which thus is zero since k 2
by assumption, and the argument of proof is similar.

0. If k 2 ϭ 0, then k 1

0

Case 3. p(a) ‫ ؍‬0, p(b) 0. Line 2 of (9) is zero. From (2b) it follows that Line 1 of (9)
is zero; this is similar to Case 2.
Case 4. p(a) 0, p(b) 0. We use both (2a) and (2b) and proceed as in Cases 2 and 3.
Case 5. p(a) ‫ ؍‬p(b). Then (9) becomes
p(b)[ ynr (b)ym(b) Ϫ ym
r (b)yn(b) Ϫ ynr (a)ym (a) ϩ ym
r (a)yn(a)].
The expression in brackets [ Á ] is zero, either by (2) used as before, or more directly by
(7). Hence in this case, (7) can be used instead of (2), as claimed. This completes the
᭿
proof of Theorem 1.
EXAMPLE 3

Application of Theorem 1. Vibrating String
The ODE in Example 1 is a Sturm–Liouville equation with p ϭ 1, q ϭ 0, and r ϭ 1. From Theorem 1 it follows
᭿
that the eigenfunctions ym ϭ sin mx (m ϭ 1, 2, Á ) are orthogonal on the interval 0 Ϲ x Ϲ p.

c11-a.qxd

10/30/10

1:25 PM

Page 503

SEC. 11.5 Sturm–Liouville Problems. Orthogonal Functions

503

Example 3 confirms, from this new perspective, that the trigonometric system underlying
the Fourier series is orthogonal, as we knew from Sec. 11.1.
EXAMPLE 4

Application of Theorem 1. Orthogonlity of the Legendre Polynomials
Legendre’s equation (1 Ϫ x 2) y s Ϫ 2xy r ϩ n (n ϩ 1) y ϭ 0 may be written
[(1 Ϫ x 2) y r ] r ϩ ly ϭ 0

l ϭ n (n ϩ 1).

Hence, this is a Sturm–Liouville equation (1) with p ϭ 1 Ϫ x 2, q ϭ 0, and r ϭ 1. Since p (Ϫ1) ϭ p (1) ϭ 0, we
need no boundary conditions, but have a “singular” Sturm–Liouville problem on the interval Ϫ1 Ϲ x Ϲ 1. We
know that for n ϭ 0, 1, Á , hence l ϭ 0, 1 # 2, 2 # 3, Á , the Legendre polynomials Pn (x) are solutions of the
problem. Hence these are the eigenfunctions. From Theorem 1 it follows that they are orthogonal on that interval,
that is,
(10)

Ύ

1

Pm (x)Pn (x) dx ϭ 0

(m

n).

᭿

Ϫ1

What we have seen is that the trigonometric system, underlying the Fourier series, is
a solution to a Sturm–Liouville problem, as shown in Example 1, and that this
trigonometric system is orthogonal, which we knew from Sec. 11.1 and confirmed in
Example 3.

PROBLEM SET 11.5
1. Proof of Theorem 1. Carry out the details in Cases 3
and 4.

2–6

set p ϭ exp ( ͐ f dx), q ϭ pg, r ϭ hp. Why would you
do such a transformation?

ORTHOGONALITY

2. Normalization of eigenfunctions ym of (1), (2) means
that we multiply ym by a nonzero constant cm such that
cmym has norm 1. Show that z m ϭ cym with any c 0
is an eigenfunction for the eigenvalue corresponding
to ym.
3. Change of x. Show that if the functions y0 (x), y1 (x), Á
form an orthogonal set on an interval a Ϲ x Ϲ b (with
r (x) ϭ 1), then the functions y0 (ct ϩ k), y1 (ct ϩ k),
Á , c Ͼ 0, form an orthogonal set on the interval
(a Ϫ k)>c Ϲ t Ϲ (b Ϫ k)>c.
4. Change of x. Using Prob. 3, derive the orthogonality
of 1, cos px, sin px, cos 2px, sin 2px, Á on
Ϫ1 Ϲ x Ϲ 1 (r (x) ϭ 1) from that of 1, cos x, sin x,
cos 2x, sin 2x, Á on Ϫp Ϲ x Ϲ p.
5. Legendre polynomials. Show that the functions
Pn(cos u), n ϭ 0, 1, Á , from an orthogonal set on the
interval 0 Ϲ u Ϲ p with respect to the weight function
sin u.
6. Tranformation to Sturm–Liouville form. Show that
y s ϩ fy r ϩ (g ϩ lh) y ϭ 0 takes the form (1) if you

7–15

STURM–LIOUVILLE PROBLEMS

Find the eigenvalues and eigenfunctions. Verify orthogonality. Start by writing the ODE in the form (1), using
Prob. 6. Show details of your work.
7. y s ϩ ly ϭ 0, y (0) ϭ 0,

y (10) ϭ 0

8. y s ϩ ly ϭ 0, y (0) ϭ 0,

y (L) ϭ 0

9. y s ϩ ly ϭ 0,
10. y s ϩ ly ϭ 0,

y (0) ϭ 0,

y r(L) ϭ 0

y (0) ϭ y (1),

y r(0) ϭ y r(1)

11. ( y r >x) r ϩ (l ϩ 1)y>x ϭ 0, y (1) ϭ 0, y (ep) ϭ 0.
(Set x ϭ et.)
3

12. y s Ϫ 2y r ϩ (l ϩ 1) y ϭ 0, y (0) ϭ 0, y (1) ϭ 0
13. y s ϩ 8y r ϩ (l ϩ 16) y ϭ 0, y (0) ϭ 0, y (p) ϭ 0
14. TEAM PROJECT. Special Functions. Orthogonal
polynomials play a great role in applications. For
this reason, Legendre polynomials and various other
orthogonal polynomials have been studied extensively;
see Refs. [GenRef1], [GenRef10] in App. 1. Consider
some of the most important ones as follows.

c11-a.qxd

10/30/10

1:25 PM

504

Page 504

CHAP. 11 Fourier Analysis
(a) Chebyshev polynomials6 of the first and second
kind are defined by

that Tn (x), n ϭ 0, 1, 2, 3, satisfy the Chebyshev
equation

Tn (x) ϭ cos (n arccos x)

(1 Ϫ x 2)y s Ϫ xy r ϩ n 2y ϭ 0.

Un (x) ϭ

sin [(n ϩ 1) arccos x]
21 Ϫ x

(b) Orthogonality on an infinite interval: Laguerre
polynomials7 are defined by L 0 ϭ 1, and

2

L n(x) ϭ

respectively, where n ϭ 0, 1, Á . Show that
T0 ϭ 1,

T1(x) ϭ x,

T2(x) ϭ 2x 2 Ϫ 1.

T3(x) ϭ 4x 3 Ϫ 3x,
U0 ϭ 1,

U1(x) ϭ 2x,

U2(x) ϭ 4x 2 Ϫ 1,

n
n ؊x
ex d (x e )
,
n!
dx n

n ϭ 1, 2, Á .

Show that
L n(x) ϭ 1 Ϫ x,

L 2 (x) ϭ 1 Ϫ 2x ϩ x 2>2,

U3(x) ϭ 8x 3 Ϫ 4x.

L 3 (x) ϭ 1 Ϫ 3x ϩ 3x 2>2 Ϫ x 3>6.

Show that the Chebyshev polynomials Tn(x) are
orthogonal on the interval Ϫ1 Ϲ x Ϲ 1 with respect
to the weight function r (x) ϭ 1> 21 Ϫ x 2 . (Hint.
To evaluate the integral, set arccos x ϭ u.) Verify

Prove that the Laguerre polynomials are orthogonal on
the positive axis 0 Ϲ x Ͻ ϱ with respect to the weight
function r (x) ϭ e؊x. Hint. Since the highest power in
L m is x m, it suffices to show that ͐ e؊xx kL n dx ϭ 0
for k Ͻ n. Do this by k integrations by parts.

11.6

Orthogonal Series.
Generalized Fourier Series
Fourier series are made up of the trigonometric system (Sec. 11.1), which is orthogonal,
and orthogonality was essential in obtaining the Euler formulas for the Fourier coefficients.
Orthogonality will also give us coefficient formulas for the desired generalized Fourier
series, including the Fourier–Legendre series and the Fourier–Bessel series. This generalization is as follows.
Let y0, y1, y2, Á be orthogonal with respect to a weight function r (x) on an interval
a Ϲ x Ϲ b, and let f (x) be a function that can be represented by a convergent series
ϱ

(1)

f (x) ϭ a am ym (x) ϭ a0 y0 (x) ϩ a1 y1 (x) ϩ Á .
mϭ0

This is called an orthogonal series, orthogonal expansion, or generalized Fourier series.
If the ym are the eigenfunctions of a Sturm–Liouville problem, we call (1) an eigenfunction
expansion. In (1) we use again m for summation since n will be used as a fixed order of
Bessel functions.
Given f (x), we have to determine the coefficients in (1), called the Fourier constants
of f (x) with respect to y0, y1, Á . Because of the orthogonality, this is simple. Similarly
to Sec. 11.1, we multiply both sides of (1) by r (x)yn (x) (n fixed ) and then integrate on
6
PAFNUTI CHEBYSHEV (1821–1894), Russian mathematician, is known for his work in approximation
theory and the theory of numbers. Another transliteration of the name is TCHEBICHEF.
7
EDMOND LAGUERRE (1834–1886), French mathematician, who did research work in geometry and in
the theory of infinite series.

c11-a.qxd

10/30/10

1:25 PM

Page 505

SEC. 11.6 Orthogonal Series. Generalized Fourier Series

505

both sides from a to b. We assume that term-by-term integration is permissible. (This is
justified, for instance, in the case of “uniform convergence,” as is shown in Sec. 15.5.)
Then we obtain
( f, yn ) ϭ

Ύ

b

r fyn dx ϭ

a

Ύ

b

a

ϱ

ϱ

mϭ 0

mϭ 0

Ύ

r a a am ym b yn dx ϭ a am

b

ϱ

rym yn dx ϭ a am (ym, yn).
mϭ 0

a

Because of the orthogonality all the integrals on the right are zero, except when m ϭ n.
Hence the whole infinite series reduces to the single term
a n (yn, yn ) ϭ an ʈ y n ʈ 2.

( f, yn ) ϭ an ʈ yn ʈ2.

Thus

Assuming that all the functions yn have nonzero norm, we can divide by ʈynʈ2; writing again
m for n, to be in agreement with (1), we get the desired formula for the Fourier constants

am ϭ

(2)

( f, ym)
ʈ ym ʈ 2

b

Ύ r (x) f (x)y

1

ϭ

m (x)

ʈ ym ʈ 2

dx

(n ϭ 0, 1, Á ).

a

This formula generalizes the Euler formulas (6) in Sec. 11.1 as well as the principle of
their derivation, namely, by orthogonality.
EXAMPLE 1

Fourier–Legendre Series
A Fourier–Legendre series is an eigenfunction expansion
ؕ

f (x) ϭ a amPm (x) ϭ a0P0 ϩ a1P1 (x) ϩ a2P2 (x) ϩ Á ϭ a0 ϩ a1x ϩ a2 ( 32 x 2 Ϫ 12 ) ϩ Á
mϭ0

in terms of Legendre polynomials (Sec. 5.3). The latter are the eigenfunctions of the Sturm–Liouville problem
in Example 4 of Sec. 11.5 on the interval Ϫ1 Ϲ x Ϲ 1. We have r (x) ϭ 1 for Legendre’s equation, and (2)
gives

am ϭ

(3)

2m ϩ 1
2

Ύ

1

m ϭ 0, 1, Á

f (x)Pm (x) dx,

Ϫ1

because the norm is

ʈ Pm ʈ ϭ

(4)

G

Ύ

1

Pm (x)2 dx ϭ

؊1

2

(m ϭ 0, 1, Á )

B 2m ϩ 1

as we state without proof. The proof of (4) is tricky; it uses Rodrigues’s formula in Problem Set 5.2 and a
reduction of the resulting integral to a quotient of gamma functions.
For instance, let f (x) ϭ sin px. Then we obtain the coefficients
am ϭ

2m ϩ 1
2

1

Ύ (sin px)P

m (x)

؊1

dx,

thus

a1 ϭ

2 Ύ
3

1

x sin px dx ϭ

؊1

3

p

ϭ 0.95493,

etc.

c11-a.qxd

11/1/10

10:39 PM

506

Page 506

CHAP. 11 Fourier Analysis
Hence the Fourier–Legendre series of sin px is
sin px ϭ 0.95493P1 (x) Ϫ 1.15824P3 (x) ϩ 0.21929P5 (x) Ϫ 0.01664P7 (x) ϩ 0.00068P9 (x)
Ϫ 0.00002P11 (x) ϩ Á .
The coefficient of P13 is about 3 # 10؊7. The sum of the first three nonzero terms gives a curve that practically
coincides with the sine curve. Can you see why the even-numbered coefficients are zero? Why a3 is the absolutely
biggest coefficient?
᭿

EXAMPLE 2

Fourier–Bessel Series
These series model vibrating membranes (Sec. 12.9) and other physical systems of circular symmetry. We derive
these series in three steps.
Step 1. Bessel’s equation as a Sturm–Liouville equation. The Bessel function Jn (x) with fixed integer n м 0
satisfies Bessel’s equation (Sec. 5.5)

##

#

ϳ
ϳ
ϳ2
2
ϳ
xϳ2J n ( xϳ) ϩ xJ
n ( x ) ϩ ( x Ϫ n )Jn( x ) ϭ 0
ϳ
2
ϳ2
ϳ
ϳ
where Jn ϭ dJn>d
by the chain rule, Jn ϭ dJn>d xϳ ϭ
## x and J n2 ϭ d Jn>d x . We set x ϭ kx. Then x ϭ x >k and
2
(dJn>dx)/k and J n ϭ Jns >k . In the first two terms of Bessel’s equation, k and k drop out and we obtain

#

##

#

x 2Jns (kx) ϩ xJ nr (kx) ϩ (k 2x 2 Ϫ n 2)Jn(kx) ϭ 0.
Dividing by x and using (xJnr (kx)) r ϭ xJ ns (kx) ϩ Jnr (kx) gives the Sturm–Liouville equation
[xJnr (kx)] r ϩ aϪ

(5)

n2
ϩ lxb Jn(kx) ϭ 0
x

l ϭ k2

with p (x) ϭ x, q (x) ϭ Ϫn 2>x, r (x) ϭ x, and parameter l ϭ k 2. Since p (0) ϭ 0, Theorem 1 in Sec. 11.5
implies orthogonality on an interval 0 Ϲ x Ϲ R (R given, fixed) of those solutions Jn(kx) that are zero at
x ϭ R, that is,
Jn(kR) ϭ 0

(6)

(n fixed).

Note that q (x) ϭ Ϫn 2>x is discontinuous at 0, but this does not affect the proof of Theorem 1.

Step 2. Orthogonality. It can be shown (see Ref. [A13]) that Jn( ϳx ) has infinitely many zeros, say,
ෂ
x ϭ an,1 Ͻ an,2 Ͻ Á (see Fig. 110 in Sec. 5.4 for n ϭ 0 and 1). Hence we must have
kR ϭ an,m

(7)

thus

k n,m ϭ an,m>R

(m ϭ 1, 2, Á ).

This proves the following orthogonality property.

THEOREM 1

Orthogonality of Bessel Functions

For each fixed nonnegative integer n the sequence of Bessel functions of the first
kind Jn(k n,1x), Jn(k n,2x), Á with k n,m as in (7) forms an orthogonal set on the
interval 0 Ϲ x Ϲ R with respect to the weight function r (x) ϭ x, that is,
(8)

Ύ

R

xJn (k n,mx)Jn(k n, jx) dx ϭ 0

(j

m, n fixed).

0

Hence we have obtained infinitely many orthogonal sets of Bessel functions, one for each of J0, J1, J2, Á .
Each set is orthogonal on an interval 0 Ϲ x Ϲ R with a fixed positive R of our choice and with respect to
the weight x. The orthogonal set for Jn is Jn(k n,1x), Jn(k n,2x), Jn(k n,3x), Á , where n is fixed and k n,m is
given by (7).

c11-a.qxd

10/30/10

1:25 PM

Page 507

SEC. 11.6 Orthogonal Series. Generalized Fourier Series

507

Step 3. Fourier–Bessel series. The Fourier–Bessel series corresponding to Jn (n fixed) is
ؕ

f (x) ϭ a amJn(k n,mx) ϭ a1Jn(k n,1x) ϩ a2Jn(k n,2x) ϩ a3Jn(k n,3x) ϩ Á

(9)

(n fixed).

mϭ1

The coefficients are (with an,m ϭ k n,mR)

am ϭ

(10)

Ύ

2

R

R2J 2nϩ1(an,m) 0

m ϭ 1, 2, Á

x f (x) Jn(k n,mx) dx,

because the square of the norm is
ʈ Jn(k n,mx) ʈ 2 ϭ

(11)

R

Ύ

xJn2 (k n,mx) dx ϭ

R2

0

2

J 2nϩ1(k n,mR)

᭿

as we state without proof (which is tricky; see the discussion beginning on p. 576 of [A13]).

EXAMPLE 3

Special Fourier–Bessel Series
For instance, let us consider f (x) ϭ 1 Ϫ x 2 and take R ϭ 1 and n ϭ 0 in the series (9), simply writing l for
a0,m. Then k n,m ϭ a0,m ϭ l ϭ 2.405, 5.520, 8.654, 11.792, etc. (use a CAS or Table A1 in App. 5). Next we
calculate the coefficients am by (10)
am ϭ

Ύ
J (l)
2

2
1

1

x(1 Ϫ x 2)J0(lx) dx.

0

This can be integrated by a CAS or by formulas as follows. First use [xJ1(lx)] r ϭ lxJ0(lx) from Theorem 1 in
Sec. 5.4 and then integration by parts,
am ϭ

Ύ
J (l)
2

2
1

1

0

x(1 Ϫ x 2)J0(lx) dx ϭ

1
1
1
(1 Ϫ x 2)xJ1(lx) ` Ϫ
2
l
l
0
J1 (l)

2

c

Ύ

1

0

xJ1(lx)(Ϫ2x) dx d .

The integral-free part is zero. The remaining integral can be evaluated by [x 2J2(lx)] r ϭ lx 2J1(lx) from Theorem 1
in Sec. 5.4. This gives
am ϭ

4J2 (l)
l2J12 (l)

(l ϭ a0,m).

Numeric values can be obtained from a CAS (or from the table on p. 409 of Ref. [GenRef1] in App. 1, together
with the formula J2 ϭ 2x ؊1J1 Ϫ J0 in Theorem 1 of Sec. 5.4). This gives the eigenfunction expansion of 1 Ϫ x 2
in terms of Bessel functions J0, that is,
1 Ϫ x 2 ϭ 1.1081J0(2.405x) Ϫ 0.1398J0(5.520x) ϩ 0.0455J0(8.654x) Ϫ 0.0210J0(11.792x) ϩ Á.
A graph would show that the curve of 1 Ϫ x 2 and that of the sum of first three terms practically coincide.

᭿

Mean Square Convergence. Completeness
Ideas on approximation in the last section generalize from Fourier series to orthogonal series
(1) that are made up of an orthonormal set that is “complete,” that is, consists of “sufficiently
many” functions so that (1) can represent large classes of other functions (definition below).
In this connection, convergence is convergence in the norm, also called mean-square
convergence; that is, a sequence of functions f k is called convergent with the limit f if
(12*)

lim ʈ f Ϫ f ʈ ϭ 0;

k :ϱ

k

c11-a.qxd

10/30/10

508

1:25 PM

Page 508

CHAP. 11 Fourier Analysis

written out by (5) in Sec. 11.5 (where we can drop the square root, as this does not affect
the limit)
b

(12)

Ύ r (x)[ f (x) Ϫ f (x)]

lim

2

k

k :ϱ

dx ϭ 0.

a

Accordingly, the series (1) converges and represents f if
b

(13)

lim

k :ϱ

Ύ r (x)[s (x) Ϫ f (x)]

2

k

dx ϭ 0

a

where sk is the kth partial sum of (1).
k

sk(x) ϭ a am ym(x).

(14)

mϭ0

Note that the integral in (13) generalizes (3) in Sec. 11.4.
We now define completeness. An orthonormal set y0, y1, Á on an interval a Ϲ x Ϲ b
is complete in a set of functions S defined on a Ϲ x Ϲ b if we can approximate every
f belonging to S arbitrarily closely in the norm by a linear combination a0y0 ϩ
a1y1 ϩ Á ϩ akyk, that is, technically, if for every P Ͼ 0 we can find constants a0, Á , ak
(with k large enough) such that
ʈ f Ϫ (a0y0 ϩ Á ϩ akyk)ʈ Ͻ P.

(15)

Ref. [GenRef7] in App. 1 uses the more modern term total for complete.
We can now extend the ideas in Sec. 11.4 that guided us from (3) in Sec. 11.4 to Bessel’s
and Parseval’s formulas (7) and (8) in that section. Performing the square in (13) and
using (14), we first have (analog of (4) in Sec. 11.4)

Ύ

b

r (x)[sk (x) Ϫ f (x)]2 dx ϭ

a

Ύ

b

Ύ

b

rsk2 dx Ϫ 2

a

ϭ

a

Ύ

b

b

rfsk dx ϩ

a

Ύ rf

2

dx

a

k

k

2

r c a am ym d dx Ϫ 2 a am
mϭ0

mϭ0

Ύ

b

a

rfym dx ϩ

Ύ

b

rf 2 dx.

a

l, and
The first integral on the right equals ga 2m because ͐ rymyl dx ϭ 0 for m
2
2
ry
dx
ϭ
1
a
,
ʈ
y
ʈ
ϭ 1.
.
In
the
second
sum
on
the
right,
the
integral
equals
by
(2)
with
͐ m
m
m
Hence the first term on the right cancels half of the second term, so that the right side
reduces to (analog of (6) in Sec. 11.4)
k
2
Ϫ a am
ϩ
mϭ0

Ύ

b

rf 2 dx.

a

This is nonnegative because in the previous formula the integrand on the left is nonnegative
(recall that the weight r (x) is positive!) and so is the integral on the left. This proves the
important Bessel’s inequality (analog of (7) in Sec. 11.4)
k

(16)

2
2
a am Ϲ ʈ f ʈ ϭ
mϭ0

Ύ

b

a

r (x) f (x)2 dx

(k ϭ 1, 2, Á ),

c11-a.qxd

10/30/10

1:25 PM

Page 509

SEC. 11.6 Orthogonal Series. Generalized Fourier Series

509

Here we can let k : ϱ , because the left sides form a monotone increasing sequence that
is bounded by the right side, so that we have convergence by the familiar Theorem 1 in
App. A.3.3 Hence
ؕ

2
2
a am Ϲ ʈ f ʈ .

(17)

mϭ0

Furthermore, if y0, y1, Á is complete in a set of functions S, then (13) holds for every f
belonging to S. By (13) this implies equality in (16) with k : ϱ. Hence in the case of
completeness every f in S saisfies the so-called Parseval equality (analog of (8) in Sec. 11.4)
ؕ

(18)

a

2
am
ϭ

ʈfʈ ϭ
2

mϭ0

Ύ

b

r (x) f (x)2 dx.

a

As a consequence of (18) we prove that in the case of completeness there is no function
orthogonal to every function of the orthonormal set, with the trivial exception of a function
of zero norm:
THEOREM 2

Completeness

Let y0, y1, Á be a complete orthonormal set on a Ϲ x Ϲ b in a set of functions S.
Then if a function f belongs to S and is orthogonal to every ym , it must have norm
zero. In particular, if f is continuous, then f must be identically zero.

PROOF

Since f is orthogonal to every ym, the left side of (18) must be zero. If f is continuous,
then ʈ f ʈ ϭ 0 implies f (x) ϵ 0, as can be seen directly from (5) in Sec. 11.5 with f instead
of ym because r (x) Ͼ 0 by assumption.
᭿

PROBLEM SET 11.6
1–7

FOURIER–LEGENDRE SERIES
Showing the details, develop

63x 5 Ϫ 90x 3 ϩ 35x
(x ϩ 1)2
1 Ϫ x4
1, x, x 2, x 3, x 4
Prove that if f (x) is even (is odd, respectively), its
Fourier–Legendre series contains only Pm (x) with even
m (only Pm (x) with odd m, respectively). Give examples.
6. What can you say about the coefficients of the Fourier–
Legendre series of f (x) if the Maclaurin series of f (x)
contains only powers x 4m (m ϭ 0, 1, 2, Á )?
7. What happens to the Fourier–Legendre series of a
polynomial f (x) if you change a coefficient of f (x)?
Experiment. Try to prove your answer.

1.
2.
3.
4.
5.

8–13

CAS EXPERIMENT

FOURIER–LEGENDRE SERIES. Find and graph (on
common axes) the partial sums up to Sm0 whose graph
practically coincides with that of f (x) within graphical
accuracy. State m 0. On what does the size of m 0 seem to
depend?
8. f ( x) ϭ sin px
9. f ( x) ϭ sin 2px
10. f ( x) ϭ e؊x

2

11. f ( x) ϭ (1 ϩ x 2)؊1
12. f ( x) ϭ J0(a0,1 x), a0,1 ϭ the first positive zero
of J0( x)
13. f (x) ϭ J0(a0,2 x), a0,2 ϭ the second positive zero
of J0(x)

c11-a.qxd

10/30/10

1:25 PM

510

Page 510

CHAP. 11 Fourier Analysis

14. TEAM PROJECT. Orthogonality on the Entire Real
Axis. Hermite Polynomials.8 These orthogonal polynomials are defined by He0 (1) ϭ 1 and
(19)

>2

2

Hen (x) ϭ (Ϫ1)nex

dn

(e؊x >2),
2

dx n

n ϭ 1, 2, Á .

(22)

REMARK. As is true for many special functions, the
literature contains more than one notation, and one sometimes defines as Hermite polynomials the functions
d ne؊x

2

H n*(x) ϭ (Ϫ1)nex

dx n

.

This differs from our definition, which is preferred in
applications.
(a) Small Values of n. Show that
He1 (x) ϭ x,
He3 (x) ϭ x 3 Ϫ 3x,

He2 (x) ϭ x 2 Ϫ 1,
He4 (x) ϭ x 4 Ϫ 6x 2 ϩ 3.

(b) Generating Function. A generating function of the
Hermite polynomials is
(20)

>2

2

etx؊t

ؕ

ϭ a an (x) t n
nϭ0

because Hen (x) ϭ n! an(x). Prove this. Hint: Use the
formula for the coefficients of a Maclaurin series and
note that tx Ϫ 12 t 2 ϭ 12 x 2 Ϫ 12 (x Ϫ t)2.
(c) Derivative. Differentiating the generating function with respect to x, show that
(21)

Henr (x) ϭ nHenϪ1 (x).

(d) Orthogonality on the x-Axis needs a weight function
that goes to zero sufficiently fast as x : Ϯϱ, (Why?)

11.7

Henr (x) ϭ xHen(x) Ϫ Henϩ1 (x).

Using this with n Ϫ 1 instead of n and (21), show that
y ϭ Hen(x) satisfies the ODE
(23)

2

H 0* ϭ 1,

Show that the Hermite polynomials are orthogonal on
Ϫϱ Ͻ x Ͻ2 ϱ with respect to the weight function
r (x) ϭ e؊x >2. Hint. Use integration by parts and (21).
(e) ODEs. Show that

y s ϭ xy r ϩ ny ϭ 0.

Show that w ϭ e ؊x
equation

>4

2

y is a solution of Weber’s

(24) w s ϩ (n ϩ 12 Ϫ 14 x 2) w ϭ 0

(n ϭ 0, 1, Á ).

15. CAS EXPERIMENT. Fourier–Bessel Series. Use
Example 2 and R ϭ 1, so that you get the series
(25)

f (x) ϭ a1J0 (a0,1x) ϩ a2J0 (a0,2x)
ϩ a3J0 (a0,3x) ϩ Á

With the zeros a0,1 a0,2, Á from your CAS (see also
Table A1 in App. 5).
(a) Graph the terms J0 (a0,1x), Á , J0 (a0,10 x) for
0 Ϲ x Ϲ 1 on common axes.
(b) Write a program for calculating partial sums of (25).
Find out for what f (x) your CAS can evaluate the
integrals. Take two such f (x) and comment empirically
on the speed of convergence by observing the decrease
of the coefficients.
(c) Take f (x) ϭ 1 in (25) and evaluate the integrals
for the coefficients analytically by (21a), Sec. 5.4, with
v ϭ 1. Graph the first few partial sums on common
axes.

Fourier Integral
Fourier series are powerful tools for problems involving functions that are periodic or are of
interest on a finite interval only. Sections 11.2 and 11.3 first illustrated this, and various further
applications follow in Chap. 12. Since, of course, many problems involve functions that are
nonperiodic and are of interest on the whole x-axis, we ask what can be done to extend the
method of Fourier series to such functions. This idea will lead to “Fourier integrals.”
In Example 1 we start from a special function fL of period 2L and see what happens to
its Fourier series if we let L : ϱ. Then we do the same for an arbitrary function fL of
period 2L. This will motivate and suggest the main result of this section, which is an
integral representation given in Theorem 1 below.

8
CHARLES HERMITE (1822–1901), French mathematician, is known for his work in algebra and number
theory. The great HENRI POINCARÉ (1854–1912) was one of his students.

c11-a.qxd

10/30/10

1:25 PM

Page 511

SEC. 11.7 Fourier Integral
EXAMPLE 1

511

Rectangular Wave
Consider the periodic rectangular wave fL (x) of period 2L Ͼ 2 given by
0

if

ϪL Ͻ x Ͻ Ϫ1

fL (x) ϭ d 1

if

Ϫ1 Ͻ x Ͻ

0

if

1

1 Ͻ x Ͻ L.

The left part of Fig. 280 shows this function for 2L ϭ 4, 8, 16 as well as the nonperiodic function f(x), which
we obtain from fL if we let L : ϱ,
f (x) ϭ lim fL (x) ϭ e
L:ؕ

1

if Ϫ1 Ͻ x Ͻ 1

0

otherwise.

We now explore what happens to the Fourier coefficients of fL as L increases. Since fL is even, bn ϭ 0 for
all n. For an the Euler formulas (6), Sec. 11.2, give
a0 ϭ

2L Ύ
1

1

dx ϭ

؊1

1
L

,

an ϭ

LΎ
1

1

cos

؊1

npx
L

dx ϭ

1

cos
LΎ
2

0

npx
L

dx ϭ

2 sin (np>L)
L

np>L

.

This sequence of Fourier coefficients is called the amplitude spectrum of fL because ƒ an ƒ is the maximum
amplitude of the wave an cos (npx>L). Figure 280 shows this spectrum for the periods 2L ϭ 4, 8, 16. We see
that for increasing L these amplitudes become more and more dense on the positive wn-axis, where wn ϭ np>L.
Indeed, for 2L ϭ 4, 8, 16 we have 1, 3, 7 amplitudes per “half-wave” of the function (2 sin wn)>(Lwn) (dashed
in the figure). Hence for 2L ϭ 2k we have 2k؊1 Ϫ 1 amplitudes per half-wave, so that these amplitudes will
eventually be everywhere dense on the positive wn-axis (and will decrease to zero).
The outcome of this example gives an intuitive impression of what about to expect if we turn from our special
function to an arbitrary one, as we shall do next.
᭿
Waveform fL(x)

1

Amplitude spectrum an(wn)
n=1

fL(x)
–2

n=5
0

x

2

wn
n=7

n=3

2L = 4
1
_
2

n=2

fL(x)
–4

n = 10
0

x

4

n=6

2L = 8

fL(x)
–8

wn = nπ/L
π

1
_
4

0

8

wn

n=4
n = 20

x
n = 12

2L = 16

f(x)
–1 0 1

n = 14

x

Fig. 280. Waveforms and amplitude spectra in Example 1

n = 28

wn

c11-a.qxd

10/30/10

512

1:25 PM

Page 512

CHAP. 11 Fourier Analysis

From Fourier Series to Fourier Integral
We now consider any periodic function fL (x) of period 2L that can be represented by a
Fourier series
ؕ

fL (x) ϭ a0 ϩ a (an cos wnx ϩ bn sin wnx),

wn ϭ

nϭ1

np
L

and find out what happens if we let L : ϱ. Together with Example 1 the present
calculation will suggest that we should expect an integral (instead of a series) involving
cos wx and sin wx with w no longer restricted to integer multiples w ϭ wn ϭ np>L
of p>L but taking all values. We shall also see what form such an integral might
have.
If we insert an and bn from the Euler formulas (6), Sec. 11.2, and denote the variable
of integration by v, the Fourier series of fL (x) becomes
1
2L

fL (x) ϭ

Ύ

L

1 ؕ
a c cos wnx
L nϭ1

fL (v) dv ϩ

؊L

Ύ

L

fL (v) cos wnv dv

؊L

ϩ sin wnx

Ύ

L

fL (v) sin wnv dv d .

؊L

We now set
¢w ϭ wnϩ1 Ϫ wn ϭ

(n ϩ 1)p
L

Ϫ

np
p
ϭ .
L
L

Then 1>L ϭ ¢w> p, and we may write the Fourier series in the form

(1)

fL (x) ϭ

1
2L

Ύ

L

ؕ

fL (v) dv ϩ 1 a c (cos wn x) ¢w

؊L

p nϭ1

ϩ (sin wnx)¢w

Ύ

L

Ύ

L

fL (v) cos wnv dv

؊L

fL (v) sin wnv dv d .

؊L

This representation is valid for any fixed L, arbitrarily large, but finite.
We now let L : ϱ and assume that the resulting nonperiodic function
f (x) ϭ lim fL (x)
L :ϱ

is absolutely integrable on the x-axis; that is, the following (finite!) limits exist:

(2)

lim

a :Ϫϱ

Ύ

0

a

ƒ f (x) ƒ dx ϩ lim

b :ϱ

Ύ

b

0

ƒ f (x) ƒ dx awritten

Ύ

ؕ

ƒ f (x) ƒ dxb.

؊ؕ

Then 1>L : 0, and the value of the first term on the right side of (1) approaches zero.
Also ¢w ϭ p>L : 0 and it seems plausible that the infinite series in (1) becomes an

c11-a.qxd

10/30/10

1:25 PM

Page 513

SEC. 11.7 Fourier Integral

513

integral from 0 to ϱ, which represents f(x), namely,
(3)

1

f (x) ϭ p

Ύ

ؕ

0

c cos wx

ؕ

Ύ

f (v) cos wv dv ϩ sin wx

؊ؕ

Ύ

ؕ

f (v) sin wv dv d dw.

؊ؕ

If we introduce the notations

(4)

1
A (w) ϭ p

Ύ

ؕ

1
B (w) ϭ p

f (v) cos wv dv,

؊ؕ

Ύ

ؕ

f (v) sin wv dv

؊ؕ

we can write this in the form
f (x) ϭ

(5)

Ύ

ؕ

[A (w) cos wx ϩ B (w) sin wx] dw.

0

This is called a representation of f (x) by a Fourier integral.
It is clear that our naive approach merely suggests the representation (5), but by no
means establishes it; in fact, the limit of the series in (1) as ¢w approaches zero is not
the definition of the integral (3). Sufficient conditions for the validity of (5) are as follows.
THEOREM 1

Fourier Integral

If f (x) is piecewise continuous (see Sec. 6.1) in every finite interval and has a righthand derivative and a left-hand derivative at every point (see Sec 11.1) and if the
integral (2) exists, then f (x) can be represented by a Fourier integral (5) with A and
B given by (4). At a point where f (x) is discontinuous the value of the Fourier integral
equals the average of the left- and right-hand limits of f (x) at that point (see Sec. 11.1).
(Proof in Ref. [C12]; see App. 1.)

Applications of Fourier Integrals
The main application of Fourier integrals is in solving ODEs and PDEs, as we shall see
for PDEs in Sec. 12.6. However, we can also use Fourier integrals in integration and in
discussing functions defined by integrals, as the next example.
EXAMPLE 2

Single Pulse, Sine Integral. Dirichlet’s Discontinuous Factor. Gibbs Phenomenon
Find the Fourier integral representation of the function
f (x) ϭ e

1

if

ƒxƒ Ͻ 1

0

if

ƒxƒ Ͼ 1

(Fig. 281)

f(x)
1
–1

0

1

Fig. 281. Example 2

x

c11-a.qxd

10/30/10

514

1:25 PM

Page 514

CHAP. 11 Fourier Analysis

Solution. From (4) we obtain
A (w) ϭ

p Ύ
1

ϱ

f (v) cos wv dv ϭ

؊ϱ

p Ύ

1

Ύ

1

1

cos wv dv ϭ

؊1

B (w) ϭ

1

p

sin wv
`
pw

1

ϭ
؊1

2 sin w
pw

sin wv dv ϭ 0

؊1

and (5) gives the answer
f (x) ϭ

(6)

p Ύ
2

ϱ

cos wx sin w
dw.
w

0

The average of the left- and right-hand limits of f (x) at x ϭ 1 is equal to (1 ϩ 0)>2, that is, 12.
Furthermore, from (6) and Theorem 1 we obtain (multiply by p>2)

Ύ

(7)

ϱ

0

p>2
cos wx sin w
dw ϭ dp>4
w
0

if

0 Ϲ x Ͻ 1,

if

x ϭ 1,

if

x Ͼ 1.

We mention that this integral is called Dirichlet’s discontinous factor. (For P. L. Dirichlet see Sec. 10.8.)
The case x ϭ 0 is of particular interest. If x ϭ 0, then (7) gives

Ύ

(8*)

ϱ

sin w

0

p

dw ϭ

w

2

.

We see that this integral is the limit of the so-called sine integral

Ύ

Si(u) ϭ

(8)

u

0

sin w
dw
w

as u : ϱ . The graphs of Si(u) and of the integrand are shown in Fig. 282.
In the case of a Fourier series the graphs of the partial sums are approximation curves of the curve of the
periodic function represented by the series. Similarly, in the case of the Fourier integral (5), approximations are
obtained by replacing ϱ by numbers a. Hence the integral
2

(9)

p

Ύ

a

0

cos wx sin w
dw
w

approximates the right side in (6) and therefore f (x).
y

Integrand

Si(u)

π–
2
1
0.5

–4π

–3π

–2π

–1π 0
–0.5

1π

2π

3π

4π u

–1
– π–
2

Fig. 282. Sine integral Si(u) and integrand

c11-a.qxd

10/30/10

1:25 PM

Page 515

SEC. 11.7 Fourier Integral

515
y

y

y
a = 16

a=8

–2 –1 0

1

–2 –1 0

2x

1

a = 32

2x

–2 –1 0

1

2x

Fig. 283. The integral (9) for a ‫ ؍‬8, 16, and 32, illustrating
the development of the Gibbs phenomenon
Figure 283 shows oscillations near the points of discontinuity of f (x). We might expect that these oscillations
disappear as a approaches infinity. But this is not true; with increasing a, they are shifted closer to the points
x ϭ Ϯ1. This unexpected behavior, which also occurs in connection with Fourier series (see Sec. 11.2), is known
as the Gibbs phenomenon. We can explain it by representing (9) in terms of sine integrals as follows. Using
(11) in App. A3.1, we have
2

p

Ύ

a

0

cos wx sin w
1
dw ϭ
w
p

Ύ

a

0

sin (w ϩ wx)
1
dw ϩ
w
p

Ύ

a

0

sin (w Ϫ wx)
dw.
w

In the first integral on the right we set w ϩ wx ϭ t. Then dw>w ϭ dt>t, and 0 Ϲ w Ϲ a corresponds to
0 Ϲ t Ϲ (x ϩ 1) a. In the last integral we set w Ϫ wx ϭ Ϫt. Then dw>w ϭ dt>t, and 0 Ϲ w Ϲ a corresponds to
0 Ϲ t Ϲ (x Ϫ 1) a. Since sin (Ϫt) ϭ Ϫsin t, we thus obtain

p Ύ
2

a

0

cos wx sin w
1
dw ϭ
w
p

Ύ

(xϩ1) a

0

sin t
1
dt Ϫ
t
p

Ύ

(x؊1) a

0

sin t
dt.
t

From this and (8) we see that our integral (9) equals
1

1

p Si(a[x ϩ 1]) Ϫ p Si(a[x Ϫ 1])
and the oscillations in Fig. 283 result from those in Fig. 282. The increase of a amounts to a transformation
of the scale on the axis and causes the shift of the oscillations (the waves) toward the points of discontinuity
᭿
Ϫ1 and 1.

Fourier Cosine Integral and Fourier Sine Integral
Just as Fourier series simplify if a function is even or odd (see Sec. 11.2), so do Fourier
integrals, and you can save work. Indeed, if f has a Fourier integral representation and is
even, then B (w) ϭ 0 in (4). This holds because the integrand of B (w) is odd. Then (5)
reduces to a Fourier cosine integral

(10)

f (x) ϭ

Ύ

ϱ

A (w) cos wx dw

where

2
A (w) ϭ p

0

Ύ

ϱ

f (v) cos wv dv.

0

Note the change in A (w): for even f the integrand is even, hence the integral from Ϫϱ to
ϱ equals twice the integral from 0 to ϱ , just as in (7a) of Sec. 11.2.
Similarly, if f has a Fourier integral representation and is odd, then A (w) ϭ 0 in (4). This
is true because the integrand of A (w) is odd. Then (5) becomes a Fourier sine integral

(11)

f (x) ϭ

Ύ

ϱ

0

B (w) sin wx dw

where

2
B (w) ϭ p

Ύ

ϱ

0

f (v) sin wv dv.

c11-a.qxd

10/30/10

1:25 PM

516

Page 516

CHAP. 11 Fourier Analysis

Note the change of B (w) to an integral from 0 to ϱ because B (w) is even (odd times odd
is even).
Earlier in this section we pointed out that the main application of the Fourier integral
representation is in differential equations. However, these representations also help in
evaluating integrals, as the following example shows for integrals from 0 to ϱ .
EXAMPLE 3
1

Laplace Integrals
We shall derive the Fourier cosine and Fourier sine integrals of f (x) ϭ e؊kx, where x Ͼ 0 and k Ͼ 0 (Fig. 284).
The result will be used to evaluate the so-called Laplace integrals.
2

Solution. (a) From (10) we have A (w) ϭ p
0

Ύe

Fig. 284. f(x)
in Example 3

؊kv

cos wv dv ϭ Ϫ

Ύ

ϱ

e؊kv cos wv dv. Now, by integration by parts,

0

e؊kv aϪ

k
k2 ϩ w2

w
k

sin wv ϩ cos wvb .

If v ϭ 0, the expression on the right equals Ϫk>(k 2 ϩ w 2). If v approaches infinity, that expression approaches
zero because of the exponential factor. Thus 2> p times the integral from 0 to ϱ gives
A (w) ϭ

(12)

2k> p
k ϩ w2
2

.

By substituting this into the first integral in (10) we thus obtain the Fourier cosine integral representation
2k

f (x) ϭ e؊kx ϭ

p

Ύ

ϱ

0

cos wx

dw

k2 ϩ w2

(x Ͼ 0, k Ͼ 0).

From this representation we see that

Ύ

(13)

ϱ

cos wx
k ϩw
2

0

(b) Similarly, from (11) we have B (w) ϭ

Ύe

؊kv

2

p

sin wv dv ϭ Ϫ

Ύ

ϱ

dw ϭ

2

p

e؊kx

2k

(x Ͼ 0, k Ͼ 0).

e؊kv sin wv dv. By integration by parts,

0

e؊kv a

w
k2 ϩ w2

k
w

sin wv ϩ cos wvb .

This equals Ϫw>(k 2 ϩ w 2) if v ϭ 0, and approaches 0 as v : ϱ . Thus
B (w) ϭ

(14)

2w> p
k ϩ w2
2

.

From (14) we thus obtain the Fourier sine integral representation
f (x) ϭ e؊kx ϭ

2

p

Ύ

ϱ

0

w sin wx
k2 ϩ w2

dw.

From this we see that

(15)

Ύ

ϱ

0

w sin wx
k2 ϩ w2

dw ϭ

The integrals (13) and (15) are called the Laplace integrals.

p
2

e؊kx

(x Ͼ 0, k Ͼ 0).

᭿

c11-a.qxd

10/30/10

1:25 PM

Page 517

SEC. 11.7 Fourier Integral

517

PROBLEM SET 11.7
EVALUATION OF INTEGRALS

1–6

Show that the integral represents the indicated function.
Hint. Use (5), (10), or (11); the integral tells you which one,
and its value tells you what function to consider. Show your
work in detail.
0
if x Ͻ 0
ϱ
cos xw ϩ w sin xw
1.
dx ϭ d p/2
if x ϭ 0
1 ϩ w2
0
؊x
pe
if x Ͼ 0

Ύ

2.

Ύ

ϱ

sin pw sin xw
1 Ϫ w2

0

3.

Ύ

ϱ

0

p

dw ϭ b

2

sin x

if

0ϹxϹp

0

if

xϾ p

1
1 Ϫ cos pw
2p
sin
xw
dw
ϭ
b
w
0

functions of x. Graph approximations obtained by
replacing ϱ with finite upper limits of your choice.
Compare the quality of the approximations. Write a
short report on your empirical results and observations.
14. PROJECT. Properties of Fourier Integrals
(a) Fourier cosine integral. Show that (10) implies

4.

Ύ

5.

Ύ

cos pw
1Ϫw

0

ϱ

1
2

Ύ

cos xw dw ϭ b

sin w Ϫ w cos w
w2

0

6.

2

ؕ

0

7–12

w 3 sin xw
w4 ϩ 4

1
2

0

ƒxƒ м p

if
1
2

px if 0 Ͻ x Ͻ 1

1
4

if

xϭ1

0

if

xϾ1

sin xw dw ϭ d p

dw ϭ 12 pe؊x cos x

if x Ͼ 0

Represent f (x) as an integral (10).

8. f ( x) ϭ b

1
0
x

if

0ϽxϽ1

if

xϾ1

2

0ϽxϽ1

if

B* ϭ Ϫ

x 2 f ( x) ϭ

11. f ( x) ϭ b
12. f ( x) ϭ b

0
e؊x
0

if

d A
.
dw 2
(b) Solve Prob. 8 by applying (a3) to the result of Prob. 7.
(c) Verify (a2) for f (x) ϭ 1 if 0 Ͻ x Ͻ a and
f (x) ϭ 0 if x Ͼ a.
(d) Fourier sine integral. Find formulas for the Fourier
sine integral similar to those in (a).
15. CAS EXPERIMENT. Sine Integral. Plot Si(u) for
positive u. Does the sequence of the maximum and
minimum values give the impression that it converges
and has the limit p>2? Investigate the Gibbs phenomenon
graphically.
A* ϭ Ϫ

16–20

FOURIER SINE INTEGRAL
REPRESENTATIONS

Represent f(x) as an integral (11).

xϾa
0ϽxϽp

if
xϾp
if 0 Ͻ x Ͻ a
if

18. f ( x) ϭ b
19. f ( x) ϭ b

xϾa

13. CAS EXPERIMENT. Approximate Fourier Cosine
Integrals. Graph the integrals in Prob. 7, 9, and 11 as

Ύ A*(w) cos xw dw,
0

17. f ( x) ϭ b

sin x

A as in (10)

2

a 2 Ϫ x 2 if 0 Ͻ x Ͻ a
if

dA
,
dw

ϱ

16. f ( x) ϭ b

0

*

0

0
if
xϾ1
9. f (x) ϭ 1>(1 ϩ x 2) [x Ͼ 0 . Hint. See (13).]
10. f ( x) ϭ b

Ύ B (w) sin xw dw,

p cos x if 0 Ͻ ƒ x ƒ Ͻ 12p

FOURIER COSINE INTEGRAL
REPRESENTATIONS

7. f ( x) ϭ b

(Scale change)

xf (x) ϭ

(a2)

xϾp

1
2

w
A a a b cos xw dw

ϱ

(a3)
ϱ

ؕ

0

(a Ͼ 0)

if 0 Ͻ x Ͻ p
if

Ύ

1
f (ax) ϭ a

(a1)

20. f ( x) ϭ b

if 0 Ͻ x Ͻ a

x

xϾa

0 if
1

if

0ϽxϽ1

0

if

xϾ1

cos x

if

0ϽxϽp

0

if

xϾp

e

x

0
e

؊x

0

if 0 Ͻ x Ͻ 1
xϾ1

if

if 0 Ͻ x Ͻ 1
if

xϾ1

c11-b.qxd

10/30/10

518

11.8

1:31 PM

Page 518

CHAP. 11 Fourier Analysis

Fourier Cosine and Sine Transforms
An integral transform is a transformation in the form of an integral that produces from
given functions new functions depending on a different variable. One is mainly interested
in these transforms because they can be used as tools in solving ODEs, PDEs, and integral
equations and can often be of help in handling and applying special functions. The Laplace
transform of Chap. 6 serves as an example and is by far the most important integral
transform in engineering.
Next in order of importance are Fourier transforms. They can be obtained from the
Fourier integral in Sec. 11.7 in a straightforward way. In this section we derive two such
transforms that are real, and in Sec. 11.9 a complex one.

Fourier Cosine Transform
The Fourier cosine transform concerns even functions f (x). We obtain it from the Fourier
cosine integral [(10) in Sec. 10.7]
f (x) ϭ

Ύ

ؕ

A(w) cos wx dw,

where

2
A (w) ϭ p

0

Ύ

ؕ

f (v) cos wv dv.

0

Namely, we set A(w) ϭ 22> p fˆc (w), where c suggests “cosine.” Then, writing v ϭ x in
the formula for A(w), we have

(1a)

fˆc(w) ϭ

Ύ

2
Bp

ؕ

f (x) cos wx dx

0

and

(1b)

f (x) ϭ

2
Bp

Ύ

ؕ

fˆc (w) cos wx dw.

0

Formula (1a) gives from f (x) a new function fˆc(w), called the Fourier cosine transform
of f (x). Formula (1b) gives us back f (x) from fˆc(w), and we therefore call f (x) the inverse
Fourier cosine transform of fˆc(w).
The process of obtaining the transform fˆc from a given f is also called the Fourier
cosine transform or the Fourier cosine transform method.

Fourier Sine Transform
Similarly, in (11), Sec. 11.7, we set B (w) ϭ 22> p fˆs(w), where s suggests “sine.” Then,
writing v ϭ x, we have from (11), Sec. 11.7, the Fourier sine transform, of f (x) given by

(2a)

fˆs(w) ϭ

2
Bp

Ύ

ؕ

0

f(x) sin wx dx,

c11-b.qxd

10/30/10

1:31 PM

Page 519

SEC. 11.8 Fourier Cosine and Sine Transforms

519

and the inverse Fourier sine transform of fˆs (w), given by
2

f (x) ϭ

(2b)

Bp

Ύ

ؕ

fˆs (w) sin wx dw.

0

The process of obtaining fs (w) from f (x) is also called the Fourier sine transform or
the Fourier sine transform method.
Other notations are
fc ( f ) ϭ fˆc,

fs ( f ) ϭ fˆs

and fc؊1 and fs؊1 for the inverses of fc and fs, respectively.
EXAMPLE 1

Fourier Cosine and Fourier Sine Transforms
Find the Fourier cosine and Fourier sine transforms of the function

k

f (x) ϭ b
a

x

Fig. 285. ƒ(x) in
Example 1

k if

0ϽxϽa

0 if

xϾa

(Fig. 285).

Solution. From the definitions (1a) and (2a) we obtain by integration
fˆc (w) ϭ
fˆs (w) ϭ

2
Bp
2

Bp

a

k

Ύ cos wx dx ϭ B p k a
2

0

a

k

Ύ sin wx dx ϭ B p k a
2

0

sin aw
b
w

1 Ϫ cos aw
b.
w

This agrees with formulas 1 in the first two tables in Sec. 11.10 (where k ϭ 1).
Note that for f (x) ϭ k ϭ const (0 Ͻ x Ͻ ϱ), these transforms do not exist. (Why?)

EXAMPLE 2

᭿

Fourier Cosine Transform of the Exponential Function
Find fc(e؊x).

Solution. By integration by parts and recursion,
fc(e؊x ) ϭ

Bp Ύ
2

ؕ

0

e؊x cos wx dx ϭ

ؕ
22> p
e؊x
ϭ
.
2 (Ϫcos wx ϩ w sin wx) `
1 ϩ w2
Bp 1 ϩ w
0

2

This agrees with formula 3 in Table I, Sec. 11.10, with a ϭ 1. See also the next example.

᭿

What did we do to introduce the two integral transforms under consideration? Actually
not much: We changed the notations A and B to get a “symmetric” distribution of the
constant 2> p in the original formulas (1) and (2). This redistribution is a standard convenience, but it is not essential. One could do without it.
What have we gained? We show next that these transforms have operational properties
that permit them to convert differentiations into algebraic operations (just as the Laplace
transform does). This is the key to their application in solving differential equations.

c11-b.qxd

10/30/10

1:31 PM

520

Page 520

CHAP. 11 Fourier Analysis

Linearity, Transforms of Derivatives
If f (x) is absolutely integrable (see Sec. 11.7) on the positive x-axis and piecewise
continuous (see Sec. 6.1) on every finite interval, then the Fourier cosine and sine
transforms of f exist.
Furthermore, if f and g have Fourier cosine and sine transforms, so does af ϩ bg for
any constants a and b, and by (1a)
fc (af ϩ bg) ϭ

Ύ

2
Bp

ϭa

ؕ

[af (x) ϩ bg (x)] cos wx dx

0

2
Bp

Ύ

ؕ

2

f (x) cos wx dx ϩ b

Bp

0

Ύ

ؕ

g (x) cos wx dx.

0

The right side is afc( f ) ϩ bfc(g). Similarly for fs, by (2). This shows that the Fourier
cosine and sine transforms are linear operations,
(3)

THEOREM 1

(a)

fc(af ϩ bg) ϭ afc( f ) ϩ bfc(g),

(b)

fs(af ϩ bg) ϭ afs( f ) ϩ bfs(g).

Cosine and Sine Transforms of Derivatives

Let f (x) be continuous and absolutely integrable on the x-axis, let f r (x) be piecewise
continuous on every finite interval, and let f (x) : 0 as x : ϱ. Then
(a)

2

fc{ f r(x)} ϭ w fs{f (x)} Ϫ

f (0),
Bp

(4)

fs{f r (x)} ϭ Ϫwfc{f (x)}.

(b)

PROOF

This follows from the definitions and by using integration by parts, namely,
fc{f r (x)} ϭ
ϭ

2
Bp
2
Bp

ϭϪ

ؕ

Ύ

f r (x) cos wx dx

0

c f (x) cos wx `

ؕ

ϩw
0

Ύ

ؕ

f (x) sin wx dx d

0

2

f (0) ϩ w fs{f (x)};
Bp
˛

and similarly,
fs{f r (x)} ϭ
ϭ

2
Bp

Ύ

ؕ

f r (x) sin wx dx

0

ؕ

c f (x) sin wx ` Ϫ w
0
Bp
2

ϭ 0 Ϫ wfc{f(x)}.

Ύ

ؕ

0

f (x) cos wx dx d
᭿

c11-b.qxd

10/30/10

1:31 PM

Page 521

SEC. 11.8 Fourier Cosine and Sine Transforms

521

Formula (4a) with f r instead of f gives (when f r , f s satisfy the respective assumptions
for f, f r in Theorem 1)
fc{f s (x)} ϭ w fs{f r (x)} Ϫ
˛

2
f r (0);
Bp

hence by (4b)

(5a)

2

fc{f s (x)} ϭ Ϫw 2 fc{f (x)} Ϫ

f r (0).
Bp

fs{f s (x)} ϭ Ϫw 2 fs{f (x)} ϩ

wf (0).
Bp

Similarly,

(5b)

2

A basic application of (5) to PDEs will be given in Sec. 12.7. For the time being we
show how (5) can be used for deriving transforms.
EXAMPLE 3

An Application of the Operational Formula (5)

fc(e؊ax) of f (x) ϭ e؊ax, where a Ͼ 0.
Solution. By differentiation, (e؊ax) s ϭ a 2e؊ax; thus
Find the Fourier cosine transform

a 2f (x) ϭ f s (x).
From this, (5a), and the linearity (3a),
a 2 fc( f ) ϭ fc( f s )
ϭ Ϫw 2 fc( f ) Ϫ

2
Bp

ϭ Ϫw 2 fc( f ) ϩ a

f r (0)

2
Bp

.

Hence
(a 2 ϩ w 2)fc( f ) ϭ a22> p.
The answer is (see Table I, Sec. 11.10)

fc(e؊ax) ϭ

2
a
a
b
B p a2 ϩ w 2

(a Ͼ 0).

Tables of Fourier cosine and sine transforms are included in Sec. 11.10.

᭿

c11-b.qxd

10/30/10

522

1:31 PM

Page 522

CHAP. 11 Fourier Analysis

PROBLEM SET 11.8
1–8

FOURIER COSINE TRANSFORM

9–15

9. Find fs(e؊ax), a Ͼ 0, by integration.

1. Find the cosine transform fˆc(w) of f (x) ϭ 1 if
0 Ͻ x Ͻ 1, f (x) ϭ Ϫ1 if 1 Ͻ x Ͻ 2, f (x) ϭ 0 if
x Ͼ 2.
2. Find f in Prob. 1 from the answer fˆc.
3. Find fˆc(w) for f (x) ϭ x if 0 Ͻ x Ͻ 2, f (x) ϭ 0 if
x Ͼ 2.
4. Derive formula 3 in Table I of Sec. 11.10 by integration.
5. Find fˆc(w) for f (x) ϭ x 2 if 0 Ͻ x Ͻ 1, f (x) ϭ 0 if x Ͼ 1.
6. Continuity assumptions. Find gˆc(w) for g (x) ϭ 2 if
0 Ͻ x Ͻ 1, g (x) ϭ 0 if x Ͼ 1. Try to obtain from it
fˆc(w) for f (x) in Prob. 5 by using (5a).
7. Existence? Does the Fourier cosine transform of
x ؊1 sin x (0 Ͻ x Ͻ ϱ) exist? Of x ؊1 cos x? Give
reasons.
8. Existence? Does the Fourier cosine transform of
f (x) ϭ k ϭ const (0 Ͻ x Ͻ ϱ) exist? The Fourier sine
transform?

11.9

FOURIER SINE TRANSFORM

10. Obtain the answer to Prob. 9 from (5b).
11. Find fs (w) for f (x) ϭ x 2 if 0 Ͻ x Ͻ 1, f (x) ϭ 0 if
x Ͼ 1.

12. Find fs(xe؊x >2) from (4b) and a suitable formula in
Table I of Sec. 11.10.
2

13. Find fs(e؊x) from (4a) and formula 3 of Table I in
Sec. 11.10.
14. Gamma function. Using formulas 2 and 4 in Table II
of Sec. 11.10, prove ⌫(12) ϭ 1p [(30) in App. A3.1],
a value needed for Bessel functions and other
applications.
15. WRITING PROJECT. Finding Fourier Cosine and
Sine Transforms. Write a short report on ways of
obtaining these transforms, with illustrations by
examples of your own.

Fourier Transform.
Discrete and Fast Fourier Transforms
In Sec. 11.8 we derived two real transforms. Now we want to derive a complex transform
that is called the Fourier transform. It will be obtained from the complex Fourier integral,
which will be discussed next.

Complex Form of the Fourier Integral
The (real) Fourier integral is [see (4), (5), Sec. 11.7]
f (x) ϭ

Ύ

ؕ

[A(w) cos wx ϩ B(w) sin wx] dw

0

where
1
A(w) ϭ p

Ύ

ؕ

f (v) cos wv dv,

؊ؕ

1
B(w) ϭ p

Ύ

ؕ

f (v) sin wv dv.

؊ؕ

Substituting A and B into the integral for f, we have
1
f (x) ϭ p

ؕ

Ύ Ύ
0

ؕ

؊ؕ

f (v)[cos wv cos wx ϩ sin wv sin wx] dv dw.

c11-b.qxd

10/30/10

1:31 PM

Page 523

SEC. 11.9 Fourier Transform. Discrete and Fast Fourier Transforms

523

By the addition formula for the cosine [(6) in App. A3.1] the expression in the brackets
[ Á ] equals cos (wv Ϫ wx) or, since the cosine is even, cos (wx Ϫ wv). We thus obtain
(1*)

1
f (x) ϭ p

ؕ

Ύ Ύ

ؕ

B

f (v) cos (wx Ϫ wv)dvR dw.

؊ؕ

0

The integral in brackets is an even function of w, call it F (w), because cos (wx Ϫ wv) is
an even function of w, the function f does not depend on w, and we integrate with respect
to v (not w). Hence the integral of F (w) from w ϭ 0 to ϱ is 12 times the integral of F (w)
from Ϫϱ to ϱ . Thus (note the change of the integration limit!)
(1)

f (x) ϭ

1
2p

ؕ

Ύ Ύ

ؕ

B

؊ؕ

f (v) cos (wx Ϫ wv) dvR dw.

؊ؕ

We claim that the integral of the form (1) with sin instead of cos is zero:
(2)

1
2p

ؕ

Ύ Ύ

ؕ

B

؊ؕ

f (v) sin (wx Ϫ wv) dvR dw ϭ 0.

؊ؕ

This is true since sin (wx Ϫ wv) is an odd function of w, which makes the integral in
brackets an odd function of w, call it G (w). Hence the integral of G (w) from Ϫϱ to ϱ
is zero, as claimed.
We now take the integrand of (1) plus i (ϭ 1Ϫ1) times the integrand of (2) and use
the Euler formula [(11) in Sec. 2.2]
eix ϭ cos x ϩ i sin x.

(3)

Taking wx Ϫ wv instead of x in (3) and multiplying by f (v) gives
f (v) cos (wx Ϫ wv) ϩ if (v) sin (wx Ϫ wv) ϭ f (v)ei(wx؊wv).
Hence the result of adding (1) plus i times (2), called the complex Fourier integral, is
f (x) ϭ

(4)

1
2p

ؕ

Ύ Ύ
؊ؕ

ؕ

f (v)eiw(x؊v) dv dw

(i ϭ 1Ϫ1).

؊ؕ

To obtain the desired Fourier transform will take only a very short step from here.

Fourier Transform and Its Inverse
Writing the exponential function in (4) as a product of exponential functions, we have
(5)

f (x) ϭ

Ύ
22p
1

ؕ

؊ؕ

B

Ύ
22p
1

ؕ

f (v)e؊iwv dvR eiwx dw.

؊ؕ

The expression in brackets is a function of w, is denoted by fˆ(w), and is called the Fourier
transform of f ; writing v ϭ x, we have
(6)

fˆ(w) ϭ

Ύ
22p
1

ؕ

؊ؕ

f (x)e؊iwx dx.

c11-b.qxd

10/30/10

1:31 PM

524

Page 524

CHAP. 11 Fourier Analysis

With this, (5) becomes
f (x) ϭ

(7)

1
22p

Ύ

ؕ

fˆ(w)eiwx dw

؊ؕ

and is called the inverse Fourier transform of fˆ(w).
Another notation for the Fourier transform is
fˆ ϭ f( f ),
so that
f ϭ f؊1( fˆ).
The process of obtaining the Fourier transform f( f ) ϭ fˆ from a given f is also called
the Fourier transform or the Fourier transform method.
Using concepts defined in Secs. 6.1 and 11.7 we now state (without proof) conditions
that are sufficient for the existence of the Fourier transform.
THEOREM 1

Existence of the Fourier Transform

If f (x) is absolutely integrable on the x-axis and piecewise continuous on every finite
interval, then the Fourier transform fˆ(w) of f (x) given by (6) exists.

EXAMPLE 1

Fourier Transform
Find the Fourier transform of f (x) ϭ 1 if ƒ x ƒ Ͻ 1 and f (x) ϭ 0 otherwise.

Solution. Using (6) and integrating, we obtain
fˆ(w) ϭ

Ύ
12p
1

1

e؊iwx dx ϭ

؊1

1
12p

؊iwx 1
#e
` ϭ

Ϫiw

؊1

1
Ϫiw 12p

(e

؊iw

Ϫ eiw).

As in (3) we have eiw ϭ cos w ϩ i sin w, e؊iw ϭ cos w Ϫ i sin w, and by subtraction
eiw Ϫ e؊iw ϭ 2i sin w.
Substituting this in the previous formula on the right, we see that i drops out and we obtain the answer
fˆ(w) ϭ

EXAMPLE 2

p sin w
B2

w

᭿

.

Fourier Transform
Find the Fourier transform f (e؊ax) of f (x) ϭ e؊ax if x Ͼ 0 and f (x) ϭ 0 if x Ͻ 0; here a Ͼ 0.

Solution. From the definition (6) we obtain by integration
f (e؊ax) ϭ

Ύ
12p
1

ؕ

e؊axe؊iwx dx

0

ϭ

1

e؊(aϩiw)x

22p Ϫ(a ϩ iw)

This proves formula 5 of Table III in Sec. 11.10.

`

ؕ
xϭ0

ϭ

1
12p(a ϩ iw)

.

᭿

c11-b.qxd

10/30/10

1:31 PM

Page 525

SEC. 11.9 Fourier Transform. Discrete and Fast Fourier Transforms

525

Physical Interpretation: Spectrum
The nature of the representation (7) of f (x) becomes clear if we think of it as a superposition
of sinusoidal oscillations of all possible frequencies, called a spectral representation.
This name is suggested by optics, where light is such a superposition of colors
(frequencies). In (7), the “spectral density” fˆ(w) measures the intensity of f (x) in the
frequency interval between w and w ϩ ¢w ( ¢w small, fixed). We claim that, in connection
with vibrations, the integral

Ύ

ؕ

ƒ fˆ(w) ƒ 2 dw

؊ؕ

can be interpreted as the total energy of the physical system. Hence an integral of ƒ fˆ(w) ƒ 2
from a to b gives the contribution of the frequencies w between a and b to the total energy.
To make this plausible, we begin with a mechanical system giving a single frequency,
namely, the harmonic oscillator (mass on a spring, Sec. 2.4)
my s ϩ ky ϭ 0.
Here we denote time t by x. Multiplication by y r gives my r y s ϩ ky r y ϭ 0. By integration,
1
2
2 mv

ϩ 12 ky 2 ϭ E 0 ϭ const

where v ϭ y r is the velocity. The first term is the kinetic energy, the second the potential
energy, and E 0 the total energy of the system. Now a general solution is (use (3) in
Sec. 11.4 with t ϭ x)
y ϭ a1 cos w0 x ϩ b1 sin w0 x ϭ c1eiw0x ϩ c؊1e؊iw0x,

w 20 ϭ k>m

where c1 ϭ (a1 Ϫ ib1)>2, c؊1 ϭ c1 ϭ (a1 ϩ ib1)>2. We write simply A ϭ c1eiw0x,
B ϭ c؊1e؊iw0x. Then y ϭ A ϩ B. By differentiation, v ϭ y r ϭ A r ϩ B r ϭ iw0 (A Ϫ B).
Substitution of v and y on the left side of the equation for E 0 gives
E 0 ϭ 12 mv2 ϩ 12 ky 2 ϭ 12 m(iw0)2(A Ϫ B)2 ϩ 12 k(A ϩ B)2.
Here w 20 ϭ k>m, as just stated; hence mw 20 ϭ k. Also i 2 ϭ Ϫ1, so that
E 0 ϭ 12 k[Ϫ(A Ϫ B)2 ϩ (A ϩ B)2] ϭ 2kAB ϭ 2kc1eiw0xc؊1e؊iw0x ϭ 2kc1c؊1 ϭ 2k ƒ c1 ƒ 2.
Hence the energy is proportional to the square of the amplitude ƒ c1 ƒ .
As the next step, if a more complicated system leads to a periodic solution y ϭ f (x)
that can be represented by a Fourier series, then instead of the single energy term ƒ c1 ƒ 2
we get a series of squares ƒ cn ƒ 2 of Fourier coefficients cn given by (6), Sec. 11.4. In this
case we have a “discrete spectrum” (or “point spectrum”) consisting of countably many
isolated frequencies (infinitely many, in general), the corresponding ƒ cn ƒ 2 being the
contributions to the total energy.
Finally, a system whose solution can be represented by an integral (7) leads to the above
integral for the energy, as is plausible from the cases just discussed.

c11-b.qxd

10/30/10

1:31 PM

526

Page 526

CHAP. 11 Fourier Analysis

Linearity. Fourier Transform of Derivatives
New transforms can be obtained from given ones by using

THEOREM 2

Linearity of the Fourier Transform

The Fourier transform is a linear operation; that is, for any functions f (x) and g(x)
whose Fourier transforms exist and any constants a and b, the Fourier transform
of af ϩ bg exists, and
f(af ϩ bg) ϭ af ( f ) ϩ bf (g).

(8)

PROOF

This is true because integration is a linear operation, so that (6) gives
f{af (x) ϩ bg (x)} ϭ

1
12p

ϭa

Ύ

ؕ

[af (x) ϩ bg (x)] e؊iwx dx

؊ؕ

1
12p

Ύ

ؕ

1
12p

f (x)e؊iwx dx ϩ b

؊ؕ

Ύ

ؕ

g (x)e؊iwx dx

؊ؕ

ϭ af{f (x)} ϩ bf{g (x)}.

᭿

In applying the Fourier transform to differential equations, the key property is that
differentiation of functions corresponds to multiplication of transforms by iw:

THEOREM 3

Fourier Transform of the Derivative of f (x)

Let f (x) be continuous on the x-axis and f (x) : 0 as ƒ x ƒ : ϱ . Furthermore, let f r (x)
be absolutely integrable on the x-axis. Then
f {f r (x)} ϭ iwf {f (x)}.

(9)

PROOF

From the definition of the Fourier transform we have
f{f r (x)} ϭ

1
12p

Ύ

ؕ

f r (x)e؊iwx dx.

؊ؕ

Integrating by parts, we obtain
ؕ

f{f r (x)} ϭ

1
Bf (x)e؊iwx `
Ϫ (Ϫiw)
12p
؊ؕ

Ύ

ؕ

f (x)e؊iwx dxR .

؊ؕ

Since f (x) : 0 as ƒ x ƒ : ϱ, the desired result follows, namely,
f{ f r (x)} ϭ 0 ϩ iw f{f (x)}.

᭿

c11-b.qxd

10/30/10

3:46 PM

Page 527

SEC. 11.9 Fourier Transform. Discrete and Fast Fourier Transforms

527

Two successive applications of (9) give
f ( f s ) ϭ iwf ( f r ) ϭ (iw)2f ( f ).
Since (iw)2 ϭ Ϫw 2, we have for the transform of the second derivative of f
f{f s (x)} ϭ Ϫw 2f{f (x)}.

(10)

Similarly for higher derivatives.
An application of (10) to differential equations will be given in Sec. 12.6. For the time
being we show how (9) can be used to derive transforms.
EXAMPLE 3

Application of the Operational Formula (9)
Find the Fourier transform of xe؊x from Table III, Sec 11.10.
2

Solution. We use (9). By formula 9 in Table III
f (xe؊x ) ϭ f{Ϫ 12 (e؊x ) r }
2

2

ϭ Ϫ 12 f{(e؊x ) r }
2

ϭ Ϫ 12 iwf(e؊x )
2

1 ؊w2>4
1
ϭ Ϫ iw
e
2
12
ϭϪ

iw
2 12

e؊w

>4

2

᭿

.

Convolution
The convolution f * g of functions f and g is defined by

(11)

h (x) ϭ ( f * g) (x) ϭ

Ύ

ؕ

f (p) g (x Ϫ p) dp ϭ

؊ؕ

Ύ

ؕ

f (x Ϫ p)g (p) dp.

؊ؕ

The purpose is the same as in the case of Laplace transforms (Sec. 6.5): taking the
convolution of two functions and then taking the transform of the convolution is the same
as multiplying the transforms of these functions (and multiplying them by 12p):

THEOREM 4

Convolution Theorem

Suppose that f (x) and g(x) are piecewise continuous, bounded, and absolutely
integrable on the x-axis. Then
(12)

f ( f * g) ϭ 12p f ( f ) f (g).

c11-b.qxd

10/30/10

1:31 PM

528

Page 528

CHAP. 11 Fourier Analysis

PROOF

By the definition,
1
f ( f * g) ϭ
12p

ؕ

Ύ Ύ

ؕ

f (p) g (x Ϫ p) dp e؊iwx dx.

؊ؕ ؊ؕ

An interchange of the order of integration gives
f ( f * g) ϭ

1
12p

ؕ

ؕ

Ύ Ύ

f (p) g (x Ϫ p) e؊iwx dx dp.

؊ؕ ؊ؕ

Instead of x we now take x Ϫ p ϭ q as a new variable of integration. Then x ϭ p ϩ q
and
f ( f * g) ϭ

1
12p

ؕ

Ύ Ύ

ؕ

f (p) g (q) e؊iw (pϩq) dq dp.

؊ؕ ؊ؕ

This double integral can be written as a product of two integrals and gives the desired
result
1
f ( f * g) ϭ
12p
ϭ

Ύ

ؕ

f (p)e

؊iwp

؊ؕ

dp

Ύ

ؕ

g (q) e؊iwq dq

؊ؕ

1
[12p f ( f )][12p f (g)] ϭ 12p f ( f ) f (g).
12p

᭿

By taking the inverse Fourier transform on both sides of (12), writing fˆ ϭ f ( f ) and
gˆ ϭ f (g) as before, and noting that 12p and 1> 12p in (12) and (7) cancel each other,
we obtain

(13)

( f * g) (x) ϭ

Ύ

ؕ

fˆ(w)gˆ (w)eiwx dw,

؊ؕ

a formula that will help us in solving partial differential equations (Sec. 12.6).

Discrete Fourier Transform (DFT),
Fast Fourier Transform (FFT)
In using Fourier series, Fourier transforms, and trigonometric approximations (Sec. 11.6)
we have to assume that a function f (x), to be developed or transformed, is given on some
interval, over which we integrate in the Euler formulas, etc. Now very often a function f (x)
is given only in terms of values at finitely many points, and one is interested in extending
Fourier analysis to this case. The main application of such a “discrete Fourier analysis”
concerns large amounts of equally spaced data, as they occur in telecommunication, time
series analysis, and various simulation problems. In these situations, dealing with sampled
values rather than with functions, we can replace the Fourier transform by the so-called
discrete Fourier transform (DFT) as follows.

c11-b.qxd

10/30/10

1:31 PM

Page 529

SEC. 11.9 Fourier Transform. Discrete and Fast Fourier Transforms

529

Let f (x) be periodic, for simplicity of period 2p. We assume that N measurements of
f (x) are taken over the interval 0 Ϲ x Ϲ 2p at regularly spaced points
xk ϭ

(14)

2pk
,
N

k ϭ 0, 1, Á , N Ϫ 1.

We also say that f (x) is being sampled at these points. We now want to determine a
complex trigonometric polynomial
NϪ1

q (x) ϭ a cneinxk

(15)

nϭ0

that interpolates f (x) at the nodes (14), that is, q (x k) ϭ f (x k), written out, with fk denoting
f (x k),
NϪ1

fk ϭ f (x k) ϭ q (x k) ϭ a cneinxk,

(16)

k ϭ 0, 1, Á , N Ϫ 1.

nϭ0

Hence we must determine the coefficients c0, Á , cN؊1 such that (16) holds. We do this
by an idea similar to that in Sec. 11.1 for deriving the Fourier coefficients by using the
orthogonality of the trigonometric system. Instead of integrals we now take sums. Namely,
we multiply (16) by e؊imxk (note the minus!) and sum over k from 0 to N Ϫ 1. Then we
interchange the order of the two summations and insert x k from (14). This gives
(17)

NϪ1

NϪ1 NϪ1

NϪ1

NϪ1

kϭ0

kϭ0 nϭ0

nϭ0

kϭ0

؊imxk
ϭ a a cnei(n؊m)xk ϭ a cn a ei (n؊m) 2pk>N.
a fke

Now
ei (n؊m)2pk>N ϭ [ei (n؊m)2p>N]k.
We donote [ Á ] by r. For n ϭ m we have r ϭ e0 ϭ 1. The sum of these terms over k
equals N, the number of these terms. For n m we have r 1 and by the formula for a
geometric sum [(6) in Sec. 15.1 with q ϭ r and n ϭ N Ϫ 1]
1 Ϫ rN
k
ar ϭ 1Ϫr ϭ0
kϭ0

NϪ1

because r N ϭ 1; indeed, since k, m, and n are integers,
r N ϭ ei(n؊m)2pk ϭ cos 2pk(n Ϫ m) ϩ i sin 2pk(n Ϫ m) ϭ 1 ϩ 0 ϭ 1.
This shows that the right side of (17) equals cmN. Writing n for m and dividing by N, we
thus obtain the desired coefficient formula

(18*)

cn ϭ

1 NϪ1
؊inxk
a fke
N kϭ0

fk ϭ f (x k),

n ϭ 0, 1, Á , N Ϫ 1.

Since computation of the cn (by the fast Fourier transform, below) involves successive
halfing of the problem size N, it is practical to drop the factor 1>N from cn and define the

c11-b.qxd

10/30/10

1:31 PM

530

Page 530

CHAP. 11 Fourier Analysis

discrete Fourier transform of the given signal f ϭ [ f0
fˆ ϭ [ fˆ0 Á fˆN؊1] with components

fN؊1]T to be the vector

Á

NϪ1

fˆn ϭ Ncn ϭ a fke؊inxk,

(18)

fk ϭ f (x k),

n ϭ 0, Á , N Ϫ 1.

kϭ0

This is the frequency spectrum of the signal.
In vector notation, fˆ ϭ FNf, where the N ϫ N Fourier matrix FN ϭ [enk] has the
entries [given in (18)]
(19)

enk ϭ e؊inxk ϭ e؊2pink>N ϭ w nk,

w ϭ wN ϭ e؊2pi>N,

where n, k ϭ 0, Á , N Ϫ 1.
EXAMPLE 4

Discrete Fourier Transform (DFT). Sample of N ‫ ؍‬4 Values
Let N ϭ 4 measurements (sample values) be given. Then w ϭ e؊2pi>N ϭ e؊pi>2 ϭ Ϫi and thus w nk ϭ (Ϫi)nk.
Let the sample values be, say f ϭ [0 1 4 9]T. Then by (18) and (19),

(20)

w0

w0

w0

w

0

w

1

w

2

w

3

w

0

w

2

w

4

w

6

fˆ ϭ F4 f ϭ E

w0

w0

w3

w6

w9

1

1

1

1

Ϫi

Ϫ1

1

Ϫ1

1

i

U fϭE

1

0

14

i

1

Ϫ4 ϩ 8i

1

Ϫ1

4

Ϫ6

Ϫ1

Ϫi

9

Ϫ4 Ϫ 8i

U E UϭE

U.

From the first matrix in (20) it is easy to infer what FN looks like for arbitrary N, which in practice may be
1000 or more, for reasons given below.
᭿

From the DFT (the frequency spectrum) fˆ ϭ FNf we can recreate the given signal
1 nk
fˆ ϭ F ؊1
[w ]
N f, as we shall now prove. Here FN and its complex conjugate FN ϭ
N
satisfy
(21a)

FNFN ϭ FNFN ϭ NI

where I is the N ϫ N unit matrix; hence FN has the inverse
(21b)

PROOF

F ؊1
N ϭ

1
FN.
N

We prove (21). By the multiplication rule (row times column) the product matrix
GN ϭ FNFN ϭ [gjk] in (21a) has the entries gjk ϭ Row j of FN times Column k of FN.
That is, writing W ϭ w jw k, we prove that
gjk ϭ (w jw k)0 ϩ (w j wk )1 ϩ Á ϩ (w j w k )N؊1
ϭ W 0 ϩ W 1 ϩ Á ϩW N؊1 ϭ b

0 if j

k

N if j ϭ k.

c11-b.qxd

10/30/10

1:31 PM

Page 531

SEC. 11.9 Fourier Transform. Discrete and Fast Fourier Transforms

531

Indeed, when j ϭ k, then w kw k ϭ (ww)k ϭ (e2pi>Ne؊2pi>N)k ϭ 1k ϭ 1, so that the sum
of these N terms equals N; these are the diagonal entries of GN. Also, when j k, then
W 1 and we have a geometric sum (whose value is given by (6) in Sec. 15.1 with q ϭ W
and n ϭ N Ϫ1)
1 Ϫ WN
W 0 ϩ W 1 ϩ Á ϩW N؊1 ϭ
ϭ0
1ϪW
because W N ϭ (w jw k)N ϭ (e2pi)j(e؊2pi)k ϭ 1j # 1k ϭ 1.

᭿

We have seen that fˆ is the frequency spectrum of the signal f (x). Thus the components
fˆn of fˆ give a resolution of the 2p-periodic function f (x) into simple (complex) harmonics.
Here one should use only n’s that are much smaller than N>2, to avoid aliasing. By this
we mean the effect caused by sampling at too few (equally spaced) points, so that, for
instance, in a motion picture, rotating wheels appear as rotating too slowly or even in the
wrong sense. Hence in applications, N is usually large. But this poses a problem. Eq. (18)
requires O (N) operations for any particular n, hence O (N 2) operations for, say, all
n Ͻ N>2. Thus, already for 1000 sample points the straightforward calculation would
involve millions of operations. However, this difficulty can be overcome by the so-called
fast Fourier transform (FFT), for which codes are readily available (e.g., in Maple). The
FFT is a computational method for the DFT that needs only O (N) log 2 N operations
instead of O (N 2). It makes the DFT a practical tool for large N. Here one chooses N ϭ 2p
( p integer) and uses the special form of the Fourier matrix to break down the given problem
into smaller problems. For instance, when N ϭ 1000, those operations are reduced by a
factor 1000>log 2 1000 Ϸ 100.
The breakdown produces two problems of size M ϭ N>2. This breakdown is possible
because for N ϭ 2M we have in (19)
2
wN2 ϭ w 2M
ϭ (e؊2pi>N)2 ϭ e؊4pi>(2M) ϭ e؊2pi>(M) ϭ wM.

The given vector f ϭ [ f0 Á fN؊1]T is split into two vectors with M components each,
namely, f ev ϭ [ f0 f2 Á fN؊2]T containing the even components of f, and f od ϭ
[ f1 f3 Á fN؊1]T containing the odd components of f. For f ev and f od we determine
the DFTs
fˆev ϭ [ fˆev,0 fˆev,2

Á

fˆev,N؊2]T ϭ FM f ev

fˆod ϭ [ fˆod,1 fˆod,3

Á

fˆod,N؊1]T ϭ FM f od

and

involving the same M ϫ M matrix FM. From these vectors we obtain the components of
the DFT of the given vector f by the formulas
(22)

(a)

ˆ
fˆn ϭ fˆev,n ϩ w n
N fod,n

n ϭ 0, Á , M Ϫ 1

(b)

ˆ
fˆnϩM ϭ fˆev,n Ϫ w n
N fod,n

n ϭ 0, Á , M Ϫ 1.

c11-b.qxd

10/30/10

1:31 PM

532

Page 532

CHAP. 11 Fourier Analysis

For N ϭ 2p this breakdown can be repeated p Ϫ 1 times in order to finally arrive at N>2
problems of size 2 each, so that the number of multiplications is reduced as indicated
above.
We show the reduction from N ϭ 4 to M ϭ N>2 ϭ 2 and then prove (22).
EXAMPLE 5

Fast Fourier Transform (FFT). Sample of N ‫ ؍‬4 Values
When N ϭ 4, then w ϭ wN ϭ Ϫi as in Example 4 and M ϭ N>2 ϭ 2, hence w ϭ wM ϭ e؊2pi>2 ϭ e؊pi ϭ Ϫ1.
Consequently,
fˆ0

fˆev ϭ

cˆ d

fˆ od ϭ

cˆ d

ϭ F2f ev ϭ

c

1

1

1

Ϫ1

ϭ F2 f od ϭ

c

1

1

1

Ϫ1

f2

fˆ1
f3

dc d

ϭ

c

f0 ϩ f2

dc d

ϭ

c

f1 ϩ f3

f0
f2

f1
f3

f0 Ϫ f2

f1 Ϫ f3

d
d.

From this and (22a) we obtain
fˆ0 ϭ fˆev,0 ϩ w 0N fˆod,0 ϭ ( f0 ϩ f2) ϩ ( f1 ϩ f3) ϭ f0 ϩ f1 ϩ f2 ϩ f3
fˆ1 ϭ fˆev,1 ϩ w 1N fˆod,1 ϭ ( f0 Ϫ f2) Ϫ i( f1 ϩ f3) ϭ f0 Ϫ if1 Ϫ f2 ϩ if3.
Similarly, by (22b),
fˆ2 ϭ fˆev,0 Ϫ w 0N fˆod,0 ϭ ( f0 ϩ f2) Ϫ ( f1 ϩ f3) ϭ f0 Ϫ f1 ϩ f2 Ϫ f3
fˆ3 ϭ fˆev,1 Ϫ w 1N fˆod,1 ϭ ( f0 Ϫ f2) Ϫ (Ϫi)( f1 Ϫ f3) ϭ f0 ϩ if1 Ϫ f2 Ϫ if3.
This agrees with Example 4, as can be seen by replacing 0, 1, 4, 9 with f0, f1, f2, f3.

᭿

We prove (22). From (18) and (19) we have for the components of the DFT
N؊1
kn
fk.
fˆn ϭ a w N
kϭ0

Splitting into two sums of M ϭ N>2 terms each gives
M؊1

M؊1

2kn
(2kϩ1)n
fˆn ϭ a w N
f2k ϩ a w N
f2kϩ1.
kϭ0

kϭ0

We now use wN2 ϭ wM and pull out w n
N from under the second sum, obtaining
M؊1

(23)

M؊1

kn
n
kn
fˆn ϭ a w M
fev,k ϩ w N
a w M fod,k.
kϭ0

kϭ0

The two sums are fev,n and fod,n, the components of the “half-size” transforms Ff ev and
Ff od.
Formula (22a) is the same as (23). In (22b) we have n ϩ M instead of n. This causes
a sign changes in (23), namely Ϫw n
N before the second sum because
؊2piM>N
wM
ϭ e؊2pi>2 ϭ e؊pi ϭ Ϫ1.
N ϭ e

This gives the minus in (22b) and completes the proof.

᭿

c11-b.qxd

10/30/10

1:31 PM

Page 533

SEC. 11.9 Fourier Transform. Discrete and Fast Fourier Transforms

533

PROBLEM SET 11.9
1. Review in complex. Show that 1>i ϭ Ϫi, e؊ix ϭ
cos x Ϫ i sin x, eix ϩ e؊ix ϭ 2 cos x, eix Ϫ e؊ix ϭ
2i sin x, eikx ϭ cos kx ϩ i sin kx.
2–11
FOURIER TRANSFORMS BY
INTEGRATION
Find the Fourier transform of f (x) (without using Table
III in Sec. 11.10). Show details.
2. f (x) ϭ e
3. f (x) ϭ e
4. f (x) ϭ e
5. f (x) ϭ e

e2ix if Ϫ1 Ͻ x Ͻ 1
otherwise

0

1 if a Ͻ x Ͻ b
0 otherwise
ekx if x Ͻ 0 (k Ͼ 0)
if x Ͼ 0

0

ex if Ϫa Ͻ x Ͻ a
otherwise

0

6. f (x) ϭ e؊ƒ x ƒ
7. f (x) ϭ e

(Ϫϱ Ͻ x Ͻ ϱ)

x if 0 Ͻ x Ͻ a
0 otherwise

8. f (x) ϭ e

xe؊x if Ϫ1 Ͻ x Ͻ 0

9. f (x) ϭ e

ƒxƒ

if Ϫ1 Ͻ x Ͻ 1

0

otherwise

10. f (x) ϭ e

otherwise

0

x if Ϫ1 Ͻ x Ͻ 1
0 otherwise
Ϫ1 if Ϫ1 Ͻ x Ͻ 0

11. f (x) ϭ μ

1 if

0ϽxϽ1

0 otherwise

USE OF TABLE III IN SEC. 11.10.
12–17
OTHER METHODS
12. Find f ( f (x)) for f (x) ϭ xe؊x if x Ͼ 0, f (x) ϭ 0 if
x Ͻ 0, by (9) in the text and formula 5 in Table III
(with a ϭ 1). Hint. Consider xe؊x and e؊x.
2
13. Obtain f(e؊x >2) from Table III.
14. In Table III obtain formula 7 from formula 8.
15. In Table III obtain formula 1 from formula 2.
16. TEAM PROJECT. Shifting (a) Show that if f (x)
has a Fourier transform, so does f (x Ϫ a), and
f{ f (x Ϫ a)} ϭ e؊iwaf{ f (x)}.
(b) Using (a), obtain formula 1 in Table III, Sec. 11.10,
from formula 2.
(c) Shifting on the w-Axis. Show that if fˆ (w) is the
Fourier transform of f (x), then fˆ (w Ϫ a) is the Fourier
transform of eiaxf (x).
(d) Using (c), obtain formula 7 in Table III from 1 and
formula 8 from 2.
17. What could give you the idea to solve Prob. 11 by using
the solution of Prob. 9 and formula (9) in the text?
Would this work?
18–25

DISCRETE FOURIER TRANSFORM

18. Verify the calculations in Example 4 of the text.
19. Find the transform of a general signal
f ϭ [ f1 f2 f3 f4]T of four values.
20. Find the inverse matrix in Example 4 of the text and
use it to recover the given signal.
21. Find the transform (the frequency spectrum) of a
general signal of two values [ f1 f2]T.
22. Recreate the given signal in Prob. 21 from the
frequency spectrum obtained.
23. Show that for a signal of eight sample values,
w ϭ e؊i>4 ϭ (1 Ϫ i)> 12. Check by squaring.
24. Write the Fourier matrix F for a sample of eight values
explicitly.
25. CAS Problem. Calculate the inverse of the 8 ϫ 8
Fourier matrix. Transform a general sample of eight
values and transform it back to the given data.

c11-b.qxd

10/30/10

1:31 PM

534

11.10

Page 534

CHAP. 11 Fourier Analysis

Tables of Transforms
Table I.

Fourier Cosine Transforms

See (2) in Sec. 11.8.
fˆc (w) ϭ fc ( f )

f (x)
if 0 Ͻ x Ͻ a

1

e

2

x a؊1 (0 Ͻ a Ͻ 1)

ap
2 ⌫ (a)
a cos
2
Bp w

3

e؊ax (a Ͼ 0)

a
2b
Bp a ϩ w

4

e؊x

5

e؊ax

6

x ne؊ax (a Ͼ 0)

7

e

8

cos (ax 2) (a Ͼ 0)

1
w2
p
Ϫ b
cos a
4a
4
12a

9

sin (ax 2) (a Ͼ 0)

1
w2
p
ϩ b
cos a
4a
4
12a

1

0 otherwise

>2

a

2

e؊w

2

2

2 sin aw
w

Bp

(⌫(a) see App. A3.1.)

2

>2

2

(a Ͼ 0)

cos x
0

if 0 Ͻ x Ͻ a
otherwise

10

sin ax
x

11

e؊x sin x
x

12

J0(ax) (a Ͼ 0)

(a Ͼ 0)

1

>(4a)

e؊w

2

12a
2

n!

2
2 nϩ1
B p (a ϩ w )

Re (a ϩ iw)nϩ1

Re ϭ
Real part

sin a(1 ϩ w)
sin a(1 Ϫ w)
1
ϩ
d
c
1Ϫw
1ϩw
12p

p
B2

(1 Ϫ u(w Ϫ a))

1
12p
2

arctan

2
w2

1

B p 2a Ϫ w 2
2

(See Sec. 6.3.)

(1 Ϫ u(w Ϫ a)) (See Secs. 5.5, 6.3.)

c11-b.qxd

10/30/10

1:31 PM

Page 535

SEC. 11.10 Tables of Transforms

535

Table II.

Fourier Sine Transforms

See (5) in Sec. 11.8.
fˆs (w) ϭ fs ( f )

f (x)
1

e

1 if 0 Ͻ x Ͻ a
0 otherwise

2

1> 1x

1> 1w

3

1>x 3>2

21w

4

x a؊1 (0 Ͻ a Ͻ 1)

5

e؊ax (a Ͼ 0)

6

e؊ax
x

7

x ne؊ax (a Ͼ 0)

8

xe؊x

9

xe؊ax

(a Ͼ 0)

>2

2

e

11

cos ax
x

12

arctan

0

sin

ap
2

(⌫(a) see App. A3.1.)

2
w
a 2
2b
Bp a ϩ w
2
Bp

w
a

arctan

2

n!

2
2 nϩ1
B p (a ϩ w )

Im (a ϩ iw)nϩ1

Im ϭ
Imaginary part

>2

2

(a Ͼ 0)

sin x if 0 Ͻ x Ͻ a

10

2 ⌫ (a)
a
Bp w

we؊w

2

1 Ϫ cos aw
d
w

c

2
Bp

otherwise

(a Ͼ 0)
2a
x

(a Ͼ 0)

w
(2a)

e؊w

>4a

2

3>2

sin a(1 ϩ w)
sin a(1 Ϫ w)
1
Ϫ
d
c
1Ϫw
1ϩw
22p

p
B2

u (w Ϫ a)

12p

sin aw ؊aw
e
w

(See Sec. 6.3.)

c11-b.qxd

10/30/10

536

1:31 PM

Page 536

CHAP. 11 Fourier Analysis

Table III. Fourier Transforms
See (6) in Sec. 11.9.
fˆ(w) ϭ f( f )

f (x)
1

e

2

e

3

1 if Ϫb Ͻ x Ͻ b
0

1 if b Ͻ x Ͻ c

e؊ibw Ϫ e؊icw
iw12p

0 otherwise
1

x 2 ϩ a2

if b Ͻ x Ͻ 2b

e

e؊ax if x Ͼ 0

6

e

eax

7

e

eiax

8

e

eiax

0

otherwise

(a Ͼ 0)

if b Ͻ x Ͻ c
otherwise

0

0

if Ϫb Ͻ x Ͻ b

Ϫ1 ϩ 2eibw Ϫ e ؊2ibw
12pw 2

1
12p(a ϩ iw)
e(a؊iw)c Ϫ e(a؊iw)b
12p(a Ϫ iw)
2 sin b(w Ϫ a)
wϪa

otherwise

Bp

if b Ͻ x Ͻ c

i eib(a؊w) Ϫ eic(a؊w)
aϪw
22p

otherwise

9

e؊ax

(a Ͼ 0)

10

sin ax
x

(a Ͼ 0)

2

a

otherwise

5

0

B2

if 0 Ͻ x Ͻ b

μ 2x Ϫ b
0

p e؊aƒwƒ

(a Ͼ 0)

x
4

2 sin bw
w

Bp

otherwise

1
12a

p
B2

e؊w

>4a

2

if ƒ w ƒ Ͻ a; 0 if ƒ w ƒ Ͼ a

c11-b.qxd

10/30/10

1:31 PM

Page 537

Chapter 11 Review Questions and Problems

537

CHAPTER 11 REVIEW QUESTIONS AND PROBLEMS
1. What is a Fourier series? A Fourier cosine series? A
half-range expansion? Answer from memory.
2. What are the Euler formulas? By what very important
idea did we obtain them?
3. How did we proceed from 2p-periodic to generalperiodic functions?
4. Can a discontinuous function have a Fourier series? A
Taylor series? Why are such functions of interest to the
engineer?
5. What do you know about convergence of a Fourier
series? About the Gibbs phenomenon?
6. The output of an ODE can oscillate several times as
fast as the input. How come?
7. What is approximation by trigonometric polynomials?
What is the minimum square error?
8. What is a Fourier integral? A Fourier sine integral?
Give simple examples.
9. What is the Fourier transform? The discrete Fourier
transform?
10. What are Sturm–Liouville problems? By what idea are
they related to Fourier series?
11–20
FOURIER SERIES. In Probs. 11, 13, 16, 20 find
the Fourier series of f (x) as given over one period and
sketch f (x) and partial sums. In Probs. 12, 14, 15, 17–19
give answers, with reasons. Show your work detail.
11. f (x) ϭ e

0 if Ϫ2 Ͻ x Ͻ 0
2 if

0ϽxϽ2

12. Why does the series in Prob. 11 have no cosine terms?
13. f (x) ϭ e

0 if Ϫ1 Ͻ x Ͻ 0
x if

0ϽxϽ1

14. What function does the series of the cosine terms in
Prob. 13 represent? The series of the sine terms?
15. What function do the series of the cosine terms and the
series of the sine terms in the Fourier series of
ex (Ϫ5 Ͻ x Ͻ 5) represent?
16. f (x) ϭ ƒ x ƒ (Ϫp Ͻ x Ͻ p)

17. Find a Fourier series from which you can conclude that
1 Ϫ 1/3 ϩ 1/5 Ϫ 1/7 ϩ Ϫ Á ϭ p/4.
18. What function and series do you obtain in Prob. 16 by
(termwise) differentiation?
19. Find the half-range expansions of f (x) ϭ x
(0 Ͻ x Ͻ 1).
20. f (x) ϭ 3x 2 (Ϫp Ͻ x Ͻ p)
21–22

GENERAL SOLUTION

Solve, y s ϩ v2y ϭ r (t), where ƒ v ƒ
2p-periodic and
21. r (t) ϭ 3t 2 (Ϫp Ͻ t Ͻ p)
22. r (t) ϭ ƒ t ƒ (Ϫp Ͻ t Ͻ p)
23–25

0, 1, 2, Á , r (t) is

MINIMUM SQUARE ERROR

23. Compute the minimum square error for f (x) ϭ x> p
(Ϫp Ͻ x Ͻ p) and trigonometric polynomials of
degree N ϭ 1, Á , 5.
24. How does the minimum square error change if you
multiply f (x) by a constant k?
25. Same task as in Prob. 23, for f (x) ϭ ƒ x ƒ > p
(Ϫp Ͻ x Ͻ p). Why is E* now much smaller (by a
factor 100, approximately!)?
26–30
FOURIER INTEGRALS AND TRANSFORMS
Sketch the given function and represent it as indicated. If you
have a CAS, graph approximate curves obtained by replacing
ϱ with finite limits; also look for Gibbs phenomena.
26. f (x) ϭ x ϩ 1 if 0 Ͻ x Ͻ 1 and 0 otherwise; by the
Fourier sine transform
27. f (x) ϭ x if 0 Ͻ x Ͻ 1 and 0 otherwise; by the Fourier
integral
28. f (x) ϭ kx if a Ͻ x Ͻ b and 0 otherwise; by the Fourier
transform
29. f (x) ϭ x if 1 Ͻ x Ͻ a and 0 otherwise; by the Fourier
cosine transform
30. f (x) ϭ e؊2x if x Ͼ 0 and 0 otherwise; by the Fourier
transform

c11-b.qxd

10/30/10

538

1:31 PM

Page 538

CHAP. 11 Fourier Analysis

11

SUMMARY OF CHAPTER

Fourier Analysis. Partial Differential Equations (PDEs)
Fourier series concern periodic functions f (x) of period p ϭ 2L, that is, by
definition f (x ϩ p) ϭ f (x) for all x and some fixed p Ͼ 0; thus, f (x ϩ np) ϭ f (x)
for any integer n. These series are of the form
ؕ
np
np
f (x) ϭ a0 ϩ a aan cos
x ϩ bn sin
xb
L
L
nϭ1

(1)

(Sec. 11.2)

with coefficients, called the Fourier coefficients of f (x), given by the Euler formulas
(Sec. 11.2)
a0 ϭ
(2)

1
2L

Ύ

L

؊L

bn ϭ

Ύ

1
L

Ύ

1
L

an ϭ

f (x) dx,
L

f (x) sin

؊L

L

f (x) cos

؊L

npx
dx
L

npx
dx
L

where n ϭ 1, 2, Á . For period 2p we simply have (Sec. 11.1)
ؕ

f (x) ϭ a0 ϩ a (an cos nx ϩ bn sin nx)

(1*)

nϭ1

with the Fourier coefficients of f (x) (Sec. 11.1)
1
a0 ϭ
2p

Ύ

p

f (x) dx, an ϭ

؊p

1

p

Ύ

p

f (x) cos nx dx, bn ϭ

؊p

1

p

Ύ

p

f (x) sin nx dx.

؊p

Fourier series are fundamental in connection with periodic phenomena, particularly
in models involving differential equations (Sec. 11.3, Chap, 12). If f (x) is even
[ f (Ϫx) ϭ f (x)] or odd [ f (Ϫx) ϭ Ϫf (x)], they reduce to Fourier cosine or Fourier
sine series, respectively (Sec. 11.2). If f (x) is given for 0 Ϲ x Ϲ L only, it has two
half-range expansions of period 2L, namely, a cosine and a sine series (Sec. 11.2).
The set of cosine and sine functions in (1) is called the trigonometric system.
Its most basic property is its orthogonality on an interval of length 2L; that is, for
all integers m and n m we have

Ύ

L

cos

؊L

mpx
npx
cos
dx ϭ 0,
L
L

Ύ

L

sin

؊L

mpx
npx
sin
dx ϭ 0
L
L

and for all integers m and n,

Ύ

L

cos

؊L

mpx
npx
sin
dx ϭ 0.
L
L

This orthogonality was crucial in deriving the Euler formulas (2).

c11-b.qxd

11/9/10

8:56 PM

Page 539

Summary of Chapter 11

539

Partial sums of Fourier series minimize the square error (Sec. 11.4).
Replacing the trigonometric system in (1) by other orthogonal systems first leads
to Sturm–Liouville problems (Sec. 11.5), which are boundary value problems for
ODEs. These problems are eigenvalue problems and as such involve a parameter
l that is often related to frequencies and energies. The solutions to Sturm–Liouville
problems are called eigenfunctions. Similar considerations lead to other orthogonal
series such as Fourier–Legendre series and Fourier–Bessel series classified as
generalized Fourier series (Sec. 11.6).
Ideas and techniques of Fourier series extend to nonperiodic functions f (x) defined
on the entire real line; this leads to the Fourier integral
(3)

f (x) ϭ

Ύ

ؕ

[ A (w) cos wx ϩ B (w) sin wx] dw

(Sec. 11.7)

0

where
(4)

1
A (w) ϭ p

Ύ

ؕ

1
B (w) ϭ p

f (v) cos wv dv,

؊ؕ

Ύ

ؕ

f (v) sin wv dv

؊ؕ

or, in complex form (Sec. 11.9),
(5)

f (x) ϭ

1
12p

Ύ

ؕ

fˆ (w)eiwx dw

(i ϭ 1Ϫ1)

؊ؕ

where
(6)

fˆ (w) ϭ

1
12p

Ύ

ؕ

f (x)e؊iwx dx.

؊ؕ

Formula (6) transforms f (x) into its Fourier transform fˆ(w), and (5) is the inverse
transform.
Related to this are the Fourier cosine transform (Sec. 11.8)
(7)

fˆc (w) ϭ

2
Bp

Ύ

ؕ

f (x) cos wx dx

0

and the Fourier sine transform (Sec. 11.8)
(8)

fˆs(w) ϭ

2
Bp

Ύ

ؕ

f (x) sin wx dx .

0

The discrete Fourier transform (DFT) and a practical method of computing it,
called the fast Fourier transform (FFT), are discussed in Sec. 11.9.

c12-a.qxd

10/30/10

1:44 PM

Page 540

CHAPTER

12

Partial Differential
Equations (PDEs)
A PDE is an equation that contains one or more partial derivatives of an unknown function
that depends on at least two variables. Usually one of these deals with time t and the
remaining with space (spatial variable(s)). The most important PDEs are the wave
equations that can model the vibrating string (Secs. 12.2, 12.3, 12.4, 12.12) and the
vibrating membrane (Secs. 12.8, 12.9, 12.10), the heat equation for temperature in a bar
or wire (Secs. 12.5, 12.6), and the Laplace equation for electrostatic potentials (Secs.
12.6, 12.10, 12.11). PDEs are very important in dynamics, elasticity, heat transfer,
electromagnetic theory, and quantum mechanics. They have a much wider range of
applications than ODEs, which can model only the simplest physical systems. Thus PDEs
are subjects of many ongoing research and development projects.
Realizing that modeling with PDEs is more involved than modeling with ODEs, we
take a gradual, well-planned approach to modeling with PDEs. To do this we carefully
derive the PDE that models the phenomena, such as the one-dimensional wave equation
for a vibrating elastic string (say a violin string) in Sec. 12.2, and then solve the PDE
in a separate section, that is, Sec. 12.3. In a similar vein, we derive the heat equation in
Sec. 12.5 and then solve and generalize it in Sec. 12.6.
We derive these PDEs from physics and consider methods for solving initial and
boundary value problems, that is, methods of obtaining solutions which satisfy the
conditions required by the physical situations. In Secs. 12.7 and 12.12 we show how PDEs
can also be solved by Fourier and Laplace transform methods.
COMMENT. Numerics for PDEs is explained in Secs. 21.4–21.7, which, for greater
teaching flexibility, is designed to be independent of the other sections on numerics in
Part E.
Prerequisites: Linear ODEs (Chap. 2), Fourier series (Chap. 11).
Sections that may be omitted in a shorter course: 12.7, 12.10–12.12.
References and Answers to Problems: App. 1 Part C, App. 2.

12.1

Basic Concepts of PDEs
A partial differential equation (PDE) is an equation involving one or more partial
derivatives of an (unknown) function, call it u, that depends on two or more variables,
often time t and one or several variables in space. The order of the highest derivative is
called the order of the PDE. Just as was the case for ODEs, second-order PDEs will be
the most important ones in applications.

540

c12-a.qxd

10/30/10

1:44 PM

Page 541

SEC. 12.1 Basic Concepts of PDEs

541

Just as for ordinary differential equations (ODEs) we say that a PDE is linear if it is
of the first degree in the unknown function u and its partial derivatives. Otherwise we
call it nonlinear. Thus, all the equations in Example 1 are linear. We call a linear PDE
homogeneous if each of its terms contains either u or one of its partial derivatives.
Otherwise we call the equation nonhomogeneous. Thus, (4) in Example 1 (with f not
identically zero) is nonhomogeneous, whereas the other equations are homogeneous.
EXAMPLE 1

Important Second-Order PDEs
0 2u

(1)

0t 2

ϭ c2

0 2u

One-dimensional wave equation

0x 2

0u
0 2u
ϭ c2 2
0t
0x

(2)

0 2u

(3)

0x 2
0 2u

(4)

0x

2

0 2u

(5)

0t 2
0 2u

(6)

0x

2

ϩ
ϩ

0 2u
0y 2
0 2u
0y 2

ϭ c2 a
ϩ

0 2u
0y

2

One-dimensional heat equation

ϭ0

Two-dimensional Laplace equation

ϭ f (x, y)

Two-dimensional Poisson equation

0 2u
0x 2
ϩ

ϩ
0 2u
0z 2

0 2u

b

Two-dimensional wave equation

ϭ0

Three-dimensional Laplace equation

0y 2

Here c is a positive constant, t is time, x, y, z are Cartesian coordinates, and dimension is the number of these
coordinates in the equation.
᭿

A solution of a PDE in some region R of the space of the independent variables is a
function that has all the partial derivatives appearing in the PDE in some domain D
(definition in Sec. 9.6) containing R, and satisfies the PDE everywhere in R.
Often one merely requires that the function is continuous on the boundary of R, has
those derivatives in the interior of R, and satisfies the PDE in the interior of R. Letting R
lie in D simplifies the situation regarding derivatives on the boundary of R, which is then
the same on the boundary as it is in the interior of R.
In general, the totality of solutions of a PDE is very large. For example, the functions
(7)

u ϭ x 2 Ϫ y 2,

u ϭ ex cos y,

u ϭ sin x cosh y,

u ϭ ln (x 2 ϩ y 2)

which are entirely different from each other, are solutions of (3), as you may verify. We
shall see later that the unique solution of a PDE corresponding to a given physical problem
will be obtained by the use of additional conditions arising from the problem. For instance,
this may be the condition that the solution u assume given values on the boundary of the
region R (“boundary conditions”). Or, when time t is one of the variables, u (or u t ϭ 0u>0t
or both) may be prescribed at t ϭ 0 (“initial conditions”).
We know that if an ODE is linear and homogeneous, then from known solutions we
can obtain further solutions by superposition. For PDEs the situation is quite similar:
THEOREM 1

Fundamental Theorem on Superposition

If u 1 and u 2 are solutions of a homogeneous linear PDE in some region R, then
u ϭ c1u 1 ϩ c2u 2
with any constants c1 and c2 is also a solution of that PDE in the region R.

c12-a.qxd

10/30/10

1:44 PM

542

Page 542

CHAP. 12 Partial Differential Equations (PDEs)

The simple proof of this important theorem is quite similar to that of Theorem 1 in Sec. 2.1
and is left to the student.
Verification of solutions in Probs. 2–13 proceeds as for ODEs. Problems 16–23 concern
PDEs solvable like ODEs. To help the student with them, we consider two typical examples.
EXAMPLE 2

Solving uxx ؊ u ‫ ؍‬0 Like an ODE
Find solutions u of the PDE u xx Ϫ u ϭ 0 depending on x and y.

Solution. Since no y-derivatives occur, we can solve this PDE like u s Ϫ u ϭ 0. In Sec. 2.2 we would have
obtained u ϭ Aex ϩ Be؊x with constant A and B. Here A and B may be functions of y, so that the answer is
u(x, y) ϭ A( y)ex ϩ B( y)e؊x
with arbitrary functions A and B. We thus have a great variety of solutions. Check the result by differentiation.

EXAMPLE 3

᭿

Solving uxy ‫ ؍‬؊ux Like an ODE
Find solutions u ϭ u (x, y) of this PDE.

ෂ
p ϭ c (x)e؊y and by
Solution. Setting u x ϭ p, we have py ϭ Ϫp, py>p ϭ Ϫ1, ln ƒ p ƒ ϭ Ϫy ϩ c(x),
integration with respect to x,

u (x, y) ϭ f (x)e؊y ϩ g (y)

where

f (x) ϭ

Ύ c (x) dx,
᭿

here, f (x) and g( y) are arbitrary.

PROBLEM SET 12.1
1. Fundamental theorem. Prove it for second-order
PDEs in two and three independent variables. Hint.
Prove it by substitution.
2–13
VERIFICATION OF SOLUTIONS
Verifiy (by substitution) that the given function is a solution
of the PDE. Sketch or graph the solution as a surface in space.
2–5
2.
3.
4.
5.

u
u
u
u

Wave Equation (1) with suitable c
ϭ
ϭ
ϭ
ϭ

6–9
6.
7.
8.
9.

x2 ϩ t 2
cos 4t sin 2x
sin kct cos kx
sin at sin bx
Heat Equation (2) with suitable c
؊t

u ϭ e sin x
2 2
u ϭ e؊v c t cos vx
؊9t
sin vx
uϭe
؊p2t
cos 25x
uϭe

10–13

Laplace Equation (3)

10. u ϭ ex cos y, ex sin y
11. u ϭ arctan ( y>x)
12. u ϭ cos y sinh x, sin y cosh x

13. u ϭ x>(x 2 ϩ y 2), y>(x 2 ϩ y 2)
14. TEAM PROJECT. Verification of Solutions
(a) Wave equation. Verify that u (x, t) ϭ v (x ϩ ct) ϩ
w (x Ϫ ct) with any twice differentiable functions v and
w satisfies (1).
(b) Poisson equation. Verify that each u satisfies (4)
with f (x, y) as indicated.
f ϭ 2y>x 3
u ϭ y>x
u ϭ sin xy
f ϭ (x 2 ϩ y 2) sin xy
2
2
x 2 ؊y2
uϭe
f ϭ 4 (x 2 ϩ y 2)ex ؊y
u ϭ 1> 2x 2 ϩ y 2
f ϭ (x 2 ϩ y 2)؊3>2
(c) Laplace equation. Verify that
u ϭ 1> 2x 2 ϩ y 2 ϩ z 2 satisfies (6) and
u ϭ ln (x 2 ϩ y 2) satisfies (3). Is u ϭ 1> 2x 2 ϩ y 2 a
solution of (3)? Of what Poisson equation?
(d) Verify that u with any (sufficiently often differentiable) v and w satisfies the given PDE.
u ϭ v (x) ϩ w (y)
u xy ϭ 0
u ϭ v (x)w (y)
uu xy ϭ u xu y
u tt ϭ 4u xx
u ϭ v (x ϩ 2t) ϩ w (x Ϫ 2t)
15. Boundary value problem. Verify that the function
u (x, y) ϭ a ln (x 2 ϩ y 2) ϩ b satisfies Laplace’s equation

c12-a.qxd

10/30/10

1:44 PM

Page 543

SEC. 12.2 Modeling: Vibrating String, Wave Equation

543
18. 25u yy Ϫ 4u ϭ 0
19. u y ϩ y 2u ϭ 0
20. 2u xx ϩ 9u x ϩ 4u ϭ Ϫ3 cos x Ϫ 29 sin x
21. u yy ϩ 6u y ϩ 13u ϭ 4e3y
22. u xy ϭ u x
23. x 2u xx ϩ 2xu x Ϫ 2u ϭ 0
24. Surface of revolution. Show that the solutions z ϭ
z (x, y) of yz x ϭ xz y represent surfaces of revolution. Give
examples. Hint. Use polar coordinates r, u and show that
the equation becomes z u ϭ 0.
25. System of PDEs. Solve u xx ϭ 0, u yy ϭ 0

(3) and determine a and b so that u satisfies the
boundary conditions u ϭ 110 on the circle
x 2 ϩ y 2 ϭ 1 and u ϭ 0 on the circle x 2 ϩ y 2 ϭ 100.
16–23
PDEs SOLVABLE AS ODEs
This happens if a PDE involves derivatives with respect to
one variable only (or can be transformed to such a form),
so that the other variable(s) can be treated as parameter(s).
Solve for u ϭ u (x, y):
16. u yy ϭ 0
17. u xx ϩ 16p2u ϭ 0

12.2

Modeling: Vibrating String, Wave Equation
In this section we model a vibrating string, which will lead to our first important PDE,
that is, equation (3) which will then be solved in Sec. 12.3. The student should pay very
close attention to this delicate modeling process and detailed derivation starting from
scratch, as the skills learned can be applied to modeling other phenomena in general and
in particular to modeling a vibrating membrane (Sec. 12.7).
We want to derive the PDE modeling small transverse vibrations of an elastic string, such
as a violin string. We place the string along the x-axis, stretch it to length L, and fasten it
at the ends x ϭ 0 and x ϭ L. We then distort the string, and at some instant, call it t ϭ 0,
we release it and allow it to vibrate. The problem is to determine the vibrations of the string,
that is, to find its deflection u (x, t) at any point x and at any time t Ͼ 0; see Fig. 286.
u (x, t) will be the solution of a PDE that is the model of our physical system to be
derived. This PDE should not be too complicated, so that we can solve it. Reasonable
simplifying assumptions (just as for ODEs modeling vibrations in Chap. 2) are as follows.

Physical Assumptions
1. The mass of the string per unit length is constant (“homogeneous string”). The string
is perfectly elastic and does not offer any resistance to bending.
2. The tension caused by stretching the string before fastening it at the ends is so large
that the action of the gravitational force on the string (trying to pull the string down
a little) can be neglected.
3. The string performs small transverse motions in a vertical plane; that is, every
particle of the string moves strictly vertically and so that the deflection and the slope
at every point of the string always remain small in absolute value.
Under these assumptions we may expect solutions u (x, t) that describe the physical
reality sufficiently well.
β

u
P

Q

Q

T2
P

α

α

T1
T1
0

x x + Δx

L

Fig. 286. Deflected string at fixed time t. Explanation on p. 544

T2

β

c12-a.qxd

10/30/10

544

1:44 PM

Page 544

CHAP. 12 Partial Differential Equations (PDEs)

Derivation of the PDE of the Model
(“Wave Equation”) from Forces
The model of the vibrating string will consist of a PDE (“wave equation”) and additional
conditions. To obtain the PDE, we consider the forces acting on a small portion of the
string (Fig. 286). This method is typical of modeling in mechanics and elsewhere.
Since the string offers no resistance to bending, the tension is tangential to the curve
of the string at each point. Let T1 and T2 be the tension at the endpoints P and Q of that
portion. Since the points of the string move vertically, there is no motion in the horizontal
direction. Hence the horizontal components of the tension must be constant. Using the
notation shown in Fig. 286, we thus obtain
(1)

T1 cos a ϭ T2 cos b ϭ T ϭ const.

In the vertical direction we have two forces, namely, the vertical components ϪT1 sin a
and T2 sin b of T1 and T2; here the minus sign appears because the component at P is
directed downward. By Newton’s second law (Sec. 2.4) the resultant of these two forces
is equal to the mass r ¢x of the portion times the acceleration 0 2u>0t 2, evaluated at some
point between x and x ϩ ¢x; here r is the mass of the undeflected string per unit length,
and ¢x is the length of the portion of the undeflected string. ( ¢ is generally used to denote
small quantities; this has nothing to do with the Laplacian ٌ2, which is sometimes also
denoted by ¢.) Hence
T2 sin b Ϫ T1 sin a ϭ r¢x

0 2u
0t 2

.

Using (1), we can divide this by T2 cos b ϭ T1 cos a ϭ T, obtaining
(2)

T2 sin b
r¢x 0 2u
T sin a
Ϫ 1
ϭ tan b Ϫ tan a ϭ
.
T2 cos b
T1 cos a
T 0t 2

Now tan a and tan b are the slopes of the string at x and x ϩ ¢x:
tan a ϭ a

0u
b`
0x x

and

tan b ϭ a

0u
b`
.
0x xϩ ¢x

Here we have to write partial derivatives because u also depends on time t. Dividing (2)
by ¢x, we thus have
r 0 2u
1
0u
0u
Ϫa b` d ϭ
.
ca b `
¢x 0x xϩ ¢x
0x x
T 0t 2
If we let ¢x approach zero, we obtain the linear PDE
(3)

0 2u
0t

2

ϭ c2

0 2u
0x

2

,

T
c2 ϭ r .

This is called the one-dimensional wave equation. We see that it is homogeneous and
of the second order. The physical constant T>r is denoted by c2 (instead of c) to indicate

c12-a.qxd

10/30/10

1:44 PM

Page 545

SEC. 12.3 Solution by Separating Variables. Use of Fourier Series

545

that this constant is positive, a fact that will be essential to the form of the solutions. “Onedimensional” means that the equation involves only one space variable, x. In the next
section we shall complete setting up the model and then show how to solve it by a general
method that is probably the most important one for PDEs in engineering mathematics.

12.3

Solution by Separating Variables.
Use of Fourier Series
We continue our work from Sec. 12.2, where we modeled a vibrating string and obtained
the one-dimensional wave equation. We now have to complete the model by adding
additional conditions and then solving the resulting model.
The model of a vibrating elastic string (a violin string, for instance) consists of the onedimensional wave equation
0 2u

(1)

0t

2

ϭ c2

0 2u
0x

T
c2 ϭ r

2

for the unknown deflection u (x, t) of the string, a PDE that we have just obtained, and
some additional conditions, which we shall now derive.
Since the string is fastened at the ends x ϭ 0 and x ϭ L (see Sec. 12.2), we have the
two boundary conditions
(2)

(a) u (0, t) ϭ 0,

(b) u (L, t) ϭ 0,

for all t м 0.

Furthermore, the form of the motion of the string will depend on its initial deflection
(deflection at time t ϭ 0), call it f (x), and on its initial velocity (velocity at t ϭ 0), call it
g (x). We thus have the two initial conditions
(3)

(a) u (x, 0) ϭ f (x),

(b) u t (x, 0) ϭ g (x)

(0 Ϲ x Ϲ L)

where u t ϭ 0u>0t. We now have to find a solution of the PDE (1) satisfying the conditions
(2) and (3). This will be the solution of our problem. We shall do this in three steps, as
follows.
Step 1. By the “method of separating variables” or product method, setting
u (x, t) ϭ F (x)G (t), we obtain from (1) two ODEs, one for F (x) and the other one
for G (t).
Step 2. We determine solutions of these ODEs that satisfy the boundary conditions (2).
Step 3. Finally, using Fourier series, we compose the solutions found in Step 2 to obtain
a solution of (1) satisfying both (2) and (3), that is, the solution of our model of the
vibrating string.

Step 1. Two ODEs from the Wave Equation (1)
In the method of separating variables, or product method, we determine solutions of the
wave equation (1) of the form
(4)

u (x, t) ϭ F (x)G (t)

c12-a.qxd

10/30/10

546

1:44 PM

Page 546

CHAP. 12 Partial Differential Equations (PDEs)

which are a product of two functions, each depending on only one of the variables x and t.
This is a powerful general method that has various applications in engineering mathematics,
as we shall see in this chapter. Differentiating (4), we obtain
0 2u
0t 2

0 2u

##

ϭ FG

and

0x 2

ϭ F sG

where dots denote derivatives with respect to t and primes derivatives with respect to x.
By inserting this into the wave equation (1) we have

##

FG ϭ c2F s G.
Dividing by c2FG and simplifying gives

##
G
Fs
ϭ
.
c2G
F
The variables are now separated, the left side depending only on t and the right side only
on x. Hence both sides must be constant because, if they were variable, then changing t
or x would affect only one side, leaving the other unaltered. Thus, say,

##
G
Fs
ϭ k.
ϭ
2
c G
F
Multiplying by the denominators gives immediately two ordinary DEs
(5)

F s Ϫ kF ϭ 0

and

##

(6)

G Ϫ c2kG ϭ 0.

Here, the separation constant k is still arbitrary.

Step 2. Satisfying the Boundary Conditions (2)
We now determine solutions F and G of (5) and (6) so that u ϭ FG satisfies the boundary
conditions (2), that is,
(7)

u (0, t) ϭ F (0)G (t) ϭ 0,

u (L, t) ϭ F (L)G (t) ϭ 0

for all t.

We first solve (5). If G ϵ 0, then u ϭ FG ϵ 0, which is of no interest. Hence G [ 0
and then by (7),
(8)

(a) F (0) ϭ 0,

(b) F (L) ϭ 0.

We show that k must be negative. For k ϭ 0 the general solution of (5) is F ϭ ax ϩ b,
and from (8) we obtain a ϭ b ϭ 0, so that F ϵ 0 and u ϭ FG ϵ 0, which is of no interest.
For positive k ϭ ␮2 a general solution of (5) is
F ϭ Ae␮x ϩ Be؊␮x

c12-a.qxd

10/30/10

1:44 PM

Page 547

SEC. 12.3 Solution by Separating Variables. Use of Fourier Series

547

and from (8) we obtain F ϵ 0 as before (verify!). Hence we are left with the possibility
of choosing k negative, say, k ϭ Ϫp 2. Then (5) becomes F s ϩ p 2F ϭ 0 and has as a
general solution
F (x) ϭ A cos px ϩ B sin px.
From this and (8) we have
F (0) ϭ A ϭ 0
We must take B

F (L) ϭ B sin pL ϭ 0.

and then

0 since otherwise F ϵ 0. Hence sin pL ϭ 0. Thus
pL ϭ np,

(9)

pϭ

so that

np
L

(n integer).

Setting B ϭ 1, we thus obtain infinitely many solutions F (x) ϭ Fn (x), where
Fn (x) ϭ sin

(10)

np
x
L

(n ϭ 1, 2, Á ).

These solutions satisfy (8). [For negative integer n we obtain essentially the same solutions,
except for a minus sign, because sin (Ϫa) ϭ Ϫsin a.]
We now solve (6) with k ϭ Ϫp 2 ϭ Ϫ(np>L)2 resulting from (9), that is,

##

G ϩ ln2G ϭ 0

(11*)

where

ln ϭ cp ϭ

cnp
.
L

A general solution is
Gn(t) ϭ Bn cos lnt ϩ Bn* sin lnt.
Hence solutions of (1) satisfying (2) are u n(x, t) ϭ Fn(x)Gn(t) ϭ Gn(t)Fn(x), written out
(11)

u n (x, t) ϭ (Bn cos lnt ϩ Bn* sin lnt) sin

np
x
L

(n ϭ 1, 2, Á ).

These functions are called the eigenfunctions, or characteristic functions, and the values
ln ϭ cnp>L are called the eigenvalues, or characteristic values, of the vibrating string.
The set {l1, l2, Á } is called the spectrum.
Discussion of Eigenfunctions. We see that each u n represents a harmonic motion having
the frequency ln>2p ϭ cn>2L cycles per unit time. This motion is called the nth normal
mode of the string. The first normal mode is known as the fundamental mode (n ϭ 1),
and the others are known as overtones; musically they give the octave, octave plus fifth,
etc. Since in (11)
npx
sin L ϭ 0

at

L 2L
nϪ1
x ϭ n , n , Á , n L,

the nth normal mode has n Ϫ 1 nodes, that is, points of the string that do not move (in
addition to the fixed endpoints); see Fig. 287.

c12-a.qxd

10/30/10

548

1:44 PM

Page 548

CHAP. 12 Partial Differential Equations (PDEs)

L

0
n=1

L

0

L

0

n=2

L

0

n=3

n=4

Fig. 287. Normal modes of the vibrating string

Figure 288 shows the second normal mode for various values of t. At any instant the
string has the form of a sine wave. When the left part of the string is moving down, the
other half is moving up, and conversely. For the other modes the situation is similar.
Tuning is done by changing the tension T. Our formula for the frequency ln>2p ϭ cn>2L
of u n with c ϭ 1T>r [see (3), Sec. 12.2] confirms that effect because it shows that the
frequency is proportional to the tension. T cannot be increased indefinitely, but can you
see what to do to get a string with a high fundamental mode? (Think of both L and r.)
Why is a violin smaller than a double-bass?

L

Fig. 288.

x

Second normal mode for various values of t

Step 3. Solution of the Entire Problem. Fourier Series
The eigenfunctions (11) satisfy the wave equation (1) and the boundary conditions (2)
(string fixed at the ends). A single u n will generally not satisfy the initial conditions (3).
But since the wave equation (1) is linear and homogeneous, it follows from Fundamental
Theorem 1 in Sec. 12.1 that the sum of finitely many solutions u n is a solution of (1). To
obtain a solution that also satisfies the initial conditions (3), we consider the infinite series
(with ln ϭ cnp>L as before)
(12)

ؕ
ؕ
np
x.
u (x, t) ϭ a u n (x, t) ϭ a (Bn cos lnt ϩ Bn* sin lnt) sin
L
nϭ1
nϭ1

Satisfying Initial Condition (3a) (Given Initial Displacement). From (12) and (3a)
we obtain
ؕ

(13)

u (x, 0) ϭ a Bn sin np x ϭ f (x).
L
nϭ1

(0 Ϲ x Ϲ L).

Hence we must choose the Bn’s so that u (x, 0) becomes the Fourier sine series of f (x).
Thus, by (4) in Sec. 11.3,

(14)

Bn ϭ

2
L

Ύ

L

0

f (x) sin

npx
dx,
L

n ϭ 1, 2, Á .

c12-a.qxd

10/30/10

1:44 PM

Page 549

SEC. 12.3 Solution by Separating Variables. Use of Fourier Series

549

Satisfying Initial Condition (3b) (Given Initial Velocity). Similarly, by differentiating
(12) with respect to t and using (3b), we obtain
ؕ
0u
npx
`
ϭ c a (ϪBnln sin lnt ϩ Bn
*ln cos lnt) sin
d
0t tϭ0
L tϭ0
nϭ1
ؕ
npx
ϭ a Bn
*ln sin
ϭ g (x).
L
nϭ1

Hence we must choose the Bn*’s so that for t ϭ 0 the derivative 0u>0t becomes the Fourier
sine series of g (x). Thus, again by (4) in Sec. 11.3,
2
Bn
*ln ϭ
L

L

Ύ g (x) sin npL x dx.
0

Since ln ϭ cnp>L, we obtain by division
2
Bn
* ϭ cnp

(15)

L

Ύ g (x) sin npL x dx,

n ϭ 1, 2, Á .

0

Result. Our discussion shows that u (x, t) given by (12) with coefficients (14) and (15)
is a solution of (1) that satisfies all the conditions in (2) and (3), provided the series (12)
converges and so do the series obtained by differentiating (12) twice termwise with respect
to x and t and have the sums 0 2u>0x 2 and 0 2u>0t 2, respectively, which are continuous.
Solution (12) Established. According to our derivation, the solution (12) is at first a
purely formal expression, but we shall now establish it. For the sake of simplicity we
consider only the case when the initial velocity g (x) is identically zero. Then the Bn
* are
zero, and (12) reduces to
ؕ

u (x, t) ϭ a Bn cos lnt sin npx ,
L
nϭ1

(16)

ln ϭ

cnp
.
L

It is possible to sum this series, that is, to write the result in a closed or finite form. For
this purpose we use the formula [see (11), App. A3.1]
cos

cnp
np
1
np
np
t sin
x ϭ c sin e
(x Ϫ ct) f ϩ sin e
(x ϩ ct) f d .
L
L
2
L
L

Consequently, we may write (16) in the form
u (x, t) ϭ

1 ؕ
np
1 ؕ
np
Bn sin e
(x Ϫ ct) f ϩ a Bn sin e
(x ϩ ct) f .
a
2 nϭ1
L
2 nϭ1
L

These two series are those obtained by substituting x Ϫ ct and x ϩ ct, respectively, for
the variable x in the Fourier sine series (13) for f (x). Thus
(17)

u(x, t) ϭ 12 3 f *(x Ϫ ct) ϩ f *(x ϩ ct)4

c12-a.qxd

10/30/10

1:44 PM

550

Page 550

CHAP. 12 Partial Differential Equations (PDEs)

where f * is the odd periodic extension of f with the period 2L (Fig. 289). Since the initial
deflection f (x) is continuous on the interval 0 Ϲ x Ϲ L and zero at the endpoints, it follows
from (17) that u (x, t) is a continuous function of both variables x and t for all values of
the variables. By differentiating (17) we see that u (x, t) is a solution of (1), provided f (x)
is twice differentiable on the interval 0 Ͻ x Ͻ L, and has one-sided second derivatives at
x ϭ 0 and x ϭ L, which are zero. Under these conditions u (x, t) is established as a solution
of (1), satisfying (2) and (3) with g (x) ϵ 0.
᭿

x

L

0

Fig. 289. Odd periodic extension of f (x)

Generalized Solution. If f r(x) and f s(x) are merely piecewise continuous (see Sec. 6.1),
or if those one-sided derivatives are not zero, then for each t there will be finitely many
values of x at which the second derivatives of u appearing in (1) do not exist. Except at
these points the wave equation will still be satisfied. We may then regard u (x, t) as a
“generalized solution,” as it is called, that is, as a solution in a broader sense. For instance,
a triangular initial deflection as in Example 1 (below) leads to a generalized solution.
Physical Interpretation of the Solution (17). The graph of f * (x Ϫ ct) is obtained from
the graph of f * (x) by shifting the latter ct units to the right (Fig. 290). This means that
f * (x Ϫ ct)(c Ͼ 0) represents a wave that is traveling to the right as t increases. Similarly,
f *(x ϩ ct) represents a wave that is traveling to the left, and u (x, t) is the superposition
of these two waves.

f *(x)

f *(x – ct)

x

ct

Fig. 290. Interpretation of (17)

EXAMPLE 1

Vibrating String if the Initial Deflection Is Triangular
Find the solution of the wave equation (1) satisfying (2) and corresponding to the triangular initial deflection
2k
L

x

if

(L Ϫ x)

if

0ϽxϽ

L
2

f (x) ϭ e
2k
L

L
2

ϽxϽL

and initial velocity zero. (Figure 291 shows f (x) ϭ u (x, 0) at the top.)

Solution. Since g (x) ϵ 0, we have Bn* ϭ 0 in (12), and from Example 4 in Sec. 11.3 we see that the Bn are
given by (5), Sec. 11.3. Thus (12) takes the form
u (x, t) ϭ

8k

1

c 2
p2 1

sin

p
L

x cos

pc
L

tϪ

1
32

sin

3p
L

x cos

3pc
L

t ϩ Ϫ Á d.

c12-a.qxd

10/30/10

1:44 PM

Page 551

SEC. 12.3 Solution by Separating Variables. Use of Fourier Series

551

For graphing the solution we may use u (x, 0) ϭ f (x) and the above interpretation of the two functions in the
representation (17). This leads to the graph shown in Fig. 291.
᭿
u(x, 0)
1
f *(x)
2

0
1
f *(x + L)
2
5

t=0

L

0

L

1
f *(x – L )
2
5

t = L/5c

1
f *( x + 2L)
2
5

1
f *(x – 2L )
2
5

t = 2L/5c
1
f *(x + L)
2
2

1
f *(x – L )
2
2

t = L/2c

1
f *( x + 3L )
2
5

1
f *(x – 3L )
2
5

1
f *( x + 4L )
2
5

1
f *(x – 4L )
2
5

1
f *(x – L)
2
1
= f *(x + L)
2

t = 3L/5c

t = 4L/5c

t = L/c

Fig. 291. Solution u(x, t) in Example 1 for various values of t (right part
of the figure) obtained as the superposition of a wave traveling to the
right (dashed) and a wave traveling to the left (left part of the figure)

PROBLEM SET 12.3
1. Frequency. How does the frequency of the fundamental
mode of the vibrating string depend on the length of the
string? On the mass per unit length? What happens if
we double the tension? Why is a contrabass larger than
a violin?
2. Physical Assumptions. How would the motion of
the string change if Assumption 3 were violated?
Assumption 2? The second part of Assumption 1? The
first part? Do we really need all these assumptions?
3. String of length p. Write down the derivation in this
section for length L ϭ p, to see the very substantial
simplification of formulas in this case that may show
ideas more clearly.

4. CAS PROJECT. Graphing Normal Modes. Write a
program for graphing u n with L ϭ p and c2 of your
choice similarly as in Fig. 287. Apply the program to
u 2, u 3, u 4. Also graph these solutions as surfaces over
the xt-plane. Explain the connection between these two
kinds of graphs.
5–13

DEFLECTION OF THE STRING

Find u (x, t) for the string of length L ϭ 1 and c2 ϭ 1 when
the initial velocity is zero and the initial deflection with small
k (say, 0.01) is as follows. Sketch or graph u (x, t) as in
Fig. 291 in the text.
5. k sin 3px
6. k (sin px Ϫ 12 sin 2px)

c12-a.qxd

10/30/10

1:44 PM

Page 552

552

CHAP. 12 Partial Differential Equations (PDEs)

7. kx (1 Ϫ x)
9.

y-axis in the figure, r ϭ density, A ϭ cross-sectional
area). (Bending of a beam under a load is discussed in
Sec. 3.3.)
15. Substituting u ϭ F (x)G (t) into (21), show that

8. kx 2 (1 Ϫ x)

0.1

10.

F (4)>F ϭ ϪG>c2 G ϭ b4 ϭ const,

##

1

0.5

F (x) ϭ A cos bx ϩ B sin bx

1
4

ϩ C cosh bx ϩ D sinh bx,
3
4

1
4

G (t) ϭ a cos cb2 t ϩ b sin cb2 t.

1

–1

x

4

11.

1
4

x=0

1
4

12.

1
2

3
4

x=L

1

(B) Clamped at both
ends

1
4

x=0

3
4

1
4

x=L

1

13. 2x Ϫ 4x 2 if 0 Ͻ x Ͻ 12, 0 if 12 Ͻ x Ͻ 1
14. Nonzero initial velocity. Find the deflection u(x, t) of
the string of length L ϭ p and c2 ϭ 1 for zero initial displacement and “triangular” initial velocity u t(x, 0) ϭ 0.01x
if 0 Ϲ x Ϲ 12 p, u t (x, 0) ϭ 0.01 (p Ϫ x) if 12 p Ϲ
x Ϲ p. (Initial conditions with u t (x, 0) 0 are hard
to realize experimentally.)

x

x=L

u

x=0

x=L

16. Simply supported beam in Fig. 293A. Find solutions
u n ϭ Fn(x)Gn(t) of (21) corresponding to zero initial
velocity and satisfying the boundary conditions (see
Fig. 293A)
u (0, t) ϭ 0, u (L, t) ϭ 0
(ends simply supported for all times t),
u xx (0, t) ϭ 0, u xx (L, t) ϭ 0
(zero moments, hence zero curvature, at the ends).
17. Find the solution of (21) that satisfies the conditions in
Prob. 16 as well as the initial condition

y

u (x, 0) ϭ f (x) ϭ x (L Ϫ x).

SEPARATION OF A FOURTH-ORDER
PDE. VIBRATING BEAM

By the principles used in modeling the string it can be
shown that small free vertical vibrations of a uniform elastic
beam (Fig. 292) are modeled by the fourth-order PDE

(21)

(C) Clamped at the left
end, free at the
right end

Fig. 293. Supports of a beam

Fig. 292. Elastic beam
15–20

(A) Simply supported

0 2u
0t 2

ϭ Ϫc2

0 4u
0x 4

(Ref. [C11])

where c2 ϭ EI>rA (E ϭ Young’s modulus of elasticity,
I ϭ moment of intertia of the cross section with respect to the

18. Compare the results of Probs. 17 and 7. What is the
basic difference between the frequencies of the normal
modes of the vibrating string and the vibrating beam?
19. Clamped beam in Fig. 293B. What are the boundary
conditions for the clamped beam in Fig. 293B? Show
that F in Prob. 15 satisfies these conditions if bL is a
solution of the equation
(22)

cosh bL cos bL ϭ 1.

Determine approximate solutions of (22), for instance,
graphically from the intersections of the curves of
cos bL and 1>cosh bL.

c12-a.qxd

10/30/10

1:44 PM

Page 553

SEC. 12.4 D’Alembert’s Solution of the Wave Equation. Characteristics

Show that F in Prob. 15 satisfies these conditions if bL
is a solution of the equation

20. Clamped-free beam in Fig. 293C. If the beam is
clamped at the left and free at the right (Fig. 293C),
the boundary conditions are
u x (0, t) ϭ 0,
u (0, t) ϭ 0,
u xx (L, t) ϭ 0,
u xxx (L, t) ϭ 0.

12.4

553

cosh bL cos bL ϭ Ϫ1.

(23)

Find approximate solutions of (23).

D’Alembert’s Solution
of the Wave Equation. Characteristics
It is interesting that the solution (17), Sec. 12.3, of the wave equation
0 2u

(1)

0t

2

ϭ c2

0 2u
0x

2

T
c2 ϭ r ,

,

can be immediately obtained by transforming (1) in a suitable way, namely, by introducing
the new independent variables
v ϭ x ϩ ct,

(2)

w ϭ x Ϫ ct.

Then u becomes a function of v and w. The derivatives in (1) can now be expressed in terms
of derivatives with respect to v and w by the use of the chain rule in Sec. 9.6. Denoting
partial derivatives by subscripts, we see from (2) that vx ϭ 1 and wx ϭ 1. For simplicity
let us denote u (x, t), as a function of v and w, by the same letter u. Then
u x ϭ u vvx ϩ u wwx ϭ u v ϩ u w.
We now apply the chain rule to the right side of this equation. We assume that all the
partial derivatives involved are continuous, so that u wv ϭ u vw. Since vx ϭ 1 and wx ϭ 1,
we obtain
u xx ϭ (u v ϩ u w)x ϭ (u v ϩ u w)vvx ϩ (u v ϩ u w)wwx ϭ u vv ϩ 2u vw ϩ u ww.
Transforming the other derivative in (1) by the same procedure, we find
u tt ϭ c2 (u vv Ϫ 2u vw ϩ u ww).
By inserting these two results in (1) we get (see footnote 2 in App. A3.2)
u vw ϵ

(3)

0 2u
ϭ 0.
0w 0v

The point of the present method is that (3) can be readily solved by two successive
integrations, first with respect to w and then with respect to v. This gives
0u
ϭ h (v)
0v

and

uϭ

Ύ h (v) dv ϩ c (w).

c12-a.qxd

10/30/10

554

1:44 PM

Page 554

CHAP. 12 Partial Differential Equations (PDEs)

Here h (v) and c (w) are arbitrary functions of v and w, respectively. Since the integral is
a function of v, say, ␾ (v), the solution is of the form u ϭ ␾ (v) ϩ c (w). In terms of x
and t, by (2), we thus have
u (x, t) ϭ ␾ (x ϩ ct) ϩ c (x Ϫ ct).

(4)

This is known as d’Alembert’s solution1 of the wave equation (1).
Its derivation was much more elegant than the method in Sec. 12.3, but d’Alembert’s method
is special, whereas the use of Fourier series applies to various equations, as we shall see.

D’Alembert’s Solution Satisfying the Initial Conditions
(5)

(a) u (x, 0) ϭ f (x),

(b) u t (x, 0) ϭ g (x).

These are the same as (3) in Sec. 12.3. By differentiating (4) we have
(6)

u t (x, t) ϭ c␾ r(x ϩ ct) Ϫ cc r(x Ϫ ct)

where primes denote derivatives with respect to the entire arguments x ϩ ct and x Ϫ ct,
respectively, and the minus sign comes from the chain rule. From (4)–(6) we have
(7)

u (x, 0) ϭ ␾ (x) ϩ c (x) ϭ f (x),

(8)

u t (x, 0) ϭ c␾ r(x) ϩ cc r(x) ϭ g (x).

Dividing (8) by c and integrating with respect to x, we obtain
(9)

1
␾ (x) Ϫ c (x) ϭ k (x 0) ϩ c

x

Ύ g (s) ds,

k (x 0) ϭ ␾ (x 0) Ϫ c (x 0).

x0

If we add this to (7), then c drops out and division by 2 gives
(10)

␾ (x) ϭ

1
1
f (x) ϩ
2
2c

x

Ύ g (s) ds ϩ 21 k (x ).
0

x0

Similarly, subtraction of (9) from (7) and division by 2 gives
(11)

c (x) ϭ

1
1
f (x) Ϫ
2
2c

x

Ύ g (s) ds Ϫ 21 k (x ).
0

x0

In (10) we replace x by x ϩ ct; we then get an integral from x 0 to x ϩ ct. In (11) we
replace x by x Ϫ ct and get minus an integral from x 0 to x Ϫ ct or plus an integral from
x Ϫ ct to x 0. Hence addition of ␾ (x ϩ ct) and c (x Ϫ ct) gives u (x, t) [see (4)] in the form

(12)

u (x, t) ϭ

1
1
[ f (x ϩ ct) ϩ f (x Ϫ ct)] ϩ
2
2c

Ύ

x؉ct

g (s) ds.

x؊ct

1
JEAN LE ROND D’ALEMBERT (1717–1783), French mathematician, also known for his important work
in mechanics.
We mention that the general theory of PDEs provides a systematic way for finding the transformation (2)
that simplifies (1). See Ref. [C8] in App. 1.

c12-a.qxd

10/30/10

1:44 PM

Page 555

SEC. 12.4 D’Alembert’s Solution of the Wave Equation. Characteristics

555

If the initial velocity is zero, we see that this reduces to
u (x, t) ϭ 12 [ f (x ϩ ct) ϩ f (x Ϫ ct)],

(13)

in agreement with (17) in Sec. 12.3. You may show that because of the boundary conditions
(2) in that section the function f must be odd and must have the period 2L.
Our result shows that the two initial conditions [the functions f (x) and g (x) in (5)]
determine the solution uniquely.
The solution of the wave equation by the Laplace transform method will be shown in
Sec. 12.11.

Characteristics. Types and Normal Forms of PDEs
The idea of d’Alembert’s solution is just a special instance of the method of characteristics.
This concerns PDEs of the form
Au xx ϩ 2Bu xy ϩ Cu yy ϭ F (x, y, u, u x, u y)

(14)

(as well as PDEs in more than two variables). Equation (14) is called quasilinear because
it is linear in the highest derivatives (but may be arbitrary otherwise). There are three
types of PDEs (14), depending on the discriminant AC Ϫ B 2, as follows.
Type

Defining Condition

Hyperbolic

AC Ϫ B 2 Ͻ 0

Wave equation (1)

Parabolic

AC Ϫ B ϭ 0

Heat equation (2)

Elliptic

AC Ϫ B 2 Ͼ 0

Laplace equation (3)

2

Example in Sec. 12.1

Note that (1) and (2) in Sec. 12.1 involve t, but to have y as in (14), we set y ϭ ct in
(1), obtaining u tt Ϫ c2u xx ϭ c2(u yy Ϫ u xx) ϭ 0. And in (2) we set y ϭ c2t, so that
u t Ϫ c2u xx ϭ c2(u y Ϫ u xx).
A, B, C may be functions of x, y, so that a PDE may be of mixed type, that is, of different
type in different regions of the xy-plane. An important mixed-type PDE is the Tricomi
equation (see Prob. 10).
Transformation of (14) to Normal Form. The normal forms of (14) and the corresponding transformations depend on the type of the PDE. They are obtained by solving the
characteristic equation of (14), which is the ODE
(15)

Ay r 2 Ϫ 2By r ϩ C ϭ 0

where y r ϭ dy>dx (note Ϫ2B, not ϩ2B). The solutions of (15) are called the characteristics
of (14), and we write them in the form £ (x, y) ϭ const and ° (x, y) ϭ const. Then the
transformations giving new variables v, w instead of x, y and the normal forms of (14) are
as follows.

c12-a.qxd

10/30/10

1:44 PM

556

Page 556

CHAP. 12 Partial Differential Equations (PDEs)

Type

New Variables

Normal Form

Hyperbolic

vϭ£

wϭ⌿

u vw ϭ F1

Parabolic

vϭx
1
v ϭ (£ ϩ °)
2

wϭ£ϭ⌿
1
w ϭ (£ Ϫ °)
2i

u ww ϭ F2

Elliptic

u vv ϩ u ww ϭ F3

Here, ⌽ ϭ ⌽(x, y), ⌿ ϭ ⌿(x, y), F1 ϭ F1(v, w, u, u v, u w), etc., and we denote u as
function of v, w again by u, for simplicity. We see that the normal form of a hyperbolic
PDE is as in d’Alembert’s solution. In the parabolic case we get just one family of solutions
⌽ ϭ ⌿. In the elliptic case, i ϭ 1Ϫ1, and the characteristics are complex and are of
minor interest. For derivation, see Ref. [GenRef3] in App. 1.
EXAMPLE 1

D’Alembert’s Solution Obtained Systematically
The theory of characteristics gives d’Alembert’s solution in a systematic fashion. To see this, we write the wave
equation u tt Ϫ c2u xx ϭ 0 in the form (14) by setting y ϭ ct. By the chain rule, u t ϭ u yyt ϭ cu y and u tt ϭ c2u yy.
Division by c2 gives u xx Ϫ u yy ϭ 0, as stated before. Hence the characteristic equation is y r 2 Ϫ 1 ϭ (y r ϩ 1)
(y r Ϫ 1) ϭ 0. The two families of solutions (characteristics) are £(x, y) ϭ y ϩ x ϭ const and °(x, y) ϭ y Ϫ x ϭ
const. This gives the new variables v ϭ ⌽ ϭ y ϩ x ϭ ct ϩ x and w ϭ ° ϭ y Ϫ x ϭ ct Ϫ x and d’Alembert’s
solution u ϭ f1(x ϩ ct) ϩ f2(x Ϫ ct).
᭿

PROBLEM SET 12.4
1. Show that c is the speed of each of the two waves given
by (4).
2. Show that, because of the boundary conditions (2), Sec.
12.3, the function f in (13) of this section must be odd
and of period 2L.
3. If a steel wire 2 m in length weighs 0.9 nt (about 0.20
lb) and is stretched by a tensile force of 300 nt (about
67.4 lb), what is the corresponding speed of transverse
waves?
4. What are the frequencies of the eigenfunctions in
Prob. 3?
5–8
GRAPHING SOLUTIONS
Using (13) sketch or graph a figure (similar to Fig. 291 in
Sec. 12.3) of the deflection u (x, t) of a vibrating string
(length L ϭ 1, ends fixed, c ϭ 1) starting with initial
velocity 0 and initial deflection (k small, say, k ϭ 0.01).
5. f (x) ϭ k sin px
6. f (x) ϭ k (1 Ϫ cos px)
7. f (x) ϭ k sin 2px
8. f (x) ϭ kx (1 Ϫ x)
9–18
NORMAL FORMS
Find the type, transform to normal form, and solve. Show
your work in detail.
9. u xx ϩ 4u yy ϭ 0
10. u xx Ϫ 16u yy ϭ 0

2

11.
13.
15.
17.
19.

u xx ϩ 2u xy ϩ u yy ϭ 0 12. u xx Ϫ 2u xy ϩ u yy ϭ 0
u xx ϩ 5u xy ϩ 4u yy ϭ 0 14. xu xy Ϫ yu yy ϭ 0
16. u xx ϩ 2u xy ϩ 10u yy ϭ 0
xu xx Ϫ yu xy ϭ 0
u xx Ϫ 4u xy ϩ 5u yy ϭ 0 18. u xx Ϫ 6u xy ϩ 9u yy ϭ 0
Longitudinal Vibrations of an Elastic Bar or Rod.
These vibrations in the direction of the x-axis are
modeled by the wave equation u tt ϭ c2u xx, c2 ϭ E>r
(see Tolstov [C9], p. 275). If the rod is fastened at one
end, x ϭ 0, and free at the other, x ϭ L, we have
u (0, t) ϭ 0 and u x (L, t) ϭ 0. Show that the motion
corresponding to initial displacement u (x, 0) ϭ f (x)
and initial velocity zero is
ؕ

u ϭ a An sin pnx cos pnct,
nϭ0

An ϭ

2
L

L

Ύ f (x) sin p x dx,
n

0

pn ϭ

(2n ϩ 1)p
2L

.

20. Tricomi and Airy equations.2 Show that the Tricomi
equation yu xx ϩ u yy ϭ 0 is of mixed type. Obtain the
Airy equation G s Ϫ yG ϭ 0 from the Tricomi
equation by separation. (For solutions, see p. 446 of
Ref. [GenRef1] listed in App. 1.)

Sir GEORGE BIDELL AIRY (1801–1892), English mathematician, known for his work in elasticity. FRANCESCO
TRICOMI (1897–1978), Italian mathematician, who worked in integral equations and functional analysis.

c12-a.qxd

10/30/10

1:44 PM

Page 557

SEC. 12.5 Modeling: Heat Flow from a Body in Space. Heat Equation

12.5

557

Modeling: Heat Flow from a Body
in Space. Heat Equation
After the wave equation (Sec. 12.2) we now derive and discuss the next “big” PDE, the
heat equation, which governs the temperature u in a body in space. We obtain this model
of temperature distribution under the following.

Physical Assumptions
1. The specific heat s and the density r of the material of the body are constant. No
heat is produced or disappears in the body.
2. Experiments show that, in a body, heat flows in the direction of decreasing
temperature, and the rate of flow is proportional to the gradient (cf. Sec. 9.7) of the
temperature; that is, the velocity v of the heat flow in the body is of the form
v ϭ ϪK grad u

(1)

where u (x, y, z, t) is the temperature at a point (x, y, z) and time t.
3. The thermal conductivity K is constant, as is the case for homogeneous material and
nonextreme temperatures.
Under these assumptions we can model heat flow as follows.
Let T be a region in the body bounded by a surface S with outer unit normal vector n
such that the divergence theorem (Sec. 10.7) applies. Then
v•n
is the component of v in the direction of n. Hence ƒ v • n ¢A ƒ is the amount of heat leaving
T (if v • n Ͼ 0 at some point P) or entering T (if v • n Ͻ 0 at P) per unit time at some
point P of S through a small portion ¢S of S of area ¢A. Hence the total amount of heat
that flows across S from T is given by the surface integral

ΎΎ v • n dA.
S

Note that, so far, this parallels the derivation on fluid flow in Example 1 of Sec. 10.8.
Using Gauss’s theorem (Sec. 10.7), we now convert our surface integral into a volume
integral over the region T. Because of (1) this gives [use (3) in Sec. 9.8]
(2)

ΎΎ v • n dA ϭ ϪKΎΎ (grad u) • n dA ϭ ϪKΎΎΎdiv (grad u) dx dy dz
S

S

T

ΎΎΎٌ u dx dy dz.

ϭ ϪK

2

T

Here,
ٌ2u ϭ

0 2u
0x

is the Laplacian of u.

2

ϩ

0 2u
0y

2

ϩ

0 2u
0z 2

c12-a.qxd

10/30/10

558

1:44 PM

Page 558

CHAP. 12 Partial Differential Equations (PDEs)

On the other hand, the total amount of heat in T is
Hϭ

ΎΎΎsru dx dy dz
T

with s and r as before. Hence the time rate of decrease of H is
Ϫ

ΎΎΎsr 0u0t dx dy dz.

0H
ϭϪ
0t

T

This must be equal to the amount of heat leaving T because no heat is produced or
disappears in the body. From (2) we thus obtain

ΎΎΎ sr 0u0t dx dy dz ϭ ϪK ΎΎΎ ٌ u dx dy dz

Ϫ

2

T

T

or (divide by Ϫsr)

ΎΎΎ a 0u0t Ϫ c ٌ ub dx dy dz ϭ 0
2

2

K
c2 ϭ sr .

T

Since this holds for any region T in the body, the integrand (if continuous) must be zero
everywhere. That is,
0u
ϭ c2ٌ2u.
0t

(3)

c2 ϭ K>rs

This is the heat equation, the fundamental PDE modeling heat flow. It gives the
temperature u (x, y, z, t) in a body of homogeneous material in space. The constant c2 is
the thermal diffusivity. K is the thermal conductivity, s the specific heat, and r the density
of the material of the body. ٌ2u is the Laplacian of u and, with respect to the Cartesian
coordinates x, y, z, is
ٌ2u ϭ

0 2u
0x 2

ϩ

0 2u
0y 2

ϩ

0 2u
0z 2

.

The heat equation is also called the diffusion equation because it also models chemical
diffusion processes of one substance or gas into another.

12.6

Heat Equation: Solution by Fourier Series.
Steady Two-Dimensional Heat Problems.
Dirichlet Problem
We want to solve the (one-dimensional) heat equation just developed in Sec. 12.5 and
give several applications. This is followed much later in this section by an extension of
the heat equation to two dimensions.

c12-a.qxd

10/30/10

1:44 PM

Page 559

SEC. 12.6 Heat Equation: Solution by Fourier Series

559

x=L

0

Fig. 294. Bar under consideration

As an important application of the heat equation, let us first consider the temperature
in a long thin metal bar or wire of constant cross section and homogeneous material, which
is oriented along the x-axis (Fig. 294) and is perfectly insulated laterally, so that heat flows
in the x-direction only. Then besides time, u depends only on x, so that the Laplacian
reduces to u xx ϭ 0 2u>0x 2, and the heat equation becomes the one-dimensional heat
equation
0u
0 2u
ϭ c2 2 .
0t
0x

(1)

This PDE seems to differ only very little from the wave equation, which has a term u tt
instead of u t, but we shall see that this will make the solutions of (1) behave quite
differently from those of the wave equation.
We shall solve (1) for some important types of boundary and initial conditions. We
begin with the case in which the ends x ϭ 0 and x ϭ L of the bar are kept at temperature
zero, so that we have the boundary conditions
(2)

u (0, t) ϭ 0,

u (L, t) ϭ 0

for all t м 0.

Furthermore, the initial temperature in the bar at time t ϭ 0 is given, say, f (x), so that we
have the initial condition
u (x, 0) ϭ f (x)

(3)

[ f (x) given].

Here we must have f (0) ϭ 0 and f (L) ϭ 0 because of (2).
We shall determine a solution u (x, t) of (1) satisfying (2) and (3)—one initial condition
will be enough, as opposed to two initial conditions for the wave equation. Technically,
our method will parallel that for the wave equation in Sec. 12.3: a separation of variables,
followed by the use of Fourier series. You may find a step-by-step comparison
worthwhile.
Step 1. Two ODEs from. the heat equation
. (1). Substitution of a product u (x, t) ϭ
F (x)G (t) into (1) gives FG ϭ c2F s G with G ϭ dG>dt and F s ϭ d 2F>dx 2. To separate
the variables, we divide by c2FG, obtaining

#
G
Fs
ϭ
.
F
c2G

(4)

The left side depends only on t and the right side only on x, so that both sides must equal
a constant k (as in Sec. 12.3). You may show that for k ϭ 0 or k Ͼ 0 the only solution
u ϭ FG satisfying (2) is u ϵ 0. For negative k ϭ Ϫp 2 we have from (4)

#
Fs
G
ϭ Ϫp 2.
ϭ
F
c2G

c12-a.qxd

10/30/10

560

1:44 PM

Page 560

CHAP. 12 Partial Differential Equations (PDEs)

Multiplication by the denominators immediately gives the two ODEs
F s ϩ p 2F ϭ 0

(5)
and

#

G ϩ c2p 2G ϭ 0.

(6)

Step 2. Satisfying the boundary conditions (2). We first solve (5). A general solution is
F(x) ϭ A cos px ϩ B sin px.

(7)

From the boundary conditions (2) it follows that
u(0, t) ϭ F(0)G(t) ϭ 0

u(L, t) ϭ F(L)G(t) ϭ 0.

and

Since G ϵ 0 would give u ϵ 0, we require F (0) ϭ 0, F (L) ϭ 0 and get F (0) ϭ A ϭ 0
by (7) and then F (L) ϭ B sin pL ϭ 0, with B 0 (to avoid F ϵ 0); thus,
sin pL ϭ 0,

np
,
L

pϭ

hence

n ϭ 1, 2, Á .

Setting B ϭ 1, we thus obtain the following solutions of (5) satisfying (2):
Fn(x) ϭ sin

npx
,
L

n ϭ 1, 2, Á .

(As in Sec. 12.3, we need not consider negative integer values of n.)
All this was literally the same as in Sec. 12.3. From now on it differs since (6) differs
from (6) in Sec. 12.3. We now solve (6). For p ϭ np>L, as just obtained, (6) becomes

#

G ϩ ln2G ϭ 0

ln ϭ

where

cnp
.
L

It has the general solution
Gn(t) ϭ Bne؊lnt,
2

n ϭ 1, 2, Á

where Bn is a constant. Hence the functions

(8)

u n (x, t) ϭ Fn(x)Gn(t) ϭ Bn sin

npx ؊ln2 t
e
L

(n ϭ 1, 2, Á )

are solutions of the heat equation (1), satisfying (2). These are the eigenfunctions of the
problem, corresponding to the eigenvalues ln ϭ cnp>L.
Step 3. Solution of the entire problem. Fourier series. So far we have solutions (8)
satisfying the boundary conditions (2). To obtain a solution that also satisfies the initial
condition (3), we consider a series of these eigenfunctions,

(9)

ؕ
ؕ
npx ؊ln2 t
u (x, t) ϭ a u n(x, t) ϭ a Bn sin
e
L
nϭ1
nϭ1

aln ϭ

cnp
b.
L

c12-a.qxd

10/30/10

1:44 PM

Page 561

SEC. 12.6 Heat Equation: Solution by Fourier Series

561

From this and (3) we have
ؕ
npx
u(x, 0) ϭ a Bn sin
ϭ f (x).
L
nϭ1

Hence for (9) to satisfy (3), the Bn’s must be the coefficients of the Fourier sine series,
as given by (4) in Sec. 11.3; thus

2
Bn ϭ
L

(10)

L

Ύ f (x) sin npL x dx

(n ϭ 1, 2, Á .)

0

The solution of our problem can be established, assuming that f (x) is piecewise continuous
(see Sec. 6.1) on the interval 0 Ϲ x Ϲ L and has one-sided derivatives (see Sec. 11.1) at all
interior points of that interval; that is, under these assumptions the series (9) with coefficients
(10) is the solution of our physical problem. A proof requires knowledge of uniform
convergence and will be given at a later occasion (Probs. 19, 20 in Problem Set 15.5).
Because of the exponential factor, all the terms in (9) approach zero as t approaches
infinity. The rate of decay increases with n.
EXAMPLE 1

Sinusoidal Initial Temperature
Find the temperature u (x, t) in a laterally insulated copper bar 80 cm long if the initial temperature is
100 sin (px>80) °C and the ends are kept at 0°C. How long will it take for the maximum temperature in the bar
to drop to 50°C? First guess, then calculate. Physical data for copper: density 8.92 g>cm3, specific heat
0.092 cal>(g °C), thermal conductivity 0.95 cal>(cm sec °C).

Solution. The initial condition gives
ؕ
npx
px
u (x, 0) ϭ a Bn sin
ϭ f (x) ϭ 100 sin
.
80
80
nϭ1

Hence, by inspection or from (9), we get B1 ϭ 100, B2 ϭ B3 ϭ Á ϭ 0. In (9) we need l21 ϭ c2p2>L2, where
c2 ϭ K>(sr) ϭ 0.95>(0.092 # 8.92) ϭ 1.158 [cm2>sec]. Hence we obtain
l21 ϭ 1.158 # 9.870>802 ϭ 0.001785 [sec؊1].
The solution (9) is
u (x, t) ϭ 100 sin

px
80

e؊0.001785t.

Also, 100e؊0.001785t ϭ 50 when t ϭ (ln 0.5)>(Ϫ0.001785) ϭ 388 [sec] Ϸ 6.5 [min]. Does your guess, or at
᭿
least its order of magnitude, agree with this result?

EXAMPLE 2

Speed of Decay
Solve the problem in Example 1 when the initial temperature is 100 sin (3px>80) °C and the other data are as
before.

Solution. In (9), instead of n ϭ 1 we now have n ϭ 3, and l23 ϭ 32l21 ϭ 9 # 0.001785 ϭ 0.01607, so that
the solution now is
u (x, t) ϭ 100 sin

3px
80

e؊0.01607t.

Hence the maximum temperature drops to 50°C in t ϭ (ln 0.5)>(Ϫ0.01607) Ϸ 43 [sec], which is much faster
(9 times as fast as in Example 1; why?).

c12-a.qxd

10/30/10

1:44 PM

562

Page 562

CHAP. 12 Partial Differential Equations (PDEs)
Had we chosen a bigger n, the decay would have been still faster, and in a sum or series of such terms, each
term has its own rate of decay, and terms with large n are practically 0 after a very short time. Our next example
is of this type, and the curve in Fig. 295 corresponding to t ϭ 0.5 looks almost like a sine curve; that is, it is
practically the graph of the first term of the solution.
᭿

u
t=0

π
u

x

t = 0.1

π

x

π

x

π

x

t = 0.5
u

u

t=2

Fig. 295. Example 3. Decrease of temperature
with time t for L ‫ ؍‬p and c ‫ ؍‬1

EXAMPLE 3

“Triangular” Initial Temperature in a Bar
Find the temperature in a laterally insulated bar of length L whose ends are kept at temperature 0, assuming that
the initial temperature is
f (x) ϭ e

x

if

0 Ͻ x Ͻ L>2,

LϪx

if

L>2 Ͻ x Ͻ L.

(The uppermost part of Fig. 295 shows this function for the special L ϭ p.)

Solution. From (10) we get
Bn ϭ

(10*)

2
L

a

Ύ

L>2

x sin

npx

0

L

dx ϩ

Ύ

L

(L Ϫ x) sin

npx

L>2

L

dxb .

Integration gives Bn ϭ 0 if n is even,
Bn ϭ

4L
2

(n ϭ 1, 5, 9, Á )

2

n p

Bn ϭ Ϫ

and

4L

(n ϭ 3, 7, 11, Á ).

2

n p2

(see also Example 4 in Sec. 11.3 with k ϭ L>2). Hence the solution is
u (x, t) ϭ

4L
2

p

Bsin

px
L

exp BϪ a

cp
L

2

b tR Ϫ

1
9

sin

3px
L

exp BϪ a

3cp
L

2

b tR ϩ Ϫ Á R .

Figure 295 shows that the temperature decreases with increasing t, because of the heat loss due to the cooling
of the ends.
Compare Fig. 295 and Fig. 291 in Sec. 12.3 and comment.
᭿

c12-a.qxd

10/30/10

1:44 PM

Page 563

SEC. 12.6 Heat Equation: Solution by Fourier Series
EXAMPLE 4

563

Bar with Insulated Ends. Eigenvalue 0
Find a solution formula of (1), (3) with (2) replaced by the condition that both ends of the bar are insulated.

Solution. Physical experiments show that the rate of heat flow is proportional to the gradient of the
temperature. Hence if the ends x ϭ 0 and x ϭ L of the bar are insulated, so that no heat can flow through the
ends, we have grad u ϭ u x ϭ 0u>0x and the boundary conditions
u x(0, t) ϭ 0,

(2*)

u x(L, t) ϭ 0

for all t.

Since u (x, t) ϭ F (x)G (t), this gives u x (0, t) ϭ F r (0)G (t) ϭ 0 and u x (L, t) ϭ F r (L)G (t) ϭ 0. Differentiating
(7), we have F r (x) ϭ ϪAp sin px ϩ Bp cos px, so that
F r(0) ϭ Bp ϭ 0

F r(L) ϭ ϪAp sin pL ϭ 0.

and then

The second of these conditions gives p ϭ pn ϭ np>L, (n ϭ 0, 1, 2, Á ). From this and (7) with A ϭ 1 and
B ϭ 0 we get Fn (x) ϭ cos (npx>L), (n ϭ 0, 1, 2, Á ). With Gn as before, this yields the eigenfunctions
u n(x, t) ϭ Fn(x)Gn(t) ϭ An cos

(11)

npx
L

e؊ln t
2

(n ϭ 0, 1, Á )

corresponding to the eigenvalues ln ϭ cnp>L. The latter are as before, but we now have the additional eigenvalue
l0 ϭ 0 and eigenfunction u 0 ϭ const, which is the solution of the problem if the initial temperature f (x) is
constant. This shows the remarkable fact that a separation constant can very well be zero, and zero can be an
eigenvalue.
Furthermore, whereas (8) gave a Fourier sine series, we now get from (11) a Fourier cosine series
ؕ
ؕ
npx ؊l2nt
u (x, t) ϭ a u n (x, t) ϭ a An cos
e
L
nϭ0
nϭ0

(12)

aln ϭ

cnp
L

b.

Its coefficients result from the initial condition (3),
ؕ
npx
u(x, 0) ϭ a An cos
ϭ f (x),
L
nϭ0

in the form (2), Sec. 11.3, that is,
LΎ
1

A0 ϭ

(13)

L

An ϭ

f (x) dx,

0

EXAMPLE 5

LΎ
2

L

f (x) cos

0

npx
L

n ϭ 1, 2, Á .

dx,

᭿

“Triangular” Initial Temperature in a Bar with Insulated Ends
Find the temperature in the bar in Example 3, assuming that the ends are insulated (instead of being kept at
temperature 0).

Solution. For the triangular initial temperature, (13) gives A0 ϭ L>4 and (see also Example 4 in Sec. 11.3
with k ϭ L>2)
An ϭ

2
L

c

Ύ

L>2

x cos

npx

0

L

dx ϩ

Ύ

L

L>2

(L Ϫ x) cos

npx
L

dx d ϭ

2L
2

n p

2

a2 cos

np
2

Ϫ cos np Ϫ 1b .

Hence the solution (12) is
u (x, t) ϭ

L
4

Ϫ

8L
2

p

e

2

2

1
2px
2cp
1
6px
6cp
cos
exp BϪ a
b tR ϩ 2 cos
exp BϪa
b tR ϩ Á f .
22
L
L
L
L
6

We see that the terms decrease with increasing t, and u : L>4 as t : ϱ; this is the mean value of the initial
temperature. This is plausible because no heat can escape from this totally insulated bar. In contrast, the cooling
᭿
of the ends in Example 3 led to heat loss and u : 0, the temperature at which the ends were kept.

c12-a.qxd

10/30/10

564

1:44 PM

Page 564

CHAP. 12 Partial Differential Equations (PDEs)

Steady Two-Dimensional Heat Problems.
Laplace’s Equation
We shall now extend our discussion from one to two space dimensions and consider the
two-dimensional heat equation
0u
0 2u
0 2u
ϭ c2ٌ2u ϭ c2 a 2 ϩ 2 b
0t
0x
0y
for steady (that is, time-independent) problems. Then 0u>0t ϭ 0 and the heat equation
reduces to Laplace’s equation
ٌ2u ϭ

(14)

0 2u
0x 2

ϩ

0 2u
0y 2

ϭ0

(which has already occurred in Sec. 10.8 and will be considered further in Secs.
12.8–12.11). A heat problem then consists of this PDE to be considered in some region
R of the xy-plane and a given boundary condition on the boundary curve C of R. This is
a boundary value problem (BVP). One calls it:

First BVP or Dirichlet Problem if u is prescribed on C (“Dirichlet boundary
condition”)
Second BVP or Neumann Problem if the normal derivative u n ϭ 0u>0n is
prescribed on C (“Neumann boundary condition”)
Third BVP, Mixed BVP, or Robin Problem if u is prescribed on a portion of C
and u n on the rest of C (“Mixed boundary condition”).

y
u = f(x)
b
R

u=0
0
0

u=0

u=0
x
a

Fig. 296. Rectangle R and given boundary values

Dirichlet Problem in a Rectangle R (Fig. 296). We consider a Dirichlet problem for
Laplace’s equation (14) in a rectangle R, assuming that the temperature u (x, y) equals a
given function f (x) on the upper side and 0 on the other three sides of the rectangle.
We solve this problem by separating variables. Substituting u(x, y) ϭ F(x)G (y) into
(14) written as u xx ϭ Ϫu yy, dividing by FG, and equating both sides to a negative
constant, we obtain

c12-a.qxd

10/30/10

1:44 PM

Page 565

SEC. 12.6 Heat Equation: Solution by Fourier Series

565

1 # d 2F
1 # d 2G
ϭ Ϫk.
2 ϭ Ϫ
F dx
G dy 2
From this we get
d 2F
dx 2

ϩ kF ϭ 0,

and the left and right boundary conditions imply
F(0) ϭ 0,

and

F(a) ϭ 0.

This gives k ϭ (np>a)2 and corresponding nonzero solutions
np
F(x) ϭ Fn(x) ϭ sin a x,

(15)

n ϭ 1, 2, Á .

The ODE for G with k ϭ (np>a)2 then becomes
d 2G
np 2
b G ϭ 0.
2 Ϫ a
a
dy
Solutions are
G( y) ϭ Gn( y) ϭ Anenpy>a ϩ Bne؊npy>a.
Now the boundary condition u ϭ 0 on the lower side of R implies that Gn(0) ϭ 0; that
is, Gn(0) ϭ An ϩ Bn ϭ 0 or Bn ϭ ϪAn. This gives
npy
Gn( y) ϭ An(enpy>a Ϫ e؊npy>a) ϭ 2An sinh a .
* , we obtain as the eigenfunctions of our problem
From this and (15), writing 2An ϭ An
(16)

npx
npy
u n(x, y) ϭ Fn(x)Gn( y) ϭ A*n sin a sinh a .

These solutions satisfy the boundary condition u ϭ 0 on the left, right, and lower sides.
To get a solution also satisfying the boundary condition u (x, b) ϭ f (x) on the upper
side, we consider the infinite series
ؕ

u(x, y) ϭ a u n (x, y).
nϭ1

From this and (16) with y ϭ b we obtain
ؕ
npx
npb
u(x, b) ϭ f (x) ϭ a An* sin a sinh a .
nϭ1

We can write this in the form
ؕ
npb
npx
u(x, b) ϭ a aA*n sinh a b sin a .
nϭ1

c12-a.qxd

10/30/10

566

1:44 PM

Page 566

CHAP. 12 Partial Differential Equations (PDEs)

This shows that the expressions in the parentheses must be the Fourier coefficients bn of
f (x); that is, by (4) in Sec. 11.3,
npb
2
bn ϭ A*n sinh a ϭ a

Ύ

a

npx
f (x) sin a dx.

0

From this and (16) we see that the solution of our problem is
(17)

ؕ
ؕ
npy
npx
u(x, y) ϭ a u n (x, y) ϭ a A*n sin a sinh a
nϭ1

nϭ1

where

(18)

A*n ϭ

2
a sinh (npb>a)

Ύ

a

n px
f (x) sin a dx.

0

We have obtained this solution formally, neither considering convergence nor showing
that the series for u, u xx, and u yy have the right sums. This can be proved if one assumes
that f and f r are continuous and f s is piecewise continuous on the interval 0 Ϲ x Ϲ a.
The proof is somewhat involved and relies on uniform convergence. It can be found in
[C4] listed in App. 1.

Unifying Power of Methods. Electrostatics, Elasticity
The Laplace equation (14) also governs the electrostatic potential of electrical charges in any
region that is free of these charges. Thus our steady-state heat problem can also be interpreted
as an electrostatic potential problem. Then (17), (18) is the potential in the rectangle R when
the upper side of R is at potential f (x) and the other three sides are grounded.
Actually, in the steady-state case, the two-dimensional wave equation (to be considered
in Secs. 12.8, 12.9) also reduces to (14). Then (17), (18) is the displacement of a rectangular
elastic membrane (rubber sheet, drumhead) that is fixed along its boundary, with three
sides lying in the xy-plane and the fourth side given the displacement f (x).
This is another impressive demonstration of the unifying power of mathematics. It
illustrates that entirely different physical systems may have the same mathematical model
and can thus be treated by the same mathematical methods.

PROBLEM SET 12.6
1. Decay. How does the rate of decay of (8) with fixed
n depend on the specific heat, the density, and the
thermal conductivity of the material?
2. Decay. If the first eigenfunction (8) of the bar
decreases to half its value within 20 sec, what is the
value of the diffusivity?

3. Eigenfunctions. Sketch or graph and compare the first
three eigenfunctions (8) with Bn ϭ 1, c ϭ 1, and
L ϭ p for t ϭ 0, 0.1, 0.2, Á , 1.0.
4. WRITING PROJECT. Wave and Heat Equations.
Compare these PDEs with respect to general behavior
of eigenfunctions and kind of boundary and initial

c12-a.qxd

10/30/10

1:44 PM

Page 567

SEC. 12.6 Heat Equation: Solution by Fourier Series
conditions. State the difference between Fig. 291 in
Sec. 12.3 and Fig. 295.
5–7
LATERALLY INSULATED BAR
Find the temperature u (x, t) in a bar of silver of length
10 cm and constant cross section of area 1 cm2 (density
10.6 g>cm3, thermal conductivity 1.04 cal>(cm sec °C),
specific heat 0.056 cal>(g °C) that is perfectly insulated
laterally, with ends kept at temperature 0°C and initial
temperature f (x) °C, where
5. f (x) ϭ sin 0.1px
6. f (x) ϭ 4 Ϫ 0.8 ƒ x Ϫ 5 ƒ
7. f (x) ϭ x (10 Ϫ x)
8. Arbitrary temperatures at ends. If the ends x ϭ 0
and x ϭ L of the bar in the text are kept at constant
temperatures U1 and U2, respectively, what is the temperature u 1(x) in the bar after a long time (theoretically,
as t : ϱ )? First guess, then calculate.
9. In Prob. 8 find the temperature at any time.
10. Change of end temperatures. Assume that the ends
of the bar in Probs. 5–7 have been kept at 100°C for a
long time. Then at some instant, call it t ϭ 0, the
temperature at x ϭ L is suddenly changed to 0°C and
kept at 0°C, whereas the temperature at x ϭ 0 is kept
at 100°C. Find the temperature in the middle of the bar
at t ϭ 1, 2, 3, 10, 50 sec. First guess, then calculate.

567
18–25

TWO-DIMENSIONAL PROBLEMS

18. Laplace equation. Find the potential in the rectangle 0 Ϲ x Ϲ 20, 0 Ϲ y Ϲ 40 whose upper side is
kept at potential 110 V and whose other sides are
grounded.
19. Find the potential in the square 0 Ϲ x Ϲ 2, 0 Ϲ y Ϲ 2
if the upper side is kept at the potential 1000 sin 12 px
and the other sides are grounded.
20. CAS PROJECT. Isotherms. Find the steady-state
solutions (temperatures) in the square plate in Fig. 297
with a ϭ 2 satisfying the following boundary conditions. Graph isotherms.
(a) u ϭ 80 sin px on the upper side, 0 on the others.
(b) u ϭ 0 on the vertical sides, assuming that the other
sides are perfectly insulated.
(c) Boundary conditions of your choice (such that the
solution is not identically zero).
y
a

a

x

Fig. 297. Square plate

BAR UNDER ADIABATIC CONDITIONS
“Adiabatic” means no heat exchange with the neighborhood, because the bar is completely insulated, also at
the ends. Physical Information: The heat flux at the ends
is proportional to the value of 0u> 0x there.
11. Show that for the completely insulated bar, u x (0, t) ϭ 0,
u x (L, t) ϭ 0, u (x, t) ϭ f (x) and separation of variables
gives the following solution, with An given by (2) in
Sec. 11.3.
ؕ
npx ؊(cnp>L)2t
u(x, t) ϭ A0 ϩ a An cos
e
L
nϭ1

12–15
Find the temperature in Prob. 11 with L ϭ p,
c ϭ 1, and
12. f (x) ϭ x
13. f (x) ϭ 1
14. f (x) ϭ cos 2x
15. f (x) ϭ 1 Ϫ x> p
16. A bar with heat generation of constant rate H ( Ͼ 0)
is modeled by u t ϭ c2u xx ϩ H. Solve this problem if
L ϭ p and the ends of the bar are kept at 0°C. Hint.
Set u ϭ v Ϫ Hx(x Ϫ p)>(2c2).
17. Heat flux. The heat flux of a solution u (x, t) across x ϭ 0
is defined by ␾ (t) ϭ ϪKu x (0, t). Find ␾ (t) for the
solution (9). Explain the name. Is it physically understandable that ␾ goes to 0 as t : ϱ ?

21. Heat flow in a plate. The faces of the thin square plate
in Fig. 297 with side a ϭ 24 are perfectly insulated.
The upper side is kept at 25°C and the other sides are
kept at 0°C. Find the steady-state temperature u (x, y)
in the plate.
22. Find the steady-state temperature in the plate in Prob.
21 if the lower side is kept at U0°C, the upper side at
U1°C, and the other sides are kept at 0°C. Hint: Split
into two problems in which the boundary temperature
is 0 on three sides for each problem.
23. Mixed boundary value problem. Find the steadystate temperature in the plate in Prob. 21 with the upper
and lower sides perfectly insulated, the left side kept
at 0°C, and the right side kept at f (y)°C.
24. Radiation. Find steady-state temperatures in the
rectangle in Fig. 296 with the upper and left sides
perfectly insulated and the right side radiating into a
medium at 0°C according to u x (a, y) ϩ hu (a, y) ϭ 0,
h Ͼ 0 constant. (You will get many solutions since no
condition on the lower side is given.)
25. Find formulas similar to (17), (18) for the temperature
in the rectangle R of the text when the lower side of R
is kept at temperature f (x) and the other sides are kept
at 0°C.

c12-a.qxd

10/30/10

568

12.7

1:44 PM

Page 568

CHAP. 12 Partial Differential Equations (PDEs)

Heat Equation: Modeling Very Long Bars.
Solution by Fourier Integrals and
Transforms
Our discussion of the heat equation
0u
0 2u
ϭ c2 2
0t
0x

(1)

in the last section extends to bars of infinite length, which are good models of very long
bars or wires (such as a wire of length, say, 300 ft). Then the role of Fourier series in the
solution process will be taken by Fourier integrals (Sec. 11.7).
Let us illustrate the method by solving (1) for a bar that extends to infinity on both
sides (and is laterally insulated as before). Then we do not have boundary conditions, but
only the initial condition
u(x, 0) ϭ f (x)

(2)

(Ϫϱ Ͻ x Ͻ ϱ)

where f (x) is the given initial temperature of the bar.
To solve this problem, we start as in the last section, substituting u(x, t) ϭ F(x)G(t)
into (1). This gives the two ODEs
(3)

F s ϩ p 2F ϭ 0

[see (5), Sec. 12.6]

and

#

(4)

G ϩ c2p 2G ϭ 0

[see (6), Sec. 12.6].

Solutions are
F(x) ϭ A cos px ϩ B sin px

and

G(t) ϭ e؊c

2

p2t

,

respectively, where A and B are any constants. Hence a solution of (1) is
(5)

u(x, t; p) ϭ FG ϭ (A cos px ϩ B sin px) e؊c

2

p2t

.

Here we had to choose the separation constant k negative, k ϭ Ϫp 2, because positive
values of k would lead to an increasing exponential function in (5), which has no physical
meaning.

Use of Fourier Integrals
Any series of functions (5), found in the usual manner by taking p as multiples of a fixed
number, would lead to a function that is periodic in x when t ϭ 0. However, since f (x)

c12-a.qxd

10/30/10

1:44 PM

Page 569

SEC. 12.7 Heat Equation: Modeling Very Long Bars. Solution by Fourier Integrals and Transforms

569

in (2) is not assumed to be periodic, it is natural to use Fourier integrals instead of Fourier
series. Also, A and B in (5) are arbitrary and we may regard them as functions of p, writing
A ϭ A ( p) and B ϭ B (P). Now, since the heat equation (1) is linear and homogeneous,
the function

(6)

u(x, t) ϭ

Ύ

ؕ

u (x, t; p) dp ϭ

0

Ύ

ؕ

[A( p) cos px ϩ B( p) sin px] e؊c

2

p2t

dp

0

is then a solution of (1), provided this integral exists and can be differentiated twice with
respect to x and once with respect to t.
Determination of A( p) and B( p) from the Initial Condition. From (6) and (2) we get
(7)

ؕ

Ύ

u(x, 0) ϭ

[A( p) cos px ϩ B( p) sin px] dp ϭ f (x).

0

This gives A ( p) and B ( p) in terms of f (x); indeed, from (4) in Sec. 11.7 we have
(8)

1
A( p) ϭ p

Ύ

ؕ

1
B( p) ϭ p

f (v) cos pv dv,

؊ؕ

Ύ

ؕ

f (v) sin pv dv.

؊ؕ

According to (1*), Sec. 11.9, our Fourier integral (7) with these A ( p) and B ( p) can be
written
1
u(x, 0) ϭ p

ؕ

Ύ cΎ

ؕ

f (v) cos ( px Ϫ pv) dv d dp.

؊ؕ

0

Similarly, (6) in this section becomes
1
u(x, t) ϭ p

ؕ

Ύ cΎ

ؕ

f (v) cos ( px Ϫ pv) e؊c

2

؊ؕ

0

p2t

dv d dp.

Assuming that we may reverse the order of integration, we obtain

(9)

1
u(x, t) ϭ p

Ύ

ؕ

؊ؕ

f (v) c

Ύ

ؕ

e؊c

0

2

p2t

cos ( px Ϫ pv) dp d dv.

Then we can evaluate the inner integral by using the formula
(10)

Ύ

ؕ

e؊s cos 2bs ds ϭ
2

0

1p ؊b2
e .
2

[A derivation of (10) is given in Problem Set 16.4 (Team Project 24).] This takes the form
of our inner integral if we choose p ϭ s>(c1t) as a new variable of integration and set
bϭ

xϪv
.
2c1t

c12-a.qxd

10/30/10

1:44 PM

570

Page 570

CHAP. 12 Partial Differential Equations (PDEs)

Then 2bs ϭ (x Ϫ v)p and ds ϭ c1t dp, so that (10) becomes

Ύ

ؕ

e؊c

2

p2t

cos ( px Ϫ pv) dp ϭ

0

2p
2c1t

exp e Ϫ

(x Ϫ v)2
4c2t

f.

By inserting this result into (9) we obtain the representation

(11)

u(x, t) ϭ

Ύ
2c1pt
1

ؕ

f (v) exp e Ϫ

(x Ϫ v)2
4c2t

؊ؕ

f dv.

Taking z ϭ (v Ϫ x)>(2c 1t) as a variable of integration, we get the alternative form

(12)

u(x, t) ϭ

1
1p

Ύ

ؕ

f (x ϩ 2cz1t) e؊z dz.
2

؊ؕ

If f (x) is bounded for all values of x and integrable in every finite interval, it can be
shown (see Ref. [C10]) that the function (11) or (12) satisfies (1) and (2). Hence this
function is the required solution in the present case.
EXAMPLE 1

Temperature in an Infinite Bar
Find the temperature in the infinite bar if the initial temperature is (Fig. 298)
f (x) ϭ e

U0 ϭ const

if

ƒ x ƒ Ͻ 1,

0

if

ƒ x ƒ Ͼ 1.

f(x)
U0

–1

x

1

Fig. 298. Initial temperature in Example 1

Solution. From (11) we have
u(x, t) ϭ

Ύ
2c1pt
U0

1

exp e Ϫ

(x Ϫ v)2

؊1

4c2t

f dv.

If we introduce the above variable of integration z, then the integration over v from Ϫ1 to 1 corresponds to the
integration over z from (Ϫ1 Ϫ x)>(2c 1t) to (1 Ϫ x)>(2c 1t), and

(13)

u(x, t) ϭ

U0

Ύ
1p

(1؊x)>(2c2t)

e؊z dz
2

(t Ͼ 0).

؊(1ϩx)>(2c2t )

We mention that this integral is not an elementary function, but can be expressed in terms of the error
function, whose values have been tabulated. (Table A4 in App. 5 contains a few values; larger tables are
listed in Ref. [GenRef1] in App. 1. See also CAS Project 1, p. 574.) Figure 299 shows u (x, t) for U0 ϭ 100°C,
c2 ϭ 1 cm2>sec, and several values of t.
᭿

c12-a.qxd

10/30/10

1:44 PM

Page 571

SEC. 12.7 Heat Equation: Modeling Very Long Bars. Solution by Fourier Integrals and Transforms

571

u(x, t)
100

t=0
t=1
8

t=1
2

t=1
t=2
t=8

–3

–2

0

–1

1

2

3

x

Fig. 299. Solution u(x, t) in Example 1 for U0 ϭ 100°C,
c 2 ϭ 1 cm2/sec, and several values of t

Use of Fourier Transforms
The Fourier transform is closely related to the Fourier integral, from which we obtained the
transform in Sec. 11.9. And the transition to the Fourier cosine and sine transform in Sec.
11.8 was even simpler. (You may perhaps wish to review this before going on.) Hence it
should not surprise you that we can use these transforms for solving our present or similar
problems. The Fourier transform applies to problems concerning the entire axis, and the
Fourier cosine and sine transforms to problems involving the positive half-axis. Let us explain
these transform methods by typical applications that fit our present discussion.
EXAMPLE 2

Temperature in the Infinite Bar in Example 1
Solve Example 1 using the Fourier transform.

Solution. The problem consists of the heat equation (1) and the initial condition (2), which in this example is
f (x) ϭ U0 ϭ const if ƒ x ƒ Ͻ 1

and 0 otherwise.

Our strategy is to take the Fourier transform with respect to x and then to solve the resulting ordinary DE in t.
The details are as follows.
Let uˆ ϭ f(u) denote the Fourier transform of u, regarded as a function of x. From (10) in Sec. 11.9 we see
that the heat equation (1) gives
f(u t) ϭ c2f(u xx) ϭ c2(Ϫw 2)f(u) ϭ Ϫc2w 2uˆ.
On the left, assuming that we may interchange the order of differentiation and integration, we have
f(u t) ϭ

1
12p

Ύ

ؕ

u te؊iwx dx ϭ

؊ؕ

1
0
12p 0t

Ύ

ؕ

ue؊iwx dx ϭ

؊ؕ

0uˆ
.
0t

Thus
0uˆ
ϭ Ϫc2w 2uˆ.
0t
Since this equation involves only a derivative with respect to t but none with respect to w, this is a first-order
ordinary DE, with t as the independent variable and w as a parameter. By separating variables (Sec. 1.3) we
get the general solution
uˆ (w, t) ϭ C (w)e؊c

2

w2t

c12-a.qxd

10/30/10

1:44 PM

572

Page 572

CHAP. 12 Partial Differential Equations (PDEs)
with the arbitrary “constant” C (w) depending on the parameter w. The initial condition (2) yields the relationship
uˆ (w, 0) ϭ C (w) ϭ fˆ(w) ϭ f( f ). Our intermediate result is
2 2
uˆ (w, t) ϭ fˆ(w)e؊c w t.

The inversion formula (7), Sec. 11.9, now gives the solution
u (x, t) ϭ

(14)

ؕ

Ύ
12p
1

؊c2w2t iwx
e
dw.
fˆ(w) e

؊ؕ

In this solution we may insert the Fourier transform
fˆ(w) ϭ

Ύ
12p
1

ؕ

f (v)eivwdv.

؊ؕ

Assuming that we may invert the order of integration, we then obtain
2p Ύ
1

u(x, t) ϭ

ؕ

Ύ

f (v) c

؊ؕ

ؕ

e؊c

w2t

ei(wx؊wv)dw d dv.

2

؊ؕ

By the Euler formula (3). Sec. 11.9, the integrand of the inner integral equals
e؊c

2

w2t

cos (wx Ϫ wv) ϩ ie؊c

2

w2t

sin (wx Ϫ wv).

We see that its imaginary part is an odd function of w, so that its integral is 0. (More precisely, this is the
principal part of the integral; see Sec. 16.4.) The real part is an even function of w, so that its integral from Ϫϱ
to ϱ equals twice the integral from 0 to ϱ :
u (x, t) ϭ

1

p

Ύ

ؕ

f (v) c

؊ؕ

Ύ

ؕ

e؊c

2

w2t

0

cos (wx Ϫ wv) dw d dv.

This agrees with (9) (with p ϭ w) and leads to the further formulas (11) and (13).

EXAMPLE 3

Solution in Example 1 by the Method of Convolution
Solve the heat problem in Example 1 by the method of convolution.

Solution. The beginning is as in Example 2 and leads to (14), that is,
(15)

u (x, t) ϭ

Ύ
12p
1

ؕ

2 2
fˆ(w)e؊c w teiwx dw.

؊ؕ

Now comes the crucial idea. We recognize that this is of the form (13) in Sec. 11.9, that is,
(16)

u(x, t) ϭ ( f * g) (x) ϭ

Ύ

ؕ

fˆ(w)gˆ (w)eiwx dw

؊ؕ

where
(17)

gˆ (w) ϭ

1
12p

e؊c

2

w2t

.

Since, by the definition of convolution [(11), Sec. 11.9],
(18)

( f * g) (x) ϭ

Ύ

ؕ

f ( p)g (x Ϫ p) dp,

؊ؕ

᭿

c12-a.qxd

10/30/10

1:44 PM

Page 573

SEC. 12.7 Heat Equation: Modeling Very Long Bars. Solution by Fourier Integrals and Transforms

573

as our next and last step we must determine the inverse Fourier transform g of gˆ. For this we can use formula
9 in Table III of Sec. 11.10,
f(e؊ax ) ϭ

1

2

>(4a)

e؊w

2

12a

with a suitable a. With c2t ϭ 1>(4a) or a ϭ 1>(4c2t), using (17) we obtain
f(e؊x

>(4c2t)

) ϭ 22c2t e؊c

2

2

w2t

ϭ 22c2t 12pgˆ (w).

Hence gˆ has the inverse
1

e؊x

>(4c2t)

2

22c t 22p
2

.

Replacing x with x Ϫ p and substituting this into (18) we finally have

(19)

Ύ
2c1pt
1

u(x, t) ϭ ( f * g) (x) ϭ

ؕ

f ( p) exp e Ϫ

؊ؕ

(x Ϫ p)2
4c2t

f dp.

This solution formula of our problem agrees with (11). We wrote ( f * g)(x), without indicating the parameter t
with respect to which we did not integrate.
᭿

EXAMPLE 4

Fourier Sine Transform Applied to the Heat Equation
If a laterally insulated bar extends from x ϭ 0 to infinity, we can use the Fourier sine transform. We let the
initial temperature be u (x, 0) ϭ f (x) and impose the boundary condition u (0, t) ϭ 0. Then from the heat equation
and (9b) in Sec. 11.8, since f (0) ϭ u (0, 0) ϭ 0, we obtain
fs(u t) ϭ

0uˆ s
0t

ϭ c2fs(u xx) ϭ Ϫc2w 2fs (u) ϭ Ϫc2w 2uˆ s(w, t).

This is a first-order ODE 0uˆ s> 0t ϩ c2w 2uˆ s ϭ 0. Its solution is
uˆ s (w, t) ϭ C(w)e؊c

2

w2t

.

From the initial condition u (x, 0) ϭ f (x) we have uˆ s (w, 0) ϭ fˆs (w) ϭ C (w). Hence
uˆ s (w, t) ϭ fˆs (w)e؊c

2

w2t

.

Taking the inverse Fourier sine transform and substituting
2

fˆs (w) ϭ

Bp

Ύ

ؕ

f ( p) sin wp dp

0

on the right, we obtain the solution formula

(20)

u (x, t) ϭ

2

p

ؕ

ΎΎ
0

ؕ

f ( p) sin wp e؊c

2

w2t

sin wx dp dw.

0

Figure 300 shows (20) with c ϭ 1 for f (x) ϭ 1 if 0 Ϲ x Ϲ 1 and 0 otherwise, graphed over the xt-plane for
0 Ϲ x Ϲ 2, 0.01 Ϲ t Ϲ 1.5. Note that the curves of u (x, t) for constant t resemble those in Fig. 299.
᭿

c12-a.qxd

10/30/10

574

1:44 PM

Page 574

CHAP. 12 Partial Differential Equations (PDEs)

1
0.5

0.5

1

1
1.5

2

t

x

Fig. 300. Solution (20) in Example 4

PROBLEM SET 12.7
1. CAS PROJECT. Heat Flow. (a) Graph the basic
Fig. 299.
(b) In (a) apply animation to “see” the heat flow in
terms of the decrease of temperature.
(c) Graph u (x, t) with c ϭ 1 as a surface over a
rectangle of the form Ϫa Ͻ x Ͻ a, 0 Ͻ y Ͻ b.
2–8

SOLUTION
IN INTEGRAL FORM

(21)

CAS PROJECT. Error Function.
erf x ϭ

Ύe

؊w2

dw ϭ

a

Ύ

1p
(erf b Ϫ erf a).
2

b

e؊w dw ϭ 1p erf b.
2

10. Obtain the Maclaurin series of erf x from that of the
integrand. Use that series to compute a table of erf x
for x ϭ 0 (0.01)3 (meaning x ϭ 0, 0.01, 0.02, Á , 3).
11. Obtain the values required in Prob. 10 by an integration
command of your CAS. Compare accuracy.
12. It can be shown that erf (ϱ) ϭ 1. Confirm this experimentally by computing erf x for large x.
13. Let f (x) ϭ 1 when x Ͼ 0 and 0 when x Ͻ 0. Using
erf (ϱ) ϭ 1, show that (12) then gives
u (x, t) ϭ

؊w2

dw

1p

Ύ

ؕ

e؊x dz
2

؊x>(2c1t)

(t Ͼ 0).

14. Express the temperature (13) in terms of the error
function.
15. Show that £(x) ϭ

0

This function is important in applied mathematics and
physics (probability theory and statistics, thermodynamics,
etc.) and fits our present discussion. Regarding it as a typical
case of a special function defined by an integral that cannot
be evaluated as in elementary calculus, do the following.

1

1
x
1
b
ϭ Ϫ erf aϪ
2c 1t
2
2

x

Ύe
1p
2

b

Ϫb

Using (6), obtain the solution of (1) in integral form
satisfying the initial condition u (x, 0) ϭ f (x), where
2. f (x) ϭ 1 if ƒ x ƒ Ͻ a and 0 otherwise
3. f (x) ϭ 1>(1 ϩ x 2).
Hint. Use (15) in Sec. 11.7.
4. f (x) ϭ e؊ƒxƒ
5. f (x) ϭ ƒ x ƒ if ƒ x ƒ Ͻ 1 and 0 otherwise
6. f (x) ϭ x if ƒ x ƒ Ͻ 1 and 0 otherwise
7. f (x) ϭ (sin x)>x.
Hint. Use Prob. 4 in Sec. 11.7.
8. Verify that u in the solution of Prob. 7 satisfies the
initial condition.
9–12

9. Graph the bell-shaped curve [the curve of the integrand in (21)]. Show that erf x is odd. Show that

Ύ
12p
1

x

e؊s

>2

2

ds

؊ؕ

1
1
x
ϩ erf a
b.
12
2
2
Here, the integral is the definition of the “distribution
function of the normal probability distribution” to be
discussed in Sec. 24.8.
ϭ

c12-b.qxd

10/30/10

1:59 PM

Page 575

SEC. 12.8 Modeling: Membrane, Two-Dimensional Wave Equation

12.8

575

Modeling: Membrane,
Two-Dimensional Wave Equation
Since the modeling here will be similar to that of Sec. 12.2, you may want to take another
look at Sec. 12.2.
The vibrating string in Sec. 12.2 is a basic one-dimensional vibrational problem. Equally
important is its two-dimensional analog, namely, the motion of an elastic membrane, such
as a drumhead, that is stretched and then fixed along its edge. Indeed, setting up the model
will proceed almost as in Sec. 12.2.

Physical Assumptions
1. The mass of the membrane per unit area is constant (“homogeneous membrane”).
The membrane is perfectly flexible and offers no resistance to bending.
2. The membrane is stretched and then fixed along its entire boundary in the xy-plane.
The tension per unit length T caused by stretching the membrane is the same at all
points and in all directions and does not change during the motion.
3. The deflection u (x, y, t) of the membrane during the motion is small compared to
the size of the membrane, and all angles of inclination are small.
Although these assumptions cannot be realized exactly, they hold relatively accurately for
small transverse vibrations of a thin elastic membrane, so that we shall obtain a good
model, for instance, of a drumhead.
Derivation of the PDE of the Model (“Two-Dimensional Wave Equation”) from Forces.
As in Sec. 12.2 the model will consist of a PDE and additional conditions. The PDE will be
obtained by the same method as in Sec. 12.2, namely, by considering the forces acting on a
small portion of the physical system, the membrane in Fig. 301 on the next page, as it is
moving up and down.
Since the deflections of the membrane and the angles of inclination are small, the sides
of the portion are approximately equal to ¢x and ¢y. The tension T is the force per unit
length. Hence the forces acting on the sides of the portion are approximately T ¢x and
T ¢y. Since the membrane is perfectly flexible, these forces are tangent to the moving
membrane at every instant.
Horizontal Components of the Forces. We first consider the horizontal components
of the forces. These components are obtained by multiplying the forces by the cosines of
the angles of inclination. Since these angles are small, their cosines are close to 1. Hence
the horizontal components of the forces at opposite sides are approximately equal.
Therefore, the motion of the particles of the membrane in a horizontal direction will be
negligibly small. From this we conclude that we may regard the motion of the membrane
as transversal; that is, each particle moves vertically.
Vertical Components of the Forces.
left side are (Fig. 301), respectively,
T ¢y sin b

These components along the right side and the
and

ϪT ¢y sin a.

Here a and b are the values of the angle of inclination (which varies slightly along the
edges) in the middle of the edges, and the minus sign appears because the force on the

c12-b.qxd

10/30/10

576

1:59 PM

Page 576

CHAP. 12 Partial Differential Equations (PDEs)
Membrane

y + Δy
y

x + Δx

x

TΔy

TΔ x
u

TΔy

β

α
β

TΔy

y+Δ

y

TΔ x

α
TΔy

y
x

x

x + Δx

x+Δ

x

Fig. 301. Vibrating membrane

left side is directed downward. Since the angles are small, we may replace their sines by
their tangents. Hence the resultant of those two vertical components is
(1)

T ¢y (sin b Ϫ sin a) Ϸ T ¢y (tan b Ϫ tan a)
ϭ T ¢y [u x (x ϩ ¢x, y1) Ϫ u x (x, y2)]

where subscripts x denote partial derivatives and y1 and y2 are values between y and
y ϩ ¢y. Similarly, the resultant of the vertical components of the forces acting on the
other two sides of the portion is
(2)

T ¢x [u y (x 1, y ϩ ¢y) Ϫ u y (x 2, y)]

where x 1 and x 2 are values between x and x ϩ ¢x.
Newton’s Second Law Gives the PDE of the Model. By Newton’s second law (see
Sec. 2.4) the sum of the forces given by (1) and (2) is equal to the mass r ¢A of that
small portion times the acceleration 0 2u>0t 2; here r is the mass of the undeflected
membrane per unit area, and ¢A ϭ ¢x ¢y is the area of that portion when it is undeflected. Thus
r¢x ¢y

0 2u
0t 2

ϭ T ¢y [u x (x ϩ ¢x, y1) Ϫ u x (x, y2)]
ϩ T ¢x [u y (x 1, y ϩ ¢y) Ϫ u y (x 2, y)]

x, ෂ
y ) corresponding
where the derivative on the left is evaluated at some suitable point ( ෂ
to that portion. Division by r¢x ¢y gives

c12-b.qxd

10/30/10

1:59 PM

Page 577

SEC. 12.9 Rectangular Membrane. Double Fourier Series

0 2u
T
ϭ
2
r
0t

c

577

u y(x 1, y ϩ ¢y) Ϫ u y(x 2, y)
u x(x ϩ ¢x, y1) Ϫ u x(x, y2)
ϩ
d.
¢x
¢y

If we let ¢x and ¢y approach zero, we obtain the PDE of the model
(3)

0 2u
0t

2

ϭ c2 a

0 2u
0x

2

0 2u

ϩ

0y

2

b

T
c2 ϭ r .

This PDE is called the two-dimensional wave equation. The expression in parentheses
is the Laplacian ¢ 2u of u (Sec. 10.8). Hence (3) can be written
0 2u

(3؅)

0t 2

ϭ c2 ¢ 2u.

Solutions of the wave equation (3) will be obtained and discussed in the next section.

12.9

Rectangular Membrane.
Double Fourier Series
Now we develop a solution for the PDE obtained in Sec. 12.8. Details are as follows.
The model of the vibrating membrane for obtaining the displacement u(x, y, t) of a point
(x, y) of the membrane from rest (u ϭ 0) at time t is

(1)

y
b
R
a

Fig. 302.
Rectangular
membrane

x

0 2u
0t 2

ϭ c2 a

0 2u
0x 2

ϩ

0 2u
0y 2

b

(2)

u ϭ 0 on the boundary

(3a)

u (x, y, 0) ϭ f (x, y)

(3b)

u t (x, y, 0) ϭ g (x, y).

Here (1) is the two-dimensional wave equation with c2 ϭ T>r just derived, (2) is
the boundary condition (membrane fixed along the boundary in the xy-plane for
all times t м 0), and (3) are the initial conditions at t ϭ 0, consisting of the given
initial displacement (initial shape) f (x, y) and the given initial velocity g(x, y), where
u t ϭ 0u>0t. We see that these conditions are quite similar to those for the string in
Sec. 12.2.
Let us consider the rectangular membrane R in Fig. 302. This is our first important
model. It is much simpler than the circular drumhead, which will follow later. First we
note that the boundary in equation (2) is the rectangle in Fig. 302. We shall solve this
problem in three steps:

c12-b.qxd

10/30/10

578

1:59 PM

Page 578

CHAP. 12 Partial Differential Equations (PDEs)

Step 1. By separating variables, first setting u(x, y, t) ϭ F(x, y)G(t) and later
F(x, y) ϭ H(x)Q(y) we obtain from (1) an ODE (4) for G and later from a PDE (5) for F
two ODEs (6) and (7) for H and Q.
Step 2. From the solutions of those ODEs we determine solutions (13) of (1)
(“eigenfunctions” u mn) that satisfy the boundary condition (2).
Step 3. We compose the u mn into a double series (14) solving the whole model (1),
(2), (3).

Step 1. Three ODEs From the Wave Equation (1)
To obtain ODEs from (1), we apply two successive separations of variables. In the first
separation we set u(x, y, t) ϭ F(x, y)G(t). Substitution into (1) gives

##

FG ϭ c2(FxxG ϩ FyyG)
where subscripts denote partial derivatives and dots denote derivatives with respect to t.
To separate the variables, we divide both sides by c2FG:

##
G
1
ϭ (Fxx ϩ Fyy).
2
F
c G
Since the left side depends only on t, whereas the right side is independent of t, both sides
must equal a constant. By a simple investigation we see that only negative values of that
constant will lead to solutions that satisfy (2) without being identically zero; this is similar
to Sec. 12.3. Denoting that negative constant by Ϫ␯2, we have

##
G
1
ϭ (Fxx ϩ Fyy) ϭ Ϫ␯2.
F
c2G
This gives two equations: for the “time function” G(t) we have the ODE

##

(4)

G ϩ l2G ϭ 0

where l ϭ c␯,

and for the “amplitude function” F (x, y) a PDE, called the two-dimensional Helmholtz3
equation
(5)

3

Fxx ϩ Fyy ϩ ␯2F ϭ 0.

HERMANN VON HELMHOLTZ (1821–1894), German physicist, known for his fundamental work in
thermodynamics, fluid flow, and acoustics.

c12-b.qxd

10/30/10

1:59 PM

Page 579

SEC. 12.9 Rectangular Membrane. Double Fourier Series

579

Separation of the Helmholtz equation is achieved if we set F(x, y) ϭ H(x)Q( y). By
substitution of this into (5) we obtain
d 2H
dx

2

Q ϭ ϪaH

d 2Q
dy

2

ϩ ␯2HQb .

To separate the variables, we divide both sides by HQ, finding
1 d 2H
H dx

2

ϭϪ

1
Q

a

d 2Q
dy

2

ϩ ␯2Qb .

Both sides must equal a constant, by the usual argument. This constant must be negative,
say, Ϫk 2, because only negative values will lead to solutions that satisfy (2) without being
identically zero. Thus
1 d 2H
H dx 2

ϭϪ

1
Q

a

d 2Q
dy 2

ϩ ␯2Qb ϭ Ϫk 2.

This yields two ODEs for H and Q, namely,
d 2H
(6)

dx 2

ϩ k 2H ϭ 0

and
d 2Q
(7)

dy

2

ϩ p 2Q ϭ 0

where p 2 ϭ ␯2 Ϫ k 2.

Step 2. Satisfying the Boundary Condition
General solutions of (6) and (7) are
H(x) ϭ A cos kx ϩ B sin kx

and

Q(y) ϭ C cos py ϩ D sin py

with constant A, B,C, D. From u ϭ FG and (2) it follows that F ϭ HQ must be zero on
the boundary, that is, on the edges x ϭ 0, x ϭ a, y ϭ 0, y ϭ b; see Fig. 302. This gives
the conditions
H(0) ϭ 0,

H(a) ϭ 0,

Q(0) ϭ 0,

Q(b) ϭ 0.

Hence H(0) ϭ A ϭ 0 and then H(a) ϭ B sin ka ϭ 0. Here we must take B
otherwise H(x) ϵ 0 and F(x, y) ϵ 0. Hence sin ka ϭ 0 or ka ϭ mp, that is,
mp
kϭ a

(m integer).

0 since

c12-b.qxd

10/30/10

580

1:59 PM

Page 580

CHAP. 12 Partial Differential Equations (PDEs)

In precisely the same fashion we conclude that C ϭ 0 and p must be restricted to the
values p ϭ np>b where n is an integer. We thus obtain the solutions H ϭ Hm, Q ϭ Q n,
where
mpx
Hm(x) ϭ sin
a

npy
Q n(y) ϭ sin
,
b

and

m ϭ 1, 2, Á ,
n ϭ 1, 2, Á .

As in the case of the vibrating string, it is not necessary to consider m, n ϭ Ϫ1, Ϫ2, Á
since the corresponding solutions are essentially the same as for positive m and n, expect
for a factor Ϫ1. Hence the functions
(8)

mpx
npy
Fmn (x, y) ϭ Hm(x)Q n( y) ϭ sin
sin
,
a
b

m ϭ 1, 2, Á ,
n ϭ 1, 2, Á ,

are solutions of the Helmholtz equation (5) that are zero on the boundary of our membrane.
Eigenfunctions and Eigenvalues. Having taken care of (5), we turn to (4). Since
p 2 ϭ ␯2 Ϫ k 2 in (7) and l ϭ cv in (4), we have
l ϭ c2k 2 ϩ p 2.
Hence to k ϭ mp>a and p ϭ np>b there corresponds the value
(9)

l ϭ lmn ϭ cp

m2
B a2

ϩ

n2

m ϭ 1, 2, Á ,

b

n ϭ 1, 2, Á ,

,
2

in the ODE (4). A corresponding general solution of (4) is
Gmn (t) ϭ Bmn cos lmnt ϩ B*mn sin lmnt.
It follows that the functions u mn(x, y, t) ϭ Fmn(x, y) Gmn(t), written out
(10)

u mn (x, y, t) ϭ (Bmn cos lmnt ϩ B*mn sin lmnt) sin

npy
mpx
sin
a
b

with lmn according to (9), are solutions of the wave equation (1) that are zero on
the boundary of the rectangular membrane in Fig. 302. These functions are called the
eigenfunctions or characteristic functions, and the numbers lmn are called the
eigenvalues or characteristic values of the vibrating membrane. The frequency of u mn
is lmn>2p.
Discussion of Eigenfunctions. It is very interesting that, depending on a and b, several
functions Fmn may correspond to the same eigenvalue. Physically this means that there
may exists vibrations having the same frequency but entirely different nodal lines (curves
of points on the membrane that do not move). Let us illustrate this with the following
example.

c12-b.qxd

10/30/10

1:59 PM

Page 581

SEC. 12.9 Rectangular Membrane. Double Fourier Series
EXAMPLE 1

581

Eigenvalues and Eigenfunctions of the Square Membrane
Consider the square membrane with a ϭ b ϭ 1. From (9) we obtain its eigenvalues
lmn ϭ cp 2m 2 ϩ n 2.

(11)
Hence lmn ϭ lnm, but for m

n the corresponding functions

Fmn ϭ sin mpx sin npy

and

Fnm ϭ sin npx sin mpy

are certainly different. For example, to l12 ϭ l21 ϭ cp 15 there correspond the two functions
F12 ϭ sin px sin 2py

and

F21 ϭ sin 2px sin py.

and

* sin cp 15t)F21
u 21 ϭ (B21 cos cp 15t ϩ B21

Hence the corresponding solutions
* sin cp 15t)F12
u 12 ϭ (B12 cos cp 15t ϩ B12

* ϭ 0, we
have the nodal lines y ϭ 12 and x ϭ 12 , respectively (see Fig. 303). Taking B12 ϭ 1 and B*12 ϭ B21
obtain
u 12 ϩ u 21 ϭ cos cp 15t (F12 ϩ B21F21)

(12)

which represents another vibration corresponding to the eigenvalue cp 15. The nodal line of this function is the
solution of the equation
F12 ϩ B21F21 ϭ sin px sin 2py ϩ B21 sin 2px sin py ϭ 0
or, since sin 2a ϭ 2 sin a cos a,
sin px sin py (cos py ϩ B21 cos px) ϭ 0.

(13)

This solution depends on the value of B21 (see Fig. 304).
From (11) we see that even more than two functions may correspond to the same numerical value of lmn.
For example, the four functions F18, F81, F47, and F74 correspond to the value
l18 ϭ l81 ϭ l47 ϭ l74 ϭ cp 165,

because

12 ϩ 82 ϭ 42 ϩ 72 ϭ 65.

This happens because 65 can be expressed as the sum of two squares of positive integers in several ways.
According to a theorem by Gauss, this is the case for every sum of two squares among whose prime factors
there are at least two different ones of the form 4n ϩ 1 where n is a positive integer. In our case we have
65 ϭ 5 # 13 ϭ (4 ϩ 1)(12 ϩ 1).
᭿

B21 = –10
u11

u12

u21

B21 = –1
B21 = – 0.5
B21 = 0
B21 = 0.5

u22

u13

u31

Fig. 303. Nodal lines of the solutions
u11, u12, u21, u22, u13, u31 in the case of
the square membrane

B21 = 1

Fig. 304. Nodal lines
of the solution (12) for
some values of B21

c12-b.qxd

10/30/10

582

1:59 PM

Page 582

CHAP. 12 Partial Differential Equations (PDEs)

Step 3. Solution of the Model (1), (2), (3).
Double Fourier Series
So far we have solutions (10) satisfying (1) and (2) only. To obtain the solutions that also
satisfies (3), we proceed as in Sec. 12.3. We consider the double series
ؕ

ؕ

u (x, y, t) ϭ a a u mn (x, y, t)
(14)

mϭ1 nϭ1
ؕ

ؕ

ϭ a a (Bmn cos lmnt ϩ B*mn sin lmnt) sin
mϭ1 nϭ1

mpx
npy
sin
a
b

(without discussing convergence and uniqueness). From (14) and (3a), setting t ϭ 0, we
have
ؕ

(15)

ؕ

u (x, y, 0) ϭ a a Bmn sin
mϭ1 nϭ1

npy
mpx
a sin b ϭ f (x, y).

Suppose that f (x, y) can be represented by (15). (Sufficient for this is the continuity of
f, 0f> 0x, 0f>0y, 0 2f>0x 0y in R.) Then (15) is called the double Fourier series of f (x, y).
Its coefficients can be determined as follows. Setting
ؕ
npy
K m(y) ϭ a Bmn sin
b
nϭ1

(16)
we can write (15) in the form

ؕ

f (x, y) ϭ a K m(y) sin
mϭ1

mpx
a .

For fixed y this is the Fourier sine series of f (x, y), considered as a function of x. From
(4) in Sec. 11.3 we see that the coefficients of this expansion are
(17)

2
K m(y) ϭ a

a

Ύ f (x, y) sin mpa x dx.
0

Furthermore, (16) is the Fourier sine series of K m(y), and from (4) in Sec. 11.3 it follows
that the coefficients are
2
Bmn ϭ
b

b

ΎK

m(y)

0

sin

npy
dy.
b

From this and (17) we obtain the generalized Euler formula

(18)

Bmn ϭ

4
ab

b

ΎΎ
0

a

0

f (x, y) sin

npy
mpx
sin
dx dy
a
b

for the Fourier coefficients of f (x, y) in the double Fourier series (15).

m ϭ 1, 2, Á
n ϭ 1, 2, Á

c12-b.qxd

10/30/10

1:59 PM

Page 583

SEC. 12.9 Rectangular Membrane. Double Fourier Series

583

The Bmn in (14) are now determined in terms of f (x, y). To determine the B *mn, we
differentiate (14) termwise with respect to t; using (3b), we obtain
ؕ
ؕ
0u
mpx
npy
`
ϭ a a B *mn lmn sin
sin
ϭ g (x, y).
0t tϭ0 mϭ1 nϭ1
a
b

Suppose that g (x, y) can be developed in this double Fourier series. Then, proceeding as
before, we find that the coefficients are

(19)

B *mn ϭ

b

a

m ϭ 1, 2, Á

Ύ Ύ g (x, y) sin mpa x sin npa y dx dy

4
ablmn

0

n ϭ 1, 2, Á .

0

Result. If f and g in (3) are such that u can be represented by (14), then (14) with
coefficients (18) and (19) is the solution of the model (1), (2), (3).
EXAMPLE 2

Vibration of a Rectangular Membrane
Find the vibrations of a rectangular membrane of sides a ϭ 4 ft and b ϭ 2 ft (Fig. 305) if the tension is 12.5 lb>ft,
the density is 2.5 slugs>ft 2 (as for light rubber), the initial velocity is 0, and the initial displacement is
f (x, y) ϭ 0.1 (4x Ϫ x 2)(2y Ϫ y 2) ft.

(20)

y

u
y

2
2

R

4 x

0

x

4
Membrane

Initial displacement

Fig. 305. Example 2
* ϭ 0 from (19). From (18) and (20),
Solution. c2 ϭ T>r ϭ 12.5>2.5 ϭ 5 [ft 2>sec2]. Also Bmn
Bmn ϭ

ϭ

2

4

Ύ Ύ 0.1(4x Ϫ x ) (2y Ϫ y ) sin

4

2

4#2

0

mpx
4

0

4

(4x Ϫ x ) sin
20 Ύ
1

2

2

0

mpx
4

2

dx

sin

Ύ (2y Ϫ y ) sin
2

128

[1 Ϫ (Ϫ1)m] ϭ

0

256
m 3p3

(m odd)

and for the second integral
16
3

3

n p

[1 Ϫ (Ϫ1)n] ϭ

32
3

n p3

2

npy

Two integrations by parts give for the first integral on the right

m 3p3

npy

(n odd).

2

dx dy

dy.

c12-b.qxd

10/30/10

584

1:59 PM

Page 584

CHAP. 12 Partial Differential Equations (PDEs)
For even m or n we get 0. Together with the factor 1>20 we thus have Bmn ϭ 0 if m or n is even and
256 # 32

Bmn ϭ

3 3

Ϸ

6

20m n p

0.426050

(m and n both odd).

m 3n 3

From this, (9), and (14) we obtain the answer
u (x, y, t) ϭ 0.426050 a a

m,n odd

(21)

ϭ 0.426050 acos
ϩ

1
27

cos

1
m 3n 3

cos a

15p 15
4

15p 113
4

t sin

25p

t sin
3px
4

4

px
4

2m 2 ϩ 4n 2 b t sin

py
sin

py
sin

2

2
ϩ

ϩ

1
27

1
729

sin

npy

4

2

15p 137

cos

cos

mpx

4

15p 145
4

t sin

t sin

px
4

3px
4

sin

sin

3py
2

3py
2

ϩ Á b.

To discuss this solution, we note that the first term is very similar to the initial shape of the membrane, has no
nodal lines, and is by far the dominating term because the coefficients of the next terms are much smaller. The
second term has two horizontal nodal lines ( y ϭ 23 , 43 ), the third term two vertical ones (x ϭ 43 , 83 ), the fourth
term two horizontal and two vertical ones, and so on.
᭿

PROBLEM SET 12.9
1. Frequency. How does the frequency of the eigenfunctions of the rectangular membrane change (a) If
we double the tension? (b) If we take a membrane of
half the density of the original one? (c) If we double
the sides of the membrane? Give reasons.
2. Assumptions. Which part of Assumption 2 cannot be
satisfied exactly? Why did we also assume that the
angles of inclination are small?
3. Determine and sketch the nodal lines of the square
membrane for m ϭ 1, 2, 3, 4 and n ϭ 1, 2, 3, 4.

0
1.0
0.8
0.4
0
0.1
0.2
y 0.3
0.4

4–8
DOUBLE FOURIER SERIES
Represent f (x, y) by a series (15), where
4. f (x, y) ϭ 1, a ϭ b ϭ 1
5. f (x, y) ϭ y, a ϭ b ϭ 1
6. f (x, y) ϭ x, a ϭ b ϭ 1
7. f (x, y) ϭ xy, a and b arbitrary
8. f (x, y) ϭ xy (a Ϫ x) (b Ϫ y), a and b arbitrary
9. CAS PROJECT. Double Fourier Series. (a) Write
a program that gives and graphs partial sums of (15).
Apply it to Probs. 5 and 6. Do the graphs show that
those partial sums satisfy the boundary condition (3a)?
Explain why. Why is the convergence rapid?
(b) Do the tasks in (a) for Prob. 4. Graph a portion,
say, 0 Ͻ x Ͻ 12, 0 Ͻ y Ͻ 12, of several partial sums on
common axes, so that you can see how they differ. (See
Fig. 306.)
(c) Do the tasks in (b) for functions of your choice.

0.5

0.5

0.4

0.3

0.2

0.1

0

x

Fig. 306. Partial sums S2,2 and S10,10
in CAS Project 9b

10. CAS EXPERIMENT. Quadruples of Fmn. Write a
program that gives you four numerically equal lmn in
Example 1, so that four different Fmn correspond to it.
Sketch the nodal lines of F18, F81, F47, F74 in Example
1 and similarly for further Fmn that you will find.
11–13
SQUARE MEMBRANE
Find the deflection u (x, y, t) of the square membrane of side
p and c2 ϭ 1 for initial velocity 0 and initial deflection
11. 0.1 sin 2x sin 4y
12. 0.01 sin x sin y
13. 0.1 xy (p Ϫ x) (p Ϫ y)

c12-b.qxd

10/30/10

1:59 PM

Page 585

SEC. 12.10 Laplacian in Polar Coordinates. Circular Membrane. Fourier–Bessel Series

14–19

RECTANGULAR MEMBRANE

19. Deflection. Find the deflection of the membrane of
sides a and b with c2 ϭ 1 for the initial deflection

14.
15.
16.
17.

Verify the discussion of (21) in Example 2.
Do Prob. 3 for the membrane with a ϭ 4 and b ϭ 2.
Verify Bmn in Example 2 by integration by parts.
Find eigenvalues of the rectangular membrane of sides
a ϭ 2 and b ϭ 1 to which there correspond two or
more different (independent) eigenfunctions.
18. Minimum property. Show that among all rectangular
membranes of the same area A ϭ ab and the same c
the square membrane is that for which u 11 [see (10)]
has the lowest frequency.

12.10

585

6px
2py
f (x, y) ϭ sin a sin
and initial velocity 0.
b
20. Forced vibrations. Show that forced vibrations of a
membrane are modeled by the PDE u tt ϭ c2ٌ2u ϩ P>r,
where P (x, y, t) is the external force per unit area acting
perpendicular to the xy-plane.

Laplacian in Polar Coordinates.
Circular Membrane.
Fourier–Bessel Series
It is a general principle in boundary value problems for PDEs to choose coordinates that
make the formula for the boundary as simple as possible. Here polar coordinates are used
for this purpose as follows. Since we want to discuss circular membranes (drumheads),
we first transform the Laplacian in the wave equation (1), Sec. 12.9,
u tt ϭ c2ٌ2u ϭ c2 (u xx ϩ u yy)

(1)

(subscripts denoting partial derivatives) into polar coordinates r, u defined by x ϭ r cos u,
y ϭ r sin u; thus,
r ϭ 2x 2 ϩ y 2,

y
tan u ϭ x .

By the chain rule (Sec. 9.6) we obtain
u x ϭ u rrx ϩ u uux .
Differentiating once more with respect to x and using the product rule and then again the
chain rule gives
u xx ϭ (u rrx)x ϩ (u uux)x
(2)

ϭ (u r)xrx ϩ u rrxx ϩ (u u)xux ϩ u uuxx
ϭ (u rrrx ϩ u ruux)rx ϩ u rrxx ϩ (u urrx ϩ u uuux)ux ϩ u uuxx.

Also, by differentiation of r and u we find
rx ϭ

x
x
ϭ ,
2
2x ϩ y
r
2

ux ϭ

1
1 ϩ ( y>x)

2

aϪ

y
x

2

bϭϪ

y
r2

.

c12-b.qxd

10/30/10

1:59 PM

586

Page 586

CHAP. 12 Partial Differential Equations (PDEs)

Differentiating these two formulas again, we obtain
rxx ϭ

1
x2
y2
r Ϫ xrx
ϭ Ϫ 3 ϭ 3,
2
r
r
r
r

uxx ϭ Ϫy aϪ

2
2xy
b rx ϭ 4 .
r3
r

We substitute all these expressions into (2). Assuming continuity of the first and second
partial derivatives, we have u ru ϭ u ur, and by simplifying,
(3)

u xx ϭ

x2
r

2

u rr Ϫ 2

xy
r

u ϩ
3 ru

y2
r

4

u uu ϩ

y2
r

3

ur ϩ 2

xy
r4

u u.

In a similar fashion it follows that
(4)

u yy ϭ

y2
r

u ϩ2
2 rr

xy
r

u ϩ
3 ru

x2
r

4

u uu ϩ

x2
r

3

ur Ϫ 2

xy
r4

u u.

By adding (3) and (4) we see that the Laplacian of u in polar coordinates is
(5)

ٌ2u ϭ

0 2u
1 0u
1 0 2u
ϩ 2 2.
2 ϩ
r 0r
0r
r 0u

Circular Membrane
Circular membranes are important parts of drums, pumps, microphones, telephones, and
other devices. This accounts for their great importance in engineering. Whenever a circular
membrane is plane and its material is elastic, but offers no resistance to bending (this
excludes thin metallic membranes!), its vibrations are modeled by the two-dimensional
wave equation in polar coordinates obtained from (1) with ٌ2u given by (5), that is,
(6)

y

R x

Fig. 307. Circular
membrane

2
0 2u
1 0u
1 0 2u
2 0 u
ϭ
c
a
ϩ
ϩ
b
r 0r
0t 2
0r 2
r 2 0u2

T
c2 ϭ r .

We shall consider a membrane of radius R (Fig. 307) and determine solutions u(r, t)
that are radially symmetric. (Solutions also depending on the angle u will be discussed in
the problem set.) Then u uu ϭ 0 in (6) and the model of the problem (the analog of (1),
(2), (3) in Sec. 12.9) is
(7)

0 2u
0t

2

ϭ c2 a

0 2u
0r

2

1 0u
ϩ r
b
0r

(8)

u (R, t) ϭ 0 for all t м 0

(9a)

u (r, 0) ϭ f (r)

(9b)

u t(r, 0) ϭ g (r).

Here (8) means that the membrane is fixed along the boundary circle r ϭ R. The initial
deflection f (r) and the initial velocity g(r) depend only on r, not on u, so that we can
expect radially symmetric solutions u(r, t).

c12-b.qxd

10/30/10

1:59 PM

Page 587

SEC. 12.10 Laplacian in Polar Coordinates. Circular Membrane. Fourier–Bessel Series

587

Step 1. Two ODEs From the Wave Equation (7).
Bessel’s Equation
Using the method of separation of variables, we first determine solutions u(r, t) ϭ
W (r)G (t). (We write W, not F because W depends on r, whereas F, used before, depended
on x.) Substituting u ϭ WG and its derivatives into (7) and dividing the result by c2WG,
we get

##

G
1
1
ϭ
aW s ϩ W r b
W
r
c2G
where dots denote derivatives with respect to t and primes denote derivatives with respect
to r. The expressions on both sides must equal a constant. This constant must be negative,
say, Ϫk 2, in order to obtain solutions that satisfy the boundary condition without being
identically zero. Thus,

##

G
1
1
ϭ
aW s ϩ W r b ϭ Ϫk 2.
2
W
r
c G
This gives the two linear ODEs

##

G ϩ l2G ϭ 0

(10)

where l ϭ ck

and
1
W s ϩ r W r ϩ k 2W ϭ 0.

(11)

We can reduce (11) to Bessel’s equation (Sec. 5.4) if we set s ϭ kr. Then 1>r ϭ k>s and,
retaining the notation W for simplicity, we obtain by the chain rule
Wr ϭ

dW
dW ds
dW
ϭ
ϭ
k
dr
ds dr
ds

and

Ws ϭ

d 2W 2
k .
ds 2

By substituting this into (11) and omitting the common factor k 2 we have

(12)

d 2W
1 dW
ϩ W ϭ 0.
2 ϩ
s ds
ds

This is Bessel’s equation (1), Sec. 5.4, with parameter ␯ ϭ 0.

Step 2. Satisfying the Boundary Condition (8)
Solutions of (12) are the Bessel functions J0 and Y0 of the first and second kind (see Secs.
5.4, 5.5). But Y0 becomes infinite at 0, so that we cannot use it because the deflection of
the membrane must always remain finite. This leaves us with
(13)

W (r) ϭ J0 (s) ϭ J0 (kr)

(s ϭ kr).

c12-b.qxd

10/30/10

588

1:59 PM

Page 588

CHAP. 12 Partial Differential Equations (PDEs)

On the boundary r ϭ R we get W (R) ϭ J0 (kR) ϭ 0 from (8) (because G ϵ 0 would imply
u ϵ 0). We can satisfy this condition because J0 has (infinitely many) positive zeros,
s ϭ a1, a2, Á (see Fig. 308), with numerical values
a1 ϭ 2.4048, a2 ϭ 5.5201, a3 ϭ 8.6537, a4 ϭ 11.7915, a5 ϭ 14.9309
and so on. (For further values, consult your CAS or Ref. [GenRef1] in App. 1.) These
zeros are slightly irregularly spaced, as we see. Equation (13) now implies
kR ϭ am

(14)

thus

k ϭ km ϭ

am
R

m ϭ 1, 2, Á .

,

Hence the functions
Wm(r) ϭ J0(k mr) ϭ J0 a

(15)

am
R

m ϭ 1, 2, Á

rb ,

are solutions of (11) that are zero on the boundary circle r ϭ R.
Eigenfunctions and Eigenvalues. For Wm in (15), a corresponding general solution of
(10) with l ϭ lm ϭ ck m ϭ cam>R is
Gm(t) ϭ Am cos lmt ϩ Bm sin lmt.
Hence the functions
u m(r, t) ϭ Wm(r)Gm(t) ϭ (Am cos lmt ϩ Bm sin lmt)J0(k mr)

(16)

with m ϭ 1, 2, Á are solutions of the wave equation (7) satisfying the boundary condition
(8). These are the eigenfunctions of our problem. The corresponding eigenvalues are lm.
The vibration of the membrane corresponding to u m is called the mth normal mode;
it has the frequency lm>2p cycles per unit time. Since the zeros of the Bessel function
J0 are not regularly spaced on the axis (in contrast to the zeros of the sine functions
appearing in the case of the vibrating string), the sound of a drum is entirely different
from that of a violin. The forms of the normal modes can easily be obtained from Fig. 308
and are shown in Fig. 309. For m ϭ 1, all the points of the membrane move up (or down)
at the same time. For m ϭ 2, the situation is as follows. The function W2 (r) ϭ J0 (a2r>R)
is zero for a2r>R ϭ a1, thus r ϭ a1R>a2. The circle r ϭ a1R>a2 is, therefore, nodal line,
and when at some instant the central part of the membrane moves up, the outer part
(r Ͼ a1R>a2) moves down, and conversely. The solution u m (r, t) has m Ϫ 1 nodal lines,
which are circles (Fig. 309).
J0(s)
1
–10
–α4

–5
–α3

–α2

5
–α1

α1

10
α2

Fig. 308. Bessel function J0 (s)

α3

α4

s

c12-b.qxd

10/30/10

1:59 PM

Page 589

SEC. 12.10 Laplacian in Polar Coordinates. Circular Membrane. Fourier–Bessel Series

m=1

m=2

589

m=3

Fig. 309. Normal modes of the circular membrane in the case of vibrations
independent of the angle

Step 3. Solution of the Entire Problem
To obtain a solution u (r, t) that also satisfies the initial conditions (9), we may proceed
as in the case of the string. That is, we consider the series

(17)

ؕ
ؕ
am
u (r, t) ϭ a Wm(r)Gm(t) ϭ a (Am cos lmt ϩ Bm sin lmt)J0 a
rb
R
mϭ1
mϭ1

(leaving aside the problems of convergence and uniqueness). Setting t ϭ 0 and using (9a),
we obtain
(18)

ؕ
am
u (r, 0) ϭ a AmJ0 a
rb ϭ f (r).
R
mϭ1

Thus for the series (17) to satisfy the condition (9a), the constants Am must be the
coefficients of the Fourier–Bessel series (18) that represents f (r) in terms of J0 (amr>R);
that is [see (9) in Sec. 11.6 with n ϭ 0, a0, m ϭ am, and x ϭ r],

(19)

Am ϭ

2

Ύ

R

R 2J 21 (am) 0

rf (r)J0 a

am
R

rb dr

(m ϭ 1, 2, Á ).

Differentiability of f (r) in the interval 0 Ϲ r Ϲ R is sufficient for the existence of the
development (18); see Ref. [A13]. The coefficients Bm in (17) can be determined from
(9b) in a similar fashion. Numeric values of Am and Bm may be obtained from a CAS or
by a numeric integration method, using tables of J0 and J1. However, numeric integration
can sometimes be avoided, as the following example shows.

c12-b.qxd

10/30/10

1:59 PM

590

Page 590

CHAP. 12 Partial Differential Equations (PDEs)

EXAMPLE 1

Vibrations of a Circular Membrane
Find the vibrations of a circular drumhead of radius 1 ft and density 2 slugs>ft 2 if the tension is 8 lb>ft, the
initial velocity is 0, and the initial displacement is
f (r) ϭ 1 Ϫ r 2 [ft].

Solution. c2 ϭ T>r ϭ 82 ϭ 4 [ft 2>sec2]. Also Bm ϭ 0, since the initial velocity is 0. From (10) in Sec. 11.6,
since R ϭ 1, we obtain
Am ϭ

ϭ
ϭ

1

Ύ r (1 Ϫ r )J (a

2

2

J 21 (am) 0

0

mr)

dr

4J2 (am)
a2mJ 21 (am)
8
a3mJ1 (am)

where the last equality follows from (21c), Sec. 5.4, with ␯ ϭ 1, that is,
J2 (am) ϭ

2
2
J (a ) Ϫ J0 (am) ϭ
J (a ).
am 1 m
am 1 m

Table 9.5 on p. 409 of [GenRef1] gives am and J0r (am). From this we get J1(am) ϭ ϪJ0r (am) by (21b), Sec. 5.4,
with ␯ ϭ 0, and compute the coefficients Am:

m

␣m

J1(␣m)

J2(␣m)

Am

1
2
3
4
5
6
7
8
9
10

2.40483
5.52008
8.65373
11.79153
14.93092
18.07106
21.21164
24.35247
27.49348
30.63461

0.51915
Ϫ0.34026
0.27145
Ϫ0.23246
0.20655
Ϫ0.18773
0.17327
Ϫ0.16170
0.15218
Ϫ0.14417

0.43176
Ϫ0.12328
0.06274
Ϫ0.03943
0.02767
Ϫ0.02078
0.01634
Ϫ0.01328
0.01107
Ϫ0.00941

1.10801
Ϫ0.13978
0.04548
Ϫ0.02099
0.01164
Ϫ0.00722
0.00484
Ϫ0.00343
0.00253
Ϫ0.00193

Thus
f (r) ϭ 1.108J0 (2.4048r) Ϫ 0.140J0 (5.5201r) ϩ 0.045J0 (8.6537r) Ϫ Á .
We see that the coefficients decrease relatively slowly. The sum of the explicitly given coefficients in the table
is 0.99915. The sum of all the coefficients should be 1. (Why?) Hence by the Leibniz test in App. A3.3 the
partial sum of those terms gives about three correct decimals of the amplitude f(r).
Since
lm ϭ ck m ϭ cam>R ϭ 2am,
from (17) we thus obtain the solution (with r measured in feet and t in seconds)
u (r, t) ϭ 1.108J0 (2.4048r) cos 4.8097t Ϫ 0.140J0 (5.5201r) cos 11.0402t ϩ 0.045J0 (8.6537r) cos 17.3075t Ϫ Á .
In Fig. 309, m ϭ 1 gives an idea of the motion of the first term of our series, m ϭ 2 of the second term, and
m ϭ 3 of the third term, so that we can “see” our result about as well as for a violin string in Sec. 12.3. ᭿

c12-b.qxd

10/30/10

1:59 PM

Page 591

SEC. 12.10 Laplacian in Polar Coordinates. Circular Membrane. Fourier–Bessel Series

591

PROBLEM SET 12.10
RADIAL SYMMETRY

1–3

with arbitrary A0 and

1. Why did we introduce polar coordinates in this
section?

2. Radial symmetry reduces (5) to ٌ2u ϭ u rr ϩ u r>r.
Derive this directly from ٌ2u ϭ u xx ϩ u yy. Show
that the only solution of ٌ2u ϭ 0 depending only on
r ϭ 2x 2 ϩ y 2 is u ϭ a ln r ϩ b with arbitrary constants a and b.

An ϭ
Bn ϭ

p

1

pnRn؊1
1

pnRn؊1

Ύ

f (u) cos nu du,

؊p

Ύ

p

f (u) sin nu du.

؊p

(e) Compatibility condition. Show that (9), Sec. 10.4,
imposes on f (u) in (d) the “compatibility condition”

3. Alternative form of (5). Show that (5) can be written
ٌ2u ϭ (ru r)r>r ϩ u uu>r 2, a form that is often practical.

Ύ

p

f (u) du ϭ 0.

؊p

BOUNDARY VALUE PROBLEMS. SERIES
4. TEAM PROJECT. Series for Dirichlet and Neumann
Problems
(a) Show that u n ϭ r ncos nu, u n ϭ r n sin nu, n ϭ 0,
1, Á , are solutions of Laplace’s equation ٌ2u ϭ 0
with ٌ2u given by (5). (What would u n be in Cartesian
coordinates? Experiment with small n.)
(b) Dirichlet problem (See Sec. 12.6) Assuming that
termwise differentiation is permissible, show that a
solution of the Laplace equation in the disk r Ͻ R
satisfying the boundary condition u(R, u) ϭ f (u) (R and
f given) is
ؕ

(20)

n

u(r, u) ϭ a 0 ϩ a c an a r b cos nu
R
nϭ1
n

r
ϩ bn a b sin nu d
R
where an, bn are the Fourier coefficients of f (see
Sec. 11.1).
(c) Dirichlet problem. Solve the Dirichlet problem
using (20) if R ϭ 1 and the boundary values are
u (u) ϭ Ϫ100 volts if Ϫp Ͻ u Ͻ 0, u (u) ϭ 100 volts
if 0 Ͻ u Ͻ p. (Sketch this disk, indicate the boundary
values.)
(d) Neumann problem. Show that the solution of the
Neumann problem ٌ2u ϭ 0 if r Ͻ R, u N (R, u) ϭ f (u)
(where u N ϭ 0u>0N is the directional derivative in the
direction of the outer normal) is
ؕ

u(r, u) ϭ A0 ϩ a r n(An cos nu ϩ Bn sin nu)
nϭ1

(f) Neumann problem. Solve ٌ2u ϭ 0 in the annulus
1 Ͻ r Ͻ 2 if u r (1, u) ϭ sin u, u r (2, u) ϭ 0.
5–8

ELECTROSTATIC POTENTIAL.
STEADY-STATE HEAT PROBLEMS

The electrostatic potential satisfies Laplace’s equation
ٌ2u ϭ 0 in any region free of charges. Also the heat
equation u t ϭ c2ٌ2u (Sec. 12.5) reduces to Laplace’s
equation if the temperature u is time-independent
(“steady-state case”). Using (20), find the potential
(equivalently: the steady-state temperature) in the disk
r Ͻ 1 if the boundary values are (sketch them, to see what
is going on).
5. u (1, u) ϭ 220 if Ϫ12 p Ͻ u Ͻ 12 p and 0 otherwise
6. u (1, u) ϭ 400 cos3 u
7. u (1, u) ϭ 110 ƒ u ƒ if Ϫp Ͻ u Ͻ p
8. u (1, u) ϭ u if Ϫ12 p Ͻ u Ͻ 12 p and 0 otherwise
9. CAS EXPERIMENT. Equipotential Lines. Guess
what the equipotential lines u (r, u) ϭ const in Probs. 5
and 7 may look like. Then graph some of them, using
partial sums of the series.
10. Semidisk. Find the electrostatic potential in the semidisk r Ͻ 1, 0 Ͻ u Ͻ p which equals 110u (p Ϫ u)
on the semicircle r ϭ 1 and 0 on the segment
Ϫ1 Ͻ x Ͻ 1.
11. Semidisk. Find the steady-state temperature in a
semicircular thin plate r Ͻ a, 0 Ͻ u Ͻ p with the
semicircle r ϭ a kept at constant temperature u 0 and
the segment Ϫa Ͻ x Ͻ a at 0.

CIRCULAR MEMBRANE
12. CAS PROJECT. Normal Modes. (a) Graph the
normal modes u 4, u 5, u 6 as in Fig. 306.

c12-b.qxd

10/30/10

1:59 PM

592

13.
14.

15.

16.

17.

18.

Page 592

CHAP. 12 Partial Differential Equations (PDEs)
(b) Write a program for calculating the Am’s in
Example 1 and extend the table to m ϭ 15. Verify
numerically that am Ϸ (m Ϫ 14 )p and compute the
error for m ϭ 1, Á , 10.
(c) Graph the initial deflection f (r) in Example 1 as
well as the first three partial sums of the series.
Comment on accuracy.
(d) Compute the radii of the nodal lines of u 2, u 3, u 4
when R ϭ 1. How do these values compare to those of
the nodes of the vibrating string of length 1? Can you
establish any empirical laws by experimentation with
further u m?
Frequency. What happens to the frequency of an
eigenfunction of a drum if you double the tension?
Size of a drum. A small drum should have a higher
fundamental frequency than a large one, tension and
density being the same. How does this follow from our
formulas?
Tension. Find a formula for the tension required
to produce a desired fundamental frequency f1 of a
drum.
Why is A1 ϩ A2 ϩ Á ϭ 1 in Example 1? Compute
the first few partial sums until you get 3-digit
accuracy. What does this problem mean in the field
of music?
Nodal lines. Is it possible that for fixed c and R two
or more u m [see (16)] with different nodal lines
correspond to the same eigenvalue? (Give a reason.)
Nonzero initial velocity is more of theoretical interest
because it is difficult to obtain experimentally. Show
that for (17) to satisfy (9b) we must have

Bm ϭ K m

(21)

Ύ

(24)

Frr ϩ

1
1
Fr ϩ 2 Fuu ϩ k 2F ϭ 0.
r
r

Show that the PDE can now be separated by substituting F ϭ W (r)Q (u), giving
(25)
(26)

Q s ϩ n 2Q ϭ 0,
r 2W s ϩ rW r ϩ (k 2r 2 Ϫ n 2)W ϭ 0.

20 Periodicity. Show that Q (u) must be periodic with
period 2p and, therefore, n ϭ 0, 1, 2, Á in (25) and
(26). Show that this yields the solutions Q n ϭ cos nu,
* ϭ sin nu, Wn ϭ Jn(kr), n ϭ 0, 1, Á .
Qn
21. Boundary condition. Show that the boundary condition
(27)

u (R, u, t) ϭ 0

leads to k ϭ k mn ϭ amn>R, where s ϭ anm is the mth
positive zero of Jn (s).
22. Solutions depending on both r and U. Show that
solutions of (22) satisfying (27) are (see Fig. 310)
u nm ϭ (Anm cos ck nmt ϩ Bnm sin ck nmt)
(28)

ϫ Jn(k nmr) cos nu
u*nm ϭ (A*nm cos ck nmt ϩ B*nm sin ck nmt)
ϫ Jn (k nmr) sin nu

R

rg (r)J0 (amr>R) dr

0

u11

where K m ϭ 2>(camR)J 21(am).

VIBRATIONS OF A CIRCULAR MEMBRANE
DEPENDING ON BOTH r AND U
19. (Separations) Show that substitution of u ϭ F (r, u)G (t)
into the wave equation (6), that is,
1
1
u tt ϭ c2 au rr ϩ r u r ϩ r u uu b,
2

(22)

gives an ODE and a PDE

##

(23)

G ϩ l2G ϭ 0,

where l ϭ ck,

u21

u32

Fig. 310. Nodal lines of some of the solutions (28)

23. Initial condition. Show that u t (r, u, 0) ϭ 0 gives
* ϭ 0 in (28).
Bnm ϭ 0, Bnm
* ϭ 0 and u 0m is identical with (16) in
24. Show that u 0m
this section.
25. Semicircular membrane. Show that u 11 represents the
fundamental mode of a semicircular membrane and
find the corresponding frequency when c2 ϭ 1 and
R ϭ 1.

c12-b.qxd

10/30/10

1:59 PM

Page 593

SEC. 12.11 Laplace’s Equation in Cylindrical and Spherical Coordinates. Potential

12.11

593

Laplace’s Equation in Cylindrical and
Spherical Coordinates. Potential
One of the most important PDEs in physics and engineering applications is Laplace’s
equation, given by
(1)

ٌ2u ϭ u xx ϩ u yy ϩ u zz ϭ 0.

Here, x, y, z are Cartesian coordinates in space (Fig. 167 in Sec. 9.1), u xx ϭ 0 2u>0x 2, etc.
The expression ٌ2u is called the Laplacian of u. The theory of the solutions of (1) is
called potential theory. Solutions of (1) that have continuous second partial derivatives
are known as harmonic functions.
Laplace’s equation occurs mainly in gravitation, electrostatics (see Theorem 3, Sec. 9.7),
steady-state heat flow (Sec. 12.5), and fluid flow (to be discussed in Sec. 18.4).
Recall from Sec. 9.7 that the gravitational potential u(x, y, z) at a point (x, y, z) resulting
from a single mass located at a point (X, Y, Z) is
(2)

c
u (x, y, z) ϭ r ϭ

c
2(x Ϫ X) ϩ (y Ϫ Y)2 ϩ (z Ϫ Z)2
2

(r Ͼ 0)

and u satisfies (1). Similarly, if mass is distributed in a region T in space with density
r (X, Y, Z), its potential at a point (x, y, z) not occupied by mass is
(3)

u (x, y, z) ϭ k

ΎΎΎ

r (X, Y, Z)
dX dY dZ.
r

T

It satisfies (1) because ٌ2 (1>r) ϭ 0 (Sec. 9.7) and r is not a function of x, y, z.
Practical problems involving Laplace’s equation are boundary value problems in a
region T in space with boundary surface S. Such problems can be grouped into three types
(see also Sec. 12.6 for the two-dimensional case):
(I) First boundary value problem or Dirichlet problem if u is prescribed on S.
(II) Second boundary value problem or Neumann problem if the normal
derivative u n ϭ 0u>0n is prescribed on S.
(III) Third or mixed boundary value problem or Robin problem if u is prescribed
on a portion of S and u n on the remaining portion of S.
In general, when we want to solve a boundary value problem, we have to first select
the appropriate coordinates in which the boundary surface S has a simple representation.
Here are some examples followed by some applications.

Laplacian in Cylindrical Coordinates
The first step in solving a boundary value problem is generally the introduction of
coordinates in which the boundary surface S has a simple representation. Cylindrical
symmetry (a cylinder as a region T ) calls for cylindrical coordinates r, u, z related to
x, y, z by
(4)

x ϭ r cos u,

y ϭ r sin u,

zϭz

(Fig. 311).

c12-b.qxd

10/30/10

594

1:59 PM

Page 594

CHAP. 12 Partial Differential Equations (PDEs)
z

z
(r, θ , z)

r

z

θ

y

r

(r, θ , φ )

φ

θ

x

y

x

Fig. 312. Spherical coordinates
(r м 0, 0 Ϲ u Ϲ 2p, 0 Ϲ ␸ Ϲ p)

Fig. 311. Cylindrical coordinates
(r м 0, 0 Ϲ u Ϲ 2p)

For these we get ٌ2u immediately by adding u zz to (5) in Sec. 12.10; thus,
ٌ2u ϭ

(5)

0 2u
1 0u
1 0 2u
0 2u
ϩ 2 2 ϩ 2.
2 ϩ
0r
r 0u
0z
r 0r

Laplacian in Spherical Coordinates
Spherical symmetry (a ball as region T bounded by a sphere S) requires spherical
coordinates r, u, ␾ related to x, y, z by
(6)

x ϭ r cos u sin ␾,

y ϭ r sin u sin ␾,

z ϭ r cos ␾

(Fig. 312).

Using the chain rule (as in Sec. 12.10), we obtain ٌ2u in spherical coordinates
(7)

ٌ2u ϭ

cot ␾ 0u
1 0 2u
0 2u
2 0u
1
0 2u
ϩ 2
ϩ 2 2
.
2 ϩ
2 ϩ
2
r 0r
0r
r 0␾
r
r sin ␾ 0u2
0␾

We leave the details as an exercise. It is sometimes practical to write (7) in the form
(7 r )

ٌ2u ϭ

1
0
1
0
0u
1
0 2u
2 0u
ar
b
ϩ
asin
␾
b
ϩ
c
d.
0r
r 2 0r
sin2 ␾ 0u2
sin ␾ 0␾
0␾

Remark on Notation. Equation (6) is used in calculus and extends the familiar notation
for polar coordinates. Unfortunately, some books use u and ␾ interchanged, an extension
of the notation x ϭ r cos ␾, y ϭ r sin ␾ for polar coordinates (used in some European
countries).

Boundary Value Problem in Spherical Coordinates
We shall solve the following Dirichlet problem in spherical coordinates:
(8)

(9)
(10)

ٌ2u ϭ

1
0
0u
1
0
0u
ar 2
bϩ
asin ␾
b d ϭ 0.
2 c
r
0r
0r
sin ␾ 0␾
0␾
u (R, ␾) ϭ f (␾)
lim u (r, ␾) ϭ 0.

r :ϱ

c12-b.qxd

10/30/10

1:59 PM

Page 595

SEC. 12.11 Laplace’s Equation in Cylindrical and Spherical Coordinates. Potential

595

The PDE (8) follows from (7) or (7 r ) by assuming that the solution u will not depend on
u because the Dirichlet condition (9) is independent of u. This may be an electrostatic
potential (or a temperature) f (␾) at which the sphere S: r ϭ R is kept. Condition (10)
means that the potential at infinity will be zero.
Separating Variables by substituting u (r, ␾) ϭ G (r)H (␾) into (8). Multiplying (8) by
r 2, making the substitution and then dividing by GH, we obtain
dG
1
dH
1 d
d
ar 2
bϭϪ
asin ␾
b.
G dr
dr
H sin ␾ d ␾
d␾
By the usual argument both sides must be equal to a constant k. Thus we get the two
ODEs
(11)

1 d
G dr

ar 2

dG
dr

bϭk

or

r2

d 2G
dr

2

ϩ 2r

dG

ϭ kG

dr

and
(12)

dH
1 d
asin ␾
b ϩ kH ϭ 0.
sin ␾ d␾
d␾

The solutions of (11) will take a simple form if we set k ϭ n (n ϩ 1). Then, writing
G r ϭ dG>dr, etc., we obtain
(13)

r 2G s ϩ 2rG r Ϫ n (n ϩ 1) G ϭ 0.

This is an Euler–Cauchy equation. From Sec. 2.5 we know that it has solutions G ϭ r a.
Substituting this and dropping the common factor r a gives
a (a Ϫ 1) ϩ 2a Ϫ n (n ϩ 1) ϭ 0.

aϭn

The roots are

and Ϫn Ϫ 1.

Hence solutions are
(14)

Gn (r) ϭ r n

and

G*n(r) ϭ

1
r

nϩ1

.

We now solve (12). Setting cos ␾ ϭ w, we have sin2 ␾ ϭ 1 Ϫ w 2 and
d
d dw
d
ϭ
ϭ Ϫsin ␾
.
d␾
dw d␾
dw
Consequently, (12) with k ϭ n (n ϩ 1) takes the form
(15)

d
2 dH
c (1 Ϫ w )
d ϩ n (n ϩ 1)H ϭ 0.
dw
dw

This is Legendre’s equation (see Sec. 5.3), written out

c12-b.qxd

10/30/10

596

1:59 PM

Page 596

CHAP. 12 Partial Differential Equations (PDEs)

(1 Ϫ w 2)

(15؅)

d 2H
dH
Ϫ 2w
ϩ n (n ϩ 1)H ϭ 0.
dw
dw 2

For integer n ϭ 0, 1, Á the Legendre polynomials
H ϭ Pn (w) ϭ Pn (cos ␾)

n ϭ 0, 1, Á ,

are solutions of Legendre’s equation (15). We thus obtain the following two sequences
of solution u ϭ GH of Laplace’s equation (8), with constant An and Bn, where
n ϭ 0, 1, Á ,
(a) u n (r, ␾) ϭ Anr nPn (cos ␾),

(16)

(b) u*n (r, ␾) ϭ

Bn
r

nϩ1

Pn (cos ␾)

Use of Fourier–Legendre Series
Interior Problem: Potential Within the Sphere S.
(16a),

We consider a series of terms from

ؕ

u(r, ␾) ϭ a Anr nPn(cos ␾)

(17)

(r Ϲ R).

nϭ0

Since S is given by r ϭ R, for (17) to satisfy the Dirichlet condition (9) on the sphere S,
we must have
ؕ

u (R, ␾) ϭ a AnRnPn (cos ␾) ϭ f (␾);

(18)

nϭ0

that is, (18) must be the Fourier–Legendre series of f (␾). From (7) in Sec. 5.8 we get
the coefficients

AnR n ϭ

(19*)

2n ϩ 1
2

Ύ

1

ෂ
f (w) Pn (w) dw

؊1

ෂ

where f (w) denotes f (␾) as a function of w ϭ cos ␾. Since dw ϭ Ϫsin ␾ d␾, and the limits
of integration Ϫ1 and 1 correspond to ␾ ϭ p and ␾ ϭ 0, respectively, we also obtain
(19)

An ϭ

2n ϩ 1
2R n

Ύ

p

f (␾)Pn (cos ␾) sin ␾ d␾,

n ϭ 0, 1, Á .

0

If f (␾) and f r (␾) are piecewise continuous on the interval 0 Ϲ ␾ Ϲ p, then the series
(17) with coefficients (19) solves our problem for points inside the sphere because it can
be shown that under these continuity assumptions the series (17) with coefficients (19)
gives the derivatives occurring in (8) by termwise differentiation, thus justifying our
derivation.

c12-b.qxd

10/30/10

1:59 PM

Page 597

SEC. 12.11 Laplace’s Equation in Cylindrical and Spherical Coordinates. Potential

597

Exterior Problem: Potential Outside the Sphere S. Outside the sphere we cannot use
the functions u n in (16a) because they do not satisfy (10). But we can use the u n* in (16b),
which do satisfy (10) (but could not be used inside S; why?). Proceeding as before leads
to the solution of the exterior problem
ؕ

Bn

u(r, ␾) ϭ a

(20)

r nϩ1

nϭ0

Pn (cos ␾)

(r м R)

satisfying (8), (9), (10), with coefficients
Bn ϭ

(21)

2n ϩ 1 nϩ1
R
2

Ύ

p

f (␾)Pn(cos ␾) sin ␾ d␾.

0

The next example illustrates all this for a sphere of radius 1 consisting of two hemispheres
that are separated by a small strip of insulating material along the equator, so that these
hemispheres can be kept at different potentials (110 V and 0 V).
EXAMPLE 1

Spherical Capacitor
Find the potential inside and outside a spherical capacitor consisting of two metallic hemispheres of radius 1 ft
separated by a small slit for reasons of insulation, if the upper hemisphere is kept at 110 V and the lower is
grounded (Fig. 313).

Solution. The given boundary condition is (recall Fig. 312)
f (␾) ϭ e

110

if

0

if

0 Ϲ ␾ Ͻ p>2

p>2 Ͻ ␾ Ϲ p.

Since R ϭ 1, we thus obtain from (19)
An ϭ

ϭ

2n ϩ 1
2
2n ϩ 1
2

ؒ 110

Ύ

p>2

Pn(cos ␾) sin ␾ d␾

0

ؒ 110

Ύ

1

Pn (w) dw

0

where w ϭ cos ␾. Hence Pn(cos ␾) sin ␾ d␾ ϭ ϪPn(w) dw, we integrate from 1 to 0, and we finally get rid of
the minus by integrating from 0 to 1. You can evaluate this integral by your CAS or continue by using (11) in
Sec. 5.2, obtaining
M

An ϭ 55 (2n ϩ 1) a (Ϫ1)m
mϭ0

(2n Ϫ 2m)!

Ύ
2 m!(n Ϫ m)!(n Ϫ 2m)!

1

n

w n؊2m dw

0

where M ϭ n>2 for even n and M ϭ (n Ϫ 1)>2 for odd n. The integral equals 1>(n Ϫ 2m ϩ 1). Thus
z

110 volts

x

y

Fig. 313. Spherical capacitor in Example 1

c12-b.qxd

10/30/10

1:59 PM

598

Page 598

CHAP. 12 Partial Differential Equations (PDEs)

An ϭ

(22)

55 (2n ϩ 1)
n

2

M
(2n Ϫ 2m)!
m
a (Ϫ1) m!(n Ϫ m)!(n Ϫ 2m ϩ 1)! .
mϭ0

Taking n ϭ 0, we get A0 ϭ 55 (since 0! ϭ 1). For n ϭ 1, 2, 3, Á we get
A1 ϭ
A2 ϭ
A3 ϭ

165
2
275
4
385
8

2!

ؒ

0!1!2!
4!

a

0!2!3!

a

0!3!4!

6!

ϭ
Ϫ
Ϫ

165
2
2!

,

1!1!1!

b ϭ 0,

4!
1!2!2!

bϭϪ

385
8

,

etc.

Hence the potential (17) inside the sphere is (since P0 ϭ 1)
u (r, ␾) ϭ 55 ϩ

(23)

165
2

r P1 (cos ␾) Ϫ

385
8

r 3P3(cos ␾) ϩ Á

(Fig. 314)

with P1, P3, Á given by (11 r ), Sec. 5.21. Since R ϭ 1, we see from (19) and (21) in this section that Bn ϭ An,
and (20) thus gives the potential outside the sphere
u (r, ␾) ϭ

(24)

55
165
385
ϩ
P1(cos ␾) Ϫ
P3(cos ␾) ϩ Á .
r
2r 2
8r 4

Partial sums of these series can now be used for computing approximate values of the inner and outer potential.
Also, it is interesting to see that far away from the sphere the potential is approximately that of a point charge,
namely, 55>r . (Compare with Theorem 3 in Sec. 9.7.)
᭿
y
110

0

π
–
2

π

t

Fig. 314. Partial sums of the first 4, 6, and 11 nonzero terms of (23) for r ϭ R ϭ 1

EXAMPLE 2

Simpler Cases. Help with Problems
The technicalities encountered in cases that are similar to the one shown in Example 1 can often be avoided.
For instance, find the potential inside the sphere S: r ϭ R ϭ 1 when S is kept at the potential f (␾) ϭ cos 2␾.
(Can you see the potential on S? What is it at the North Pole? The equator? The South Pole?)

Solution. w ϭ cos ␾, cos 2␾ ϭ 2 cos2 ␾ Ϫ 1 ϭ 2w 2 Ϫ 1 ϭ 43P2(w) Ϫ 13 ϭ 43 (32w 2 Ϫ 12 ) Ϫ 13 . Hence the
potential in the interior of the sphere is
u ϭ 43 r 2P2(w) Ϫ 13 ϭ 43 r 2P2(cos ␾) Ϫ

1
3

ϭ 23 r 2(3 cos2 ␾ Ϫ 1) Ϫ 13 .

᭿

PROBLEM SET 12.11
1. Spherical coordinates. Derive (7) from ٌ2u in
spherical coordinates.
2. Cylindrical coordinates. Verify (5) by transforming
ٌ2u back into Cartesian coordinates.

3. Sketch Pn(cos u), 0 Ϲ u Ϲ 2p, for n ϭ 0, 1, 2. (Use
(11 r ) in Sec. 5.2.)
4. Zero surfaces. Find the surfaces on which u 1, u 2, u 3
in (16) are zero.

c12-b.qxd

10/30/10

1:59 PM

Page 599

SEC. 12.11 Laplace’s Equation in Cylindrical and Spherical Coordinates. Potential
5. CAS PROBLEM. Partial Sums. In Example 1 in the
text verify the values of A0, A1, A2, A3 and compute
A4, Á , A10. Try to find out graphically how well the
corresponding partial sums of (23) approximate the
given boundary function.
6. CAS EXPERIMENT. Gibbs Phenomenon. Study the
Gibbs phenomenon in Example 1 (Fig. 314) graphically.
7. Verify that u n and u*n in (16) are solutions of (8).
8–15

POTENTIALS DEPENDING ONLY ON r

8. Dimension 3. Verify that the potential u ϭ c>r, r ϭ
2x 2 ϩ y 2 ϩ z 2 satisfies Laplace’s equation in spherical
coordinates.
9. Spherical symmetry. Show that the only solution
of Laplace’s equation depending only on r ϭ
2x 2 ϩ y 2 ϩ z 2 is u ϭ c>r ϩ k with constant c and k.
10. Cylindrical symmetry. Show that the only solution of
Laplace’s equation depending only on r ϭ 2x 2 ϩ y 2
is u ϭ c ln r ϩ k.
11. Verification. Substituting u (r) with r as in Prob. 9 into
u xx ϩ u yy ϩ u zz ϭ 0, verify that u s ϩ 2u r >r ϭ 0, in
agreement with (7).
12. Dirichlet problem. Find the electrostatic potential
between coaxial cylinders of radii r1 ϭ 2 cm and
r2 ϭ 4 cm kept at the potentials U1 ϭ 220 V and
U2 ϭ 140 V, respectively.
13. Dirichlet problem. Find the electrostatic potential
between two concentric spheres of radii r1 ϭ 2 cm
and r2 ϭ 4 cm kept at the potentials U1 ϭ 220 V and
U2 ϭ 140 V, respectively. Sketch and compare the
equipotential lines in Probs. 12 and 13. Comment.
14. Heat problem. If the surface of the ball r 2 ϭ
x 2 ϩ y 2 ϩ z 2 Ϲ R2 is kept at temperature zero and the
initial temperature in the ball is f (r), show that the
temperature u (r, t) in the ball is a solution of u t ϭ
c2(u rr ϩ 2u r>r) satisfying the conditions u (R, t) ϭ
0, u (r, 0) ϭ f (r). Show that setting v ϭ ru gives
vt ϭ c2vrr, v (R, t) ϭ 0, v (r, 0) ϭ rf (r). Include the
condition v (0, t) ϭ 0 (which holds because u must be
bounded at r ϭ 0), and solve the resulting problem by
separating variables.
15. What are the analogs of Probs. 12 and 13 in heat
conduction?
16–20

BOUNDARY VALUE PROBLEMS
IN SPHERICAL COORDINATES r, U, ␾

Find the potential in the interior of the sphere r ϭ R ϭ 1
if the interior is free of charges and the potential on the
sphere is
16. f (␾) ϭ cos ␾
17. f (␾) ϭ 1
18. f (␾) ϭ 1 Ϫ cos2 ␾
19. f (␾) ϭ cos 2␾
20. f (␾) ϭ 10 cos3 ␾ Ϫ 3 cos2 ␾ Ϫ 5 cos ␾ Ϫ 1

599

21. Point charge. Show that in Prob. 17 the potential exterior
to the sphere is the same as that of a point charge at the
origin.
22. Exterior potential. Find the potentials exterior to the
sphere in Probs. 16 and 19.
23. Plane intersections. Sketch the intersections of the
equipotential surfaces in Prob. 16 with xz-plane.
24. TEAM PROJECT. Transmission Line and Related
PDEs. Consider a long cable or telephone wire (Fig. 315)
that is imperfectly insulated, so that leaks occur along the
entire length of the cable. The source S of the current
i (x, t) in the cable is at x ϭ 0, the receiving end T at
x ϭ l. The current flows from S to T and through the
load, and returns to the ground. Let the constants R, L,
C, and G denote the resistance, inductance, capacitance
to ground, and conductance to ground, respectively, of
the cable per unit length.
S

T

Load

x=0

x=l

Fig. 315. Transmission line
(a) Show that (“first transmission line equation”)
Ϫ

0u
0i
ϭ Ri ϩ L
0x
0t

where u (x, t) is the potential in the cable. Hint: Apply
Kirchhoff’s voltage law to a small portion of the cable
between x and x ϩ ¢x (difference of the potentials at
x and x ϩ ¢x ϭ resistive drop ϩ inductive drop).
(b) Show that for the cable in (a) (“second transmission line equation”),
Ϫ

0i
0u
ϭ Gu ϩ C .
0x
0t

Hint: Use Kirchhoff’s current law (difference of the
currents at x and x ϩ ¢x ϭ loss due to leakage to
ground ϩ capacitive loss).
(c) Second-order PDEs. Show that elimination of i
or u from the transmission line equations leads to
u xx ϭ LCu tt ϩ (RC ϩ GL)u t ϩ RGu,
i xx ϭ LCi tt ϩ (RC ϩ GL)i t ϩ RGi.
(d) Telegraph equations. For a submarine cable, G
is negligible and the frequencies are low. Show that
this leads to the so-called submarine cable equations
or telegraph equations
u xx ϭ RCu t,

i xx ϭ RCi t.

c12-b.qxd

10/30/10

1:59 PM

600

Page 600

CHAP. 12 Partial Differential Equations (PDEs)

Find the potential in a submarine cable with ends
(x ϭ 0, x ϭ l) grounded and initial voltage distribution
U0 ϭ const.
(e) High-frequency line equations. Show that in the
case of alternating currents of high frequencies the
equations in (c) can be approximated by the so-called
high-frequency line equations
u xx ϭ LCu tt,

12.12

Solve the first of them, assuming that the initial
potential is
U0 sin (px>l),
and u t (x, 0) ϭ 0 and u ϭ 0 at the ends x ϭ 0 and x ϭ l
for all t.
25. Reflection in a sphere. Let r, u, ␾ be spherical
coordinates. If u (r, u, ␾) satisfies ٌ2u ϭ 0, show that
v (r, u, ␾) ϭ u (1>r, u, ␾)>r satisfies ٌ2v ϭ 0.

i xx ϭ LCi tt.

Solution of PDEs by Laplace Transforms
Readers familiar with Chap. 6 may wonder whether Laplace transforms can also be used
for solving partial differential equations. The answer is yes, particularly if one of the
independent variables ranges over the positive axis. The steps to obtain a solution are
similar to those in Chap. 6. For a PDE in two variables they are as follows.
1. Take the Laplace transform with respect to one of the two variables, usually t. This
gives an ODE for the transform of the unknown function. This is so since the
derivatives of this function with respect to the other variable slip into the
transformed equation. The latter also incorporates the given boundary and initial
conditions.
2. Solving that ODE, obtain the transform of the unknown function.
3. Taking the inverse transform, obtain the solution of the given problem.
If the coefficients of the given equation do not depend on t, the use of Laplace transforms
will simplify the problem.
We explain the method in terms of a typical example.

EXAMPLE 1

Semi-Infinite String
Find the displacement w (x, t) of an elastic string subject to the following conditions. (We write w since we need
u to denote the unit step function.)
(i) The string is initially at rest on the x-axis from x ϭ 0 to ϱ (“semi-infinite string”).
(ii) For t Ͼ 0 the left end of the string (x ϭ 0) is moved in a given fashion, namely, according to a single
sine wave

w (0, t) ϭ f (t) ϭ e

sin t

if 0 Ϲ t Ϲ 2p

0

otherwise

(Fig. 316).

(iii) Furthermore, xlim w (x, t) ϭ 0 for t м 0.
:ϱ

f(t)
1

π

2π

t

–1

Fig. 316. Motion of the left end of the string in Example 1 as a function of time t

c12-b.qxd

10/30/10

1:59 PM

Page 601

SEC. 12.12 Solution of PDEs by Laplace Transforms

601

Of course there is no infinite string, but our model describes a long string or rope (of negligible weight) with
its right end fixed far out on the x-axis.

Solution. We have to solve the wave equation (Sec. 12.2)
0 2w

(1)

0t

0 2w

ϭ c2

2

0x

2

c2 ϭ

,

T
r

for positive x and t, subject to the “boundary conditions”
w (0, t) ϭ f (t),

(2)

lim w (x, t) ϭ 0

(t м 0)

x :ϱ

with f as given above, and the initial conditions
(3)

(a)

w (x, 0) ϭ 0,

wt (x, 0) ϭ 0.

(b)

We take the Laplace transform with respect to t. By (2) in Sec. 6.2,
le

0 2w
0t

2

f ϭ s 2l{w} Ϫ sw (x, 0) Ϫ wt (x, 0) ϭ c2l e

0 2w
0x 2

f.

The expression Ϫsw (x, 0) Ϫ wt (x, 0) drops out because of (3). On the right we assume that we may interchange
integration and differentiation. Then
le

0 2w
0x

2

f ϭ

Ύ

ؕ

e؊st

0 2w
0x

0

dt ϭ

2

02
0x

2

Ύ

ؕ

e؊stw (x, t) dt ϭ

0

02
0x 2

l{w (x, t)}.

Writing W (x, s) ϭ l{w (x, t)}, we thus obtain
s 2W ϭ c2

0 2W
0x 2

,

thus

0 2W

Ϫ

0x 2

s2
c2

W ϭ 0.

Since this equation contains only a derivative with respect to x, it may be regarded as an ordinary differential
equation for W (x, s) considered as a function of x. A general solution is
W (x, s) ϭ A (s)esx>c ϩ B (s)e؊sx>c.

(4)

From (2) we obtain, writing F (s) ϭ l{f (t)},
W (0, s) ϭ l{w (0, t)} ϭ l{ f (t)} ϭ F (s).
Assuming that we can interchange integration and taking the limit, we have
lim W (x, s) ϭ xlim
x
:ؕ

:ؕ

Ύ

ؕ

0

e؊stw (x, t) dt ϭ

Ύ

ؕ

e؊st xlim w (x, t) dt ϭ 0.
:ؕ

0

This implies A (s) ϭ 0 in (4) because c Ͼ 0, so that for every fixed positive s the function esx>c increases as x
increases. Note that we may assume s Ͼ 0 since a Laplace transform generally exists for all s greater than some
fixed k (Sec. 6.2). Hence we have
W (0, s) ϭ B (s) ϭ F (s),
so that (4) becomes
W (x, s) ϭ F (s)e؊sx>c.
From the second shifting theorem (Sec. 6.3) with a ϭ x>c we obtain the inverse transform
(5)

x
x
w (x, t) ϭ f at Ϫ b u at Ϫ b
c
c

(Fig. 317)

c12-b.qxd

10/30/10

602

1:59 PM

Page 602

CHAP. 12 Partial Differential Equations (PDEs)
that is,
x
w (x, t) ϭ sin at Ϫ b
c

if

x
x
Ͻ t Ͻ ϩ 2p
c
c

or

ct Ͼ x Ͼ (t Ϫ 2p)c

and zero otherwise. This is a single sine wave traveling to the right with speed c. Note that a point x remains
at rest until t ϭ x>c, the time needed to reach that x if one starts at t ϭ 0 (start of the motion of the left end)
and travels with speed c. The result agrees with our physical intuition. Since we proceeded formally, we must
verify that (5) satisfies the given conditions. We leave this to the student.
᭿
(t = 0)
x
(t = 2π)

2π c

x

(t = 4π)

x

(t = 6π)

x

Fig. 317. Traveling wave in Example 1

We have reached the end of Chapter 12, in which we concentrated on the most important
partial differential equations (PDEs) in physics and engineering. We have also reached
the end of Part C on Fourier Analysis and PDEs.

Outlook
We have seen that PDEs underlie the modeling process of various important engineering
application. Indeed, PDEs are the subject of many ongoing research projects.
Numerics for PDEs follows in Secs. 21.4–21.7, which, by design for greater flexibility
in teaching, are independent of the other sections in Part E on numerics.
In the next part, that is, Part D on complex analysis, we turn to an area of a different
nature that is also highly important to the engineer. The rich vein of examples and problems
will signify this. It is of note that Part D includes another approach to the two-dimensional
Laplace equation with applications, as shown in Chap. 18.

PROBLEM SET 12.12
1. Verify the solution in Example 1. What traveling wave
do we obtain in Example 1 for a nonterminating
sinusoidal motion of the left end starting at t ϭ 2p?
2. Sketch a figure similar to Fig. 317 when c ϭ 1 and
f (x) is “triangular,” say, f (x) ϭ x if 0 Ͻ x Ͻ 12 , f (x) ϭ
1 Ϫ x if 12 Ͻ x Ͻ 1 and 0 otherwise.
3. How does the speed of the wave in Example 1 of the
text depend on the tension and on the mass of the string?
4–8

SOLVE BY LAPLACE TRANSFORMS

0w
0w
ϩx
ϭ x, w (x, 0) ϭ 1, w (0, t) ϭ 1
4.
0x
0t

5. x

0w
0w
ϩ
ϭ xt,
0x
0t

0w
0w
ϩ 2x
ϭ 2x,
6.
0x
0t

w (x, 0) ϭ 0 if x м 0,
w (0, t) ϭ 0 if t м 0
w (x, 0) ϭ 1, w (0, t) ϭ 1

7. Solve Prob. 5 by separating variables.
8.

0 2w
0 2w
0w
ϩ 100
ϩ 25w,
2 ϭ 100
0x
0t 2
0t
w (x, 0) ϭ 0 if x м 0, wt(x, 0) ϭ 0 if t м 0,
w (0, t) ϭ sin t if t м 0

c12-b.qxd

10/30/10

1:59 PM

Page 603

Chapter 12 Review Questions and Problems

9–12

603

HEAT PROBLEM

Find the temperature w (x, t) in a semi-infinite laterally
insulated bar extending from x ϭ 0 along the x-axis to
infinity, assuming that the initial temperature is 0, w (x, t) : 0
as x : ϱ for every fixed t м 0, and w (0, t) ϭ f (t). Proceed
as follows.
9. Set up the model and show that the Laplace transform
leads to
sW ϭ c2

0 2W
0x 2

Ύ
2c1p
x

t

x
2c 1p

Ύt

؊3>2 ؊x2>(4c2t)

e

dt

0

ϭ 1 Ϫ erf a

x
2c 1t

b

1
W0 (x, s) ϭ s e؊1sx>c

(F ϭ l{f}).

10. Applying the convolution theorem, show that in Prob. 9,
w (x, t) ϭ

w0 (x, t) ϭ

with the error function erf as defined in Problem Set
12.7.
12. Duhamel’s formula.4 Show that in Prob. 11,

(W ϭ l{w})

and
W ϭ F (s)e؊1sx>c

11. Let w (0, t) ϭ f (t) ϭ u (t) (Sec. 6.3). Denote the corresponding w, W, and F by w0, W0, and F0. Show that
then in Prob. 10,

t

f (t Ϫ t)t؊3>2e؊x

>(4c2t)

2

dt.

0

and the convolution theorem gives Duhamel’s formula
t

W (x, t) ϭ

Ύ f (t Ϫ t)
0

0w0

dt.

0t

CHAPTER 12 REVIEW QUESTIONS AND PROBLEMS
1. For what kinds of problems will modeling lead to an
ODE? To a PDE?
2. Mention some of the basic physical principles or laws
that will give a PDE in modeling.
3. State three or four of the most important PDEs and their
main applications.
4. What is “separating variables” in a PDE? When did we
apply it twice in succession?
5. What is d’Alembert’s solution method? To what PDE
does it apply?
6. What role did Fourier series play in this chapter? Fourier
integrals?
7. When and why did Legendre’s equation occur? Bessel’s
equation?
8. What are the eigenfunctions and their frequencies of the
vibrating string? Of the vibrating membrane?
9. What do you remember about types of PDEs? Normal
forms? Why is this important?
10. When did we use polar coordinates? Cylindrical coordinates? Spherical coordinates?
11. Explain mathematically (not physically) why we got
exponential functions in separating the heat equation,
but not for the wave equation.
12. Why and where did the error function occur?

4

13. How do problems for the wave equation and the heat
equation differ regarding additional conditions?
14. Name and explain the three kinds of boundary conditions
for Laplace’s equation.
15. Explain how the Laplace transform applies to PDEs.
16–18
Solve for u ϭ u (x, y):
16. u xx ϩ 25u ϭ 0
17. u yy ϩ u y Ϫ 6u ϭ 18
18. u xx ϩ u x ϭ 0, u (0, y) ϭ f (y),

u x (0, y) ϭ g(y)

19–21
NORMAL FORM
Transform to normal form and solve:
19. u xy ϭ u yy
20. u xx ϩ 6u xy ϩ 9u yy ϭ 0
21. u xx Ϫ 4u yy ϭ 0
22–24
VIBRATING STRING
Find and sketch or graph (as in Fig. 288 in Sec. 12.3) the
deflection u (x, t) of a vibrating string of length p, extending
from x ϭ 0 to x ϭ p, and c2 ϭ T>r ϭ 4 starting with
velocity zero and deflection:
22. sin 4x
23. sin3 x
1
1
24. 2 p Ϫ ƒ x Ϫ 2 p ƒ

JEAN–MARIE CONSTANT DUHAMEL (1797–1872), French mathematician.

c12-b.qxd

10/30/10

1:59 PM

Page 604

604

CHAP. 12 Partial Differential Equations (PDEs)

HEAT

25–27

Find the temperature distribution in a laterally insulated thin
copper bar (c2 ϭ K>(sr) ϭ 1.158 cm2>sec) of length 100
cm and constant cross section with endpoints at x ϭ 0 and
100 kept at 0°C and initial temperature:
25. sin 0.01px
26. 50 Ϫ ƒ 50 Ϫ x ƒ
27. sin3 0.01px

ADIABATIC CONDITIONS

28–30

Find the temperature distribution in a laterally insulated
bar of length p with c2 ϭ 1 for the adiabatic boundary
condition (see Problem Set 12.6) and initial temperature:
28. 3x 2
29. 100 cos 2x
30. 2p Ϫ 4 ƒ x Ϫ 12 p ƒ

TEMPERATURE IN A PLATE

31–32

ؕ

u (x, y, t) ϭ a a Bmn sin mx sin ny e؊c

(m2 ϩn2)t

2

mϭ1 nϭ1

where
Bmn ϭ

4

p2

p

ΎΎ
0

33–37

p

f (x, y) sin mx sin ny dx dy.

MEMBRANES

Show that the following membranes of area 1 with c2 ϭ 1
have the frequencies of the fundamental mode as given
(4-decimal values). Compare.
33. Circle: a1>(2 1p) ϭ 0.6784
34. Square: 1> 12 ϭ 0.7071
35. Rectangle with sides 1:2:15>8 ϭ 0.7906
36. Semicircle: 3.832> 18p ϭ 0.7643
37. Quadrant of circle: a21>(4 1p) ϭ 0.7244
(a21 ϭ 5.13562 ϭ first positive zero of J2)
38–40

31. Let f (x, y) ϭ u (x, y, 0) be the initial temperature in a
thin square plate of side p with edges kept at 0°C and
faces perfectly insulated. Separating variables, obtain
from u t ϭ c2ٌ2u the solution
ؕ

32. Find the temperature in Prob. 31 if
f (x, y) ϭ x (p Ϫ x)y (p Ϫ y).

ELECTROSTATIC POTENTIAL

Find the potential in the following charge-free regions.
38. Between two concentric spheres of radii r0 and r1 kept
at potentials u 0 and u 1, respectively.
39. Between two coaxial circular cylinders of radii r0 and
r1 kept at the potentials u 0 and u 1, respectively.
Compare with Prob. 38.
40. In the interior of a sphere of radius 1 kept at the
potential f (␾) ϭ cos 3␾ ϩ 3 cos ␾ (referred to our
usual spherical coordinates).

0

SUMMARY OF CHAPTER

12

Partial Differential Equations (PDEs)
Whereas ODEs (Chaps. 1–6) serve as models of problems involving only one
independent variable, problems involving two or more independent variables (space
variables or time t and one or several space variables) lead to PDEs. This accounts for
the enormous importance of PDEs to the engineer and physicist. Most important are:
(1)

u tt ϭ c2u xx

One-dimensional wave equation (Secs. 12.2–12.4)

(2)

u tt ϭ c2(u xx ϩ u yy)

Two-dimensional wave equation (Secs. 12.8–12.10)

(3) u t ϭ c2u xx

One-dimensional heat equation (Secs. 12.5, 12.6, 12.7)

(4) ٌ2u ϭ u xx ϩ u yy ϭ 0 Two-dimensional Laplace equation (Secs. 12.6, 12.10)
(5)

ٌ2u ϭ u xx ϩ u yy ϩ u zz ϭ 0

Three-dimensional Laplace equation
(Sec. 12.11).

Equations (1) and (2) are hyperbolic, (3) is parabolic, (4) and (5) are elliptic.

c12-b.qxd

10/30/10

1:59 PM

Page 605

Summary of Chapter 12

605

In practice, one is interested in obtaining the solution of such an equation in a
given region satisfying given additional conditions, such as initial conditions
(conditions at time t ϭ 0) or boundary conditions (prescribed values of the solution
u or some of its derivatives on the boundary surface S, or boundary curve C, of the
region) or both. For (1) and (2) one prescribes two initial conditions (initial
displacement and initial velocity). For (3) one prescribes the initial temperature
distribution. For (4) and (5) one prescribes a boundary condition and calls the
resulting problem a (see Sec. 12.6)
Dirichlet problem if u is prescribed on S,
Neumann problem if u n ϭ 0u>0n is prescribed on S,
Mixed problem if u is prescribed on one part of S and u n on the other.
A general method for solving such problems is the method of separating
variables or product method, in which one assumes solutions in the form of
products of functions each depending on one variable only. Thus equation (1) is
solved by setting u (x, t) ϭ F (x)G (t); see Sec. 12.3; similarly for (3) (see Sec. 12.6).
Substitution into the given equation yields ordinary differential equations for F and
G, and from these one gets infinitely many solutions F ϭ Fn and G ϭ Gn such that
the corresponding functions
u n(x, t) ϭ Fn(x)Gn(t)
are solutions of the PDE satisfying the given boundary conditions. These are the
eigenfunctions of the problem, and the corresponding eigenvalues determine the
frequency of the vibration (or the rapidity of the decrease of temperature in the case
of the heat equation, etc.). To satisfy also the initial condition (or conditions), one
must consider infinite series of the u n, whose coefficients turn out to be the Fourier
coefficients of the functions f and g representing the given initial conditions (Secs.
12.3, 12.6). Hence Fourier series (and Fourier integrals) are of basic importance
here (Secs. 12.3, 12.6, 12.7, 12.9).
Steady-state problems are problems in which the solution does not depend on
time t. For these, the heat equation u t ϭ c2ٌ2u becomes the Laplace equation.
Before solving an initial or boundary value problem, one often transforms the
PDE into coordinates in which the boundary of the region considered is given by
simple formulas. Thus in polar coordinates given by x ϭ r cos u, y ϭ r sin u, the
Laplacian becomes (Sec. 12.11)
(6)

ٌ2u ϭ u rr ϩ

1
1
u r ϩ 2 u uu ;
r
r

for spherical coordinates see Sec. 12.10. If one now separates the variables, one gets
Bessel’s equation from (2) and (6) (vibrating circular membrane, Sec. 12.10) and
Legendre’s equation from (5) transformed into spherical coordinates (Sec. 12.11).

c12-b.qxd

10/30/10

1:59 PM

Page 606

c13.qxd

10/30/10

2:14 PM

Page 607

PART

D

Complex
Analysis
CHAPTER
CHAPTER
CHAPTER
CHAPTER
CHAPTER
CHAPTER

13
14
15
16
17
18

Complex Numbers and Functions. Complex Differentiation
Complex Integration
Power Series, Taylor Series
Laurent Series. Residue Integration
Conformal Mapping
Complex Analysis and Potential Theory
Complex analysis has many applications in heat conduction, fluid flow, electrostatics, and
in other areas. It extends the familiar “real calculus” to “complex calculus” by introducing
complex numbers and functions. While many ideas carry over from calculus to complex
analysis, there is a marked difference between the two. For example, analytic functions,
which are the “good functions” (differentiable in some domain) of complex analysis, have
derivatives of all orders. This is in contrast to calculus, where real-valued functions of
real variables may have derivatives only up to a certain order. Thus, in certain ways,
problems that are difficult to solve in real calculus may be much easier to solve in complex
analysis. Complex analysis is important in applied mathematics for three main reasons:
1. Two-dimensional potential problems can be modeled and solved by methods of
analytic functions. This reason is the real and imaginary parts of analytic functions satisfy
Laplace’s equation in two real variables.
2. Many difficult integrals (real or complex) that appear in applications can be solved
quite elegantly by complex integration.
3. Most functions in engineering mathematics are analytic functions, and their study
as functions of a complex variable leads to a deeper understanding of their properties and
to interrelations in complex that have no analog in real calculus.

607

c13.qxd

10/30/10

2:14 PM

Page 608

CHAPTER

13

Complex Numbers
and Functions. Complex
Differentiation
The transition from “real calculus” to “complex calculus” starts with a discussion of
complex numbers and their geometric representation in the complex plane. We then
progress to analytic functions in Sec. 13.3. We desire functions to be analytic because
these are the “useful functions” in the sense that they are differentiable in some domain
and operations of complex analysis can be applied to them. The most important equations
are therefore the Cauchy–Riemann equations in Sec. 13.4 because they allow a test of
analyticity of such functions. Moreover, we show how the Cauchy–Riemann equations
are related to the important Laplace equation.
The remaining sections of the chapter are devoted to elementary complex functions
(exponential, trigonometric, hyperbolic, and logarithmic functions). These generalize the
familiar real functions of calculus. Detailed knowledge of them is an absolute necessity
in practical work, just as that of their real counterparts is in calculus.
Prerequisite: Elementary calculus.
References and Answers to Problems: App. 1 Part D, App. 2.

13.1

Complex Numbers and
Their Geometric Representation
The material in this section will most likely be familiar to the student and serve as a
review.
Equations without real solutions, such as x 2 ϭ Ϫ1 or x 2 Ϫ 10x ϩ 40 ϭ 0, were
observed early in history and led to the introduction of complex numbers.1 By definition,
a complex number z is an ordered pair (x, y) of real numbers x and y, written
z ϭ (x, y).
1

First to use complex numbers for this purpose was the Italian mathematician GIROLAMO CARDANO
(1501–1576), who found the formula for solving cubic equations. The term “complex number” was introduced
by CARL FRIEDRICH GAUSS (see the footnote in Sec. 5.4), who also paved the way for a general use of
complex numbers.

608

c13.qxd

10/30/10

2:14 PM

Page 609

SEC. 13.1 Complex Numbers and Their Geometric Representation

609

x is called the real part and y the imaginary part of z, written
x ϭ Re z,

y ϭ Im z.

By definition, two complex numbers are equal if and only if their real parts are equal
and their imaginary parts are equal.
(0, 1) is called the imaginary unit and is denoted by i,
i ϭ (0, 1).

(1)

Addition, Multiplication. Notation z ϭ x ϩ iy
Addition of two complex numbers z 1 ϭ (x 1, y1) and z 2 ϭ (x 2, y2) is defined by
(2)

z 1 ϩ z 2 ϭ (x 1, y1) ϩ (x 2, y2) ϭ (x 1 ϩ x 2,

y1 ϩ y2).

Multiplication is defined by
(3)

z 1z 2 ϭ (x 1, y1)(x 2, y2) ϭ (x 1x 2 Ϫ y1 y2,

x 1 y2 ϩ x 2 y1).

These two definitions imply that
(x 1, 0) ϩ (x 2, 0) ϭ (x 1 ϩ x 2, 0)
and
(x 1, 0)(x 2, 0) ϭ (x 1x 2, 0)
as for real numbers x 1, x 2. Hence the complex numbers “extend” the real numbers. We
can thus write
(x, 0) ϭ x.

Similarly,

(0, y) ϭ iy

because by (1), and the definition of multiplication, we have
iy ϭ (0, 1)y ϭ (0, 1)( y, 0) ϭ (0 # y Ϫ 1 # 0,

0 # 0 ϩ 1 # y) ϭ (0, y).

Together we have, by addition, (x, y) ϭ (x, 0) ϩ (0, y) ϭ x ϩ iy.
In practice, complex numbers z ‫( ؍‬x, y) are written
(4)

z ϭ x ϩ iy

or z ϭ x ϩ yi, e.g., 17 ϩ 4i (instead of i4).
Electrical engineers often write j instead of i because they need i for the current.
If x ϭ 0, then z ϭ iy and is called pure imaginary. Also, (1) and (3) give
(5)

i 2 ϭ Ϫ1

because, by the definition of multiplication, i 2 ϭ ii ϭ (0, 1)(0, 1) ϭ (Ϫ1, 0) ϭ Ϫ1.

c13.qxd

10/30/10

2:14 PM

610

Page 610

CHAP. 13 Complex Numbers and Functions. Complex Differentiation

For addition the standard notation (4) gives [see (2)]
(x 1 ϩ iy1) ϩ (x 2 ϩ iy2) ϭ (x 1 ϩ x 2) ϩ i( y1 ϩ y2).
For multiplication the standard notation gives the following very simple recipe. Multiply
each term by each other term and use i 2 ϭ Ϫ1 when it occurs [see (3)]:
(x 1 ϩ iy1)(x 2 ϩ iy2) ϭ x 1x 2 ϩ ix 1 y2 ϩ iy1x 2 ϩ i 2y1 y2
ϭ (x 1x 2 Ϫ y1 y2) ϩ i(x 1 y2 ϩ x 2 y1).
This agrees with (3). And it shows that x ϩ iy is a more practical notation for complex
numbers than (x, y).
If you know vectors, you see that (2) is vector addition, whereas the multiplication (3)
has no counterpart in the usual vector algebra.
EXAMPLE 1

Real Part, Imaginary Part, Sum and Product of Complex Numbers
Let z 1 ϭ 8 ϩ 3i and z 2 ϭ 9 Ϫ 2i. Then Re z 1 ϭ 8, Im z 1 ϭ 3, Re z 2 ϭ 9, Im z 2 ϭ Ϫ2 and
z 1 ϩ z 2 ϭ (8 ϩ 3i) ϩ (9 Ϫ 2i) ϭ 17 ϩ i,

᭿

z 1z 2 ϭ (8 ϩ 3i)(9 Ϫ 2i) ϭ 72 ϩ 6 ϩ i (Ϫ16 ϩ 27) ϭ 78 ϩ 11i.

Subtraction, Division
Subtraction and division are defined as the inverse operations of addition and multiplication, respectively. Thus the difference z ϭ z 1 Ϫ z 2 is the complex number z for which
z 1 ϭ z ϩ z 2. Hence by (2),
z 1 Ϫ z 2 ϭ (x 1 Ϫ x 2) ϩ i ( y1 Ϫ y2).

(6)

The quotient z ϭ z 1>z 2 (z 2 0) is the complex number z for which z 1 ϭ zz 2. If we
equate the real and the imaginary parts on both sides of this equation, setting z ϭ x ϩ iy,
we obtain x 1 ϭ x 2 x Ϫ y2 y, y1 ϭ y2 x ϩ x 2 y. The solution is
z1
z ϭ z ϭ x ϩ iy,
2

(7*)

xϭ

x 1x 2 ϩ y1 y2
x 22 ϩ y 22

yϭ

,

x 2 y1 Ϫ x 1 y2
x 22 ϩ y 22

.

The practical rule used to get this is by multiplying numerator and denominator of z 1>z 2
by x 2 Ϫ iy2 and simplifying:
(7)

EXAMPLE 2

zϭ

x 1 ϩ iy1
x 2 ϩ iy2

ϭ

(x 1 ϩ iy1)(x 2 Ϫ iy2)
(x 2 ϩ iy2)(x 2 Ϫ iy2)

ϭ

x 1x 2 ϩ y1 y2
x 22

ϩ

y 22

ϩi

x 2 y1 Ϫ x 1 y2
x 22 ϩ y 22

.

Difference and Quotient of Complex Numbers
For z 1 ϭ 8 ϩ 3i and z 2 ϭ 9 Ϫ 2i we get z 1 Ϫ z 2 ϭ (8 ϩ 3i) Ϫ (9 Ϫ 2i) ϭ Ϫ1 ϩ 5i and
z1
(8 ϩ 3i)(9 ϩ 2i)
66 ϩ 43i
66
43
8 ϩ 3i
ϭ
ϭ
ϭ
ϩ
i.
ϭ
z2
9 Ϫ 2i
(9 Ϫ 2i)(9 ϩ 2i)
81 ϩ 4
85
85
Check the division by multiplication to get 8 ϩ 3i.

᭿

c13.qxd

10/30/10

2:14 PM

Page 611

SEC. 13.1 Complex Numbers and Their Geometric Representation

611

Complex numbers satisfy the same commutative, associative, and distributive laws as real
numbers (see the problem set).

Complex Plane
So far we discussed the algebraic manipulation of complex numbers. Consider the
geometric representation of complex numbers, which is of great practical importance. We
choose two perpendicular coordinate axes, the horizontal x-axis, called the real axis, and
the vertical y-axis, called the imaginary axis. On both axes we choose the same unit of
length (Fig. 318). This is called a Cartesian coordinate system.
y

(Imaginary
axis)
y

1
P

5

x

z = x + iy
–1

1
(Real
x axis)

1

4 – 3i

–3

Fig. 319. The number 4 Ϫ 3i in
the complex plane

Fig. 318. The complex plane

We now plot a given complex number z ϭ (x, y) ϭ x ϩ iy as the point P with coordinates
x, y. The xy-plane in which the complex numbers are represented in this way is called the
complex plane.2 Figure 319 shows an example.
Instead of saying “the point represented by z in the complex plane” we say briefly and
simply “the point z in the complex plane.” This will cause no misunderstanding.
Addition and subtraction can now be visualized as illustrated in Figs. 320 and 321.
y
z2

y

z1

z1 + z2

z2

x
z1– z2
z1
x

Fig. 320. Addition of complex numbers

2

– z2

Fig. 321. Subtraction of complex numbers

Sometimes called the Argand diagram, after the French mathematician JEAN ROBERT ARGAND
(1768–1822), born in Geneva and later librarian in Paris. His paper on the complex plane appeared in 1806,
nine years after a similar memoir by the Norwegian mathematician CASPAR WESSEL (1745–1818), a surveyor
of the Danish Academy of Science.

c13.qxd

10/30/10

2:14 PM

612

Page 612

CHAP. 13 Complex Numbers and Functions. Complex Differentiation

Complex Conjugate Numbers
The complex conjugate z of a complex number z ϭ x ϩ iy is defined by
z ϭ x Ϫ iy.
It is obtained geometrically by reflecting the point z in the real axis. Figure 322 shows
this for z ϭ 5 ϩ 2i and its conjugate z ϭ 5 Ϫ 2i.
y
z = x + iy = 5 + 2i

2
5

x
z = x – iy = 5 – 2i

–2

Fig. 322. Complex conjugate numbers

The complex conjugate is important because it permits us to switch from complex
to real. Indeed, by multiplication, zz ϭ x 2 ϩ y 2 (verify!). By addition and subtraction,
z ϩ z ϭ 2x, z Ϫ z ϭ 2iy. We thus obtain for the real part x and the imaginary part y
(not iy!) of z ϭ x ϩ iy the important formulas
(8)

Re z ϭ x ϭ 12 (z ϩ z),

Im z ϭ y ϭ

1
(z Ϫ z).
2i

If z is real, z ϭ x, then z ϭ z by the definition of z, and conversely. Working with
conjugates is easy, since we have
(z 1 ϩ z 2) ϭ z1 ϩ z 2,
(9)
(z 1z 2) ϭ z1z 2,
EXAMPLE 3

(z 1 Ϫ z 2) ϭ z 1 Ϫ z 2,
z1
z1
az b ϭ
.
2
z2

Illustration of (8) and (9)
Let z 1 ϭ 4 ϩ 3i and z 2 ϭ 2 ϩ 5i. Then by (8),
Im z 1 ϭ

1
2i

[(4 ϩ 3i) Ϫ (4 Ϫ 3i)] ϭ

3i ϩ 3i
2i

ϭ 3.

Also, the multiplication formula in (9) is verified by
(z 1z 2) ϭ (4 ϩ 3i)(2 ϩ 5i) ϭ (Ϫ7 ϩ 26i) ϭ Ϫ7 Ϫ 26i,
z 1z 2 ϭ (4 Ϫ 3i)(2 Ϫ 5i) ϭ Ϫ7 Ϫ 26i.

᭿

PROBLEM SET 13.1
1. Powers of i. Show that i 2 ϭ Ϫ1, i 3 ϭ Ϫi, i 4 ϭ 1,
i 5 ϭ i, Á and 1>i ϭ Ϫi, 1>i 2 ϭ Ϫ1, 1>i 3 ϭ i, Á .
2. Rotation. Multiplication by i is geometrically a
counterclockwise rotation through p>2 (90°). Verify

this by graphing z and iz and the angle of rotation for
z ϭ 1 ϩ i, z ϭ Ϫ1 ϩ 2i, z ϭ 4 Ϫ 3i.
3. Division. Verify the calculation in (7). Apply (7) to
(26 Ϫ 18i)>(6 Ϫ 2i).

c13.qxd

10/30/10

2:14 PM

Page 613

SEC. 13.2 Polar Form of Complex Numbers. Powers and Roots
4. Law for conjugates. Verify (9) for z 1 ϭ Ϫ11 ϩ 10i,
z 2 ϭ Ϫ1 ϩ 4i.
5. Pure imaginary number. Show that z ϭ x ϩ iy is
pure imaginary if and only if z ϭ Ϫz.
6. Multiplication. If the product of two complex numbers
is zero, show that at least one factor must be zero.
7. Laws of addition and multiplication. Derive the
following laws for complex numbers from the corresponding laws for real numbers.
z 1 ϩ z 2 ϭ z 2 ϩ z 1, z 1z 2 ϭ z 2z 1 (Commutative laws)
(z 1 ϩ z 2) ϩ z 3 ϭ z 1 ϩ (z 2 ϩ z 3),
(Associative laws)
(z 1z 2)z 3 ϭ z 1(z 2z 3)
z 1(z 2 ϩ z 3) ϭ z 1z 2 ϩ z 1z 3

(Distributive law)

0 ϩ z ϭ z ϩ 0 ϭ z,
z ϩ (Ϫz) ϭ (Ϫz) ϩ z ϭ 0,

13.2

z # 1 ϭ z.

613

8–15
COMPLEX ARITHMETIC
Let z 1 ϭ Ϫ2 ϩ 11i, z 2 ϭ 2 Ϫ i. Showing the details of
your work, find, in the form x ϩ iy:
8. z 1z 2, (z 1z 2)
9. Re (z 21), (Re z 1)2
2
2
10. Re (1>z 2), 1>Re (z 2)
11. (z 1 Ϫ z 2)2>16, (z 1>4 Ϫ z 2>4)2
12. z 1>z 2, z 2>z 1
13. (z 1 ϩ z 2)(z 1 Ϫ z 2), z 21 Ϫ z 22
14. z 1>z 2, (z 1>z 2)
15. 4 (z 1 ϩ z 2)>(z 1 Ϫ z 2)
16–20
Let z ϭ x ϩ iy. Showing details, find, in terms
of x and y:
16. Im (1>z), Im (1>z 2)
18. Re [(1 ϩ i)16z 2]
20. Im (1>z 2)

17. Re z 4 Ϫ (Re z 2)2
19. Re (z>z), Im (z>z)

Polar Form of Complex Numbers.
Powers and Roots
We gain further insight into the arithmetic operations of complex numbers if, in addition
to the xy-coordinates in the complex plane, we also employ the usual polar coordinates
r, u defined by
(1)

x ϭ r cos u,

y ϭ r sin u.

We see that then z ϭ x ϩ iy takes the so-called polar form
(2)

z ϭ r(cos u ϩ i sin u).

r is called the absolute value or modulus of z and is denoted by ƒ z ƒ . Hence
(3)

ƒ z ƒ ϭ r ϭ 2x 2 ϩ y 2 ϭ 1zz.

Geometrically, ƒ z ƒ is the distance of the point z from the origin (Fig. 323). Similarly,
ƒ z 1 Ϫ z 2 ƒ is the distance between z 1 and z 2 (Fig. 324).
u is called the argument of z and is denoted by arg z. Thus u ϭ arg z and (Fig. 323)
(4)

y
tan u ϭ x

(z

0).

Geometrically, u is the directed angle from the positive x-axis to OP in Fig. 323. Here, as
in calculus, all angles are measured in radians and positive in the counterclockwise sense.

10/30/10

2:14 PM

Page 614

614

CHAP. 13 Complex Numbers and Functions. Complex Differentiation

For z ϭ 0 this angle u is undefined. (Why?) For a given z 0 it is determined only up
to integer multiples of 2p since cosine and sine are periodic with period 2p. But one
often wants to specify a unique value of arg z of a given z 0. For this reason one defines
the principal value Arg z (with capital A!) of arg z by the double inequality
Ϫp Ͻ Arg z Ϲ p.

(5)

Then we have Arg z ϭ 0 for positive real z ϭ x, which is practical, and Arg z ϭ p (not
Ϫp!) for negative real z, e.g., for z ϭ Ϫ4. The principal value (5) will be important in
connection with roots, the complex logarithm (Sec. 13.7), and certain integrals. Obviously,
for a given z 0, the other values of arg z are arg z ϭ Arg z Ϯ 2np (n ϭ Ϯ1, Ϯ2, Á ).
Imaginary
axis
y

r
|=

|z

θ
x

O

z2

z = x + iy

| z1 – z

|

2

| z2
|

P

y

z1

|z 1|

Real
axis

x

Fig. 323. Complex plane, polar form
of a complex number

Fig. 324. Distance between two
points in the complex plane

Polar Form of Complex Numbers. Principal Value Arg z

EXAMPLE 1
y

z ϭ 1 ϩ i (Fig. 325) has the polar form z ϭ 22 (cos 14 p ϩ i sin 14 p). Hence we obtain
ƒ z ƒ ϭ 22,

1+i

1
2

c13.qxd

arg z ϭ 14 p Ϯ 2np (n ϭ 0, 1, Á ),

and

Arg z ϭ 14 p (the principal value).

Similarly, z ϭ 3 ϩ 3 23i ϭ 6 (cos 13 p ϩ i sin 13 p), ƒ z ƒ ϭ 6, and Arg z ϭ 13 p.
π /4

᭿

CAUTION! In using (4), we must pay attention to the quadrant in which z lies, since
tan u has period p, so that the arguments of z and Ϫz have the same tangent. Example:
Fig. 325. Example 1
for u1 ϭ arg (1 ϩ i) and u2 ϭ arg (Ϫ1 Ϫ i) we have tan u1 ϭ tan u2 ϭ 1.
1

x

Triangle Inequality
Inequalities such as x 1 Ͻ x 2 make sense for real numbers, but not in complex because there
is no natural way of ordering complex numbers. However, inequalities between absolute values
(which are real!), such as ƒ z 1 ƒ Ͻ ƒ z 2 ƒ (meaning that z 1 is closer to the origin than z 2) are of
great importance. The daily bread of the complex analyst is the triangle inequality
(6)

ƒ z1 ϩ z2 ƒ Ϲ ƒ z1 ƒ ϩ ƒ z2 ƒ

(Fig. 326)

which we shall use quite frequently. This inequality follows by noting that the three
points 0, z 1, and z 1 ϩ z 2 are the vertices of a triangle (Fig. 326) with sides ƒ z 1 ƒ , ƒ z 2 ƒ , and
ƒ z 1 ϩ z 2 ƒ , and one side cannot exceed the sum of the other two sides. A formal proof is
left to the reader (Prob. 33). (The triangle degenerates if z 1 and z 2 lie on the same straight
line through the origin.)

c13.qxd

10/30/10

2:14 PM

Page 615

SEC. 13.2 Polar Form of Complex Numbers. Powers and Roots

615

y

z1 + z2

z2
z1

x

Fig. 326. Triangle inequality

By induction we obtain from (6) the generalized triangle inequality
ƒ z1 ϩ z2 ϩ Á ϩ zn ƒ Ϲ ƒ z1 ƒ ϩ ƒ z2 ƒ ϩ Á ϩ ƒ zn ƒ ;

(6*)

that is, the absolute value of a sum cannot exceed the sum of the absolute values of the terms.
EXAMPLE 2

Triangle Inequality
If z 1 ϭ 1 ϩ i and z 2 ϭ Ϫ2 ϩ 3i, then (sketch a figure!)

᭿

ƒ z 1 ϩ z 2 ƒ ϭ ƒ Ϫ1 ϩ 4i ƒ ϭ 117 ϭ 4.123 Ͻ 12 ϩ 113 ϭ 5.020.

Multiplication and Division in Polar Form
This will give us a “geometrical” understanding of multiplication and division. Let
z 1 ϭ r1(cos u1 ϩ i sin u1)

and

z 2 ϭ r2(cos u2 ϩ i sin u2).

Multiplication. By (3) in Sec. 13.1 the product is at first
z 1z 2 ϭ r1r2 [(cos u1 cos u2 Ϫ sin u1 sin u2) ϩ i(sin u1 cos u2 ϩ cos u1 sin u2)].
The addition rules for the sine and cosine [(6) in App. A3.1] now yield
(7)

z 1z 2 ϭ r1r2 [cos (u1 ϩ u2) ϩ i sin (u1 ϩ u2)].

Taking absolute values on both sides of (7), we see that the absolute value of a product
equals the product of the absolute values of the factors,
(8)

ƒ z 1z 2 ƒ ϭ ƒ z 1 ƒ ƒ z 2 ƒ .

Taking arguments in (7) shows that the argument of a product equals the sum of the
arguments of the factors,
(9)

arg (z 1z 2) ϭ arg z 1 ϩ arg z 2

(up to multiples of 2p).

Division. We have z 1 ϭ (z 1>z 2)z 2. Hence ƒ z 1 ƒ ϭ ƒ (z 1>z 2) z 2 ƒ ϭ ƒ z 1>z 2 ƒ ƒ z 2 ƒ and by
division by ƒ z 2 ƒ

(10)

`

ƒ z1 ƒ
z1
` ϭ
z2
ƒ z2 ƒ

(z 2

0).

c13.qxd

10/30/10

2:14 PM

616

Page 616

CHAP. 13 Complex Numbers and Functions. Complex Differentiation

Similarly, arg z 1 ϭ arg [(z 1>z 2)z 2] ϭ arg (z 1>z 2) ϩ arg z 2 and by subtraction of arg z 2
z1
arg z ϭ arg z 1 Ϫ arg z 2

(11)

2

(up to multiples of 2p).

Combining (10) and (11) we also have the analog of (7),
z1
r1
z 2 ϭ r2 [cos (u1 Ϫ u2) ϩ i sin (u1 Ϫ u2)].

(12)

To comprehend this formula, note that it is the polar form of a complex number of absolute
value r1>r2 and argument u1 Ϫ u2. But these are the absolute value and argument of z 1>z 2,
as we can see from (10), (11), and the polar forms of z 1 and z 2.
EXAMPLE 3

Illustration of Formulas (8)–(11)
Let z 1 ϭ Ϫ2 ϩ 2i and z 2 ϭ 3i. Then z 1z 2 ϭ Ϫ6 Ϫ 6i, z 1>z 2 ϭ 23 ϩ ( 23 )i. Hence (make a sketch)
ƒ z 1z 2 ƒ ϭ 612 ϭ 318 ϭ ƒ z 1 ƒ ƒ z 2 ƒ ,

ƒ z 1>z 2 ƒ ϭ 212>3 ϭ ƒ z 1 ƒ > ƒ z 2 ƒ ,

and for the arguments we obtain Arg z 1 ϭ 3p>4, Arg z 2 ϭ p>2,
Arg (z 1z 2) ϭ Ϫ

EXAMPLE 4

3p
4

ϭ Arg z 1 ϩ Arg z 2 Ϫ 2p,

Arg a

z1
p
b ϭ ϭ Arg z 1 Ϫ Arg z 2.
z2
4

᭿

Integer Powers of z. De Moivre’s Formula
From (8) and (9) with z 1 ϭ z 2 ϭ z we obtain by induction for n ϭ 0, 1, 2, Á
(13)

z n ϭ r n (cos nu ϩ i sin nu).

Similarly, (12) with z 1 ϭ 1 and z 2 ϭ z n gives (13) for n ϭ Ϫ1, Ϫ2, Á . For ƒ z ƒ ϭ r ϭ 1, formula (13) becomes
De Moivre’s formula3
(13*)

(cos u ϩ i sin u)n ϭ cos nu ϩ i sin nu.

We can use this to express cos nu and sin nu in terms of powers of cos u and sin u. For instance, for n ϭ 2 we
have on the left cos2 u ϩ 2i cos u sin u Ϫ sin2 u. Taking the real and imaginary parts on both sides of (13*)
with n ϭ 2 gives the familiar formulas
cos 2u ϭ cos2 u Ϫ sin2 u,

sin 2u ϭ 2 cos u sin u.

This shows that complex methods often simplify the derivation of real formulas. Try n ϭ 3.

᭿

Roots
If z ϭ w n (n ϭ 1, 2, Á ), then to each value of w there corresponds one value of z. We
shall immediately see that, conversely, to a given z 0 there correspond precisely n
distinct values of w. Each of these values is called an nth root of z, and we write
3
ABRAHAM DE MOIVRE (1667–1754), French mathematician, who pioneered the use of complex numbers
in trigonometry and also contributed to probability theory (see Sec. 24.8).

c13.qxd

10/30/10

2:14 PM

Page 617

SEC. 13.2 Polar Form of Complex Numbers. Powers and Roots

617
n

w ϭ 1z .

(14)

n

Hence this symbol is multivalued, namely, n-valued. The n values of 1 z can be obtained
as follows. We write z and w in polar form
z ϭ r(cos u ϩ i sin u)

w ϭ R(cos ␾ ϩ i sin ␾).

and

Then the equation w n ϭ z becomes, by De Moivre’s formula (with ␾ instead of u),
w n ϭ Rn(cos n␾ ϩ i sin n␾) ϭ z ϭ r(cos u ϩ i sin u).
n

The absolute values on both sides must be equal; thus, R n ϭ r, so that R ϭ 1 r , where
n
1 r is positive real (an absolute value must be nonnegative!) and thus uniquely determined.
Equating the arguments n␾ and u and recalling that u is determined only up to integer
multiples of 2p, we obtain
n␾ ϭ u ϩ 2kp,

2kp
u
␾ϭnϩ n

thus

where k is an integer. For k ϭ 0, 1, Á , n Ϫ 1 we get n distinct values of w. Further integers
of k would give values already obtained. For instance, k ϭ n gives 2kp>n ϭ 2p, hence
n
the w corresponding to k ϭ 0, etc. Consequently, 1 z, for z 0, has the n distinct values
n
n
1 z ϭ 1 r acos

(15)

u ϩ 2kp
u ϩ 2kp
ϩ i sin
b
n
n
n

where k ϭ 0, 1, Á , n Ϫ 1. These n values lie on a circle of radius 1 r with center at the
n
origin and constitute the vertices of a regular polygon of n sides. The value of 1 z obtained
by taking the principal value of arg z and k ϭ 0 in (15) is called the principal value of
n
w ϭ 1z.
Taking z ϭ 1 in (15), we have ƒ z ƒ ϭ r ϭ 1 and Arg z ϭ 0. Then (15) gives
2kp
2kp
n
2 1 ϭ cos n ϩ i sin n ,

(16)

k ϭ 0, 1, Á , n Ϫ 1.

These n values are called the nth roots of unity. They lie on the circle of radius 1 and
center 0, briefly called the unit circle (and used quite frequently!). Figures 327–329 show
1
1
3
4
5
21 ϭ 1, Ϫ2 Ϯ 2 23i, 21 ϭ Ϯ1, Ϯi, and 21.
y

y

y
ω

ω

ω
ω2

ω2
1

1

x
ω3

ω2

Fig. 327.

1

x
ω3

3
2
1

Fig. 328.

ω4

4
2
1

Fig. 329.

5
2
1

x

c13.qxd

10/30/10

2:14 PM

618

Page 618

CHAP. 13 Complex Numbers and Functions. Complex Differentiation
n

If v denotes the value corresponding to k ϭ 1 in (16), then the n values of 2 1 can be
written as
1, v, v2, Á , vn؊1.
More generally, if w1 is any nth root of an arbitrary complex number z (
n
values of 1 z in (15) are
(17)

w1,

w1v,

w1v2,

Á,

0), then the n

w1vn؊1

because multiplying w1 by vk corresponds to increasing the argument of w1 by 2kp>n.
Formula (17) motivates the introduction of roots of unity and shows their usefulness.

PROBLEM SET 13.2
1–8
POLAR FORM
Represent in polar form and graph in the complex plane as
in Fig. 325. Do these problems very carefully because polar
forms will be needed frequently. Show the details.
1. 1 ϩ i
2. Ϫ4 ϩ 4i
3. 2i, Ϫ2i
4. Ϫ5
5.

22 ϩ i>3
Ϫ 28 Ϫ 2i>3

7. 1 ϩ 12pi

6.

8.

23 Ϫ 10i
Ϫ12 23 ϩ 5i
Ϫ4 ϩ 19i
2 ϩ 5i

9–14
PRINCIPAL ARGUMENT
Determine the principal value of the argument and graph it
as in Fig. 325.
9. Ϫ1 ϩ i
10. Ϫ5, Ϫ5 Ϫ i, Ϫ5 ϩ i
11. 3 Ϯ 4i
12. Ϫp Ϫ pi
13. (1 ϩ i)20
14. Ϫ1 ϩ 0.1i, Ϫ1 Ϫ 0.1i
15–18
CONVERSION TO x ؉ iy
Graph in the complex plane and represent in the form x ϩ iy:
15. 3 (cos 12p Ϫ i sin 12p) 16. 6 (cos 13p ϩ i sin 13p)
17. 28 (cos 14p ϩ i sin 14p)
18. 250 (cos 34p ϩ i sin 34p)

ROOTS
19. CAS PROJECT. Roots of Unity and Their Graphs.
Write a program for calculating these roots and for
graphing them as points on the unit circle. Apply the
program to z n ϭ 1 with n ϭ 2, 3, Á , 10. Then extend
the program to one for arbitrary roots, using an idea
near the end of the text, and apply the program to
examples of your choice.

20. TEAM PROJECT. Square Root. (a) Show that
w ϭ 1z has the values
w1 ϭ 1r c cos
(18)

u
u
ϩ i sin d ,
2
2

u
u
w2 ϭ 1r c cos a ϩ p b ϩ i sin a ϩ p b d
2
2
ϭ Ϫw1.

(b) Obtain from (18) the often more practical formula
( 1 9 ) 2z ϭ Ϯ[212( ƒ z ƒ ϩ x) ϩ (sign y)i212( ƒ z ƒ ϩ x)]
where sign y ϭ 1 if y м 0, sign y ϭ Ϫ1 if y Ͻ 0, and
all square roots of positive numbers are taken with
positive sign. Hint: Use (10) in App. A3.1 with x ϭ u>2.
(c) Find the square roots of Ϫ14i, Ϫ9 Ϫ 40i, and
1 ϩ 248i by both (18) and (19) and comment on the
work involved.
(d) Do some further examples of your own and apply
a method of checking your results.
21–27
ROOTS
Find and graph all roots in the complex plane.
3
3
21. 2
1 ϩ i 22. 2
3 ϩ 4i
3
4
23. 2216
24. 2
Ϫ4
8
4
5
25. 2
26. ͙1ෆ
27. 2
i
Ϫ1
28–31
EQUATIONS
Solve and graph the solutions. Show details.
28. z 2 Ϫ (6 Ϫ 2i) z ϩ 17 Ϫ 6i ϭ 0
29. z 2 ϩ z ϩ 1 Ϫ i ϭ 0
30. z 4 ϩ 324 ϭ 0. Using the solutions, factor z 4 ϩ 324
into quadratic factors with real coefficients.
31. z 4 Ϫ 6iz 2 ϩ 16 ϭ 0

c13.qxd

10/30/10

2:14 PM

Page 619

SEC. 13.3 Derivative. Analytic Function
32–35

619
34. Re and Im. Prove ƒ Re z ƒ Ϲ ƒ z ƒ ,

INEQUALITIES AND EQUALITY

32. Triangle inequality. Verify (6) for z 1 ϭ 3 ϩ i,
z 2 ϭ Ϫ2 ϩ 4i
33. Triangle inequality. Prove (6).

13.3

ƒ Im z ƒ Ϲ ƒ z ƒ .

35. Parallelogram equality. Prove and explain the name
ƒ z 1 ϩ z 2 ƒ 2 ϩ ƒ z 1 Ϫ z 2 ƒ 2 ϭ 2 ( ƒ z 1 ƒ 2 ϩ ƒ z 2 ƒ 2).

Derivative. Analytic Function
Just as the study of calculus or real analysis required concepts such as domain,
neighborhood, function, limit, continuity, derivative, etc., so does the study of complex
analysis. Since the functions live in the complex plane, the concepts are slightly more
difficult or different from those in real analysis. This section can be seen as a reference
section where many of the concepts needed for the rest of Part D are introduced.

Circles and Disks. Half-Planes
The unit circle ƒ z ƒ ϭ 1 (Fig. 330) has already occurred in Sec. 13.2. Figure 331 shows a
general circle of radius r and center a. Its equation is
ƒz Ϫ aƒ ϭ r
y

y
y

ρ2
ρ
1 x

ρ1

a
a

x

Fig. 330. Unit circle

Fig. 331. Circle in the
complex plane

x

Fig. 332. Annulus in the
complex plane

because it is the set of all z whose distance ƒ z Ϫ a ƒ from the center a equals r. Accordingly,
its interior (“open circular disk”) is given by ƒ z Ϫ a ƒ Ͻ r, its interior plus the circle
itself (“closed circular disk”) by ƒ z Ϫ a ƒ Ϲ r, and its exterior by ƒ z Ϫ a ƒ Ͼ r. As an
example, sketch this for a ϭ 1 ϩ i and r ϭ 2, to make sure that you understand these
inequalities.
An open circular disk ƒ z Ϫ a ƒ Ͻ r is also called a neighborhood of a or, more precisely,
a r-neighborhood of a. And a has infinitely many of them, one for each value of r (Ͼ 0),
and a is a point of each of them, by definition!
In modern literature any set containing a r-neighborhood of a is also called a neighborhood of a.
Figure 332 shows an open annulus (circular ring) r1 Ͻ ƒ z Ϫ a ƒ Ͻ r2, which we shall
need later. This is the set of all z whose distance ƒ z Ϫ a ƒ from a is greater than r1 but
less than r2. Similarly, the closed annulus r1 Ϲ ƒ z Ϫ a ƒ Ϲ r2 includes the two circles.
Half-Planes. By the (open) upper half-plane we mean the set of all points z ϭ x ϩ iy
such that y Ͼ 0. Similarly, the condition y Ͻ 0 defines the lower half-plane, x Ͼ 0 the
right half-plane, and x Ͻ 0 the left half-plane.

c13.qxd

10/30/10

620

2:14 PM

Page 620

CHAP. 13 Complex Numbers and Functions. Complex Differentiation

For Reference: Concepts on Sets
in the Complex Plane
To our discussion of special sets let us add some general concepts related to sets that we
shall need throughout Chaps. 13–18; keep in mind that you can find them here.
By a point set in the complex plane we mean any sort of collection of finitely many
or infinitely many points. Examples are the solutions of a quadratic equation, the
points of a line, the points in the interior of a circle as well as the sets discussed just
before.
A set S is called open if every point of S has a neighborhood consisting entirely of
points that belong to S. For example, the points in the interior of a circle or a square form
an open set, and so do the points of the right half-plane Re z ϭ x Ͼ 0.
A set S is called connected if any two of its points can be joined by a chain of finitely
many straight-line segments all of whose points belong to S. An open and connected set
is called a domain. Thus an open disk and an open annulus are domains. An open square
with a diagonal removed is not a domain since this set is not connected. (Why?)
The complement of a set S in the complex plane is the set of all points of the complex
plane that do not belong to S. A set S is called closed if its complement is open. For example,
the points on and inside the unit circle form a closed set (“closed unit disk”) since its
complement |z ƒ Ͼ 1 is open.
A boundary point of a set S is a point every neighborhood of which contains both points
that belong to S and points that do not belong to S. For example, the boundary points of
an annulus are the points on the two bounding circles. Clearly, if a set S is open, then no
boundary point belongs to S; if S is closed, then every boundary point belongs to S. The
set of all boundary points of a set S is called the boundary of S.
A region is a set consisting of a domain plus, perhaps, some or all of its boundary points.
WARNING! “Domain” is the modern term for an open connected set. Nevertheless, some
authors still call a domain a “region” and others make no distinction between the two terms.

Complex Function
Complex analysis is concerned with complex functions that are differentiable in some
domain. Hence we should first say what we mean by a complex function and then define
the concepts of limit and derivative in complex. This discussion will be similar to that in
calculus. Nevertheless it needs great attention because it will show interesting basic
differences between real and complex calculus.
Recall from calculus that a real function f defined on a set S of real numbers (usually an
interval) is a rule that assigns to every x in S a real number f(x), called the value of f at x.
Now in complex, S is a set of complex numbers. And a function f defined on S is a rule
that assigns to every z in S a complex number w, called the value of f at z. We write
w ϭ f(z).
Here z varies in S and is called a complex variable. The set S is called the domain of
definition of f or, briefly, the domain of f. (In most cases S will be open and connected,
thus a domain as defined just before.)
Example: w ϭ f(z) ϭ z 2 ϩ 3z is a complex function defined for all z; that is, its domain
S is the whole complex plane.
The set of all values of a function f is called the range of f.

c13.qxd

10/30/10

2:14 PM

Page 621

SEC. 13.3 Derivative. Analytic Function

621

w is complex, and we write w ϭ u ϩ iv, where u and v are the real and imaginary
parts, respectively. Now w depends on z ϭ x ϩ iy. Hence u becomes a real function of x
and y, and so does v. We may thus write
w ϭ f (z) ϭ u(x, y) ϩ iv(x, y).
This shows that a complex function f (z) is equivalent to a pair of real functions u(x, y)
and v(x, y), each depending on the two real variables x and y.
EXAMPLE 1

Function of a Complex Variable
Let w ϭ f (z) ϭ z 2 ϩ 3z. Find u and v and calculate the value of f at z ϭ 1 ϩ 3i.

Solution. u ϭ Re f (z) ϭ x 2 Ϫ y 2 ϩ 3x and v ϭ 2xy ϩ 3y. Also,
f (1 ϩ 3i) ϭ (1 ϩ 3i)2 ϩ 3(1 ϩ 3i) ϭ 1 Ϫ 9 ϩ 6i ϩ 3 ϩ 9i ϭ Ϫ5 ϩ 15i.
This shows that u(1, 3) ϭ Ϫ5 and v(1, 3) ϭ 15. Check this by using the expressions for u and v.

EXAMPLE 2

᭿

Function of a Complex Variable
Let w ϭ f (z) ϭ 2iz ϩ 6z . Find u and v and the value of f at z ϭ 12 ϩ 4i.

Solution. f (z) ϭ 2i(x ϩ iy) ϩ 6(x Ϫ iy) gives u(x, y) ϭ 6x Ϫ 2y and v(x, y) ϭ 2x Ϫ 6y. Also,
f (12 ϩ 4i) ϭ 2i(12 ϩ 4i) ϩ 6(12 Ϫ 4i) ϭ i Ϫ 8 ϩ 3 Ϫ 24i ϭ Ϫ5 Ϫ 23i.

᭿

Check this as in Example 1.

Remarks on Notation and Terminology
1. Strictly speaking, f(z) denotes the value of f at z, but it is a convenient abuse of
language to talk about the function f (z) (instead of the function f ), thereby exhibiting the
notation for the independent variable.
2. We assume all functions to be single-valued relations, as usual: to each z in S there
corresponds but one value w ϭ f (z) (but, of course, several z may give the same value
w ϭ f (z), just as in calculus). Accordingly, we shall not use the term “multivalued
function” (used in some books on complex analysis) for a multivalued relation, in which
to a z there corresponds more than one w.

Limit, Continuity
A function f (z) is said to have the limit l as z approaches a point z0, written
(1)

lim f (z) ϭ l,

z : z0

if f is defined in a neighborhood of z 0 (except perhaps at z0 itself) and if the values of
f are “close” to l for all z “close” to z 0; in precise terms, if for every positive real P we can
find a positive real d such that for all z z 0 in the disk ƒ z Ϫ z 0 ƒ Ͻ d (Fig. 333) we have
(2)

ƒ f (z) Ϫ l ƒ Ͻ P;

geometrically, if for every z z 0 in that d-disk the value of f lies in the disk (2).
Formally, this definition is similar to that in calculus, but there is a big difference.
Whereas in the real case, x can approach an x0 only along the real line, here, by definition,

c13.qxd

10/30/10

2:14 PM

622

Page 622

CHAP. 13 Complex Numbers and Functions. Complex Differentiation

z may approach z 0 from any direction in the complex plane. This will be quite essential
in what follows.
If a limit exists, it is unique. (See Team Project 24.)
A function f (z) is said to be continuous at z ϭ z 0 if f (z 0) is defined and
lim f (z) ϭ f (z 0).

(3)

z:z0

Note that by definition of a limit this implies that f (z) is defined in some neighborhood
of z 0.
f(z) is said to be continuous in a domain if it is continuous at each point of this domain.
v

y
z

δ

z0

Œ

l

f(z)

x

u

Fig. 333. Limit

Derivative
The derivative of a complex function f at a point z 0 is written f r(z 0) and is defined by

f r(z 0) ϭ lim

(4)

f (z 0 ϩ ¢z) Ϫ f(z 0)

¢z :0

¢z

provided this limit exists. Then f is said to be differentiable at z 0. If we write ¢z ϭ z Ϫ z 0,
we have z ϭ z 0 ϩ ¢z and (4) takes the form

f r(z 0) ϭ lim

(4 r )

z : z0

f(z) Ϫ f (z 0)
z Ϫ z0 .

Now comes an important point. Remember that, by the definition of limit, f (z) is defined
in a neighborhood of z 0 and z in (4 r ) may approach z 0 from any direction in the complex
plane. Hence differentiability at z0 means that, along whatever path z approaches z 0, the
quotient in (4 r ) always approaches a certain value and all these values are equal. This is
important and should be kept in mind.
EXAMPLE 3

Differentiability. Derivative
The function f (z) ϭ z 2 is differentiable for all z and has the derivative f r(z) ϭ 2z because

f r(z) ϭ lim

¢z:0

(z ϩ ¢z)2 Ϫ z 2
¢z

ϭ lim

¢z:0

z 2 ϩ 2z ¢z ϩ (¢z)2 Ϫ z 2
¢z

ϭ lim (2z ϩ ¢z) ϭ 2z.
¢z: 0

᭿

c13.qxd

10/30/10

2:14 PM

Page 623

SEC. 13.3 Derivative. Analytic Function

623

The differentiation rules are the same as in real calculus, since their proofs are literally
the same. Thus for any differentiable functions f and g and constant c we have
(cf ) r ϭ cf r ,

( f ϩ g) r ϭ f r ϩ gr ,

( fg) r ϭ f rg ϩ fg r ,

f r
f rg Ϫ fgr
agb ϭ
g2

as well as the chain rule and the power rule (z n) r ϭ nz n؊1 (n integer).
Also, if f(z) is differentiable at z0, it is continuous at z 0. (See Team Project 24.)
EXAMPLE 4

z not Differentiable
It may come as a surprise that there are many complex functions that do not have a derivative at any point. For
instance, f (z) ϭ z ϭ x Ϫ iy is such a function. To see this, we write ¢z ϭ ¢x ϩ i ¢y and obtain
f (z ϩ ¢z) Ϫ f (z)

(5)

ϭ

(z ϩ ¢z) Ϫ z

¢z

¢z

ϭ

¢z

ϭ

¢z

¢x Ϫ i ¢y
¢x ϩ i ¢y

.

If ¢y ϭ 0, this is ϩ1. If ¢x ϭ 0, this is Ϫ1. Thus (5) approaches ϩ1 along path I in Fig. 334 but Ϫ1 along
path II. Hence, by definition, the limit of (5) as ¢z : 0 does not exist at any z.
᭿
y
ΙΙ

z

z + Δz

Ι
x

Fig. 334. Paths in (5)

Surprising as Example 4 may be, it merely illustrates that differentiability of a complex
function is a rather severe requirement.
The idea of proof (approach of z from different directions) is basic and will be used
again as the crucial argument in the next section.

Analytic Functions
Complex analysis is concerned with the theory and application of “analytic functions,”
that is, functions that are differentiable in some domain, so that we can do “calculus in
complex.” The definition is as follows.
DEFINITION

Analyticity

A function f(z) is said to be analytic in a domain D if f(z) is defined and differentiable
at all points of D. The function f (z) is said to be analytic at a point z ϭ z 0 in D if
f(z) is analytic in a neighborhood of z 0.
Also, by an analytic function we mean a function that is analytic in some domain.
Hence analyticity of f (z) at z 0 means that f(z) has a derivative at every point in some
neighborhood of z 0 (including z 0 itself since, by definition, z 0 is a point of all its
neighborhoods). This concept is motivated by the fact that it is of no practical interest
if a function is differentiable merely at a single point z 0 but not throughout some
neighborhood of z 0. Team Project 24 gives an example.
A more modern term for analytic in D is holomorphic in D.

c13.qxd

10/30/10

2:14 PM

624

Page 624

CHAP. 13 Complex Numbers and Functions. Complex Differentiation

EXAMPLE 5

Polynomials, Rational Functions
The nonnegative integer powers 1, z, z 2, Á are analytic in the entire complex plane, and so are polynomials,
that is, functions of the form
f (z) ϭ c0 ϩ c1z ϩ c2z 2 ϩ Á ϩ cnz n
where c0, Á , cn are complex constants.
The quotient of two polynomials g(z) and h(z),
f (z) ϭ

g(z)
h(z)

,

is called a rational function. This f is analytic except at the points where h(z) ϭ 0; here we assume that common
factors of g and h have been canceled.
Many further analytic functions will be considered in the next sections and chapters.
᭿

The concepts discussed in this section extend familiar concepts of calculus. Most
important is the concept of an analytic function, the exclusive concern of complex
analysis. Although many simple functions are not analytic, the large variety of remaining
functions will yield a most beautiful branch of mathematics that is very useful in
engineering and physics.

PROBLEM SET 13.3
1–8
REGIONS OF PRACTICAL INTEREST
Determine and sketch or graph the sets in the complex plane
given by
1. ƒ z ϩ 1 Ϫ 5i ƒ Ϲ 32
2. 0 Ͻ ƒ z ƒ Ͻ 1
3. p Ͻ ƒ z Ϫ 4 ϩ 2i ƒ Ͻ 3p
4. Ϫp Ͻ Im z Ͻ p
5. ƒ arg z ƒ Ͻ 14p
6. Re (1>z) Ͻ 1
7. Re z м Ϫ1
8. ƒ z ϩ i ƒ м ƒ z Ϫ i ƒ
9. WRITING PROJECT. Sets in the Complex Plane.
Write a report by formulating the corresponding
portions of the text in your own words and illustrating
them with examples of your own.

COMPLEX FUNCTIONS AND THEIR DERIVATIVES
10–12
Function Values. Find Re f, and Im f and their
values at the given point z.
10.
11.
12.
13.

f (z) ϭ 5z 2 Ϫ 12z ϩ 3 ϩ 2i at 4 Ϫ 3i
f (z) ϭ 1>(1 Ϫ z) at 1 Ϫ i
f (z) ϭ (z Ϫ 2)>(z ϩ 2) at 8i
CAS PROJECT. Graphing Functions. Find and graph
Re f, Im f, and ƒ f ƒ as surfaces over the z-plane. Also
graph the two families of curves Re f (z) ϭ const and

Im f (z) ϭ const in the same figure, and the curves
ƒ f (z) ƒ ϭ const in another figure, where (a) f (z) ϭ z 2 ,
(b) f (z) ϭ 1>z, (c) f (z) ϭ z 4.
14–17
Continuity. Find out, and give reason, whether
f (z) is continuous at z ϭ 0 if f (0) ϭ 0 and for z 0 the
function f is equal to:
14. (Re z 2)> ƒ z ƒ
16. (Im z 2)> ƒ z ƒ 2
18–23
of
18.
20.
21.
22.
24.

15. ƒ z ƒ 2 Im (1>z)
17. (Re z)>(1 Ϫ ƒ z ƒ )

Differentiation. Find the value of the derivative

19. (z Ϫ 4i)8 at ϭ 3 ϩ 4i
(z Ϫ i)>(z ϩ i) at i
(1.5z ϩ 2i)>(3iz Ϫ 4) at any z. Explain the result.
i(1 Ϫ z)n at 0
23. z 3>(z ϩ i)3 at i
(iz 3 ϩ 3z 2)3 at 2i
TEAM PROJECT. Limit, Continuity, Derivative
(a) Limit. Prove that (1) is equivalent to the pair of
relations
lim Re f (z) ϭ Re l,

z: z0

lim Im f (z) ϭ Im l.

z:z0

(b) Limit. If lim f (x) exists, show that this limit is
z:z0
unique.
(c) Continuity. If z 1, z 2, Á are complex numbers for
which lim zn ϭ a, and if f(z) is continuous at z ϭ a,
n:ؕ
show that lim f (z n) ϭ f (a).
n:ؕ

c13.qxd

10/30/10

2:14 PM

Page 625

SEC. 13.4 Cauchy–Riemann Equations. Laplace’s Equation
(d) Continuity. If f (z) is differentiable at z 0, show that
f(z) is continuous at z 0.
(e) Differentiability. Show that f (z) ϭ Re z ϭ x is not
differentiable at any z. Can you find other such functions?
(f) Differentiability. Show that f (z) ϭ ƒ z ƒ 2 is differentiable only at z ϭ 0; hence it is nowhere analytic.

13.4

625
25. WRITING PROJECT. Comparison with Calculus.
Summarize the second part of this section beginning with
Complex Function, and indicate what is conceptually
analogous to calculus and what is not.

Cauchy–Riemann Equations.
Laplace’s Equation
As we saw in the last section, to do complex analysis (i.e., “calculus in the complex”) on
any complex function, we require that function to be analytic on some domain that is
differentiable in that domain.
The Cauchy–Riemann equations are the most important equations in this chapter
and one of the pillars on which complex analysis rests. They provide a criterion (a test)
for the analyticity of a complex function
w ϭ f (z) ϭ u(x, y) ϩ iv(x, y).
Roughly, f is analytic in a domain D if and only if the first partial derivatives of u and v
satisfy the two Cauchy–Riemann equations4
(1)

u x ϭ vy,

u y ϭ Ϫvx

everywhere in D; here u x ϭ 0u>0x and u y ϭ 0u>0y (and similarly for v) are the usual
notations for partial derivatives. The precise formulation of this statement is given in
Theorems 1 and 2.
Example: f(z) ϭ z 2 ϭ x 2 Ϫ y 2 ϩ 2ixy is analytic for all z (see Example 3 in Sec. 13.3),
and u ϭ x 2 Ϫ y 2 and v ϭ 2xy satisfy (1), namely, u x ϭ 2x ϭ vy as well as u y ϭ
Ϫ2y ϭ Ϫvx. More examples will follow.
THEOREM 1

Cauchy–Riemann Equations

Let f(z) ϭ u(x, y) ϩ iv(x, y) be defined and continuous in some neighborhood of a
point z ϭ x ϩ iy and differentiable at z itself. Then, at that point, the first-order
partial derivatives of u and v exist and satisfy the Cauchy–Riemann equations (1).
Hence, if f(z) is analytic in a domain D, those partial derivatives exist and satisfy
(1) at all points of D.

4
The French mathematician AUGUSTIN-LOUIS CAUCHY (see Sec. 2.5) and the German mathematicians
BERNHARD RIEMANN (1826–1866) and KARL WEIERSTRASS (1815–1897; see also Sec. 15.5) are the
founders of complex analysis. Riemann received his Ph.D. (in 1851) under Gauss (Sec. 5.4) at Göttingen, where
he also taught until he died, when he was only 39 years old. He introduced the concept of the integral as it is
used in basic calculus courses, and made important contributions to differential equations, number theory, and
mathematical physics. He also developed the so-called Riemannian geometry, which is the mathematical
foundation of Einstein’s theory of relativity; see Ref. [GenRef9] in App. 1.

c13.qxd

10/30/10

2:14 PM

626

Page 626

CHAP. 13 Complex Numbers and Functions. Complex Differentiation

PROOF

By assumption, the derivative f r(z) at z exists. It is given by
f (z ϩ ¢z) Ϫ f (z)

f r(z) ϭ lim

(2)

¢z:0

.

¢z

The idea of the proof is very simple. By the definition of a limit in complex (Sec. 13.3),
we can let ¢z approach zero along any path in a neighborhood of z. Thus we may choose
the two paths I and II in Fig. 335 and equate the results. By comparing the real parts we
shall obtain the first Cauchy–Riemann equation and by comparing the imaginary parts the
second. The technical details are as follows.
We write ¢z ϭ ¢x ϩ i ¢y. Then z ϩ ¢z ϭ x ϩ ¢x ϩ i(y ϩ ¢y), and in terms of u
and v the derivative in (2) becomes
(3)

f r(z) ϭ lim

[u(x ϩ ¢x, y ϩ ¢y) ϩ iv(x ϩ ¢x, y ϩ ¢y)] Ϫ [u(x, y) ϩ iv(x, y)]
¢x ϩ i ¢y

¢z: 0

.

We first choose path I in Fig. 335. Thus we let ¢y : 0 first and then ¢x : 0. After ¢y
is zero, ¢z ϭ ¢x. Then (3) becomes, if we first write the two u-terms and then the two
v-terms,
f r(z) ϭ lim

¢x: 0

u(x ϩ ¢x, y) Ϫ u(x, y)

ϩ i lim

v(x ϩ ¢x, y) Ϫ v(x, y)

¢x: 0

¢x

.

¢x

y
ΙΙ

z

z + Δz

Ι
x

Fig. 335. Paths in (2)

Since f r(z) exists, the two real limits on the right exist. By definition, they are the partial
derivatives of u and v with respect to x. Hence the derivative f r(z) of f(z) can be written
f r(z) ϭ u x ϩ ivx.

(4)

Similarly, if we choose path II in Fig. 335, we let ¢x : 0 first and then ¢y : 0. After
¢x is zero, ¢z ϭ i ¢y, so that from (3) we now obtain
f r(z) ϭ lim

¢y: 0

u(x, y ϩ ¢y) Ϫ u(x, y)
i ¢y

ϩ i lim

¢y:0

v(x, y ϩ ¢y) Ϫ v(x, y)
i ¢y

.

Since f r(z) exists, the limits on the right exist and give the partial derivatives of u and v
with respect to y; noting that 1>i ϭ Ϫi, we thus obtain
(5)

f r(z) ϭ Ϫiu y ϩ vy.

The existence of the derivative f r(z) thus implies the existence of the four partial derivatives
in (4) and (5). By equating the real parts u x and vy in (4) and (5) we obtain the first

c13.qxd

10/30/10

2:14 PM

Page 627

SEC. 13.4 Cauchy–Riemann Equations. Laplace’s Equation

627

Cauchy–Riemann equation (1). Equating the imaginary parts gives the other. This proves
the first statement of the theorem and implies the second because of the definition of
᭿
analyticity.
Formulas (4) and (5) are also quite practical for calculating derivatives f r(z), as we shall see.
EXAMPLE 1

Cauchy–Riemann Equations
f (z) ϭ z 2 is analytic for all z. It follows that the Cauchy–Riemann equations must be satisfied (as we have
verified above).
For f (z) ϭ z ϭ x Ϫ iy we have u ϭ x, v ϭ Ϫy and see that the second Cauchy–Riemann equation is satisfied,
u y ϭ Ϫvx ϭ 0, but the first is not: u x ϭ 1 vy ϭ Ϫ1. We conclude that f (z) ϭ z is not analytic, confirming
᭿
Example 4 of Sec. 13.3. Note the savings in calculation!

The Cauchy–Riemann equations are fundamental because they are not only necessary but
also sufficient for a function to be analytic. More precisely, the following theorem holds.
THEOREM 2

Cauchy–Riemann Equations

If two real-valued continuous functions u(x, y) and v(x, y) of two real variables x
and y have continuous first partial derivatives that satisfy the Cauchy–Riemann
equations in some domain D, then the complex function f (z) ϭ u(x, y) ϩ iv(x, y) is
analytic in D.
The proof is more involved than that of Theorem 1 and we leave it optional (see App. 4).
Theorems 1 and 2 are of great practical importance, since, by using the Cauchy–Riemann
equations, we can now easily find out whether or not a given complex function is analytic.
EXAMPLE 2

Cauchy–Riemann Equations. Exponential Function
Is f (z) ϭ u(x, y) ϩ iv(x, y) ϭ ex(cos y ϩ i sin y) analytic?

Solution. We have u ϭ ex cos y, v ϭ ex sin y and by differentiation
u x ϭ ex cos y,

vy ϭ ex cos y

u y ϭ Ϫex sin y,

vx ϭ ex sin y.

We see that the Cauchy–Riemann equations are satisfied and conclude that f (z) is analytic for all z. ( f (z) will
᭿
be the complex analog of ex known from calculus.)

EXAMPLE 3

An Analytic Function of Constant Absolute Value Is Constant
The Cauchy–Riemann equations also help in deriving general properties of analytic functions.
For instance, show that if f (z) is analytic in a domain D and ƒ f (z) ƒ ϭ k ϭ const in D, then f (z) ϭ const in
D. (We shall make crucial use of this in Sec. 18.6 in the proof of Theorem 3.)

Solution. By assumption, ƒ f ƒ 2 ϭ ƒ u ϩ iv ƒ 2 ϭ u 2 ϩ v2 ϭ k 2. By differentiation,
uu x ϩ vvx ϭ 0,
uu y ϩ vvy ϭ 0.
Now use vx ϭ Ϫu y in the first equation and vy ϭ u x in the second, to get
(a)

uu x Ϫ vu y ϭ 0,

(b)

uu y Ϫ vu x ϭ 0.

(6)

c13.qxd

10/30/10

2:14 PM

628

Page 628

CHAP. 13 Complex Numbers and Functions. Complex Differentiation
To get rid of u y, multiply (6a) by u and (6b) by v and add. Similarly, to eliminate u x, multiply (6a) by Ϫv and
(6b) by u and add. This yields
(u 2 ϩ v2)u x ϭ 0 ,
(u 2 ϩ v2)u y ϭ 0.
If k 2 ϭ u 2 ϩ v2 ϭ 0, then u ϭ v ϭ 0; hence f ϭ 0. If k 2 ϭ u 2 ϩ v2 0, then u x ϭ u y ϭ 0. Hence, by the
Cauchy–Riemann equations, also u x ϭ vy ϭ 0. Together this implies u ϭ const and v ϭ const; hence
f ϭ const.
᭿

We mention that, if we use the polar form z ϭ r(cos u ϩ i sin u) and set f (z) ϭ u(r, u) ϩ
iv(r, u), then the Cauchy–Riemann equations are (Prob. 1)
1
u r ϭ r vu,
1
vr ϭ Ϫ r u u

(7)

(r Ͼ 0).

Laplace’s Equation. Harmonic Functions
The great importance of complex analysis in engineering mathematics results mainly from
the fact that both the real part and the imaginary part of an analytic function satisfy Laplace’s
equation, the most important PDE of physics. It occurs in gravitation, electrostatics, fluid
flow, heat conduction, and other applications (see Chaps. 12 and 18).
THEOREM 3

Laplace’s Equation

If f (z) ϭ u(x, y) ϩ iv(x, y) is analytic in a domain D, then both u and v satisfy
Laplace’s equation
(8)

ٌ2u ϭ u xx ϩ u yy ϭ 0

(ٌ2 read “nabla squared”) and
(9)

ٌ2v ϭ vxx ϩ vyy ϭ 0,

in D and have continuous second partial derivatives in D.

PROOF

Differentiating u x ϭ vy with respect to x and u y ϭ Ϫvx with respect to y, we have
(10)

u xx ϭ vyx ,

u yy ϭ Ϫvxy.

Now the derivative of an analytic function is itself analytic, as we shall prove later (in
Sec. 14.4). This implies that u and v have continuous partial derivatives of all orders; in
particular, the mixed second derivatives are equal: vyx ϭ vxy. By adding (10) we thus
obtain (8). Similarly, (9) is obtained by differentiating u x ϭ vy with respect to y and
᭿
u y ϭ Ϫvx with respect to x and subtracting, using u xy ϭ u yx.
Solutions of Laplace’s equation having continuous second-order partial derivatives are called
harmonic functions and their theory is called potential theory (see also Sec. 12.11). Hence
the real and imaginary parts of an analytic function are harmonic functions.

c13.qxd

10/30/10

2:14 PM

Page 629

SEC. 13.4 Cauchy–Riemann Equations. Laplace’s Equation

629

If two harmonic functions u and v satisfy the Cauchy–Riemann equations in a domain
D, they are the real and imaginary parts of an analytic function f in D. Then v is said to
be a harmonic conjugate function of u in D. (Of course, this has absolutely nothing to
do with the use of “conjugate” for z.)
EXAMPLE 4

How to Find a Harmonic Conjugate Function by the Cauchy–Riemann Equations
Verify that u ϭ x 2 Ϫ y 2 Ϫ y is harmonic in the whole complex plane and find a harmonic conjugate function
v of u.

Solution. ٌ2u ϭ 0 by direct calculation. Now u x ϭ 2x and u y ϭ Ϫ2y Ϫ 1. Hence because of the Cauchy–
Riemann equations a conjugate v of u must satisfy
vy ϭ u x ϭ 2x,

vx ϭ Ϫu y ϭ 2y ϩ 1.

Integrating the first equation with respect to y and differentiating the result with respect to x, we obtain
v ϭ 2xy ϩ h(x),

vx ϭ 2y ϩ

dh
dx

.

A comparison with the second equation shows that dh>dx ϭ 1. This gives h(x) ϭ x ϩ c. Hence v ϭ 2xy ϩ x ϩ c
(c any real constant) is the most general harmonic conjugate of the given u. The corresponding analytic function is
f (z) ϭ u ϩ iv ϭ x 2 Ϫ y 2 Ϫ y ϩ i(2xy ϩ x ϩ c) ϭ z 2 ϩ iz ϩ ic.

᭿

Example 4 illustrates that a conjugate of a given harmonic function is uniquely determined
up to an arbitrary real additive constant.
The Cauchy–Riemann equations are the most important equations in this chapter. Their
relation to Laplace’s equation opens a wide range of engineering and physical applications,
as shown in Chap. 18.

PROBLEM SET 13.4
1. Cauchy–Riemann equations in polar form. Derive (7)
from (1).
2–11
CAUCHY–RIEMANN EQUATIONS
Are the following functions analytic? Use (1) or (7).
2. f (z) ϭ izz
3. f (z) ϭ e؊2x (cos 2y Ϫ i sin 2y)
4. f (z) ϭ ex (cos y Ϫ i sin y)
5. f (z) ϭ Re (z 2) Ϫ i Im (z 2)
6. f (z) ϭ 1>(z Ϫ z 5)
7. f (z) ϭ i>z 8
8. f (z) ϭ Arg 2pz
9. f (z) ϭ 3p2>(z 3 ϩ 4p2z)
10. f (z) ϭ ln ƒ z ƒ ϩ i Arg z
11. f (z) ϭ cos x cosh y Ϫ i sin x sinh y
12–19
HARMONIC FUNCTIONS
Are the following functions harmonic? If your answer
is yes, find a corresponding analytic function f (z) ϭ
u(x, y) ϩ iv(x, y).
12. u ϭ x 2 ϩ y 2
13. u ϭ xy

14.
16.
18.
19.
20.

v ϭ xy
15. u ϭ x>(x 2 ϩ y 2)
17. v ϭ (2x ϩ 1)y
u ϭ sin x cosh y
u ϭ x 3 Ϫ 3xy 2
v ϭ ex sin 2y
Laplace’s equation. Give the details of the derivative
of (9).

21–24
Determine a and b so that the given function is
harmonic and find a harmonic conjugate.
u ϭ epx cos av
u ϭ cos ax cosh 2y
u ϭ ax 3 ϩ bxy
u ϭ cosh ax cos y
CAS PROJECT. Equipotential Lines. Write a
program for graphing equipotential lines u ϭ const of
a harmonic function u and of its conjugate v on the
same axes. Apply the program to (a) u ϭ x 2 Ϫ y 2,
v ϭ 2xy, (b) u ϭ x 3 Ϫ 3xy 2, v ϭ 3x 2y Ϫ y 3.
26. Apply the program in Prob. 25 to u ϭ ex cos y,
v ϭ ex sin y and to an example of your own.
21.
22.
23.
24.
25.

c13.qxd

10/30/10

2:14 PM

630

Page 630

CHAP. 13 Complex Numbers and Functions. Complex Differentiation
30. TEAM PROJECT. Conditions for f (z) ϭ const . Let
f (z) be analytic. Prove that each of the following
conditions is sufficient for f (z) ϭ const.
(a) Re f (z) ϭ const
(b) Im f (z) ϭ const
(c) f r(z) ϭ 0
(d) ƒ f (z) ƒ ϭ const (see Example 3)

27. Harmonic conjugate. Show that if u is harmonic and
v is a harmonic conjugate of u, then u is a harmonic
conjugate of Ϫv.
28. Illustrate Prob. 27 by an example.
29. Two further formulas for the derivative. Formulas (4),
(5), and (11) (below) are needed from time to time. Derive
(11)

13.5

f r(z) ϭ u x Ϫ iu y,

f r(z) ϭ vy ϩ ivx.

Exponential Function
In the remaining sections of this chapter we discuss the basic elementary complex
functions, the exponential function, trigonometric functions, logarithm, and so on. They
will be counterparts to the familiar functions of calculus, to which they reduce when z ϭ x
is real. They are indispensable throughout applications, and some of them have interesting
properties not shared by their real counterparts.
We begin with one of the most important analytic functions, the complex exponential
function
ez,

also written

exp z.

The definition of ez in terms of the real functions ex, cos y, and sin y is
(1)

ez ϭ ex(cos y ϩ i sin y).

This definition is motivated by the fact the ez extends the real exponential function ex of
calculus in a natural fashion. Namely:
(A) ez ϭ ex for real z ϭ x because cos y ϭ 1 and sin y ϭ 0 when y ϭ 0.
(B) ez is analytic for all z. (Proved in Example 2 of Sec. 13.4.)
(C) The derivative of ez is ez, that is,
(ez) r ϭ ez.

(2)
This follows from (4) in Sec. 13.4,

(ez) r ϭ (ex cos y)x ϩ i(ex sin y)x ϭ ex cos y ϩ iex sin y ϭ ez.
REMARK. This definition provides for a relatively simple discussion. We could define ez
by the familiar series 1 ϩ x ϩ x 2>2! ϩ x 3>3! ϩ Á with x replaced by z, but we would
then have to discuss complex series at this very early stage. (We will show the connection
in Sec. 15.4.)
Further Properties. A function f (z) that is analytic for all z is called an entire function.
Thus, ez is entire. Just as in calculus the functional relation
(3)

ez1ϩz2 ϭ ez1ez2

c13.qxd

10/30/10

2:14 PM

Page 631

SEC. 13.5 Exponential Function

631

holds for any z 1 ϭ x 1 ϩ iy1 and z 2 ϭ x 2 ϩ iy2. Indeed, by (1),
ez1ez2 ϭ ex1(cos y1 ϩ i sin y1) ex2(cos y2 ϩ i sin y2).
Since ex1ex2 ϭ ex1ϩx2 for these real functions, by an application of the addition formulas
for the cosine and sine functions (similar to that in Sec. 13.2) we see that
ez1ez2 ϭ ex1 ϩx2[cos ( y1 ϩ y2) ϩ i sin ( y1 ϩ y2)] ϭ ez1ϩz2
as asserted. An interesting special case of (3) is z 1 ϭ x, z 2 ϭ iy; then
ez ϭ exeiy.

(4)

Furthermore, for z ϭ iy we have from (1) the so-called Euler formula
eiy ϭ cos y ϩ i sin y.

(5)

Hence the polar form of a complex number, z ϭ r (cos u ϩ i sin u), may now be written
z ϭ reiu.

(6)
From (5) we obtain

e2pi ϭ 1

(7)

as well as the important formulas (verify!)
(8)

epi>2 ϭ i,

e؊pi>2 ϭ Ϫi,

epi ϭ Ϫ1,

e؊pi ϭ Ϫ1.

Another consequence of (5) is
ƒ eiy ƒ ϭ ƒ cos y ϩ i sin y ƒ ϭ 2cos2 y ϩ sin2 y ϭ 1.

(9)

That is, for pure imaginary exponents, the exponential function has absolute value 1, a
result you should remember. From (9) and (1),
(10)

ƒ ez ƒ ϭ ex.

Hence

arg ez ϭ y Ϯ 2np

(n ϭ 0, 1, 2, Á ),

since ƒ ez ƒ ϭ ex shows that (1) is actually ez in polar form.
From ƒ ez ƒ ϭ ex 0 in (10) we see that
(11)

ex

0

for all z.

So here we have an entire function that never vanishes, in contrast to (nonconstant)
polynomials, which are also entire (Example 5 in Sec. 13.3) but always have a zero, as
is proved in algebra.

c13.qxd

10/30/10

2:14 PM

632

Page 632

CHAP. 13 Complex Numbers and Functions. Complex Differentiation

Periodicity of ex with period 2␲ i,
ezϩ2pi ϭ ez

(12)

for all z

is a basic property that follows from (1) and the periodicity of cos y and sin y. Hence all
the values that w ϭ ez can assume are already assumed in the horizontal strip of width 2p
Ϫp Ͻ y Ϲ p

(13)

(Fig. 336).

This infinite strip is called a fundamental region of ez.
EXAMPLE 1

Function Values. Solution of Equations
Computation of values from (1) provides no problem. For instance,
e1.4؊0.6i ϭ e1.4(cos 0.6 Ϫ i sin 0.6) ϭ 4.055(0.8253 Ϫ 0.5646i) ϭ 3.347 Ϫ 2.289i
ƒ e1.4؊1.6i ƒ ϭ e1.4 ϭ 4.055,

Arg e1.4–0.6i ϭ Ϫ0.6.

To illustrate (3), take the product of
e2ϩi ϭ e2(cos 1 ϩ i sin 1)

e4؊i ϭ e4(cos 1 Ϫ i sin 1)

and

and verify that it equals e2e4(cos2 1 ϩ sin2 1) ϭ e6 ϭ e(2ϩi)ϩ(4؊i).
To solve the equation ez ϭ 3 ϩ 4i, note first that ƒ ez ƒ ϭ ex ϭ 5, x ϭ ln 5 ϭ 1.609 is the real part of all
solutions. Now, since ex ϭ 5,
ex cos y ϭ 3,

ex sin y ϭ 4,

cos y ϭ 0.6,

sin y ϭ 0.8,

y ϭ 0.927.

Ans. z ϭ 1.609 ϩ 0.927i Ϯ 2npi (n ϭ 0, 1, 2, Á ). These are infinitely many solutions (due to the periodicity
᭿
of ez). They lie on the vertical line x ϭ 1.609 at a distance 2p from their neighbors.

To summarize: many properties of ez ϭ exp z parallel those of ex; an exception is the
periodicity of ez with 2pi, which suggested the concept of a fundamental region. Keep
in mind that ez is an entire function. (Do you still remember what that means?)
y

π

x

–π

Fig. 336. Fundamental region of the
exponential function e z in the z-plane

PROBLEM SET 13.5
1. ez is entire. Prove this.

8–13

2–7
Function Values. Find ez in the form u ϩ iv
z
and ƒ e ƒ if z equals
2. 3 ϩ 4i
4. 0.6 Ϫ 1.8i
6. 11pi> 2

3. 2pi(1 ϩ i)
5. 2 ϩ 3pi
7. 22 ϩ 12pi

8.

Polar Form. Write in exponential form (6):
9. 4 ϩ 3i

1z
n

10. 1i, 1Ϫi
12. 1>(1 Ϫ z)
14–17
14. e

؊pz

11. Ϫ6.3
13. 1 ϩ i

Real and Imaginary Parts. Find Re and Im of
15. exp (z 2)

c13.qxd

10/30/10

2:14 PM

Page 633

SEC. 13.6 Trigonometric and Hyperbolic Functions. Euler’s Formula
16. e1>z
17. exp (z 3)
18. TEAM PROJECT. Further Properties of the Exponential Function. (a) Analyticity. Show that ez is
entire. What about e1>z? ez? ex(cos ky ϩ i sin ky)? (Use
the Cauchy–Riemann equations.)
(b) Special values. Find all z such that (i) ez is real,
(ii) ƒ e؊z ƒ Ͻ 1, (iii) ez ϭ ez.
(c) Harmonic function. Show that u ϭ exy cos
(x 2>2 Ϫ y 2>2) is harmonic and find a conjugate.

13.6

633

(d) Uniqueness. It is interesting that f (z) ϭ ez is
uniquely determined by the two properties f (x ϩ i0) ϭ
ex and f r(z) ϭ f (z), where f is assumed to be entire.
Prove this using the Cauchy–Riemann equations.
19–22
Equations. Find all solutions and graph some
of them in the complex plane.
19. ez ϭ 1
21. ez ϭ 0

20. ez ϭ 4 ϩ 3i
22. ez ϭ Ϫ2

Trigonometric and Hyperbolic Functions.
Euler’s Formula
Just as we extended the real ex to the complex ez in Sec. 13.5, we now want to extend
the familiar real trigonometric functions to complex trigonometric functions. We can do
this by the use of the Euler formulas (Sec. 13.5)
eix ϭ cos x ϩ i sin x,

e؊ix ϭ cos x Ϫ i sin x.

By addition and subtraction we obtain for the real cosine and sine
cos x ϭ 12 (eix ϩ e؊ix),

sin x ϭ

1 ix
(e Ϫ e؊ix).
2i

This suggests the following definitions for complex values z ϭ x ϩ iy:
(1)

cos z ϭ 12 (eiz ϩ e؊iz),

sin z ϭ

1 iz
(e Ϫ e؊iz).
2i

It is quite remarkable that here in complex, functions come together that are unrelated in
real. This is not an isolated incident but is typical of the general situation and shows the
advantage of working in complex.
Furthermore, as in calculus we define
(2)

sin z
tan z ϭ cos z ,

cos z
cot z ϭ sin z

1
sec z ϭ cos z ,

1
csc z ϭ sin z .

and
(3)

Since ez is entire, cos z and sin z are entire functions. tan z and sec z are not entire; they
are analytic except at the points where cos z is zero; and cot z and csc z are analytic except

c13.qxd

10/30/10

2:14 PM

634

Page 634

CHAP. 13 Complex Numbers and Functions. Complex Differentiation

where sin z is zero. Formulas for the derivatives follow readily from (ez) r ϭ ez and (1)–(3);
as in calculus,
(4)

(cos z) r ϭ Ϫsin z,

(sin z) r ϭ cos z,

(tan z) r ϭ sec2 z,

etc. Equation (1) also shows that Euler’s formula is valid in complex:
eiz ϭ cos z ϩ i sin z

(5)

for all z.

The real and imaginary parts of cos z and sin z are needed in computing values, and they
also help in displaying properties of our functions. We illustrate this with a typical example.
EXAMPLE 1

Real and Imaginary Parts. Absolute Value. Periodicity
Show that
(a)

cos z ϭ cos x cosh y Ϫ i sin x sinh y

(b)

sin z ϭ sin x cosh y ϩ i cos x sinh y

(6)
and
(7)

(a)

ƒ cos z ƒ 2 ϭ cos2 x ϩ sinh2 y

(b)

ƒ sin z ƒ 2 ϭ sin2 x ϩ sinh2 y

and give some applications of these formulas.

Solution. From (1),
cos z ϭ 12 (ei(xϩiy) ϩ e؊i(xϩiy))
ϭ 12 e؊y(cos x ϩ i sin x) ϩ 12 ey(cos x Ϫ i sin x)
ϭ 12 (ey ϩ e؊y) cos x Ϫ 12 i(ey Ϫ e؊y) sin x.
This yields (6a) since, as is known from calculus,
(8)

cosh y ϭ 12 (ey ϩ e؊y),

sinh y ϭ 12 (ey Ϫ e؊y);

(6b) is obtained similarly. From (6a) and cosh2 y ϭ 1 ϩ sinh2 y we obtain
ƒ cos z ƒ 2 ϭ (cos2 x) (1 ϩ sinh2 y) ϩ sin2 x sinh2 y.
Since sin2 x ϩ cos2 x ϭ 1, this gives (7a), and (7b) is obtained similarly.
For instance, cos (2 ϩ 3i) ϭ cos 2 cosh 3 Ϫ i sin 2 sinh 3 ϭ Ϫ4.190 Ϫ 9.109i.
From (6) we see that sin z and cos z are periodic with period 2␲, just as in real. Periodicity of tan z and cot z
with period p now follows.
Formula (7) points to an essential difference between the real and the complex cosine and sine; whereas
ƒ cos x ƒ Ϲ 1 and ƒ sin x ƒ Ϲ 1, the complex cosine and sine functions are no longer bounded but approach infinity
᭿
in absolute value as y : ϱ, since then sinh y : ϱ in (7).

EXAMPLE 2

Solutions of Equations. Zeros of cos z and sin z
Solve (a) cos z ϭ 5 (which has no real solution!), (b) cos z ϭ 0, (c) sin z ϭ 0.

Solution. (a) e2iz Ϫ 10eiz ϩ 1 ϭ 0 from (1) by multiplication by eiz. This is a quadratic equation in eiz,
with solutions (rounded off to 3 decimals)
eiz ϭ e؊yϩix ϭ 5 Ϯ 125 Ϫ 1 ϭ 9.899 and

0.101.

Thus e؊y ϭ 9.899 or 0.101, eix ϭ 1, y ϭ Ϯ2.292, x ϭ 2np. Ans. z ϭ Ϯ2np Ϯ 2.292i (n ϭ 0, 1, 2, Á ).
Can you obtain this from (6a)?

c13.qxd

10/30/10

2:14 PM

Page 635

SEC. 13.6 Trigonometric and Hyperbolic Functions. Euler’s Formula

635

(b) cos x ϭ 0, sinh y ϭ 0 by (7a), y ϭ 0. Ans. z ϭ Ϯ12 (2n ϩ 1)p (n ϭ 0, 1, 2, Á ).
(c) sin x ϭ 0, sinh y ϭ 0 by (7b), Ans. z ϭ Ϯnp (n ϭ 0, 1, 2, Á ).
Hence the only zeros of cos z and sin z are those of the real cosine and sine functions.

᭿

General formulas for the real trigonometric functions continue to hold for complex
values. This follows immediately from the definitions. We mention in particular the
addition rules
(9)

cos (z 1 Ϯ z 2) ϭ cos z 1 cos z 2 ϯ sin z 1 sin z 2
sin (z 1 Ϯ z 2) ϭ sin z 1 cos z 2 Ϯ sin z 2 cos z 1

and the formula
cos2 z ϩ sin2 z ϭ 1.

(10)

Some further useful formulas are included in the problem set.

Hyperbolic Functions
The complex hyperbolic cosine and sine are defined by the formulas
(11)

cosh z ϭ 12(ez ϩ e؊z),

sinh z ϭ 12(ez Ϫ e؊z).

This is suggested by the familiar definitions for a real variable [see (8)]. These functions
are entire, with derivatives
(12)

(cosh z) r ϭ sinh z,

(sinh z) r ϭ cosh z,

as in calculus. The other hyperbolic functions are defined by
tanh z ϭ

sinh z
,
cosh z

coth z ϭ

cosh z
,
sinh z

sech z ϭ

1
,
cosh z

csch z ϭ

1
.
sinh z

(13)

Complex Trigonometric and Hyperbolic Functions Are Related. If in (11), we replace z
by iz and then use (1), we obtain
(14)

cosh iz ϭ cos z,

sinh iz ϭ i sin z.

Similarly, if in (1) we replace z by iz and then use (11), we obtain conversely
(15)

cos iz ϭ cosh z,

sin iz ϭ i sinh z.

Here we have another case of unrelated real functions that have related complex analogs,
pointing again to the advantage of working in complex in order to get both a more unified
formalism and a deeper understanding of special functions. This is one of the main reasons
for the importance of complex analysis to the engineer and physicist.

c13.qxd

10/30/10

2:14 PM

636

Page 636

CHAP. 13 Complex Numbers and Functions. Complex Differentiation

PROBLEM SET 13.6
1–4
FORMULAS FOR HYPERBOLIC FUNCTIONS
Show that
1.

cosh z ϭ cosh x cos y ϩ i sinh x sin y
sinh z ϭ sinh x cos y ϩ i cosh x sin y.

2.

cosh (z 1 ϩ z 2) ϭ cosh z 1 cosh z 2 ϩ sinh z 1 sinh z 2
sinh (z 1 ϩ z 2) ϭ sinh z 1 cosh z 2 ϩ cosh z 1 sinh z 2.

3. cosh2 z Ϫ sinh2 z ϭ 1, cosh2 z ϩ sinh2 z ϭ cosh 2z
4. Entire Functions. Prove that cos z, sin z, cosh z, and
sinh z are entire.
5. Harmonic Functions. Verify by differentiation that
Im cos z and Re sin z are harmonic.
6–12
6.
8.
9.
10.

Function Values. Find, in the form u ϩ iv,

sin 2pi
7. cos i,
cos pi, cosh pi
cosh (Ϫ1 ϩ 2i), cos (Ϫ2 Ϫ i)
sinh (3 ϩ 4i), cosh (3 ϩ 4i)

13.7

11. sin pi, cos (12p Ϫ pi)
12. cos 12p i, cos [12p(1 ϩ i)]
13–15
Equations and Inequalities. Using the definitions, prove:
13. cos z is even, cos (Ϫz) ϭ cos z, and sin z is odd,
sin (Ϫz) ϭ Ϫsin z.
14. ƒ sinh y ƒ Ϲ ƒ cos z ƒ Ϲ cosh y, ƒ sinh yƒ Ϲ ƒ sin z ƒ Ϲ cosh y.
Conclude that the complex cosine and sine are not
bounded in the whole complex plane.
15. sin z 1 cos z 2 ϭ 12[sin (z 1 ϩ z 2) ϩ sin (z 1 Ϫ z 2)]
16–19

Equations. Find all solutions.

17. cosh z ϭ 0
16. sin z ϭ 100
18. cosh z ϭ Ϫ1
19. sinh z ϭ 0
20. Re tan z and Im tan z. Show that
Re tan z ϭ

sin i

Im tan z ϭ

sin x cos x
cos2 x ϩ sinh2 y
sinh y cosh y
cos2 x ϩ sinh2 y

,
.

Logarithm. General Power. Principal Value
We finally introduce the complex logarithm, which is more complicated than the real
logarithm (which it includes as a special case) and historically puzzled mathematicians
for some time (so if you first get puzzled—which need not happen!—be patient and work
through this section with extra care).
The natural logarithm of z ϭ x ϩ iy is denoted by ln z (sometimes also by log z) and
is defined as the inverse of the exponential function; that is, w ϭ ln z is defined for z 0
by the relation
ew ϭ z.
(Note that z ϭ 0 is impossible, since ew
and z ϭ reiu, this becomes

0 for all w; see Sec. 13.5.) If we set w ϭ u ϩ iv

ew ϭ euϩiv ϭ reiu.
Now, from Sec. 13.5, we know that euϩiv has the absolute value eu and the argument v.
These must be equal to the absolute value and argument on the right:
eu ϭ r,

v ϭ u.

c13.qxd

10/30/10

2:14 PM

Page 637

SEC. 13.7 Logarithm. General Power. Principal Value

637

eu ϭ r gives u ϭ ln r, where ln r is the familiar real natural logarithm of the positive
number r ϭ ƒ z ƒ . Hence w ϭ u ϩ iv ϭ ln z is given by
ln z ϭ ln r ϩ iu

(1)

(r ϭ ƒ z ƒ Ͼ 0, u ϭ arg z).

Now comes an important point (without analog in real calculus). Since the argument of
z is determined only up to integer multiples of 2p, the complex natural logarithm
ln z (z 0) is infinitely many-valued.
The value of ln z corresponding to the principal value Arg z (see Sec. 13.2) is denoted
by Ln z (Ln with capital L) and is called the principal value of ln z. Thus
Ln z ϭ ln ƒ z ƒ ϩ i Arg z

(2)

(z

0).

The uniqueness of Arg z for given z ( 0) implies that Ln z is single-valued, that is, a
function in the usual sense. Since the other values of arg z differ by integer multiples of 2p,
the other values of ln z are given by
In z ϭ Ln z Ϯ 2npi

(3)

(n ϭ 1, 2, Á ).

They all have the same real part, and their imaginary parts differ by integer multiples
of 2p.
If z is positive real, then Arg z ϭ 0, and Ln z becomes identical with the real natural
logarithm known from calculus. If z is negative real (so that the natural logarithm of
calculus is not defined!), then Arg z ϭ p and
Ln z ϭ ln ƒ z ƒ ϩ pi

(z negative real).

From (1) and eln r ϭ r for positive real r we obtain
eln z ϭ z

(4a)

as expected, but since arg (ez) ϭ y Ϯ 2np is multivalued, so is
ln (ez) ϭ z Ϯ 2npi,

(4b)
EXAMPLE 1

n ϭ 0, 1, Á .

Natural Logarithm. Principal Value
ln 1 ϭ 0, Ϯ2pi, Ϯ4pi, Á

Ln 1 ϭ 0

ln 4 ϭ 1.386294 Ϯ 2npi

Ln 4 ϭ 1.386294

ln (Ϫ1) ϭ Ϯpi, Ϯ3pi, Ϯ5pi, Á

Ln (Ϫ1) ϭ pi

ln (Ϫ4) ϭ 1.386294 Ϯ (2n ϩ 1)pi

Ln (Ϫ4) ϭ 1.386294 ϩ pi

ln i ϭ pi>2, Ϫ3p>2, 5pi>2, Á

Ln i ϭ pi>2

ln 4i ϭ 1.386294 ϩ pi>2 Ϯ 2npi

Ln 4i ϭ 1.386294 ϩ pi>2

ln (Ϫ4i) ϭ 1.386294 Ϫ pi>2 Ϯ 2npi

Ln (Ϫ4i) ϭ 1.386294 Ϫ pi>2

ln (3 Ϫ 4i) ϭ ln 5 ϩ i arg (3 Ϫ 4i)
ϭ 1.609438 Ϫ 0.927295i Ϯ 2npi

Ln (3 Ϫ 4i) ϭ 1.609438 Ϫ 0.927295i
(Fig. 337)

᭿

c13.qxd

10/30/10

2:14 PM

638

Page 638

CHAP. 13 Complex Numbers and Functions. Complex Differentiation
v
–0.9 + 6π
–0.9 + 4π
–0.9 + 2π
0
–0.9

1

2

u

–0.9 – 2π

Fig. 337. Some values of ln (3 Ϫ 4i) in Example 1

The familiar relations for the natural logarithm continue to hold for complex values, that is,
(a) ln (z 1z 2) ϭ ln z 1 ϩ ln z 2,

(5)

(b) ln (z 1>z 2) ϭ ln z 1 Ϫ ln z 2

but these relations are to be understood in the sense that each value of one side is also
contained among the values of the other side; see the next example.
EXAMPLE 2

Illustration of the Functional Relation (5) in Complex
Let
z 1 ϭ z 2 ϭ epi ϭ Ϫ1.
If we take the principal values
Ln z 1 ϭ Ln z 2 ϭ pi,
then (5a) holds provided we write ln (z 1z 2) ϭ ln 1 ϭ 2pi; however, it is not true for the principal value,
Ln (z 1z 2) ϭ Ln 1 ϭ 0.
᭿

THEOREM 1

Analyticity of the Logarithm

For every n ϭ 0, Ϯ1, Ϯ2, Á formula (3) defines a function, which is analytic,
except at 0 and on the negative real axis, and has the derivative
1
(ln z) r ϭ z

(6)

PROOF

(z not 0 or negative real).

We show that the Cauchy–Riemann equations are satisfied. From (1)–(3) we have
ln z ϭ ln r ϩ i(u ϩ c) ϭ

1
y
ln (x 2 ϩ y 2) ϩ i aarctan ϩ cb
2
x

where the constant c is a multiple of 2p. By differentiation,
ux ϭ
uy ϭ

x
1
#1
2 ϭ vy ϭ
2
x ϩy
1 ϩ (y>x) x
2

y
x ϩy
2

2

ϭ Ϫvx ϭ Ϫ

1
1 ϩ (y>x)

2

aϪ

y
x2

b.

c13.qxd

10/30/10

2:14 PM

Page 639

SEC. 13.7 Logarithm. General Power. Principal Value

639

Hence the Cauchy–Riemann equations hold. [Confirm this by using these equations in polar
form, which we did not use since we proved them only in the problems (to Sec. 13.4).]
Formula (4) in Sec. 13.4 now gives (6),
(ln z) r ϭ u x ϩ ivx ϭ

y
x
1
x Ϫ iy
1
.
2 ϩ i
2 aϪ 2 b ϭ 2
2 ϭ
z
x ϩy
1 ϩ (y>x)
x
x ϩy
2

᭿

Each of the infinitely many functions in (3) is called a branch of the logarithm. The
negative real axis is known as a branch cut and is usually graphed as shown in Fig. 338.
The branch for n ϭ 0 is called the principal branch of ln z.
y

x

Fig. 338. Branch cut for ln z

General Powers
General powers of a complex number z ϭ x ϩ iy are defined by the formula
z c ϭ ec ln z

(7)

(c complex, z

0).

Since ln z is infinitely many-valued, z c will, in general, be multivalued. The particular value
z c ϭ ec Ln z
is called the principal value of z c.
If c ϭ n ϭ 1, 2, Á , then z n is single-valued and identical with the usual nth power of z.
If c ϭ Ϫ1, Ϫ2, Á , the situation is similar.
If c ϭ 1>n, where n ϭ 2, 3, Á , then
n
ϭ e(1>n) ln z
z c ϭ 1z

(z

0),

the exponent is determined up to multiples of 2pi>n and we obtain the n distinct values
of the nth root, in agreement with the result in Sec. 13.2. If c ϭ p>q, the quotient of two
positive integers, the situation is similar, and z c has only finitely many distinct values.
However, if c is real irrational or genuinely complex, then z c is infinitely many-valued.
EXAMPLE 3

General Power
i i ϭ ei ln i ϭ exp (i ln i) ϭ exp c i a

p
2

i Ϯ 2npib d ϭ e؊(p>2)ϯ2np.

All these values are real, and the principal value (n ϭ 0) is e؊p>2.
Similarly, by direct calculation and multiplying out in the exponent,
(1 ϩ i)2؊i ϭ exp 3(2 Ϫ i) ln (1 ϩ i)4 ϭ exp 3(2 Ϫ i) {ln 12 ϩ 14 pi Ϯ 2npi}4
ϭ 2ep>4Ϯ2np 3sin (12 ln 2) ϩ i cos (12 ln 2)4.

᭿

c13.qxd

10/30/10

2:14 PM

640

Page 640

CHAP. 13 Complex Numbers and Functions. Complex Differentiation

It is a convention that for real positive z ϭ x the expression z c means ec ln x where ln x
is the elementary real natural logarithm (that is, the principal value Ln z (z ϭ x Ͼ 0) in
the sense of our definition). Also, if z ϭ e, the base of the natural logarithm, z c ϭ ec is
conventionally regarded as the unique value obtained from (1) in Sec. 13.5.
From (7) we see that for any complex number a,
a z ϭ ez ln a.

(8)

We have now introduced the complex functions needed in practical work, some of them
(ez, cos z, sin z, cosh z, sinh z) entire (Sec. 13.5), some of them (tan z, cot z, tanh z, coth z)
analytic except at certain points, and one of them (ln z) splitting up into infinitely many
functions, each analytic except at 0 and on the negative real axis.
For the inverse trigonometric and hyperbolic functions see the problem set.

PROBLEM SET 13.7
1–4
VERIFICATIONS IN THE TEXT
1. Verify the computations in Example 1.
2. Verify (5) for z 1 ϭ Ϫi and z 2 ϭ Ϫ1.
3. Prove analyticity of Ln z by means of the Cauchy–
Riemann equations in polar form (Sec. 13.4).
4. Prove (4a) and (4b).

COMPLEX NATURAL LOGARITHM ln z
5–11
Principal Value Ln z. Find Ln z when z equals
5.
7.
9.
11.

Ϫ11
4 Ϫ 4i
0.6 ϩ 0.8i
ei

6. 4 ϩ 4i
8. 1 Ϯ i
10. Ϫ15 Ϯ 0.1i

12–16
All Values of ln z. Find all values and graph
some of them in the complex plane.
12.
14.
16.
17.

ln e
13. ln 1
15. ln (ei)
ln (Ϫ7)
ln (4 ϩ 3i)
Show that the set of values of ln (i 2) differs from the
set of values of 2 ln i.

18–21

Equations. Solve for z.

18. ln z ϭ Ϫpi>2
20. ln z ϭ e Ϫ pi

19. ln z ϭ 4 Ϫ 3i
21. ln z ϭ 0.6 ϩ 0.4i

22–28
General Powers. Find the principal value.
Show details.
22. (2i)2i
24. (1 Ϫ i)1ϩi

23. (1 ϩ i)1؊i
25. (Ϫ3)3؊i

26. (i)i>2

27. (Ϫ1)2؊i

28. (3 ϩ 4i)1>3
29. How can you find the answer to Prob. 24 from the
answer to Prob. 23?
30. TEAM PROJECT. Inverse Trigonometric and
Hyperbolic Functions. By definition, the inverse sine
w ϭ arcsin z is the relation such that sin w ϭ z. The
inverse cosine w ϭ arccos z is the relation such that
cos w ϭ z. The inverse tangent, inverse cotangent,
inverse hyperbolic sine, etc., are defined and denoted
in a similar fashion. (Note that all these relations are
multivalued.) Using sin w ϭ (eiw Ϫ e؊iw)>(2i) and
similar representations of cos w, etc., show that
(a) arccos z ϭ Ϫi ln (z ϩ 2z 2 Ϫ 1)
(b) arcsin z ϭ Ϫi ln (iz ϩ 21 Ϫ z 2)
(c) arccosh z ϭ ln (z ϩ 2z 2 Ϫ 1)
(d) arcsinh z ϭ ln (z ϩ 2z 2 ϩ 1)
(e) arctan z ϭ

i iϩz
ln
2 iϪz

(f) arctanh z ϭ

1 1ϩz
ln
2 1Ϫz

(g) Show that w ϭ arcsin z is infinitely many-valued,
and if w1 is one of these values, the others are of the
form w1 Ϯ 2np and p Ϫ w1 Ϯ 2np, n ϭ 0, 1, Á .
(The principal value of w ϭ u ϩ iv ϭ arcsin z is defined
to be the value for which Ϫp>2 Ϲ u Ϲ p>2 if v м 0
and Ϫp>2 Ͻ u Ͻ p>2 if v Ͻ 0.)

c13.qxd

10/30/10

2:14 PM

Page 641

Summary of Chapter 13

641

CHAPTER 13 REVIEW QUESTIONS AND PROBLEMS
1. Divide 15 ϩ 23i by Ϫ3 ϩ 7i. Check the result by
multiplication.
2. What happens to a quotient if you take the complex
conjugates of the two numbers? If you take the absolute
values of the numbers?
3. Write the two numbers in Prob. 1 in polar form. Find
the principal values of their arguments.
4. State the definition of the derivative from memory.
Explain the big difference from that in calculus.
5. What is an analytic function of a complex variable?
6. Can a function be differentiable at a point without being
analytic there? If yes, give an example.
7. State the Cauchy–Riemann equations. Why are they of
basic importance?
8. Discuss how ez, cos z, sin z, cosh z, sinh z are related.
9. ln z is more complicated than ln x. Explain. Give
examples.
10. How are general powers defined? Give an example.
Convert it to the form x ϩ iy.
11–16
Complex Numbers. Find, in the form x ϩ iy,
showing details,
11. (2 ϩ 3i)2
13. 1>(4 ϩ 3i)

12. (1 Ϫ i)10
14. 2i

SUMMARY OF CHAPTER

15. (1 ϩ i)>(1 Ϫ i)

16. epi>2, e؊pi>2

17–20
Polar Form. Represent in polar form, with the
principal argument.
17. Ϫ4 Ϫ 4i
18. 12 ϩ i, 12 Ϫ i
19. Ϫ15i
20. 0.6 ϩ 0.8i
21–24
Roots. Find and graph all values of:
21. 181
22. 2Ϫ32i
4
3
23. 2
24. 2
Ϫ1
1
25–30 Analytic Functions. Find f (z) ϭ u(x, y) ϩ iv(x, y)
with u or v as given. Check by the Cauchy–Riemann equations
for analyticity.
25
27.
29.
30.

26. v ϭ y>(x 2 ϩ y 2)
u ϭ xy
؊2x
28. u ϭ cos 3x cosh 3y
sin 2y
v ϭ Ϫe
u ϭ exp(Ϫ(x 2 Ϫ y 2)>2) cos xy
v ϭ cos 2x sinh 2y

31–35
31.
33.
34.
35.

Special Function Values. Find the value of:

32. Ln (0.6 ϩ 0.8i)
cos (3 Ϫ i)
tan i
sinh (1 ϩ pi), sin (1 ϩ pi)
cosh (p ϩ pi)

13

Complex Numbers and Functions. Complex Differentiation
For arithmetic operations with complex numbers
(1)

z ϭ x ϩ iy ϭ reiu ϭ r (cos u ϩ i sin u),

r ϭ ƒ z ƒ ϭ 2x 2 ϩ y 2, u ϭ arctan (y>x), and for their representation in the complex
plane, see Secs. 13.1 and 13.2.
A complex function f (z) ϭ u(x, y) ϩ iv(x, y) is analytic in a domain D if it has
a derivative (Sec. 13.3)
(2)

f r(z) ϭ lim

¢z :0

f (z ϩ ¢z) Ϫ f (z)
¢z

everywhere in D. Also, f(z) is analytic at a point z ϭ z 0 if it has a derivative in a
neighborhood of z 0 (not merely at z 0 itself).

c13.qxd

10/30/10

642

2:14 PM

Page 642

CHAP. 13 Complex Numbers and Functions. Complex Differentiation

If f (z) is analytic in D, then u(x, y) and v(x, y) satisfy the (very important!)
Cauchy–Riemann equations (Sec. 13.4)
0v
0u
ϭ
,
0x
0y

(3)

0u
0v
ϭϪ
0y
0x

everywhere in D. Then u and v also satisfy Laplace’s equation
u xx ϩ u yy ϭ 0,

(4)

vxx ϩ vyy ϭ 0

everywhere in D. If u(x, y) and v(x, y) are continuous and have continuous partial
derivatives in D that satisfy (3) in D, then f (z) ϭ u(x, y) ϩ iv(x, y) is analytic in
D. See Sec. 13.4. (More on Laplace’s equation and complex analysis follows in
Chap. 18.)
The complex exponential function (Sec. 13.5)
ez ϭ exp z ϭ ex (cos y ϩ i sin y)

(5)

reduces to ex if z ϭ x (y ϭ 0). It is periodic with 2pi and has the derivative ez.
The trigonometric functions are (Sec. 13.6)
cos z ϭ 12 (eiz ϩ e؊iz) ϭ cos x cosh y Ϫ i sin x sinh y
(6)

sin z ϭ

1 iz
(e Ϫ e؊iz) ϭ sin x cosh y ϩ i cos x sinh y
2i

and, furthermore,
tan z ϭ (sin z)>cos z,

cot z ϭ 1>tan z,

etc.

The hyperbolic functions are (Sec. 13.6)
(7)

cosh z ϭ 12(ez ϩ e؊z) ϭ cos iz,

sinh z ϭ 12(ez Ϫ e؊z) ϭ Ϫi sin iz

etc. The functions (5)–(7) are entire, that is, analytic everywhere in the complex
plane.
The natural logarithm is (Sec. 13.7)
(8)

ln z ϭ ln ƒ z ƒ ϩ i arg z ϭ ln ƒ z ƒ ϩ i Arg z Ϯ 2npi

where z 0 and n ϭ 0, 1, Á . Arg z is the principal value of arg z, that is,
Ϫp Ͻ Arg z Ϲ p. We see that ln z is infinitely many-valued. Taking n ϭ 0 gives
the principal value Ln z of ln z; thus Ln z ϭ ln ƒ z ƒ ϩ i Arg z.
General powers are defined by (Sec. 13.7)
(9)

z c ϭ ec ln z

(c complex, z

0).

c14.qxd

11/1/10

6:02 PM

Page 643

CHAPTER

14

Complex Integration
Chapter 13 laid the groundwork for the study of complex analysis, covered complex numbers in the complex plane, limits, and differentiation, and introduced the most important
concept of analyticity. A complex function is analytic in some domain if it is differentiable
in that domain. Complex analysis deals with such functions and their applications. The
Cauchy–Riemann equations, in Sec. 13.4, were the heart of Chapter 13 and allowed a means
of checking whether a function is indeed analytic. In that section, we also saw that analytic
functions satisfy Laplace’s equation, the most important PDE in physics.
We now consider the next part of complex calculus, that is, we shall discuss the first
approach to complex integration. It centers around the very important Cauchy integral
theorem (also called the Cauchy–Goursat theorem) in Sec. 14.2. This theorem is important
because it allows, through its implied Cauchy integral formula of Sec. 14.3, the evaluation
of integrals having an analytic integrand. Furthermore, the Cauchy integral formula shows
the surprising result that analytic functions have derivatives of all orders. Hence, in this
respect, complex analytic functions behave much more simply than real-valued functions
of real variables, which may have derivatives only up to a certain order.
Complex integration is attractive for several reasons. Some basic properties of analytic
functions are difficult to prove by other methods. This includes the existence of derivatives
of all orders just discussed. A main practical reason for the importance of integration in
the complex plane is that such integration can evaluate certain real integrals that appear
in applications and that are not accessible by real integral calculus.
Finally, complex integration is used in connection with special functions, such as
gamma functions (consult [GenRef1]), the error function, and various polynomials (see
[GenRef10]). These functions are applied to problems in physics.
The second approach to complex integration is integration by residues, which we shall
cover in Chapter 16.
Prerequisite: Chap. 13.
Section that may be omitted in a shorter course: 14.1, 14.5.
References and Answers to Problems: App. 1 Part D, App. 2.

14.1

Line Integral in the Complex Plane
As in calculus, in complex analysis we distinguish between definite integrals and indefinite
integrals or antiderivatives. Here an indefinite integral is a function whose derivative
equals a given analytic function in a region. By inverting known differentiation formulas
we may find many types of indefinite integrals.
Complex definite integrals are called (complex) line integrals. They are written

Ύ f (z) dz.
C

643

c14.qxd

11/1/10

644

6:02 PM

Page 644

CHAP. 14 Complex Integration

Here the integrand f (z) is integrated over a given curve C or a portion of it (an arc, but
we shall say “curve” in either case, for simplicity). This curve C in the complex plane is
called the path of integration. We may represent C by a parametric representation
z(t) ϭ x(t) ϭ iy(t)

(1)

(a Ϲ t Ϲ b).

The sense of increasing t is called the positive sense on C, and we say that C is oriented
by (1).
For instance, z(t) ϭ t ϩ 3it (0 Ϲ t Ϲ 2) gives a portion (a segment) of the line y ϭ 3x.
The function z(t) ϭ 4 cos t ϩ 4i sin t (Ϫp Ϲ t Ϲ p) represents the circle ƒ z ƒ ϭ 4, and so
on. More examples follow below.
We assume C to be a smooth curve, that is, C has a continuous and nonzero derivative

#

z(t) ϭ

dz
#
#
ϭ x(t) ϩ iy (t)
dt

at each point. Geometrically this means that C has everywhere a continuously turning
tangent, as follows directly from the definition

#

z(t) ϭ lim

¢t:0

z(t ϩ ¢t) Ϫ z(t)

(Fig. 339).

¢t

Here we use a dot since a prime r denotes the derivative with respect to z.

Definition of the Complex Line Integral
This is similar to the method in calculus. Let C be a smooth curve in the complex plane
given by (1), and let f (z) be a continuous function given (at least) at each point of C. We
now subdivide (we “partition”) the interval a Ϲ t Ϲ b in (1) by points
t 0 (ϭ a), t 1,

Á , t n؊1,

t n (ϭ b)

where t 0 Ͻ t 1 Ͻ Á Ͻ t n. To this subdivision there corresponds a subdivision of C by
points
z 0,

z 1,

Á , z n؊1,

z n (ϭ Z )

(Fig. 340),

z(t)
z(t +

z(t + Δt)
z(t)
0

zm – 1

(t)
Δt) – z

.
..
z2

ζm

|Δ zm|

zm

..

z1
z0

.

Fig. 339. Tangent vector z (t) of a curve C in the
complex plane given by z(t). The arrowhead on the
curve indicates the positive sense (sense of increasing t)

Fig. 340. Complex line integral

.
Z

c14.qxd

11/1/10

6:02 PM

Page 645

SEC. 14.1 Line Integral in the Complex Plane

645

where z j ϭ z(t j). On each portion of subdivision of C we choose an arbitrary point, say,
a point z1 between z 0 and z1 (that is, z1 ϭ z(t) where t satisfies t 0 Ϲ t Ϲ t 1), a point z2
between z1 and z 2, etc. Then we form the sum
n

Sn ϭ a f (zm) ¢z m

(2)

¢z m ϭ z m Ϫ z m؊1.

where

mϭ1

We do this for each n ϭ 2, 3, Á in a completely independent manner, but so that the
greatest ƒ ¢t m ƒ ϭ ƒ t m Ϫ t m؊1 ƒ approaches zero as n : ϱ. This implies that the greatest
ƒ ¢z m ƒ also approaches zero. Indeed, it cannot exceed the length of the arc of C from
z m؊1 to z m and the latter goes to zero since the arc length of the smooth curve C is a
continuous function of t. The limit of the sequence of complex numbers S2, S3, Á thus
obtained is called the line integral (or simply the integral) of f (z) over the path of
integration C with the orientation given by (1). This line integral is denoted by

Ύ f (z) dz,

(3)

Ώ f (z) dz

or by

C

C

if C is a closed path (one whose terminal point Z coincides with its initial point z 0, as
for a circle or for a curve shaped like an 8).
General Assumption. All paths of integration for complex line integrals are assumed to
be piecewise smooth, that is, they consist of finitely many smooth curves joined end to end.

Basic Properties Directly Implied by the Definition
1. Linearity. Integration is a linear operation, that is, we can integrate sums term by
term and can take out constant factors from under the integral sign. This means that
if the integrals of f1 and f2 over a path C exist, so does the integral of k 1 f1 ϩ k 2 f2
over the same path and
(4)

Ύ [k

1 f1(z)

ϩ k 2 f2(z)] dz ϭ k 1

C

Ύ f (z) dz ϩ k Ύ f (z) dz.
1

2

C

2

C

2. Sense reversal in integrating over the same path, from z 0 to Z (left) and from Z to
z 0 (right), introduces a minus sign as shown,

Ύ

(5)

Z

f (z) dz ϭ Ϫ

Ύ

z0

f (z) dz.

Z

z0

3. Partitioning of path (see Fig. 341)
(6)

Ύ f (z) dz ϭ Ύ f (z) dz ϩ Ύ f (z) dz.
C

C1

C2

C1
C2

Z

z0

Fig. 341. Partitioning of path [formula (6)]

c14.qxd

11/1/10

646

6:02 PM

Page 646

CHAP. 14 Complex Integration

Existence of the Complex Line Integral
Our assumptions that f (z) is continuous and C is piecewise smooth imply the existence
of the line integral (3). This can be seen as follows.
As in the preceding chapter let us write f (z) ϭ u(x, y) ϩ iv(x, y). We also set
zm ϭ ␰m ϩ ihm

and

¢z m ϭ ¢x m ϩ i¢ym.

Then (2) may be written
Sn ϭ a (u ϩ iv)(¢x m ϩ i¢ym)

(7)

where u ϭ u(zm, hm), v ϭ v(zm, hm) and we sum over m from 1 to n. Performing the
multiplication, we may now split up Sn into four sums:

[

]

Sn ϭ a u ¢x m Ϫ a v ¢ym ϩ i a u ¢ym ϩ a v ¢x m .
These sums are real. Since f is continuous, u and v are continuous. Hence, if we let n
approach infinity in the aforementioned way, then the greatest ¢x m and ¢ym will approach
zero and each sum on the right becomes a real line integral:
(8)

lim Sn ϭ

n :ϱ

Ύ f (z) dz
C

ϭ

Ύ u dx Ϫ Ύ v dy ϩ i c Ύ u dy ϩ Ύ v dx d .
C

C

C

C

This shows that under our assumptions on f and C the line integral (3) exists and its value
᭿
is independent of the choice of subdivisions and intermediate points zm.

First Evaluation Method:
Indefinite Integration and Substitution of Limits
This method is the analog of the evaluation of definite integrals in calculus by the wellknown formula
b

Ύ f (x) dx ϭ F(b) Ϫ F(a)
a

where [F r(x) ϭ f (x)].
It is simpler than the next method, but it is suitable for analytic functions only. To
formulate it, we need the following concept of general interest.
A domain D is called simply connected if every simple closed curve (closed curve
without self-intersections) encloses only points of D.
For instance, a circular disk is simply connected, whereas an annulus (Sec. 13.3) is not
simply connected. (Explain!)

c14.qxd

11/1/10

6:02 PM

Page 647

SEC. 14.1 Line Integral in the Complex Plane

THEOREM 1

647

Indefinite Integration of Analytic Functions

Let f (z) be analytic in a simply connected domain D. Then there exists an indefinite
integral of f (z) in the domain D, that is, an analytic function F(z) such that
F r(z) ϭ f (z) in D, and for all paths in D joining two points z 0 and z 1 in D we have

Ύ

(9)

z1

f (z) dz ϭ F(z 1) Ϫ F(z 0)

[F r(z) ϭ f (z)].

z0

(Note that we can write z 0 and z 1 instead of C, since we get the same value for all
those C from z 0 to z 1.)
This theorem will be proved in the next section.
Simple connectedness is quite essential in Theorem 1, as we shall see in Example 5.
Since analytic functions are our main concern, and since differentiation formulas will often
help in finding F(z) for a given f (z) ϭ F r(z), the present method is of great practical interest.
If f (z) is entire (Sec. 13.5), we can take for D the complex plane (which is certainly
simply connected).
EXAMPLE 1

Ύ

1ϩi

Ύ

pi

Ύ

8؊3pi

z 2 dz ϭ

0

EXAMPLE 2

؊pi

EXAMPLE 3

1
3

z3 `

1ϩi

ϭ
0

cos z dz ϭ sin z `

8ϩpi

1
3

(1 ϩ i)3 ϭ Ϫ

2
3

ϩ

2
3

᭿

i

pi
؊pi

ez>2 dz ϭ 2ez>2 `

᭿

ϭ 2 sin pi ϭ 2i sinh p ϭ 23.097i

8؊3pi

ϭ 2(e4–3pi>2 Ϫ e4ϩpi>2) ϭ 0
8ϩpi

᭿

since ez is periodic with period 2pi.

EXAMPLE 4

Ύ

i

dz
ip
ip
ϭ Ln i Ϫ Ln (Ϫi) ϭ
Ϫ aϪ b ϭ ip. Here D is the complex plane without 0 and the negative real
z
2
2
؊i

᭿

axis (where Ln z is not analytic). Obviously, D is a simply connected domain.

Second Evaluation Method:
Use of a Representation of a Path
This method is not restricted to analytic functions but applies to any continuous complex
function.
THEOREM 2

Integration by the Use of the Path

Let C be a piecewise smooth path, represented by z ϭ z(t), where a Ϲ t Ϲ b. Let
f (z) be a continuous function on C. Then

(10)

Ύ

C

b

f (z) dz ϭ

Ύ f [z(t)]z# (t) dt
a

az ϭ

#

dz
b.
dt

c14.qxd

11/1/10

6:02 PM

648

Page 648

CHAP. 14 Complex Integration

PROOF

The left side of (10) is given by (8) in terms of real line integrals, and we show that
# # #
the right side of (10) also equals (8). We have z ϭ x ϩ iy, hence z ϭ x ϩ iy. We simply
#
#
write u for u[x(t), y(t)] and v for v[x(t), y(t)]. We also have dx ϭ x dt and dy ϭ y dt.
Consequently, in (10)

Ύ

b

b

Ύ (u ϩ iv)(x# ϩ iy# ) dt

#

f [z(t)]z (t) dt ϭ

a

a

Ύ [u dx Ϫ v dy ϩ i (u dy ϩ v dx)]

ϭ

C

Ύ (u dx Ϫ v dy) ϩ i Ύ (u dy ϩ v dx).

ϭ

C

᭿

C

COMMENT. In (7) and (8) of the existence proof of the complex line integral we referred
to real line integrals. If one wants to avoid this, one can take (10) as a definition of the
complex line integral.

Steps in Applying Theorem 2
(A)
(B)
(C)
(D)
EXAMPLE 5

Represent the path C in the form z(t) (a Ϲ t Ϲ b).
#
Calculate the derivative z(t) ϭ dz>dt.
Substitute z(t) for every z in f (z) (hence x(t) for x and y(t) for y).
#
Integrate f [z(t)]z(t) over t from a to b.

A Basic Result: Integral of 1/z Around the Unit Circle
We show that by integrating 1>z counterclockwise around the unit circle (the circle of radius 1 and center 0;
see Sec. 13.3) we obtain

Ώ

(11)

C

dz
ϭ 2pi
z

(C the unit circle, counterclockwise).

This is a very important result that we shall need quite often.

Solution. (A) We may represent the unit circle C in Fig. 330 of Sec. 13.3 by
z(t) ϭ cos t ϩ i sin t ϭ eit

(0 Ϲ t Ϲ 2p),

so that counterclockwise integration corresponds to an increase of t from 0 to 2p.

#

(B) Differentiation gives z(t) ϭ ieit (chain rule!).
(C) By substitution, f (z(t)) ϭ 1>z(t) ϭ e؊it.
(D) From (10) we thus obtain the result

Ώ

C

dz
ϭ
z

Ύ

2p

0

e؊itieit dt ϭ i

Ύ

2p

dt ϭ 2pi.

0

Check this result by using z(t) ϭ cos t ϩ i sin t.
Simple connectedness is essential in Theorem 1. Equation (9) in Theorem 1 gives 0 for any closed path
because then z 1 ϭ z 0, so that F(z 1) Ϫ F(z 0) ϭ 0. Now 1>z is not analytic at z ϭ 0. But any simply connected
domain containing the unit circle must contain z ϭ 0, so that Theorem 1 does not apply—it is not enough that
1>z is analytic in an annulus, say, 12 Ͻ ƒ z ƒ Ͻ 32 , because an annulus is not simply connected!
᭿

c14.qxd

11/1/10

6:02 PM

Page 649

SEC. 14.1 Line Integral in the Complex Plane
EXAMPLE 6

649

Integral of 1/z m with Integer Power m
Let f (z) ϭ (z Ϫ z 0)m where m is the integer and z 0 a constant. Integrate counterclockwise around the circle C
of radius r with center at z 0 (Fig. 342).
y

C

ρ
z0

x

Fig. 342. Path in Example 6

Solution. We may represent C in the form
z(t) ϭ z 0 ϩ r(cos t ϩ i sin t) ϭ z 0 ϩ reit

(0 Ϲ t Ϲ 2p).

Then we have
(z Ϫ z 0)m ϭ rmeimt,

dz ϭ ireit dt

and obtain

Ώ (z Ϫ z )
0

m

dz ϭ

Ύ

2p

rmeimt ireit dt ϭ irmϩ1

0

C

Ύ

2p

ei(mϩ1)t dt.

0

By the Euler formula (5) in Sec. 13.6 the right side equals
irmϩ1 c

Ύ

2p

cos (m ϩ 1)t dt ϩ i

0

Ύ

2p

0

sin (m ϩ 1)t dt d .

If m ϭ Ϫ1, we have rmϩ1 ϭ 1, cos 0 ϭ 1, sin 0 ϭ 0. We thus obtain 2pi. For integer m Ϫ1 each of the two
integrals is zero because we integrate over an interval of length 2p, equal to a period of sine and cosine. Hence
the result is

(12)

Ώ (z Ϫ z )
0

m

dz ϭ b

C

2pi
0

(m ϭ Ϫ1),
(m

᭿

Ϫ1 and integer).

Dependence on path. Now comes a very important fact. If we integrate a given function
f (z) from a point z 0 to a point z 1 along different paths, the integrals will in general have
different values. In other words, a complex line integral depends not only on the endpoints
of the path but in general also on the path itself. The next example gives a first impression
of this, and a systematic discussion follows in the next section.
EXAMPLE 7

Integral of a Nonanalytic Function. Dependence on Path
Integrate f (z) ϭ Re z ϭ x from 0 to 1 ϩ 2i (a) along C* in Fig. 343, (b) along C consisting of C1 and C2.

#

Solution. (a) C* can be represented by z(t) ϭ t ϩ 2it (0 Ϲ t Ϲ 1). Hence z(t) ϭ 1 ϩ 2i and f [z(t)] ϭ
x(t) ϭ t on C*. We now calculate

1

Ύ Re z dz ϭ Ύ t(1 ϩ 2i) dt ϭ 2 (1 ϩ 2i) ϭ 2 ϩ i.
C*

0

1

1

c14.qxd

11/1/10

6:02 PM

650

Page 650

CHAP. 14 Complex Integration
y
z = 1 + 2i

2

C*
C2
C1
1

x

Fig. 343. Paths in Example 7

(b) We now have
C1: z(t) ϭ t,

#
#
z(t) ϭ i,

z(t) ϭ 1,

C2: z(t) ϭ 1 ϩ it,

f (z(t)) ϭ x(t) ϭ t

(0 Ϲ t Ϲ 1)

f (z(t)) ϭ x(t) ϭ 1

(0 Ϲ t Ϲ 2).

Using (6) we calculate
1

2

Ύ Re z dz ϭ Ύ Re z dz ϩ Ύ Re z dz ϭ Ύ t dt ϩ Ύ 1 # i dt ϭ 2 ϩ 2i.
C

C1

C2

0

1

0

᭿

Note that this result differs from the result in (a).

Bounds for Integrals. ML-Inequality
There will be a frequent need for estimating the absolute value of complex line integrals.
The basic formula is

2

(13)

Ύ f (z) dz 2 Ϲ ML

(ML-inequality);

C

L is the length of C and M a constant such that ƒ f (z) ƒ Ϲ M everywhere on C.
PROOF

Taking the absolute value in (2) and applying the generalized inequality (6*) in Sec. 13.2,
we obtain
n

n

n

mϭ1

mϭ1

mϭ1

ƒ Sn ƒ ϭ 2 a f (zm) ¢z m 2 Ϲ a ƒ f (zm) ƒ ƒ ¢z m ƒ Ϲ M a ƒ ¢z m ƒ .
Now ƒ ¢z m ƒ is the length of the chord whose endpoints are z m؊1 and z m (see Fig. 340).
Hence the sum on the right represents the length L* of the broken line of chords whose
endpoints are z 0, z 1, Á , z n (ϭ Z ). If n approaches infinity in such a way that the greatest
ƒ ¢t m ƒ and thus ƒ ¢z m ƒ approach zero, then L* approaches the length L of the curve C, by
the definition of the length of a curve. From this the inequality (13) follows.
᭿
We cannot see from (13) how close to the bound ML the actual absolute value of the
integral is, but this will be no handicap in applying (13). For the time being we explain
the practical use of (13) by a simple example.

c14.qxd

11/1/10

6:02 PM

Page 651

SEC. 14.1 Line Integral in the Complex Plane
EXAMPLE 8

651

Estimation of an Integral
Find an upper bound for the absolute value of the integral

1

Ύz

C

2

C the straight-line segment from 0 to 1 ϩ i, Fig. 344.

dz,

C

1

Solution. L ϭ 12 and ƒ f (z) ƒ ϭ ƒ z 2 ƒ Ϲ 2 on C gives by (13)

Ύ

Fig. 344. Path in
Example 8

2 z 2 dz 2 Ϲ 212 ϭ 2.8284.
C

The absolute value of the integral is

ƒ Ϫ 23

᭿

ϩ 23 i ƒ ϭ 23 12 ϭ 0.9428 (see Example 1).

Summary on Integration. Line integrals of f (z) can always be evaluated by (10), using
a representation (1) of the path of integration. If f (z) is analytic, indefinite integration by
(9) as in calculus will be simpler (proof in the next section).

PROBLEM SET 14.1
1–10
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.

FIND THE PATH and sketch it.

z(t) ϭ (1 ϩ 12 i)t (2 Ϲ t Ϲ 5)
z(t) ϭ 3 ϩ i ϩ (1 Ϫ i)t (0 Ϲ t Ϲ 3)
z(t) ϭ t ϩ 2it 2 (1 Ϲ t Ϲ 2)
z(t) ϭ t ϩ (1 Ϫ t)2i (Ϫ1 Ϲ t Ϲ 1)
z(t) ϭ 3 Ϫ i ϩ 110e؊it (0 Ϲ t Ϲ 2p)
z(t) ϭ 1 ϩ i ϩ e؊pit (0 Ϲ t Ϲ 2)
z(t) ϭ 2 ϩ 4epit>2 (0 Ϲ t Ϲ 2)
z(t) ϭ 5e؊it (0 Ϲ t Ϲ p>2)
z(t) ϭ t ϩ it 3 (Ϫ2 Ϲ t Ϲ 2)
z(t) ϭ 2 cos t ϩ i sin t (0 Ϲ t Ϲ 2p)

11–20

FIND A PARAMETRIC REPRESENTATION

and sketch the path.
11. Segment from (Ϫ1, 1) to (1, 3)
12. From (0, 0) to (2, 1) along the axes
13. Upper half of ƒ z Ϫ 2 ϩ i ƒ ϭ 2 from (4, Ϫ1) to (0, Ϫ1)
14. Unit circle, clockwise
15. x 2 Ϫ 4y 2 ϭ 4, the branch through (2, 0)
16. Ellipse 4x 2 ϩ 9y 2 ϭ 36, counterclockwise
17. ƒ z ϩ a ϩ ib ƒ ϭ r, clockwise
18. y ϭ 1>x from (1, 1) to (5, 15)
19. Parabola y ϭ 1 Ϫ 14 x 2 (Ϫ2 Ϲ x Ϲ 2)
20. 4(x Ϫ 2)2 ϩ 5( y ϩ 1)2 ϭ 20
21–30

INTEGRATION

Integrate by the first method or state why it does not apply
and use the second method. Show the details.
21.

Ύ Re z dz, C the shortest path from 1 ϩ i to 3 ϩ 3i
C

22.

Ύ Re z dz, C the parabola y ϭ 1 ϩ

1
2 (x

Ϫ 1)2 from

C

1 ϩ i to 3 ϩ 3i
23.

Ύe

z

dz, C the shortest path from pi to 2pi

C

24.

Ύ cos 2z dz, C the semicircle

ƒ z ƒ ϭ p, x м 0 from

C

Ϫpi to pi
25.

Ύ z exp (z ) dz, C from 1 along the axes to i
2

C

26.

Ύ (z ϩ z

؊1

) dz, C the unit circle, counterclockwise

C

27.

Ύ sec

2

C

28.

z dz, any path from p>4 to pi>4

Ύ a z Ϫ 2i Ϫ (z Ϫ 2i) b dz, C the circle ƒ z Ϫ 2i ƒ ϭ 4,
6

5

2

C

clockwise
29.

Ύ Im z

2

dz counterclockwise around the triangle with

C

vertices 0, 1, i
30.

Ύ Re z
C

2

dz clockwise around the boundary of the square

with vertices 0, i, 1 ϩ i, 1
31. CAS PROJECT. Integration. Write programs for the
two integration methods. Apply them to problems of
your choice. Could you make them into a joint program
that also decides which of the two methods to use in a
given case?

c14.qxd

11/1/10

6:02 PM

652

Page 652

CHAP. 14 Complex Integration

32. Sense reversal. Verify (5) for f (z) ϭ z 2, where C is
the segment from Ϫ1 Ϫ i to 1 ϩ i.
33. Path partitioning. Verify (6) for f (z) ϭ 1>z and C1
and C2 the upper and lower halves of the unit circle.
34. TEAM EXPERIMENT. Integration. (a) Comparison.
First write a short report comparing the essential points
of the two integration methods.
(b) Comparison. Evaluate

Ύ f (z) dz by Theorem 1
C

and check the result by Theorem 2, where:
(i) f (z) ϭ z 4 and C is the semicircle ƒ z ƒ ϭ 2 from
Ϫ2i to 2i in the right half-plane,

14.2

(ii) f (z) ϭ e2z and C is the shortest path from 0 to
1 ϩ 2i.
(c) Continuous deformation of path. Experiment
with a family of paths with common endpoints, say,
z(t) ϭ t ϩ ia sin t, 0 Ϲ t Ϲ p, with real parameter a.
Integrate nonanalytic functions (Re z, Re (z 2), etc.) and
explore how the result depends on a. Then take analytic
functions of your choice. (Show the details of your
work.) Compare and comment.
(d) Continuous deformation of path. Choose another
family, for example, semi-ellipses z(t) ϭ a cos t ϩ
i sin t, Ϫp>2 Ϲ t Ϲ p>2, and experiment as in (c).
35. ML-inequality. Find an upper bound of the absolute
value of the integral in Prob. 21.

Cauchy’s Integral Theorem
This section is the focal point of the chapter. We have just seen in Sec. 14.1 that a line
integral of a function f (z) generally depends not merely on the endpoints of the path, but
also on the choice of the path itself. This dependence often complicates situations. Hence
conditions under which this does not occur are of considerable importance. Namely, if
f (z) is analytic in a domain D and D is simply connected (see Sec. 14.1 and also below),
then the integral will not depend on the choice of a path between given points. This result
(Theorem 2) follows from Cauchy’s integral theorem, along with other basic consequences
that make Cauchy’s integral theorem the most important theorem in this chapter and
fundamental throughout complex analysis.
Let us continue our discussion of simple connectedness which we started in Sec. 14.1.
1. A simple closed path is a closed path (defined in Sec. 14.1) that does not intersect
or touch itself as shown in Fig. 345. For example, a circle is simple, but a curve
shaped like an 8 is not simple.

Simple

Simple

Not simple

Not simple

Fig. 345. Closed paths

2. A simply connected domain D in the complex plane is a domain (Sec. 13.3) such
that every simple closed path in D encloses only points of D. Examples: The interior
of a circle (“open disk”), ellipse, or any simple closed curve. A domain that is not
simply connected is called multiply connected. Examples: An annulus (Sec. 13.3),
a disk without the center, for example, 0 Ͻ ƒ z ƒ Ͻ 1. See also Fig. 346.
More precisely, a bounded domain D (that is, a domain that lies entirely in some
circle about the origin) is called p-fold connected if its boundary consists of p closed

c14.qxd

11/1/10

6:02 PM

Page 653

SEC. 14.2 Cauchy’s Integral Theorem

653

Simply
connected

Simply
connected

Doubly
connected

Triply
connected

Fig. 346. Simply and multiply connected domains

connected sets without common points. These sets can be curves, segments, or single
points (such as z ϭ 0 for 0 Ͻ ƒ z ƒ Ͻ 1, for which p ϭ 2). Thus, D has p Ϫ 1 “holes,”
where “hole” may also mean a segment or even a single point. Hence an annulus
is doubly connected ( p ϭ 2).
THEOREM 1

Cauchy’s Integral Theorem

If f (z) is analytic in a simply connected domain D, then for every simple closed path
C in D,

Ώ f (z) dz ϭ 0.

(1)

See Fig. 347.

C

C

D

Fig. 347. Cauchy’s integral theorem

Before we prove the theorem, let us consider some examples in order to really understand
what is going on. A simple closed path is sometimes called a contour and an integral over
such a path a contour integral. Thus, (1) and our examples involve contour integrals.
EXAMPLE 1

Entire Functions

Ώe
C

z

dz ϭ 0,

Ώ cos z dz ϭ 0,
C

Ώz

n

dz ϭ 0

for any closed path, since these functions are entire (analytic for all z).

EXAMPLE 2

(n ϭ 0, 1, Á )

C

᭿

Points Outside the Contour Where f (x) is Not Analytic

Ώ sec z dz ϭ 0,
C

Ώ

C

dz
z2 ϩ 4

ϭ0

where C is the unit circle, sec z ϭ 1>cos z is not analytic at z ϭ Ϯp>2, Ϯ3p>2, Á , but all these points lie
outside C; none lies on C or inside C. Similarly for the second integral, whose integrand is not analytic at
z ϭ Ϯ2i outside C.
᭿

c14.qxd

11/1/10

6:02 PM

654

Page 654

CHAP. 14 Complex Integration

EXAMPLE 3

Nonanalytic Function

Ώ z dz ϭ Ύ
C

2p

e؊itieit dt ϭ 2pi

0

where C: z(t) ϭ eit is the unit circle. This does not contradict Cauchy’s theorem because f (z) ϭ z is not
analytic.
᭿

EXAMPLE 4

Analyticity Sufficient, Not Necessary

Ώ

dz

C

z2

ϭ0

where C is the unit circle. This result does not follow from Cauchy’s theorem, because f (z) ϭ 1>z 2 is not analytic
at z ϭ 0 . Hence the condition that f be analytic in D is sufficient rather than necessary for (1) to be true. ᭿

EXAMPLE 5

Simple Connectedness Essential

Ώ

C

dz
ϭ 2pi
z

for counterclockwise integration around the unit circle (see Sec. 14.1). C lies in the annulus 12 Ͻ ƒ z ƒ Ͻ 32 where
1>z is analytic, but this domain is not simply connected, so that Cauchy’s theorem cannot be applied. Hence the
condition that the domain D be simply connected is essential.
In other words, by Cauchy’s theorem, if f (z) is analytic on a simple closed path C and everywhere inside C,
with no exception, not even a single point, then (1) holds. The point that causes trouble here is z ϭ 0 where 1>z
is not analytic.
᭿

PROOF

Cauchy proved his integral theorem under the additional assumption that the derivative
f r(z) is continuous (which is true, but would need an extra proof). His proof proceeds as
follows. From (8) in Sec. 14.1 we have

Ώ f (z) dz ϭ Ώ (u dx Ϫ v dy) ϩ i Ώ (u dy ϩ v dx).
C

C

C

Since f (z) is analytic in D, its derivative f r(z) exists in D. Since f r(z) is assumed to be
continuous, (4) and (5) in Sec. 13.4 imply that u and v have continuous partial derivatives
in D. Hence Green’s theorem (Sec. 10.4) (with u and Ϫv instead of F1 and F2) is applicable
and gives

Ώ (u dx Ϫ v dy) ϭ Ύ Ύ aϪ 0v0x Ϫ 0u0y b dx dy
C

R

where R is the region bounded by C. The second Cauchy–Riemann equation (Sec. 13.4)
shows that the integrand on the right is identically zero. Hence the integral on the left is
zero. In the same fashion it follows by the use of the first Cauchy–Riemann equation that
the last integral in the above formula is zero. This completes Cauchy’s proof.
᭿
Goursat’s proof without the condition that f r(z) is continuous1 is much more complicated.
We leave it optional and include it in App. 4.
1

ÉDOUARD GOURSAT (1858–1936), French mathematician who made important contributions to complex
analysis and PDEs. Cauchy published the theorem in 1825. The removal of that condition by Goursat (see Transactions
Amer. Math Soc., vol. 1, 1900) is quite important because, for instance, derivatives of analytic functions are also
analytic. Because of this, Cauchy’s integral theorem is also called Cauchy–Goursat theorem.

c14.qxd

11/1/10

6:02 PM

Page 655

SEC. 14.2 Cauchy’s Integral Theorem

655

Independence of Path
We know from the preceding section that the value of a line integral of a given function
f (z) from a point z 1 to a point z 2 will in general depend on the path C over which we
integrate, not merely on z 1 and z 2. It is important to characterize situations in which this
difficulty of path dependence does not occur. This task suggests the following concept.
We call an integral of f (z) independent of path in a domain D if for every z 1, z 2 in D
its value depends (besides on f (z), of course) only on the initial point z 1 and the terminal
point z 2, but not on the choice of the path C in D [so that every path in D from z 1 to z 2
gives the same value of the integral of f (z)].
THEOREM 2

Independence of Path

If f (z) is analytic in a simply connected domain D, then the integral of f (z) is
independent of path in D.

PROOF

Let z 1 and z 2 be any points in D. Consider two paths C1 and C2 in D from z1 to z 2 without
further common points, as in Fig. 348. Denote by C2* the path C2 with the orientation
reversed (Fig. 349). Integrate from z1 over C1 to z 2 and over C2* back to z1. This is a
simple closed path, and Cauchy’s theorem applies under our assumptions of the present
theorem and gives zero:

Ύ

(2 r )

f dz ϩ

Ύ

f dz ϭ 0,

Ύ

thus

C2*

C1

f dz ϭ Ϫ

Ύ

f dz.

C2*

C1

But the minus sign on the right disappears if we integrate in the reverse direction, from
z 1 to z 2, which shows that the integrals of f (z) over C1 and C2 are equal,

Ύ

(2)

f (z) dz ϭ

C1

Ύ

f (z) dz

(Fig. 348).

C2

This proves the theorem for paths that have only the endpoints in common. For paths that
have finitely many further common points, apply the present argument to each “loop”
(portions of C1 and C2 between consecutive common points; four loops in Fig. 350). For
paths with infinitely many common points we would need additional argumentation not
to be presented here.
C1
C1

z2

z2

z2

C2
C1
C2

z1

Fig. 348. Formula (2)

C2*

z1

z1

Fig. 349. Formula (2Ј)

Fig. 350. Paths with more
common points

c14.qxd

11/1/10

6:02 PM

656

Page 656

CHAP. 14 Complex Integration

Principle of Deformation of Path
This idea is related to path independence. We may imagine that the path C2 in (2) was
obtained from C1 by continuously moving C1 (with ends fixed!) until it coincides with
C2. Figure 351 shows two of the infinitely many intermediate paths for which the integral
always retains its value (because of Theorem 2). Hence we may impose a continuous
deformation of the path of an integral, keeping the ends fixed. As long as our deforming
path always contains only points at which f (z) is analytic, the integral retains the same
value. This is called the principle of deformation of path.
C1
z2

C2
z1

Fig. 351. Continuous deformation of path

EXAMPLE 6

A Basic Result: Integral of Integer Powers
From Example 6 in Sec. 14.1 and the principle of deformation of path it follows that
(3)

Ώ (z Ϫ z )
0

m

dz ϭ b

2pi
0

(m ϭ Ϫ1)
(m

Ϫ1 and integer)

for counterclockwise integration around any simple closed path containing z 0 in its interior.
Indeed, the circle ƒ z Ϫ z 0 ƒ ϭ r in Example 6 of Sec. 14.1 can be continuously deformed in two steps into a path
as just indicated, namely, by first deforming, say, one semicircle and then the other one. (Make a sketch).
᭿

Existence of Indefinite Integral
We shall now justify our indefinite integration method in the preceding section [formula
(9) in Sec. 14.1]. The proof will need Cauchy’s integral theorem.
THEOREM 3

Existence of Indefinite Integral

If f (z) is analytic in a simply connected domain D, then there exists an indefinite
integral F(z) of f (z) in D—thus, F r(z) ϭ f (z)—which is analytic in D, and for all
paths in D joining any two points z 0 and z1 in D, the integral of f (z) from z 0 to z1
can be evaluated by formula (9) in Sec. 14.1.

PROOF

The conditions of Cauchy’s integral theorem are satisfied. Hence the line integral of f (z)
from any z 0 in D to any z in D is independent of path in D. We keep z 0 fixed. Then this
integral becomes a function of z, call if F(z),
(4)

F(z) ϭ

Ύ

z

z0

f (z*) dz*

c14.qxd

11/1/10

6:02 PM

Page 657

SEC. 14.2 Cauchy’s Integral Theorem

657

which is uniquely determined. We show that this F(z) is analytic in D and F r(z) ϭ f (z).
The idea of doing this is as follows. Using (4) we form the difference quotient
(5)

F(z ϩ ¢z) Ϫ F(z)
1
ϭ
c
¢z
¢z

Ύ

zϩ¢z

f (z*) dz* Ϫ

z0

Ύ

z

f (z*) dz* d ϭ

z0

1
¢z

Ύ

zϩ¢z

f (z*) dz*.

z

We now subtract f (z) from (5) and show that the resulting expression approaches zero as
¢z : 0. The details are as follows.
We keep z fixed. Then we choose z ϩ ¢z in D so that the whole segment with endpoints
z and z ϩ ¢z is in D (Fig. 352). This can be done because D is a domain, hence it contains
a neighborhood of z. We use this segment as the path of integration in (5). Now we subtract
f (z). This is a constant because z is kept fixed. Hence we can write

Ύ

zϩ¢z

f (z) dz* ϭ f (z)

z

Ύ

zϩ¢z

dz* ϭ f (z) ¢z.

1
f (z) ϭ
¢z

Thus

z

Ύ

zϩ¢z

f (z) dz*.

z

By this trick and from (5) we get a single integral:
F(z ϩ ¢z) Ϫ F(z)
1
Ϫ f (z) ϭ
¢z
¢z

Ύ

zϩ¢z

[ f (z*) Ϫ f (z)] dz*.

z

Since f (z) is analytic, it is continuous (see Team Project (24d) in Sec. 13.3). An P Ͼ 0
being given, we can thus find a d Ͼ 0 such that ƒ f (z*) Ϫ f (z) ƒ Ͻ P when ƒ z* Ϫ z ƒ Ͻ d.
Hence, letting ƒ ¢z ƒ Ͻ d, we see that the ML-inequality (Sec. 14.1) yields
`

F(z ϩ ¢z) Ϫ F(z)
¢z

Ϫ f (z) ` ϭ

1
ƒ ¢z ƒ

`

Ύ

z

zϩ¢z

[ f (z*) Ϫ f (z)] dz* ` Ϲ

1

P ƒ ¢z ƒ ϭ P.

ƒ ¢z ƒ

By the definition of limit and derivative, this proves that
F r(z) ϭ lim

F(z ϩ ¢z) Ϫ F(z)

¢z : 0

ϭ f (z).

¢z

Since z is any point in D, this implies that F(z) is analytic in D and is an indefinite integral
or antiderivative of f (z) in D, written
F(z) ϭ

Ύ f (z) dz.
z

z+ z

D
z0

Fig. 352. Path of integration

c14.qxd

11/1/10

6:02 PM

658

Page 658

CHAP. 14 Complex Integration

Also, if G r(z) ϭ f (z), then F r(z) Ϫ G r(z) ϵ 0 in D; hence F(z) Ϫ G(z) is constant in D
(see Team Project 30 in Problem Set 13.4). That is, two indefinite integrals of f (z) can
differ only by a constant. The latter drops out in (9) of Sec. 14.1, so that we can use any
indefinite integral of f (z). This proves Theorem 3.
᭿

Cauchy’s Integral Theorem
for Multiply Connected Domains
Cauchy’s theorem applies to multiply connected domains. We first explain this for a
doubly connected domain D with outer boundary curve C1 and inner C2 (Fig. 353). If
a function f (z) is analytic in any domain D* that contains D and its boundary curves, we
claim that
(6)

Ώ f (z) dz ϭ Ώ f (z) dz
C1

(Fig. 353)

C2

both integrals being taken counterclockwise (or both clockwise, and regardless of whether
or not the full interior of C2 belongs to D*).

C2
C1

Fig. 353. Paths in (5)

PROOF

ෂ
ෂ
By two cuts C1 and C2 (Fig. 354) we cut D into two simply connected domains D1 and
D2 in which and on whose boundaries f (z) is analytic. By Cauchy’s integral theorem the
integral over the entire boundary of D1 (taken in the sense of the arrows in Fig. 354) is
zero, and so is the integral over the boundary of D2, and thus their sum. In this sum the
ෂ
ෂ
integrals over the cuts C1 and C2 cancel because we integrate over them in both
directions—this is the key—and we are left with the integrals over C1 (counterclockwise)
and C2 (clockwise; see Fig. 354); hence by reversing the integration over C2 (to
counterclockwise) we have

Ώ

C1

and (6) follows.

f dz Ϫ

Ώ

f dz ϭ 0

C2

᭿

For domains of higher connectivity the idea remains the same. Thus, for a triply connected
ෂ ෂ ෂ
domain we use three cuts C1, C2, C3 (Fig. 355). Adding integrals as before, the integrals
over the cuts cancel and the sum of the integrals over C1 (counterclockwise) and C2, C3
(clockwise) is zero. Hence the integral over C1 equals the sum of the integrals over C2
and C3, all three now taken counterclockwise. Similarly for quadruply connected domains,
and so on.

c14.qxd

11/1/10

6:02 PM

Page 659

SEC. 14.2 Cauchy’s Integral Theorem

659

~

~

D1

C3

C2

~

~

C2

C2

C1
D2

~

C2

C1

C3

C1

C1

Fig. 354. Doubly connected domain

Fig. 355. Triply connected domain

PROBLEM SET 14.2
1–8

with common endpoints, say, z(t) ϭ t ϩ ia(t Ϫ t 2),
0 Ϲ t Ϲ 1, a a real constant, and experiment with the
integration of analytic and nonanalytic functions of
your choice over these paths (e.g., z, Im z, z 2, Re z 2,
Im z 2, etc.).

COMMENTS ON TEXT AND EXAMPLES

1. Cauchy’s Integral Theorem. Verify Theorem 1 for
the integral of z 2 over the boundary of the square with
vertices Ϯ1 Ϯ i. Hint. Use deformation.
2. For what contours C will it follow from Theorem 1 that
(a)

Ύ

dz
ϭ 0,
z
C

(b)

Ύ

C

exp (1>z 2)
z ϩ 16
2

CAUCHY’S THEOREM APPLICABLE?

9–19
dz ϭ 0 ?

3. Deformation principle. Can we conclude from
Example 4 that the integral is also zero over the contour
in Prob. 1?
4. If the integral of a function over the unit circle equals
2 and over the circle of radius 3 equals 6, can the
function be analytic everywhere in the annulus
1 Ͻ ƒ z ƒ Ͻ 3?
5. Connectedness. What is the connectedness of the
domain in which (cos z 2)>(z 4 ϩ 1) is analytic?

Integrate f (z) counterclockwise around the unit circle.
Indicate whether Cauchy’s integral theorem applies. Show
the details.
9. f (z) ϭ exp (Ϫz 2)
10. f (z) ϭ tan 14 z
11. f (z) ϭ 1>(2z Ϫ 1)
12. f (z) ϭ z 3
4
13. f (z) ϭ 1>(z Ϫ 1.1)
14. f (z) ϭ 1> z
15. f (z) ϭ Im z
16. f (z) ϭ 1>(pz Ϫ 1)
17. f (z) ϭ 1> ƒ z ƒ 2
18. f (z) ϭ 1>(4z Ϫ 3)
19. f (z) ϭ z 3 cot z

FURTHER CONTOUR INTEGRALS

20–30

6. Path independence. Verify Theorem 2 for the integral
of ez from 0 to 1 ϩ i (a) over the shortest path and
(b) over the x-axis to 1 and then straight up to 1 ϩ i.

Evaluate the integral. Does Cauchy’s theorem apply? Show
details.

7. Deformation. Can we conclude in Example 2 that
the integral of 1>(z 2 ϩ 4) over (a) ƒ z Ϫ 2 ƒ ϭ 2 and
(b) ƒ z Ϫ 2 ƒ ϭ 3 is zero?

20.

8. TEAM EXPERIMENT. Cauchy’s Integral Theorem.
(a) Main Aspects. Each of the problems in Examples
1–5 explains a basic fact in connection with Cauchy’s
theorem. Find five examples of your own, more
complicated ones if possible, each illustrating one of
those facts.
(b) Partial fractions. Write f (z) in terms of partial
fractions and integrate it counterclockwise over the unit
circle, where
(i)

f (z) ϭ

2z ϩ 3i
1

z2 ϩ 4

(ii)

f (z) ϭ

zϩ1
z 2 ϩ 2z

.

(c) Deformation of path. Review (c) and (d) of Team
Project 34, Sec. 14.1, in the light of the principle of deformation of path. Then consider another family of paths

Ώ Ln (1 Ϫ z) dz, C the boundary of the parallelogram
C

with vertices Ϯi, Ϯ(1 ϩ i).
21.

Ώ

C

22.

dz
, C the circle ƒ z ƒ ϭ p counterclockwise.
z Ϫ 3i

Ώ Re z dz,

y

C:

C

C

1 x

–1

23.

Ώ

C

2z Ϫ 1
z Ϫz
2

y

dz, C:

C

2

Use partial fractions.

x

c14.qxd

11/1/10

6:02 PM

Page 660

660

24.

CHAP. 14 Complex Integration

Ώ

C

y

dz
z2 Ϫ 1

,

C:

27.

C

C

–1

x

1

Ώ

C

ez
z dz,

Ώ
Ώ

C consists of ƒ z ƒ ϭ 2 counterclockwise and

ƒ z ƒ ϭ 1 clockwise.
26.

Ώ coth

cos z
z dz, C consists of ƒ z ƒ ϭ 1 counterclockwise

and ƒ z ƒ ϭ 3 clockwise.
tan 12 z
28.
dz, C the boundary of the square with
4
C z Ϫ 16
vertices Ϯ1, Ϯi clockwise.
sin z
29.
dz, C: ƒ z Ϫ 4 Ϫ 2i ƒ ϭ 5.5 clockwise.
z ϩ 2iz
C

Use partial fractions.
25.

Ώ

30.

Ώ

2z 3 ϩ z 2 ϩ 4

z 4 ϩ 4z 2
partial fractions.

dz,

C: ƒ z Ϫ 2 ƒ ϭ 4 clockwise. Use

C

1
2z

dz, C the circle ƒ z Ϫ 12 pi ƒ ϭ 1 clockwise.

C

14.3

Cauchy’s Integral Formula
Cauchy’s integral theorem leads to Cauchy’s integral formula. This formula is useful for
evaluating integrals as shown in this section. It has other important roles, such as in proving
the surprising fact that analytic functions have derivatives of all orders, as shown in the
next section, and in showing that all analytic functions have a Taylor series representation
(to be seen in Sec. 15.4).

THEOREM 1

Cauchy’s Integral Formula

Let f (z) be analytic in a simply connected domain D. Then for any point z 0 in D
and any simple closed path C in D that encloses z 0 (Fig. 356),

Ώ

(1)

C

f (z)
z Ϫ z 0 dz ϭ 2pif (z 0)

(Cauchy’s integral formula)

the integration being taken counterclockwise. Alternatively (for representing f (z 0)
by a contour integral, divide (1) by 2pi),
f (z 0) ϭ

(1*)

2pi Ώ
1

C

PROOF

f (z)
dz
z Ϫ z0

(Cauchy’s integral formula).

By addition and subtraction, f (z) ϭ f (z 0) ϩ [ f (z) Ϫ f (z 0)]. Inserting this into (1) on the
left and taking the constant factor f (z 0) out from under the integral sign, we have
(2)

Ώ

C

f (z)
z Ϫ z 0 dz ϭ f (z 0)

Ώ

C

dz
z Ϫ z0 ϩ

Ώ

C

f (z) Ϫ f (z 0)
z Ϫ z 0 dz.

The first term on the right equals f (z 0) # 2pi, which follows from Example 6 in Sec. 14.2
with m ϭ Ϫ1. If we can show that the second integral on the right is zero, then it would
prove the theorem. Indeed, we can. The integrand of the second integral is analytic, except

c14.qxd

11/1/10

6:02 PM

Page 661

SEC. 14.3 Cauchy’s Integral Formula

661

at z 0. Hence, by (6) in Sec. 14.2, we can replace C by a small circle K of radius r and
center z 0 (Fig. 357), without altering the value of the integral. Since f (z) is analytic, it is
continuous (Team Project 24, Sec. 13.3). Hence, an P Ͼ 0 being given, we can find a
d Ͼ 0 such that ƒ f (z) Ϫ f (z 0) ƒ Ͻ P for all z in the disk ƒ z Ϫ z 0 ƒ Ͻ d. Choosing the radius
r of K smaller than d, we thus have the inequality

D
z0

ρ
C

C

Fig. 356. Cauchy’s integral formula

z0

K

Fig. 357. Proof of Cauchy’s integral formula

2

f (z) Ϫ f (z 0)
2ϽP
z Ϫ z0
r

at each point of K. The length of K is 2pr. Hence, by the ML-inequality in Sec. 14.1,

2

Ώ

K

f (z) Ϫ f (z 0)
2 P
z Ϫ z 0 dz Ͻ r 2pr ϭ 2pP.

Since P (Ͼ 0) can be chosen arbitrarily small, it follows that the last integral in (2) must
᭿
have the value zero, and the theorem is proved.
EXAMPLE 1

Cauchy’s Integral Formula

Ώ

C

ez
zϪ2

dz ϭ 2piez `

ϭ 2pie2 ϭ 46.4268i
zϭ2

for any contour enclosing z 0 ϭ 2 (since ez is entire), and zero for any contour for which z 0 ϭ 2 lies outside
᭿
(by Cauchy’s integral theorem).

EXAMPLE 2

Cauchy’s Integral Formula

Ώ

C

z3 Ϫ 6
2z Ϫ i

dz ϭ

Ώ

C

1 3
2z

Ϫ3

z Ϫ 12 i

dz

ϭ 2pi 312 z 3 Ϫ 34 ƒ zϭi>2
ϭ

EXAMPLE 3

p
8

Ϫ 6pi

Integration Around Different Contours
Integrate
g(z) ϭ

z2 ϩ 1
z Ϫ1
2

ϭ

z2 ϩ 1
(z ϩ 1)(z Ϫ 1)

counterclockwise around each of the four circles in Fig. 358.

(z 0 ϭ 12 i inside C ).

᭿

c14.qxd

11/1/10

662

6:02 PM

Page 662

CHAP. 14 Complex Integration

Solution. g(z) is not analytic at Ϫ1 and 1. These are the points we have to watch for. We consider each
circle separately.
(a) The circle ƒ z Ϫ 1 ƒ ϭ 1 encloses the point z 0 ϭ 1 where g(z) is not analytic. Hence in (1) we have to
write
g(z) ϭ

z2 ϩ 1
z Ϫ1
2

ϭ

z2 ϩ 1

1

zϩ1 zϪ1

;

thus
f (z) ϭ

z2 ϩ 1
zϩ1

and (1) gives

Ώ

C

z2 ϩ 1
z Ϫ1
2

dz ϭ 2pif (1) ϭ 2pi c

z2 ϩ 1
zϩ1

d

ϭ 2pi.
zϭ1

(b) gives the same as (a) by the principle of deformation of path.
(c) The function g(z) is as before, but f (z) changes because we must take z 0 ϭ Ϫ1 (instead of 1). This gives
a factor z Ϫ z 0 ϭ z ϩ 1 in (1). Hence we must write
g(z) ϭ

z2 ϩ 1

1

zϪ1 zϩ1

;

thus
f (z) ϭ

z2 ϩ 1
zϪ1

.

Compare this for a minute with the previous expression and then go on:

Ώ

C

z2 ϩ 1
z Ϫ1
2

dz ϭ 2pif (Ϫ1) ϭ 2pi c

z2 ϩ 1
zϪ1

d

ϭ Ϫ2pi.
zϭ؊1

᭿

(d) gives 0. Why?

y
(d)
(c)

(a)

–1

1

x

(b)

Fig. 358. Example 3

Multiply connected domains can be handled as in Sec. 14.2. For instance, if f (z) is
analytic on C1 and C2 and in the ring-shaped domain bounded by C1 and C2 (Fig. 359)
and z 0 is any point in that domain, then
(3)

f (z 0) ϭ

1
2pi

Ώ

C1

f (z)
1
dz ϩ
z Ϫ z0
2pi

Ώ

C2

f (z)
dz,
z Ϫ z0

c14.qxd

11/1/10

6:02 PM

Page 663

SEC. 14.3 Cauchy’s Integral Formula

663

where the outer integral (over C1) is taken counterclockwise and the inner clockwise, as
indicated in Fig. 359.

z0
C2

C1

Fig. 359. Formula (3)

PROBLEM SET 14.3
CONTOUR INTEGRATION

1–4

Integrate z 2>(z 2 Ϫ 1) by Cauchy’s formula counterclockwise
around the circle.
1. ƒ z ϩ 1 ƒ ϭ 1
2. ƒ z Ϫ 1 Ϫ i ƒ ϭ p>2
3. ƒ z ϩ i ƒ ϭ 1.4
4. ƒ z ϩ 5 Ϫ 5i ƒ ϭ 7
5–8

15.

16.

Integrate the given function around the unit circle.

5. (cos 3z)>(6z)
6. e2z>(pz Ϫ i)
3
7. z >(2z Ϫ i)
8. (z 2 sin z)>(4z Ϫ 1)
9. CAS EXPERIMENT. Experiment to find out to what
extent your CAS can do contour integration. For this,
use (a) the second method in Sec. 14.1 and (b) Cauchy’s
integral formula.
10. TEAM PROJECT. Cauchy’s Integral Theorem.
Gain additional insight into the proof of Cauchy’s
integral theorem by producing (2) with a contour
enclosing z 0 (as in Fig. 356) and taking the limit as in
the text. Choose
(a)

Ώ

C

z Ϫ6
3

zϪ

1
2i

dz,

(b)

Ώ

C

sin z
1
2

zϪ p

17.

cosh (z 2 Ϫ pi)
dz, C the boundary of the square
z Ϫ pi
C
with vertices Ϯ2, Ϯ2, Ϯ4i.

Ώ
Ώ

tan z
dz, C the boundary of the triangle with
z
Ϫi
C
vertices 0 and Ϯ1 ϩ 2i.

Ώ

C

18.

Ώ

Ln (z ϩ 1)
z2 ϩ 1
sin z

dz, C consists of the boundaries of the
4z Ϫ 8iz
squares with vertices Ϯ3, Ϯ3i counterclockwise and
Ϯ1, Ϯi clockwise (see figure).
C

y
3i
2i

dz,
–3

z

dz,

C the circle with center Ϫ1 and

z 2 ϩ 4z ϩ 3
radius 2
zϩ2
13.
dz, C: ƒ z Ϫ 1 ƒ ϭ 2
zϪ2
C
C

Ώ

14.

Ώ

C

x

–3i

Problem 18

Ώ
Ώ

3

FURTHER CONTOUR INTEGRALS

Integrate counterclockwise or as indicated. Show the
details.
dz
11.
, C: 4x 2 ϩ ( y Ϫ 2)2 ϭ 4
2
z
ϩ
4
C
12.

C: ƒ z Ϫ i ƒ ϭ 1.4

2

and (c) another example of your choice.
11–19

dz,

ez
dz, C: ƒ z ƒ ϭ 0.6
z
ze Ϫ 2iz

19.

Ώ

exp z 2

dz, C consists of ƒ z ƒ ϭ 2 counterz 2(z Ϫ 1 Ϫ i)
clockwise and ƒ z ƒ ϭ 1 clockwise.
C

20. Show that

Ώ (z Ϫ z )
1

؊1

(z Ϫ z 2)؊1 dz ϭ 0 for a simple

C

closed path C enclosing z 1 and z 2, which are
arbitrary.

c14.qxd

11/1/10

6:02 PM

664

Page 664

CHAP. 14 Complex Integration

14.4

Derivatives of Analytic Functions
As mentioned, a surprising fact is that complex analytic functions have derivatives of all
orders. This differs completely from real calculus. Even if a real function is once
differentiable we cannot conclude that it is twice differentiable nor that any of its higher
derivatives exist. This makes the behavior of complex analytic functions simpler than real
functions in this aspect. To prove the surprising fact we use Cauchy’s integral formula.

THEOREM 1

Derivatives of an Analytic Function

If f (z) is analytic in a domain D, then it has derivatives of all orders in D, which
are then also analytic functions in D. The values of these derivatives at a point z 0
in D are given by the formulas
(1 r )

(1 s )

f r(z 0) ϭ

f s(z 0) ϭ

1
2pi

Ώ

2!
2pi

Ώ

n!
2pi

Ώ

C

C

f (z)
dz
(z Ϫ z 0)2
f (z)
(z Ϫ z 0)3

dz

and in general
(1)

f (n)(z 0) ϭ

C

f (z)
(z Ϫ z 0)nϩ1

dz

(n ϭ 1, 2, Á );

here C is any simple closed path in D that encloses z 0 and whose full interior belongs
to D; and we integrate counterclockwise around C (Fig. 360).

D

d
z0
C

Fig. 360. Theorem 1 and its proof

COMMENT. For memorizing (1), it is useful to observe that these formulas are obtained
formally by differentiating the Cauchy formula (1*), Sec. 14.3, under the integral sign
with respect to z 0.
PROOF

We prove (1 r ), starting from the definition of the derivative
f r(z 0) ϭ lim

¢z:0

f (z 0 ϩ ¢z) Ϫ f (z 0)
¢z

.

c14.qxd

11/1/10

6:02 PM

Page 665

SEC. 14.4 Derivatives of Analytic Functions

665

On the right we represent f (z 0 ϩ ¢z) and f (z 0) by Cauchy’s integral formula:
f (z 0 ϩ ¢z) Ϫ f (z 0)

ϭ

¢z

1
B
2pi¢z

Ώ

C

f (z)

dz Ϫ

z Ϫ (z 0 ϩ ¢z)

Ώ

C

f (z)
z Ϫ z0

dzR .

We now write the two integrals as a single integral. Taking the common denominator
gives the numerator f (z){z Ϫ z 0 Ϫ [z Ϫ (z 0 ϩ ¢z)]} ϭ f (z) ¢z, so that a factor ¢z drops
out and we get
f (z 0 ϩ ¢z) Ϫ f (z 0)

ϭ

¢z

2pi Ώ

f (z)

1

C

(z Ϫ z 0 Ϫ ¢z)(z Ϫ z 0)

dz.

Clearly, we can now establish (1 r ) by showing that, as ¢z : 0, the integral on the right
approaches the integral in (1 r ). To do this, we consider the difference between these two
integrals. We can write this difference as a single integral by taking the common
denominator and simplifying the numerator (as just before). This gives

Ώ

C

f (z)
(z Ϫ z 0 Ϫ ¢z)(z Ϫ z 0)

dz Ϫ

Ώ

C

f (z)
(z Ϫ z 0)

2

dz ϭ

Ώ

C

f (z) ¢z
(z Ϫ z 0 Ϫ ¢z)(z Ϫ z 0)2

dz.

We show by the ML-inequality (Sec. 14.1) that the integral on the right approaches zero
as ¢z : 0.
Being analytic, the function f (z) is continuous on C, hence bounded in absolute value,
say, ƒ f (z) ƒ Ϲ K. Let d be the smallest distance from z 0 to the points of C (see Fig. 360).
Then for all z on C,
ƒ z Ϫ z 0 ƒ 2 м d 2,

hence

1
ƒ z Ϫ z0 ƒ

2

Ϲ

1
d2

.

Furthermore, by the triangle inequality for all z on C we then also have
d Ϲ ƒ z Ϫ z 0 ƒ ϭ ƒ z Ϫ z 0 Ϫ ¢z ϩ ¢z ƒ Ϲ ƒ z Ϫ z 0 Ϫ ¢z ƒ ϩ ƒ ¢z ƒ .
We now subtract ƒ ¢z ƒ on both sides and let ƒ ¢z ƒ Ϲ d>2, so that Ϫ ƒ ¢z ƒ м Ϫd>2. Then
1
2d

Ϲ d Ϫ ƒ ¢z ƒ Ϲ ƒ z Ϫ z 0 Ϫ ¢z ƒ .

Hence

2
1
Ϲ .
ƒ z Ϫ z 0 Ϫ ¢z ƒ
d

Let L be the length of C. If ƒ ¢z ƒ Ϲ d>2, then by the ML-inequality

2

Ώ (z Ϫ z
C

f (z) ¢z
0

Ϫ ¢z)(z Ϫ z 0)

2

dz 2 Ϲ KL ƒ ¢z ƒ

2 # 1
.
d d2

This approaches zero as ¢z : 0. Formula (1 r ) is proved.
Note that we used Cauchy’s integral formula (1*), Sec. 14.3, but if all we had known
about f (z 0) is the fact that it can be represented by (1*), Sec. 14.3, our argument would
have established the existence of the derivative f r(z 0) of f (z). This is essential to the

c14.qxd

11/1/10

6:02 PM

666

Page 666

CHAP. 14 Complex Integration

continuation and completion of this proof, because it implies that (1 s ) can be proved by
a similar argument, with f replaced by f r , and that the general formula (1) follows by
induction.
᭿

Applications of Theorem 1
EXAMPLE 1

Evaluation of Line Integrals
From (1 r ), for any contour enclosing the point pi (counterclockwise)

Ώ

C

EXAMPLE 2

᭿

From (1 s ), for any contour enclosing the point Ϫi we obtain by counterclockwise integration

Ώ

C

EXAMPLE 3

cos z
dz ϭ 2pi(cos z) r `
ϭ Ϫ2pi sin pi ϭ 2p sinh p.
(z Ϫ pi)2
zϭpi

z 4 Ϫ 3z 2 ϩ 6
dz ϭ pi(z 4 Ϫ 3z 2 ϩ 6) s `
ϭ pi [12z 2 Ϫ 6]zϭ؊i ϭ Ϫ18pi.
(z ϩ i)3
zϭ؊i

᭿

By (1 r ), for any contour for which 1 lies inside and Ϯ2i lie outside (counterclockwise),

Ώ

r
ez
ez
dz ϭ 2pi a 2
b `
2
(z Ϫ 1) (z ϩ 4)
z ϩ4
zϭ1
2

C

ϭ 2pi

ez(z 2 ϩ 4) Ϫ ez2z
(z 2 ϩ 4)2

`

ϭ
zϭ1

6ep
25

i Ϸ 2.050i.

᭿

Cauchy’s Inequality. Liouville’s and Morera’s Theorems
We develop other general results about analytic functions, further showing the versatility
of Cauchy’s integral theorem.
Cauchy’s Inequality. Theorem 1 yields a basic inequality that has many applications.
To get it, all we have to do is to choose for C in (1) a circle of radius r and center z 0 and
apply the ML-inequality (Sec. 14.1); with ƒ f (z) ƒ Ϲ M on C we obtain from (1)
ƒ f (n)(z 0) ƒ ϭ

n!
2p

2

Ώ (z Ϫ z )
f (z)

C

0

nϩ1

dz 2 Ϲ

n!
2p

M

1
r

nϩ1

2pr.

This gives Cauchy’s inequality
ƒ f (n)(z 0) ƒ Ϲ

(2)

n!M
.
rn

To gain a first impression of the importance of this inequality, let us prove a famous
theorem on entire functions (definition in Sec. 13.5). (For Liouville, see Sec. 11.5.)
THEOREM 2

Liouville’s Theorem

If an entire function is bounded in absolute value in the whole complex plane, then
this function must be a constant.

c14.qxd

11/1/10

6:02 PM

Page 667

SEC. 14.4 Derivatives of Analytic Functions

PROOF

667

By assumption, ƒ f (z) ƒ is bounded, say, ƒ f (z) ƒ Ͻ K for all z. Using (2), we see that
ƒ f r(z 0) ƒ Ͻ K>r. Since f (z) is entire, this holds for every r, so that we can take r as large
as we please and conclude that f r(z 0) ϭ 0. Since z 0 is arbitrary, f r(z) ϭ u x ϩ ivx ϭ 0 for
all z (see (4) in Sec. 13.4), hence u x ϭ vx ϭ 0, and u y ϭ vy ϭ 0 by the Cauchy–Riemann
equations. Thus u ϭ const, v ϭ const, and f ϭ u ϩ iv ϭ const for all z. This completes
the proof.
᭿
Another very interesting consequence of Theorem 1 is
Morera’s2 Theorem (Converse of Cauchy’s Integral Theorem)

THEOREM 3

If f (z) is continuous in a simply connected domain D and if

Ώ f (z) dz ϭ 0

(3)

C

for every closed path in D, then f (z) is analytic in D.

PROOF

In Sec. 14.2 we showed that if f (z) is analytic in a simply connected domain D, then
z

F(z) ϭ

Ύ f (z*) dz*
z0

is analytic in D and F r(z) ϭ f (z). In the proof we used only the continuity of f (z) and the
property that its integral around every closed path in D is zero; from these assumptions
we concluded that F(z) is analytic. By Theorem 1, the derivative of F(z) is analytic, that
᭿
is, f (z) is analytic in D, and Morera’s theorem is proved.
This completes Chapter 14.

PROBLEM SET 14.4
CONTOUR INTEGRATION. UNIT CIRCLE

1–7

Integrate counterclockwise around the unit circle.
1.

Ώ

sin z

Ώ

ez
dz,
zn

Ώ

cosh 2z

C

3.

C

5.

C

7.

Ώ

C

z4

2.

dz

C

n ϭ 1, 2, Á

z

dz,
2nϩ1

4.

Ώ

C

6.

dz
(z Ϫ 12)4
cos z

Ώ
Ώ

C

n ϭ 0, 1, Á

2

z6
(2z Ϫ 1)6

8–19

Integrate. Show the details. Hint. Begin by sketching the
contour. Why?

dz

ez cos z

(z Ϫ p>4)3

INTEGRATION. DIFFERENT CONTOURS

8.

Ώ

z 3 ϩ sin z

Ώ

tan pz

dz, C the boundary of the square with
(z Ϫ i)3
vertices Ϯ2, Ϯ2i counterclockwise.
C

dz

dz

9.

C

(z Ϫ 2i)2(z Ϫ i>2)2
10.

Ώ

z2

dz, C the ellipse 16x 2 ϩ y 2 ϭ 1 clockwise.

4z 3 Ϫ 6

dz, C consists of ƒ z ƒ ϭ 3 counterz(z Ϫ 1 Ϫ i)2
clockwise and ƒ z ƒ ϭ 1 clockwise.
C

GIACINTO MORERA (1856–1909), Italian mathematician who worked in Genoa and Turin.

c14.qxd

11/1/10

6:02 PM

Page 668

668

11.

CHAP. 14 Complex Integration

Ώ

C

12.

Ώ

C

13.

Ώ

C

14.

Ώ

(1 ϩ z) sin z
(2z Ϫ 1)2

dz, C: ƒ z Ϫ i ƒ ϭ 2 counterclockwise.

exp (z )
z(z Ϫ 2i)

2

dz, C: z Ϫ 3i ƒ ϭ 2 clockwise.

Ln z

dz,
(z Ϫ 2)2

C: ƒ z Ϫ 3 ƒ ϭ 2 counterclockwise.

Ln (z ϩ 3)

cosh 4z

dz, C consists of ƒ z ƒ ϭ 6 counterclock(z Ϫ 4)3
wise and ƒ z Ϫ 3 ƒ ϭ 2 clockwise.
C

16.

Ώ

e4z

Ώ

e؊z sin z

dz, C consists of ƒ z Ϫ i ƒ ϭ 3 counterz(z Ϫ 2i)2
clockwise and ƒ z ƒ ϭ 1 clockwise.
C

17.

sinh z
dz, C: ƒ z ƒ ϭ 1 counterclockwise, n integer.
zn

19.

Ώ

e3z
dz,
(4z Ϫ pi)3

C

dz, C the boundary of the square
(z Ϫ 2)(z ϩ 1)2
with vertices Ϯ1.5, Ϯ1.5i, counterclockwise.

Ώ

Ώ

C

2

C

15.

18.

dz, C consists of ƒ z ƒ ϭ 5 counterclock(z Ϫ 4)3
wise and ƒ z Ϫ 3 ƒ ϭ 32 clockwise.
C

C: ƒ z ƒ ϭ 1, counterclockwise.

20. TEAM PROJECT. Theory on Growth
(a) Growth of entire functions. If f (z) is not a
constant and is analytic for all (finite) z, and R and
M are any positive real numbers (no matter how
large), show that there exist values of z for which
ƒ z ƒ Ͼ R and ƒ f (z) ƒ Ͼ M. Hint. Use Liouville’s
theorem.
(b) Growth of polynomials. If f (z) is a polynomial
of degree n Ͼ 0 and M is an arbitrary positive
real number (no matter how large), show that
there exists a positive real number R such that
ƒ f (z) ƒ Ͼ M for all ƒ z ƒ Ͼ R.
(c) Exponential function. Show that f (z) ϭ ex has
the property characterized in (a) but does not have
that characterized in (b).
(d) Fundamental theorem of algebra. If f (z) is a
polynomial in z, not a constant, then f (z) ϭ 0 for
at least one value of z. Prove this. Hint. Use (a).

CHAPTER 14 REVIEW QUESTIONS AND PROBLEMS
1. What is a parametric representation of a curve? What
is its advantage?
2. What did we assume about paths of integration z ϭ z(t)?
#
What is z ϭ dz>dt geometrically?
3. State the definition of a complex line integral from
memory.
4. Can you remember the relationship between complex
and real line integrals discussed in this chapter?
5. How can you evaluate a line integral of an analytic
function? Of an arbitrary continous complex function?
6. What value do you get by counterclockwise integration
of 1>z around the unit circle? You should remember
this. It is basic.
7. Which theorem in this chapter do you regard as most
important? State it precisely from memory.
8. What is independence of path? Its importance? State a
basic theorem on independence of path in complex.
9. What is deformation of path? Give a typical example.
10. Don’t confuse Cauchy’s integral theorem (also known
as Cauchy–Goursat theorem) and Cauchy’s integral
formula. State both. How are they related?
11. What is a doubly connected domain? How can you
extend Cauchy’s integral theorem to it?

12. What do you know about derivatives of analytic
functions?
13. How did we use integral formulas for derivatives in
evaluating integrals?
14. How does the situation for analytic functions differ
with respect to derivatives from that in calculus?
15. What is Liouville’s theorem? To what complex functions does it apply?
16. What is Morera’s theorem?
17. If the integrals of a function f (z) over each of the two
boundary circles of an annulus D taken in the same
sense have different values, can f (z) be analytic everywhere in D? Give reason.
18. Is Im

Ώ f (z) dz ϭ Ώ Im f (z) dz? Give reason.
C

19. Is 2

C

Ώ f (z) dz 2 ϭ Ώ ƒ f (z) ƒ dz?
C

C

20. How would you find a bound for the left side in Prob. 19?
21–30

INTEGRATION

Integrate by a suitable method.
21.

Ύ z sinh (z ) dz from 0 to pi>2.
2

C

c14.qxd

11/1/10

6:02 PM

Page 669

Summary of Chapter 14

22.

669

Ύ ( ƒ z ƒ ϩ z) dz clockwise around the unit circle.

27.

Ύz

28.

C

23.

؊5 z

e dz counterclockwise around ƒ z ƒ ϭ p.

Ώ

C

Ύ Re z dz from 0 to 3 ϩ 27i along y ϭ x .
3

29.

C

Ύ

26.

Ύ (z

Ln z
(z Ϫ 2i)2

dz counterclockwise around ƒ z Ϫ 1 ƒ ϭ 12.

Ώ a z ϩ 2i ϩ z ϩ 4i b dz clockwise around
2

1

ƒ z Ϫ 1 ƒ ϭ 2.5.

dz clockwise around ƒ z Ϫ 1 ƒ ϭ 0.1.
2
C (z Ϫ 1)
2

ϩ z 2) dz from 0 to 2 ϩ 2i, shortest path.

C

tan pz

25.

2

C

C

24.

Ύ (z

30.

ϩ z 2) dz from z ϭ 0 horizontally to z ϭ 2, then

Ύ sin z dz from 0 to (1 ϩ i).
C

C

vertically upward to 2 ϩ 2i.

SUMMARY OF CHAPTER

14

Complex Integration
The complex line integral of a function f (z) taken over a path C is denoted by
(1)

Ύ f (z) dz

Ώ f (z)

or, if C is closed, also by

(Sec. 14.1).

C

C

If f (z) is analytic in a simply connected domain D, then we can evaluate (1) as in
calculus by indefinite integration and substitution of limits, that is,

Ύ f (z) dz ϭ F(z ) Ϫ F(z )

(2)

1

[F r(z) ϭ f (z)]

0

C

for every path C in D from a point z 0 to a point z1 (see Sec. 14.1). These assumptions
imply independence of path, that is, (2) depends only on z 0 and z1 (and on f (z),
of course) but not on the choice of C (Sec. 14.2). The existence of an F(z) such that
F r(z) ϭ f (z) is proved in Sec. 14.2 by Cauchy’s integral theorem (see below).
A general method of integration, not restricted to analytic functions, uses the
equation z ϭ z(t) of C, where a Ϲ t Ϲ b,
(3)

Ύ

C

b

f (z) dz ϭ

Ύ f (z(t))z# (t) dt
a

az ϭ

#

dz
b.
dt

Cauchy’s integral theorem is the most important theorem in this chapter. It states
that if f (z) is analytic in a simply connected domain D, then for every closed path
C in D (Sec. 14.2),
(4)

Ώ f (z) dz ϭ 0.
C

c14.qxd

11/1/10

670

6:02 PM

Page 670

CHAP. 14 Complex Integration

Under the same assumptions and for any z 0 in D and closed path C in D containing
z 0 in its interior we also have Cauchy’s integral formula
(5)

f (z 0) ϭ

2pi Ώ
1

C

f (z)
dz.
z Ϫ z0

Furthermore, under these assumptions f (z) has derivatives of all orders in D that are
themselves analytic functions in D and (Sec. 14.4)
(6)

f (n)(z 0) ϭ

Ώ
2pi
n!

C

f (z)
(z Ϫ z 0)nϩ1

dz

(n ϭ 1, 2, Á ).

This implies Morera’s theorem (the converse of Cauchy’s integral theorem) and
Cauchy’s inequality (Sec. 14.4), which in turn implies Liouville’s theorem that an
entire function that is bounded in the whole complex plane must be constant.

c15.qxd

11/9/10

3:35 PM

Page 671

CHAPTER

15

Power Series, Taylor Series
In Chapter 14, we evaluated complex integrals directly by using Cauchy’s integral formula,
which was derived from the famous Cauchy integral theorem. We now shift from the
approach of Cauchy and Goursat to another approach of evaluating complex integrals,
that is, evaluating them by residue integration. This approach, discussed in Chapter 16,
first requires a thorough understanding of power series and, in particular, Taylor series.
(To develop the theory of residue integration, we still use Cauchy’s integral theorem!)
In this chapter, we focus on complex power series and in particular Taylor series. They
are analogs of real power series and Taylor series in calculus. Section 15.1 discusses
convergence tests for complex series, which are quite similar to those for real series. Thus,
if you are familiar with convergence tests from calculus, you may use Sec. 15.1 as a
reference section. The main results of this chapter are that complex power series represent
analytic functions, as shown in Sec. 15.3, and that, conversely, every analytic function
can be represented by power series, called a Taylor series, as shown in Sec. 15.4. The last
section (15.5) on uniform convergence is optional.
Prerequisite: Chaps. 13, 14.
Sections that may be omitted in a shorter course: 15.1, 15.5.
References and Answers to Problems: App. 1 Part D, App. 2.

15.1

Sequences, Series, Convergence Tests
The basic concepts for complex sequences and series and tests for convergence and
divergence are very similar to those concepts in (real) calculus. Thus if you feel at home
with real sequences and series and want to take for granted that the ratio test also holds
in complex, skip this section and go to Section 15.2.

Sequences
The basic definitions are as in calculus. An infinite sequence or, briefly, a sequence, is
obtained by assigning to each positive integer n a number z n, called a term of the sequence,
and is written
z 1, z 2, Á

or

{z 1, z 2, Á }

or briefly

{z n}.

We may also write z 0, z 1, Á or z 2, z 3, Á or start with some other integer if convenient.
A real sequence is one whose terms are real.

671

c15.qxd

11/1/10

6:41 PM

672

Page 672

CHAP. 15 Power Series, Taylor Series

Convergence.

A convergent sequence z 1, z 2, Á is one that has a limit c, written
lim z n ϭ c

or simply

n:ϱ

z n : c.

By definition of limit this means that for every P Ͼ 0 we can find an N such that
ƒ zn Ϫ c ƒ Ͻ P

(1)

for all n Ͼ N;

geometrically, all terms z n with n Ͼ N lie in the open disk of radius P and center c (Fig. 361)
and only finitely many terms do not lie in that disk. [For a real sequence, (1) gives an open
interval of length 2P and real midpoint c on the real line as shown in Fig. 362.]
A divergent sequence is one that does not converge.
y
∈

c

c –∈

x

c +∈

Convergent and Divergent Sequences
The sequence {i n>n} ϭ {i, Ϫ 12 , Ϫi>3, 14 , Á } is convergent with limit 0.
The sequence {i n} ϭ {i, Ϫ1, Ϫi, 1, Á } is divergent, and so is {z n} with z n ϭ (1 ϩ i)n.

EXAMPLE 2

x

Fig. 362. Convergent real sequence

Fig. 361. Convergent complex sequence

EXAMPLE 1

c

᭿

Sequences of the Real and the Imaginary Parts
Á.
The sequence {z n} with z n ϭ x n ϩ iyn ϭ 1 Ϫ 1>n 2 ϩ i(2 ϩ 4>n) is 6i, 34 ϩ 4i, 89 ϩ 10i>3, 15
16 ϩ 3i,
(Sketch it.) It converges with the limit c ϭ 1 ϩ 2i. Observe that {x n} has the limit 1 ϭ Re c and {yn} has
the limit 2 ϭ Im c. This is typical. It illustrates the following theorem by which the convergence of a
complex sequence can be referred back to that of the two real sequences of the real parts and the imaginary
parts.
᭿

THEOREM 1

Sequences of the Real and the Imaginary Parts

A sequence z 1, z 2, Á , z n, Á of complex numbers z n ϭ x n ϩ iyn (where n ϭ 1,
2, Á ) converges to c ϭ a ϩ ib if and only if the sequence of the real parts x 1, x 2, Á
converges to a and the sequence of the imaginary parts y1, y2, Á converges to b.

PROOF

Convergence z n : c ϭ a ϩ ib implies convergence x n : a and yn : b because if
ƒ z n Ϫ c ƒ Ͻ P, then z n lies within the circle of radius P about c ϭ a ϩ ib, so that (Fig. 363a)
ƒ x n Ϫ a ƒ Ͻ P,

ƒ yn Ϫ b ƒ Ͻ P.

Conversely, if x n : a and yn : b as n : ϱ, then for a given P Ͼ 0 we can choose
N so large that, for every n Ͼ N,
ƒ xn Ϫ a ƒ Ͻ

P
,
2

ƒ yn Ϫ b ƒ Ͻ

P
.
2

c15.qxd

11/1/10

6:41 PM

Page 673

SEC. 15.1 Sequences, Series, Convergence Tests

673

y

y

b +∈

b+∈
2

c

b

c

b

b–∈
2

b –∈
a –∈

a +∈

a

x

a

a–∈
2

a+∈

x

2

(a)

(b)

Fig. 363. Proof of Theorem 1

These two inequalities imply that z n ϭ x n ϩ iyn lies in a square with center c and side
P. Hence, z n must lie within a circle of radius P with center c (Fig. 363b).
᭿

Series
Given a sequence z 1, z 2, Á , z m, Á , we may form the sequence of the sums
s1 ϭ z 1,

s2 ϭ z 1 ϩ z 2,

s3 ϭ z 1 ϩ z 2 ϩ z 3,

Á

and in general
sn ϭ z 1 ϩ z 2 ϩ Á ϩ z n

(2)

(n ϭ 1, 2, Á ).

Here sn is called the nth partial sum of the infinite series or series
ؕ

Á.
a zm ϭ z1 ϩ z2 ϩ

(3)

mϭ1

The z 1, z 2, Á are called the terms of the series. (Our usual summation letter is n, unless
we need n for another purpose, as here, and we then use m as the summation letter.)
A convergent series is one whose sequence of partial sums converges, say,
lim sn ϭ s.

n:ϱ

ؕ

Then we write

s ϭ a zm ϭ z1 ϩ z2 ϩ Á
mϭ1

and call s the sum or value of the series. A series that is not convergent is called a divergent
series.
If we omit the terms of sn from (3), there remains
(4)

Rn ϭ z nϩ1 ϩ z nϩ2 ϩ z nϩ3 ϩ Á .

This is called the remainder of the series (3) after the term z n. Clearly, if (3) converges
and has the sum s, then
s ϭ sn ϩ Rn,

thus

Rn ϭ s Ϫ sn.

Now sn : s by the definition of convergence; hence Rn : 0. In applications, when s is
unknown and we compute an approximation sn of s, then ƒ Rn ƒ is the error, and Rn : 0
means that we can make ƒ Rn ƒ as small as we please, by choosing n large enough.

c15.qxd

11/1/10

6:41 PM

674

Page 674

CHAP. 15 Power Series, Taylor Series

An application of Theorem 1 to the partial sums immediately relates the convergence
of a complex series to that of the two series of its real parts and of its imaginary parts:
THEOREM 2

Real and Imaginary Parts

A series (3) with z m ϭ x m ϩ iym converges and has the sum s ϭ u ϩ iv if and
only if x 1 ϩ x 2 ϩ Á converges and has the sum u and y1 ϩ y2 ϩ Á converges
and has the sum v.

Tests for Convergence and Divergence of Series
Convergence tests in complex are practically the same as in calculus. We apply them
before we use a series, to make sure that the series converges.
Divergence can often be shown very simply as follows.
THEOREM 3

Divergence

If a series z1 ϩ z 2 ϩ Á converges, then lim z m ϭ 0. Hence if this does not hold,
m:ϱ
the series diverges.

PROOF

If z 1 ϩ z 2 ϩ Á converges, with the sum s, then, since z m ϭ sm Ϫ sm؊1,
lim z m ϭ lim (sm Ϫ sm؊1) ϭ lim sm Ϫ lim sm؊1 ϭ s Ϫ s ϭ 0.

m:ϱ

m:ϱ

m:ϱ

m:ϱ

᭿

CAUTION! z m : 0 is necessary for convergence but not sufficient, as we see from the
harmonic series 1 ϩ 12 ϩ 13 ϩ 14 ϩ Á , which satisfies this condition but diverges, as is
shown in calculus (see, for example, Ref. [GenRef11] in App. 1).
The practical difficulty in proving convergence is that, in most cases, the sum of a series
is unknown. Cauchy overcame this by showing that a series converges if and only if its
partial sums eventually get close to each other:
THEOREM 4

Cauchy’s Convergence Principle for Series

A series z 1 ϩ z 2 ϩ Á is convergent if and only if for every given P Ͼ 0 (no matter
how small) we can find an N (which depends on P, in general) such that
(5)

ƒ z nϩ1 ϩ z nϩ2 ϩ Á ϩ z nϩp ƒ Ͻ P

for every n Ͼ N and p ϭ 1, 2, Á

The somewhat involved proof is left optional (see App. 4).
Absolute Convergence. A series z1 ϩ z 2 ϩ Á is called absolutely convergent if the
series of the absolute values of the terms
ؕ

Á
a ƒ zm ƒ ϭ ƒ z1 ƒ ϩ ƒ z2 ƒ ϩ
mϭ1

is convergent.

c15.qxd

11/1/10

6:41 PM

Page 675

SEC. 15.1 Sequences, Series, Convergence Tests

675

If z 1 ϩ z 2 ϩ Á converges but ƒ z 1 ƒ ϩ ƒ z 2 ƒ ϩ Á diverges, then the series z 1 ϩ z 2 ϩ Á
is called, more precisely, conditionally convergent.
EXAMPLE 3

A Conditionally Convergent Series
The series 1 Ϫ 12 ϩ 13 Ϫ 14 ϩ Ϫ Á converges, but only conditionally since the harmonic series diverges, as
mentioned above (after Theorem 3).
᭿

If a series is absolutely convergent, it is convergent.
This follows readily from Cauchy’s principle (see Prob. 29). This principle also yields
the following general convergence test.
THEOREM 5

Comparison Test

If a series z1 ϩ z 2 ϩ Á is given and we can find a convergent series b1 ϩ b2 ϩ Á
with nonnegative real terms such that ƒ z 1 ƒ Ϲ b1, ƒ z 2 ƒ Ϲ b2, Á , then the given series
converges, even absolutely.

PROOF

By Cauchy’s principle, since b1 ϩ b2 ϩ Á converges, for any given P Ͼ 0 we can find
an N such that
bnϩ1 ϩ Á ϩ bnϩp Ͻ P

for every n Ͼ N and p ϭ 1, 2, Á .

From this and ƒ z 1 ƒ Ϲ b1, ƒ z 2 ƒ Ϲ b2, Á we conclude that for those n and p,
ƒ z nϩ1 ƒ ϩ Á ϩ ƒ z nϩp ƒ Ϲ bnϩ1 ϩ Á ϩ bnϩp Ͻ P.
Hence, again by Cauchy’s principle, ƒ z 1 ƒ ϩ ƒ z 2 ƒ ϩ Á converges, so that z 1 ϩ z 2 ϩ Á is
absolutely convergent.
᭿
A good comparison series is the geometric series, which behaves as follows.
THEOREM 6

Geometric Series

The geometric series
ؕ

(6*)

m
2
Á
a q ϭ1ϩqϩq ϩ
mϭ0

converges with the sum 1>(1 Ϫ q) if ƒ q ƒ Ͻ 1 and diverges if ƒ q ƒ м 1.

PROOF

If ƒ q ƒ м 1, then ƒ q m ƒ м 1 and Theorem 3 implies divergence.
Now let ƒ q ƒ Ͻ 1. The nth partial sum is
sn ϭ 1 ϩ q ϩ Á ϩ q n.
From this,
qsn ϭ

q ϩ Á ϩ q n ϩ q nϩ1.

c15.qxd

11/1/10

6:41 PM

676

Page 676

CHAP. 15 Power Series, Taylor Series

On subtraction, most terms on the right cancel in pairs, and we are left with
sn Ϫ qsn ϭ (1 Ϫ q)sn ϭ 1 Ϫ q nϩ1.
Now 1 Ϫ q

0 since q

1, and we may solve for sn, finding
nϩ1

1Ϫq
sn ϭ
1Ϫq

(6)

q nϩ1
1
ϭ
Ϫ
.
1Ϫq
1Ϫq

Since ƒ q ƒ Ͻ 1, the last term approaches zero as n : ϱ. Hence if ƒ q ƒ Ͻ 1, the series is
᭿
convergent and has the sum 1>(1 Ϫ q). This completes the proof.

Ratio Test
This is the most important test in our further work. We get it by taking the geometric
series as comparison series b1 ϩ b2 ϩ Á in Theorem 5:

THEOREM 7

Ratio Test

If a series z1 ϩ z 2 ϩ Á with z n
n greater than some N,

0 (n ϭ 1, 2, Á ) has the property that for every
z nϩ1
` z ` ϹqϽ1
n

(7)

(n Ͼ N)

(where q Ͻ 1 is fixed ), this series converges absolutely. If for every n Ͼ N,
z nϩ1
` z ` м1
n

(8)

(n Ͼ N),

the series diverges.

PROOF

If (8) holds, then ƒ z nϩ1 ƒ м ƒ z n ƒ for n Ͼ N, so that divergence of the series follows from
Theorem 3.
If (7) holds, then ƒ z nϩ1 ƒ Ϲ ƒ z n ƒ q for n Ͼ N, in particular,
ƒ z Nϩ2 ƒ Ϲ ƒ z Nϩ1 ƒ q,

ƒ z Nϩ3 ƒ Ϲ ƒ z Nϩ2 ƒ q Ϲ ƒ z Nϩ1 ƒ q 2,

etc.,

and in general, ƒ z Nϩp ƒ Ϲ ƒ z Nϩ1 ƒ q p؊1. Since q Ͻ 1, we obtain from this and Theorem 6
ƒ z Nϩ1 ƒ ϩ ƒ z Nϩ2 ƒ ϩ ƒ z Nϩ3 ƒ ϩ Á Ϲ ƒ z Nϩ1 ƒ (1 ϩ q ϩ q 2 ϩ Á ) Ϲ ƒ z Nϩ1 ƒ
Absolute convergence of z1 ϩ z 2 ϩ Á now follows from Theorem 5.

1
.
1Ϫq
᭿

c15.qxd

11/1/10

6:41 PM

Page 677

SEC. 15.1 Sequences, Series, Convergence Tests

677

CAUTION! The inequality (7) implies ƒ z nϩ1>z n ƒ Ͻ 1, but this does not imply convergence, as we see from the harmonic series, which satisfies z nϩ1>z n ϭ n>(n ϩ 1) Ͻ 1 for
all n but diverges.
If the sequence of the ratios in (7) and (8) converges, we get the more convenient
THEOREM 8

Ratio Test

If a series z 1 ϩ z 2 ϩ Á with z n
then:

z nϩ1
0 (n ϭ 1, 2, Á ) is such that lim ` z ` ϭ L,
n :ϱ
n

(a) If L Ͻ 1, the series converges absolutely.
(b) If L Ͼ 1, the series diverges.
(c) If L ϭ 1, the series may converge or diverge, so that the test fails and
permits no conclusion.
PROOF

(a) We write k n ϭ ƒ z nϩ1>z n ƒ and let L ϭ 1 Ϫ b Ͻ 1. Then by the definition of limit, the
k n must eventually get close to 1 Ϫ b, say, k n Ϲ q ϭ 1 Ϫ 12 b Ͻ 1 for all n greater than
some N. Convergence of z 1 ϩ z 2 ϩ Á now follows from Theorem 7.
(b) Similarly, for L ϭ 1 ϩ c Ͼ 1 we have k n м 1 ϩ 12 c Ͼ 1 for all n Ͼ N* (sufficiently
large), which implies divergence of z1 ϩ z 2 ϩ Á by Theorem 7.
(c) The harmonic series 1 ϩ 12 ϩ 13 ϩ Á has z nϩ1>z n ϭ n>(n ϩ 1), hence L ϭ 1, and
diverges. The series
1ϩ

1
1
1
1
ϩ ϩ
ϩ
ϩ Á
4
9
16
25

z nϩ1

has

zn

ϭ

n2
,
(n ϩ 1)2

hence also L ϭ 1, but it converges. Convergence follows from (Fig. 364)
sn ϭ 1 ϩ

1
1
ϩÁϩ 2Ϲ1ϩ
4
n

Ύ

n

1

dx
1
,
2 ϭ 2 Ϫ
n
x

so that s1, s2, Á is a bounded sequence and is monotone increasing (since the terms of
the series are all positive); both properties together are sufficient for the convergence of
the real sequence s1, s2, Á . (In calculus this is proved by the so-called integral test, whose
idea we have used.)
᭿
y
y = 12
x
1

Area 1

1

Area 9

Area 16

1

Area 4
0

1

2

3

4

Fig. 364. Convergence of the series 1 ϩ ϩ
1
4

1
9

ϩ

1
16

ϩÁ

x

c15.qxd

11/1/10

6:41 PM

678

Page 678

CHAP. 15 Power Series, Taylor Series

EXAMPLE 4

Ratio Test
Is the following series convergent or divergent? (First guess, then calculate.)
ؕ

a
nϭ0

(100 ϩ 75i)n
n!

ϭ 1 ϩ (100 ϩ 75i) ϩ

1
2!

(100 ϩ 75i)2 ϩ Á

Solution. By Theorem 8, the series is convergent, since
`

EXAMPLE 5

z nϩ1
zn

` ϭ

ƒ 100 ϩ 75i ƒ nϩ1>(n ϩ 1)!
ƒ 100 ϩ 75i ƒ n>n!

ϭ

ƒ 100 ϩ 75i ƒ
nϩ1

ϭ

125
nϩ1

:

᭿

L ϭ 0.

Theorem 7 More General Than Theorem 8
Let an ϭ i>23n and bn ϭ 1>23nϩ1. Is the following series convergent or divergent?
1
i
1
i
1
a0 ϩ b0 ϩ a1 ϩ b1 ϩ Á ϭ i ϩ ϩ ϩ
ϩ
ϩ
ϩÁ
2
8
16
64
128

Solution. The ratios of the absolute values of successive terms are 12 , 14 , 12 , 14 , Á . Hence convergence follows
from Theorem 7. Since the sequence of these ratios has no limit, Theorem 8 is not applicable.
᭿

Root Test
The ratio test and the root test are the two practically most important tests. The ratio test
is usually simpler, but the root test is somewhat more general.
THEOREM 9

Root Test

If a series z 1 ϩ z 2 ϩ Á is such that for every n greater than some N,
(9)

n

2ƒ z n ƒ Ϲ q Ͻ 1

(n Ͼ N)

(where q Ͻ 1 is fixed ), this series converges absolutely. If for infinitely many n,
n

2 ƒ z n ƒ м 1,

(10)
the series diverges.

PROOF

If (9) holds, then ƒ z n ƒ Ϲ q n Ͻ 1 for all n Ͼ N. Hence the series ƒ z 1 ƒ ϩ ƒ z 2 ƒ ϩ Á
converges by comparison with the geometric series, so that the series z 1 ϩ z 2 ϩ Á
converges absolutely. If (10) holds, then ƒ z n ƒ м 1 for infinitely many n. Divergence of
z 1 ϩ z 2 ϩ Á now follows from Theorem 3.
᭿
n

CAUTION! Equation (9) implies 2 ƒ z n ƒ Ͻ 1, but this does not imply convergence, as
n
we see from the harmonic series, which satisfies 21>n Ͻ 1 (for n Ͼ 1) but diverges.

c15.qxd

11/1/10

6:41 PM

Page 679

SEC. 15.1 Sequences, Series, Convergence Tests

679

If the sequence of the roots in (9) and (10) converges, we more conveniently have
THEOREM 10

Root Test
n
If a series z 1 ϩ z 2 ϩ Á is such that lim 2 ƒ z n ƒ ϭ L, then:
n:ϱ

(a) The series converges absolutely if L Ͻ 1.
(b) The series diverges if L Ͼ 1.
(c) If L ϭ 1, the test fails; that is, no conclusion is possible.

PROBLEM SET 15.1
1–10

ؕ

SEQUENCES

Is the given sequence z 1, z 2, Á , z n, Á bounded? Convergent? Find its limit points. Show your work in detail.
1. z n ϭ (1 ϩ i) >2
2n

n

3. z n ϭ np>(4 ϩ 2ni)

2. z n ϭ (3 ϩ 4i) >n!
n

4. z n ϭ (1 ϩ 2i)n

5. z n ϭ (Ϫ1) ϩ 10i

6. z n ϭ (cos npi)>n

7. z n ϭ n 2 ϩ i>n 2

8. z n ϭ [(1 ϩ 3i)> 110 ]n

n

9. z n ϭ (3 ϩ 3i)؊n

10. z n ϭ sin (14 np) ϩ i n

11. CAS EXPERIMENT. Sequences. Write a program
for graphing complex sequences. Use the program to
discover sequences that have interesting “geometric”
properties, e.g., lying on an ellipse, spiraling to its limit,
having infinitely many limit points, etc.
12. Addition of sequences. If z 1, z 2, Á converges with
the limit l and z*1, z*2, Á converges with the limit l*,
show that z 1 ϩ z1*, z 2 ϩ z*2, Á is convergent with the
limit l ϩ l*.
13. Bounded sequence. Show that a complex sequence
is bounded if and only if the two corresponding
sequences of the real parts and of the imaginary parts
are bounded.
14. On Theorem 1. Illustrate Theorem 1 by an example
of your own.
15. On Theorem 2. Give another example illustrating
Theorem 2.

SERIES

16–25

Is the given series convergent or divergent? Give a reason.
Show details.
ؕ

(20 ϩ 30i)n
n!
nϭ0

16. a
ؕ

n

i
18. a n a b
4
nϭ1
2

ؕ

17. a
nϭ2
ؕ

19. a
nϭ0

(Ϫi)n
ln n
in
n2 Ϫ i

20. a
nϭ0
ؕ

nϩi
3n 2 ϩ 2i

(p ϩ pi)2nϩ1
(2n ϩ 1)!
nϭ0

21. a
ؕ

22. a
nϭ1
ؕ

1
1n

(Ϫ1)n(1 ϩ i)2n
23. a
(2n)!
nϭ0
ؕ

(3i)nn!
nn
nϭ1

24. a

ؕ
in
25. a n
nϭ1

26. Significance of (7). What is the difference between (7)
and just stating ƒ z nϩ1>z n ƒ Ͻ 1?
27. On Theorems 7 and 8. Give another example showing
that Theorem 7 is more general than Theorem 8.
28. CAS EXPERIMENT. Series. Write a program for
computing and graphing numeric values of the first n
partial sums of a series of complex numbers. Use the
program to experiment with the rapidity of convergence
of series of your choice.
29. Absolute convergence. Show that if a series converges
absolutely, it is convergent.

30. Estimate of remainder. Let ƒ z nϩ1>z n ƒ Ϲ q Ͻ 1, so
that the series z 1 ϩ z 2 ϩ Á converges by the ratio test.
Show that the remainder Rn ϭ z nϩ1 ϩ z nϩ2 ϩ Á
satisfies the inequality ƒ Rn ƒ Ϲ ƒ z nϩ1 ƒ > (1 Ϫ q). Using
this, find how many terms suffice for computing the
sum s of the series
ؕ

a
nϭ1

nϩi
2nn

with an error not exceeding 0.05 and compute s to this
accuracy.

c15.qxd

11/1/10

6:41 PM

680

15.2

Page 680

CHAP. 15 Power Series, Taylor Series

Power Series
The student should pay close attention to the material because we shall show how power
series play an important role in complex analysis. Indeed, they are the most important series
in complex analysis because their sums are analytic functions (Theorem 5, Sec. 15.3), and
every analytic function can be represented by power series (Theorem 1, Sec. 15.4).
A power series in powers of z Ϫ z 0 is a series of the form
ؕ

(1)

n
2
Á
a an(z Ϫ z 0) ϭ a0 ϩ a1(z Ϫ z 0) ϩ a2(z Ϫ z 0) ϩ
nϭ0

where z is a complex variable, a0, a1, Á are complex (or real) constants, called the
coefficients of the series, and z 0 is a complex (or real) constant, called the center of the
series. This generalizes real power series of calculus.
If z 0 ϭ 0, we obtain as a particular case a power series in powers of z:
ؕ

n
2
Á.
a anz ϭ a0 ϩ a1z ϩ a2z ϩ

(2)

nϭ0

Convergence Behavior of Power Series
Power series have variable terms (functions of z), but if we fix z, then all the concepts
for series with constant terms in the last section apply. Usually a series with variable
terms will converge for some z and diverge for others. For a power series the situation is
simple. The series (1) may converge in a disk with center z 0 or in the whole z-plane or
only at z 0. We illustrate this with typical examples and then prove it.
EXAMPLE 1

Convergence in a Disk. Geometric Series
The geometric series
ؕ

n
2
Á
a z ϭ1ϩzϩz ϩ
nϭ0

converges absolutely if ƒ z ƒ Ͻ 1 and diverges if ƒ z ƒ м 1 (see Theorem 6 in Sec. 15.1).

EXAMPLE 2

᭿

Convergence for Every z
The power series (which will be the Maclaurin series of ez in Sec. 15.4)
ؕ zn
z3
z2
Á
a n! ϭ 1 ϩ z ϩ 2! ϩ 3! ϩ
nϭ0

is absolutely convergent for every z. In fact, by the ratio test, for any fixed z,
`

z nϩ1>(n ϩ 1)!
z n>n!

` ϭ

ƒzƒ
nϩ1

:

0

as

n : ϱ.

᭿

c15.qxd

11/1/10

6:41 PM

Page 681

SEC. 15.2 Power Series
EXAMPLE 3

681

Convergence Only at the Center. (Useless Series)
The following power series converges only at z ϭ 0, but diverges for every z

0, as we shall show.

ؕ

n
2
3
Á
a n!z ϭ 1 ϩ z ϩ 2z ϩ 6z ϩ
nϭ0

In fact, from the ratio test we have
`

THEOREM 1

(n ϩ 1)!z nϩ1
n!z n

` ϭ (n ϩ 1) ƒ z ƒ

ϱ

:

as

n:ϱ

(z fixed and

0).

᭿

Convergence of a Power Series

(a) Every power series (1) converges at the center z 0.
(b) If (1) converges at a point z ϭ z1 z 0, it converges absolutely for every z
closer to z 0 than z1, that is, ƒ z Ϫ z 0 ƒ Ͻ ƒ z1 Ϫ z 0 ƒ . See Fig. 365.
(c) If (1) diverges at z ϭ z 2, it diverges for every z farther away from z 0 than
z 2. See Fig. 365.

y
Divergent
Conv.
z0

z1
z2
x

Fig. 365. Theroem 1

PROOF

(a) For z ϭ z 0 the series reduces to the single term a0.
(b) Convergence at z ϭ z1 gives by Theorem 3 in Sec. 15.1 an(z1 Ϫ z 0)n : 0 as n : ϱ.
This implies boundedness in absolute value,
ƒ an(z1 Ϫ z 0)n ƒ Ͻ M

for every n ϭ 0, 1, Á .

Multiplying and dividing an(z Ϫ z 0)n by (z1 Ϫ z 0)n we obtain from this
z Ϫ z0 n
z Ϫ z0 n
ƒ an(z Ϫ z 0)n ƒ ϭ ` an(z1 Ϫ z 0)n a z Ϫ z b ` Ϲ M ` z Ϫ z ` .
1
0
1
0
Summation over n gives
(3)

ؕ
ؕ
z Ϫ z0 n
n
a ƒ an(z Ϫ z 0) ƒ Ϲ M a ` z1 Ϫ z 0 ` .

nϭ1

nϭ1

Now our assumption ƒ z Ϫ z 0 ƒ Ͻ ƒ z1 Ϫ z 0 ƒ implies that ƒ (z Ϫ z 0)>(z1 Ϫ z 0) ƒ Ͻ 1. Hence
the series on the right side of (3) is a converging geometric series (see Theorem 6 in

c15.qxd

11/1/10

6:41 PM

682

Page 682

CHAP. 15 Power Series, Taylor Series

Sec. 15.1). Absolute convergence of (1) as stated in (b) now follows by the comparison
test in Sec. 15.1.
(c) If this were false, we would have convergence at a z 3 farther away from z 0 than z 2.
This would imply convergence at z 2, by (b), a contradiction to our assumption of divergence
᭿
at z 2.

Radius of Convergence of a Power Series
Convergence for every z (the nicest case, Example 2) or for no z z 0 (the useless case,
Example 3) needs no further discussion, and we put these cases aside for a moment. We
consider the smallest circle with center z 0 that includes all the points at which a given
power series (1) converges. Let R denote its radius. The circle
ƒ z Ϫ z0 ƒ ϭ R

(Fig. 366)

is called the circle of convergence and its radius R the radius of convergence of (1). Theorem
1 then implies convergence everywhere within that circle, that is, for all z for which
ƒ z Ϫ z0 ƒ Ͻ R

(4)

(the open disk with center z 0 and radius R). Also, since R is as small as possible, the series
(1) diverges for all z for which
ƒ z Ϫ z 0 ƒ Ͼ R.

(5)

No general statements can be made about the convergence of a power series (1) on the
circle of convergence itself. The series (1) may converge at some or all or none of the
points. Details will not be important to us. Hence a simple example may just give us
the idea.

y
Divergent
Conv.
z0

R

x

Fig. 366. Circle of convergence

EXAMPLE 4

Behavior on the Circle of Convergence
On the circle of convergence (radius R ϭ 1 in all three series),
S z n>n 2 converges everywhere since S 1>n 2 converges,

S z n>n converges at Ϫ1 (by Leibniz’s test) but diverges at 1,
S zn

diverges everywhere.

᭿

c15.qxd

11/2/10

3:12 PM

Page 683

SEC. 15.2 Power Series

683

Notations R ‫ ؕ ؍‬and R ‫ ؍‬0. To incorporate these two excluded cases in the present
notation, we write
R ϭ ϱ if the series (1) converges for all z (as in Example 2),
R ϭ 0 if (1) converges only at the center z ϭ z 0 (as in Example 3).
These are convenient notations, but nothing else.
Real Power Series. In this case in which powers, coefficients, and center are real,
formula (4) gives the convergence interval ƒ x Ϫ x 0 ƒ Ͻ R of length 2R on the real line.
Determination of the Radius of Convergence from the Coefficients. For this important
practical task we can use
THEOREM 2

Radius of Convergence R

Suppose that the sequence ƒ anϩ1>an ƒ , n ϭ 1, 2, Á , converges with limit L*. If
L* ϭ 0, then R ϭ ϱ; that is, the power series (1) converges for all z. If L* 0
(hence L* Ͼ 0), then
an
1
R ϭ * ϭ lim `
`
n
:ϱ
L
anϩ1

(6)

(Cauchy–Hadamard formula1).

If ƒ anϩ1>an ƒ : ϱ, then R ϭ 0 (convergence only at the center z 0).
PROOF

For (1) the ratio of the terms in the ratio test (Sec. 15.1) is
anϩ1(z Ϫ z 0)nϩ1
anϩ1
`
` ϭ `
` ƒ z Ϫ z0 ƒ .
an(z Ϫ z 0)n
an

The limit is

L ϭ L* ƒ z Ϫ z 0 ƒ .

Let L* 0, thus L* Ͼ 0. We have convergence if L ϭ L* ƒ z Ϫ z 0 ƒ Ͻ 1, thus
ƒ z Ϫ z 0 ƒ Ͻ 1>L*, and divergence if ƒ z Ϫ z 0 ƒ Ͼ 1>L*. By (4) and (5) this shows that 1>L*
is the convergence radius and proves (6).
If L* ϭ 0, then L ϭ 0 for every z, which gives convergence for all z by the ratio test.
If ƒ anϩ1>an ƒ : ϱ, then ƒ anϩ1>an ƒ ƒ z Ϫ z 0 ƒ Ͼ 1 for any z z 0 and all sufficiently large
n. This implies divergence for all z z 0 by the ratio test (Theorem 7, Sec. 15.1).
᭿
Formula (6) will not help if L* does not exist, but extensions of Theorem 2 are still
possible, as we discuss in Example 6 below.
EXAMPLE 5

Radius of Convergence

ؕ

(2n)!

nϭ0

(n!)2

By (6) the radius of convergence of the power series a
R ϭ lim c
n:ϱ

(2n!)
2

(n!)

^

(2n ϩ 2)!
((n ϩ 1)!)

2

d ϭ nlim
c
:ϱ

(2n!)
(2n ϩ 2)!

ؒ

(z Ϫ 3i)n is

((n ϩ 1)!)2

The series converges in the open disk ƒ z Ϫ 3i ƒ Ͻ 14 of radius

2

(n!)
1
4

d ϭ nlim
:ϱ

and center 3i.

(n ϩ 1)2
(2n ϩ 2)(2n ϩ 1)

ϭ

1
4

.

᭿

1
Named after the French mathematicians A. L. CAUCHY (see Sec. 2.5) and JACQUES HADAMARD
(1865–1963). Hadamard made basic contributions to the theory of power series and devoted his lifework to
partial differential equations.

c15.qxd

11/1/10

6:41 PM

Page 684

684

CHAP. 15 Power Series, Taylor Series

EXAMPLE 6

Extension of Theorem 2
Find the radius of convergence R of the power series
ؕ
1
1
1 2 1 3
1
n
n
4
Á.
a c 1 ϩ (Ϫ1) ϩ 2n d z ϭ 3 ϩ 2 z ϩ a2 ϩ 4 b z ϩ 8 z ϩ a2 ϩ 16 b z ϩ
nϭ0

Solution. The sequence of the ratios 16 , 2(2 ϩ 14 ), 1>(8(2 ϩ 14 )), Á does not converge, so that Theorem 2 is
of no help. It can be shown that
ෂ
R ϭ 1>L,

(6*)

ෂ
n
L ϭ lim 2 ƒ an ƒ .
n:ϱ

n

n

n

This still does not help here, since ( 2 ƒ an ƒ ) does not converge because 2 ƒ an ƒ ϭ 21>2n ϭ 12 for odd n, whereas
for even n we have
n

n

2 ƒ an ƒ ϭ 2 2 ϩ 1>2n : 1
n

so that 2 ƒ an ƒ has the two limit points
ෂ
R ϭ 1>l ,

(6**)

1
2

as

n : ϱ,

and 1. It can further be shown that

n
ෂ
l the greatest limit point of the sequence {2 ƒ an ƒ }.

ෂ
Here l ϭ I , so that R ϭ 1. Answer. The series converges for ƒ z ƒ Ͻ 1.

᭿

Summary. Power series converge in an open circular disk or some even for every z (or
some only at the center, but they are useless); for the radius of convergence, see (6) or
Example 6.
Except for the useless ones, power series have sums that are analytic functions (as we
show in the next section); this accounts for their importance in complex analysis.

PROBLEM SET 15.2
1. Power series. Are 1>z ϩ z ϩ z 2 ϩ Á and z ϩ z 3>2 ϩ
z 2 ϩ z 3 ϩ Á power series? Explain.
2. Radius of convergence. What is it? Its role? What
motivates its name? How can you find it?
3. Convergence. What are the only basically different
possibilities for the convergence of a power series?
4. On Examples 1–3. Extend them to power series in
powers of z Ϫ 4 ϩ 3pi. Extend Example 1 to the case
of radius of convergence 6.
5. Powers z 2n. Show that if Sanz n has radius of
convergence R (assumed finite), then Sanz 2n has
radius of convergence 1R.
6–18

RADIUS OF CONVERGENCE

6. a 4 (z ϩ 1)
nϭ0

n

n

ؕ

(Ϫ1)n

ؕ

(z Ϫ 2i)n
10. a
nn
nϭ0
ؕ

12. a
nϭ0
ؕ

(Ϫ1)nn n
z
8n

ؕ

9. a
nϭ0

n(n Ϫ 1)
(z Ϫ i)2n
3n

ؕ
2Ϫi
11. a a
b zn
1
ϩ
5i
nϭ0
ؕ

13. a 16n(z ϩ i)4n
nϭ0

(Ϫ1)
14. a 2n 2 z 2n
2
(n!)
nϭ0

ؕ
(2n)!
15. a n 2 (z Ϫ 2i)n
4
(n!)
nϭ0

ؕ
(3n)!
16. a n 3 z n
2
(n!)
nϭ0

ؕ
2n
17. a n(n ϩ 1) z 2nϩ1

n

nϭ1

ؕ

2(Ϫ1)n
18. a
z 2nϩ1
1p(2n ϩ 1)n!

Find the center and the radius of convergence.
ؕ

ؕ
nn
8. a
(z Ϫ pi)n
n!
nϭ0

2n

1
7. a
az Ϫ p b
(2n)!
2
nϭ0

nϭ0

19. CAS PROJECT. Radius of Convergence. Write a
program for computing R from (6), (6*), or (6**), in

c15.qxd

11/1/10

6:41 PM

Page 685

SEC. 15.3 Functions Given by Power Series

685

this order, depending on the existence of the limits
needed. Test the program on some series of your choice
such that all three formulas (6), (6*), and (6**) will
come up.
20. TEAM PROJECT. Radius of Convergence.
(a) Understanding (6). Formula (6) for R contains
ƒ an>anϩ1 ƒ , not ƒ anϩ1>an ƒ . How could you memorize
this by using a qualitative argument?
(b) Change of coefficients. What happens to R
(0 Ͻ R Ͻ ϱ) if you (i) multiply all an by k 0,

15.3

(ii) multiply all an by k n 0, (iii) replace an by
1>an? Can you think of an application of this?
(c) Understanding Example 6, which extends
Theorem 2 to nonconvergent cases of an>anϩ1.
Do you understand the principle of “mixing” by
which Example 6 was obtained? Make up further
examples.
(d) Understanding (b) and (c) in Theorem 1. Does
there exist a power series in powers of z that converges
at z ϭ 30 ϩ 10i and diverges at z ϭ 31 Ϫ 6i? Give
reason.

Functions Given by Power Series
Here, our main goal is to show that power series represent analytic functions. This fact
(Theorem 5) and the fact that power series behave nicely under addition, multiplication,
differentiation, and integration accounts for their usefulness.
To simplify the formulas in this section, we take z 0 ϭ 0 and write
ؕ

n
a anz .

(1)

nϭ0

There is no loss of generality because a series in powers of zˆ Ϫ z 0 with any z 0 can always
be reduced to the form (1) if we set zˆ Ϫ z 0 ϭ z.
Terminology and Notation. If any given power series (1) has a nonzero radius of
convergence R (thus R Ͼ 0), its sum is a function of z, say f (z). Then we write
ؕ

(2)

f (z) ϭ a anz n ϭ a0 ϩ a1z ϩ a2z 2 ϩ Á

( ƒ z ƒ Ͻ R).

nϭ0

We say that f (z) is represented by the power series or that it is developed in the power
series. For instance, the geometric series represents the function f (z) ϭ 1>(1 Ϫ z) in the
interior of the unit circle ƒ z ƒ ϭ 1. (See Theorem 6 in Sec. 15.1.)
Uniqueness of a Power Series Representation. This is our next goal. It means that a
function f (z) cannot be represented by two different power series with the same center.
We claim that if f (z) can at all be developed in a power series with center z 0, the
development is unique. This important fact is frequently used in complex analysis (as well
as in calculus). We shall prove it in Theorem 2. The proof will follow from

THEOREM 1

Continuity of the Sum of a Power Series

If a function f (z) can be represented by a power series (2) with radius of convergence
R Ͼ 0, then f (z) is continuous at z ϭ 0.

c15.qxd

11/1/10

6:41 PM

686

Page 686

CHAP. 15 Power Series, Taylor Series
PROOF

From (2) with z ϭ 0 we have f (0) ϭ a0. Hence by the definition of continuity we
must show that lim z:0 f (z) ϭ f (0) ϭ a0. That is, we must show that for a given P Ͼ 0
there is a d Ͼ 0 such that ƒ z ƒ Ͻ d implies ƒ f (z) Ϫ a0 ƒ Ͻ P. Now (2) converges absolutely for ƒ z ƒ Ϲ r with any r such that 0 Ͻ r Ͻ R, by Theorem 1 in Sec. 15.2. Hence
the series
ؕ
1 ؕ
n؊1
ϭ r a ƒ an ƒ r n
a ƒ an ƒ r

nϭ1

converges. Let S

nϭ1

0 be its sum. (S ϭ 0 is trivial.) Then for 0 Ͻ ƒ z ƒ Ϲ r,
ؕ

ؕ

ؕ

nϭ1

nϭ1

nϭ1

ƒ f (z) Ϫ a0 ƒ ϭ 2 a anz n 2 Ϲ ƒ z ƒ a ƒ an ƒ ƒ z ƒ n؊1 Ϲ ƒ z ƒ a ƒ an ƒ r n؊1 ϭ ƒ z ƒ S
and ƒ z ƒ S Ͻ P when ƒ z ƒ Ͻ d, where d Ͼ 0 is less than r and less than P>S. Hence
ƒ z ƒ S Ͻ dS Ͻ (P>S)S ϭ P. This proves the theorem.
᭿
From this theorem we can now readily obtain the desired uniqueness theorem (again
assuming z 0 ϭ 0 without loss of generality):

THEOREM 2

Identity Theorem for Power Series. Uniqueness

Let the power series a0 ϩ a1 z ϩ a2 z 2 ϩ Á and b0 ϩ b1 z ϩ b2 z 2 ϩ Á both be
convergent for ƒ z ƒ Ͻ R, where R is positive, and let them both have the same sum
for all these z. Then the series are identical, that is, a0 ϭ b0, a1 ϭ b1, a2 ϭ b2, Á .
Hence if a function f (z) can be represented by a power series with any center z 0,
this representation is unique.

PROOF

We proceed by induction. By assumption,
a0 ϩ a1z ϩ a2 z 2 ϩ Á ϭ b0 ϩ b1z ϩ b2 z 2 ϩ Á

( ƒ z ƒ Ͻ R).

The sums of these two power series are continuous at z ϭ 0, by Theorem 1. Hence if we
consider ƒ z ƒ Ͼ 0 and let z : 0 on both sides, we see that a0 ϭ b0: the assertion is true
for n ϭ 0. Now assume that an ϭ bn for n ϭ 0, 1, Á , m. Then on both sides we may
omit the terms that are equal and divide the result by z mϩ1 ( 0); this gives
amϩ1 ϩ amϩ2 z ϩ amϩ3 z 2 ϩ Á ϭ bmϩ1 ϩ bmϩ2 z ϩ bmϩ3 z 2 ϩ Á .
Similarly as before by letting z : 0 we conclude from this that amϩ1 ϭ bmϩ1. This
completes the proof.
᭿

Operations on Power Series
Interesting in itself, this discussion will serve as a preparation for our main goal, namely,
to show that functions represented by power series are analytic.

c15.qxd

11/1/10

6:41 PM

Page 687

SEC. 15.3 Functions Given by Power Series

687

Termwise addition or subtraction of two power series with radii of convergence R1 and
R2 yields a power series with radius of convergence at least equal to the smaller of R1
and R2. Proof. Add (or subtract) the partial sums sn and sn* term by term and use
lim (sn Ϯ s*n) ϭ lim sn Ϯ lim s*n .
Termwise multiplication of two power series
ؕ

f (z) ϭ a akz k ϭ a0 ϩ a1z ϩ Á
kϭ0

and
ؕ

g(z) ϭ a bmz m ϭ b0 ϩ b1z ϩ Á
mϭ0

means the multiplication of each term of the first series by each term of the second series
and the collection of like powers of z. This gives a power series, which is called the
Cauchy product of the two series and is given by
a0 b0 ϩ (a0 b1 ϩ a1 b0)z ϩ (a0 b2 ϩ a1 b1 ϩ a2 b0)z 2 ϩ Á
ؕ

ϭ a (a0 bn ϩ a1 bn؊1 ϩ Á ϩ an b0)z n.
nϭ0

We mention without proof that this power series converges absolutely for each z within
the smaller circle of convergence of the two given series and has the sum s(z) ϭ f (z)g(z).
For a proof, see [D5] listed in App. 1.
Termwise differentiation and integration of power series is permissible, as we show
next. We call derived series of the power series (1) the power series obtained from (1)
by termwise differentiation, that is,
ؕ

n؊1
ϭ a1 ϩ 2a2z ϩ 3a3z 2 ϩ Á .
a nan z

(3)

nϭ1

THEOREM 3

Termwise Differentiation of a Power Series

The derived series of a power series has the same radius of convergence as the
original series.

PROOF

This follows from (6) in Sec. 15.2 because

lim

n:ϱ

n ƒ an ƒ
(n ϩ 1) ƒ anϩ1 ƒ

ϭ lim

n:ϱ

an
an
n
lim `
` ϭ lim `
`
n:ϱ anϩ1
n ϩ 1 n:ϱ anϩ1
n

or, if the limit does not exist, from (6**) in Sec. 15.2 by noting that 2 n : 1 as n : ϱ. ᭿

c15.qxd

11/1/10

6:41 PM

688

Page 688

CHAP. 15 Power Series, Taylor Series

EXAMPLE 1

Application of Theorem 3
Find the radius of convergence R of the following series by applying Theorem 3.
ؕ

n n
2
3
4
5
Á.
a a 2 b z ϭ z ϩ 3z ϩ 6z ϩ 10z ϩ
nϭ2
Solution. Differentiate the geometric series twice term by term and multiply the result by z 2>2. This yields
the given series. Hence R ϭ 1 by Theorem 3.
᭿
THEOREM 4

Termwise Integration of Power Series

The power series
ؕ

an nϩ1
a1 2 a2 3
ϭ a0z ϩ
z ϩ
z ϩÁ
a nϩ1z
2
3
nϭ0
obtained by integrating the series a0 ϩ a1z ϩ a2z 2 ϩ Á term by term has the same
radius of convergence as the original series.
The proof is similar to that of Theorem 3.
With the help of Theorem 3, we establish the main result in this section.

Power Series Represent Analytic Functions
THEOREM 5

Analytic Functions. Their Derivatives

A power series with a nonzero radius of convergence R represents an analytic
function at every point interior to its circle of convergence. The derivatives of this
function are obtained by differentiating the original series term by term. All the
series thus obtained have the same radius of convergence as the original series.
Hence, by the first statement, each of them represents an analytic function.

PROOF

(a) We consider any power series (1) with positive radius of convergence R. Let f (z) be
its sum and f1(z) the sum of its derived series; thus
ؕ

(4)

f (z) ϭ a anz n

ؕ

and

f1(z) ϭ a nanz n؊1.

nϭ0

nϭ1

We show that f (z) is analytic and has the derivative f1(z) in the interior of the circle of
convergence. We do this by proving that for any fixed z with ƒ z ƒ Ͻ R and ¢z : 0 the
difference quotient [ f (z ϩ ¢z) Ϫ f (z)]>¢z approaches f1(z). By termwise addition we first
have from (4)
(5)

f (z ϩ ¢z) Ϫ f (z)
¢z

ؕ

(z ϩ ¢z)n Ϫ z n

nϭ2

¢z

Ϫ f1(z) ϭ a an c

Ϫ nz n؊1 d .

Note that the summation starts with 2, since the constant term drops out in taking the
difference f (z ϩ ¢z) Ϫ f (z), and so does the linear term when we subtract f1(z) from the
difference quotient.

c15.qxd

11/2/10

3:12 PM

Page 689

SEC. 15.3 Functions Given by Power Series

689

(b) We claim that the series in (5) can be written
ؕ

(6)

n؊2
ϩ 2z(z ϩ ¢z)n؊3 ϩ Á ϩ (n Ϫ 2)z n؊3(z ϩ ¢z)
a an ¢z[(z ϩ ¢z)
nϭ2

ϩ (n Ϫ 1)z n؊2].
The somewhat technical proof of this is given in App. 4.
(c) We consider (6). The brackets contain n Ϫ 1 terms, and the largest coefficient is
n Ϫ 1. Since (n Ϫ 1)2 Ϲ n(n Ϫ 1), we see that for ƒ z ƒ Ϲ R0 and ƒ z ϩ ¢z ƒ Ϲ R0, R0 Ͻ R,
the absolute value of this series (6) cannot exceed
ؕ

ƒ ¢z ƒ a ƒ an ƒ n(n Ϫ 1)Rn؊2
.
0

(7)

nϭ2

This series with an instead of ƒ an ƒ is the second derived series of (2) at z ϭ R0 and
converges absolutely by Theorem 3 of this section and Theorem 1 of Sec. 15.2. Hence
our present series (7) converges. Let the sum of (7) (without the factor ƒ ¢z ƒ ) be K (R0).
Since (6) is the right side of (5), our present result is
`

f (z ϩ ¢z) Ϫ f (z)
¢z

Ϫ f1(z) ` Ϲ ƒ ¢z ƒ K(R0).

Letting ¢z : 0 and noting that R0 (Ͻ R) is arbitrary, we conclude that f (z) is analytic at
any point interior to the circle of convergence and its derivative is represented by the derived
series. From this the statements about the higher derivatives follow by induction.
᭿
Summary. The results in this section show that power series are about as nice as we
could hope for: we can differentiate and integrate them term by term (Theorems 3 and 4).
Theorem 5 accounts for the great importance of power series in complex analysis: the
sum of such a series (with a positive radius of convergence) is an analytic function and
has derivatives of all orders, which thus in turn are analytic functions. But this is only
part of the story. In the next section we show that, conversely, every given analytic function
f (z) can be represented by power series, called Taylor series and being the complex analog
of the real Taylor series of calculus.

PROBLEM SET 15.3
1. Relation to Calculus. Material in this section generalizes calculus. Give details.
2. Termwise addition. Write out the details of the proof
on termwise addition and subtraction of power series.
n

3. On Theorem 3. Prove that 1 n : 1 as n : ϱ, as
claimed.
؊2

4. Cauchy product. Show that (1 Ϫ z)

ؕ

ϭ a (n ϩ 1)z

n

nϭ0

(a) by using the Cauchy product, (b) by differentiating
a suitable series.

5–15

RADIUS OF CONVERGENCE
BY DIFFERENTIATION OR INTEGRATION

Find the radius of convergence in two ways: (a) directly by
the Cauchy–Hadamard formula in Sec. 15.2, and (b) from a
series of simpler terms by using Theorem 3 or Theorem 4.
ؕ

5. a
nϭ2

n(n Ϫ 1)
2n

2nϩ1

(z Ϫ 2i)n

ؕ
n
7. a n (z ϩ 2i)2n
3
nϭ1

ؕ (Ϫ1)n
z
6. a
a b
2n
ϩ
1
2
p
nϭ0
ؕ
5n
8. a
zn
n(n
ϩ
1)
nϭ1

c15.qxd

11/1/10

6:41 PM

Page 690

690

CHAP. 15 Power Series, Taylor Series

ؕ
(Ϫ2)n
z 2n
9. a
n(n
ϩ
1)(n
ϩ
2)
nϭ1

17. Odd function. If f (z) in (2) is odd (i.e., f (Ϫz) ϭ Ϫf (z)),
show that an ϭ 0 for even n. Give examples.
18. Binomial coefficients. Using (1 ϩ z)p(1 ϩ z)q ϭ
(1 ϩ z)pϩq, obtain the basic relation

n

ؕ

n z
10. a a b a b
k 2
nϭk
ؕ

11. a

3nn(n ϩ 1)

nϭ1

7n

ؕ

2n(2n Ϫ 1)

12. a

nn

nϭ1
ؕ

13. a c a
nϭ0
ؕ

14. a a
nϭ0

r

p
q
pϩq
a an b ar Ϫ n b ϭ a r b .
nϭ0

(z ϩ 2)

2n

z 2n؊2

19. Find applications of Theorem 2 in differential equations and elsewhere.

n ϩ k ؊1 nϩk
bd z
k

20. TEAM PROJECT. Fibonacci numbers.2 (a) The
Fibonacci numbers are recursively defined by
a0 ϭ a1 ϭ 1, anϩ1 ϭ an ϩ an؊1 if n ϭ 1, 2, Á .
Find the limit of the sequence (anϩ1>an).

nϩm n
bz
m

(b) Fibonacci’s rabbit problem. Compute a list of
a1, Á , a12. Show that a12 ϭ 233 is the number
of pairs of rabbits after 12 months if initially there
is 1 pair and each pair generates 1 pair per month,
beginning in the second month of existence (no deaths
occurring).

ؕ

4 nn(n Ϫ 1)
(z Ϫ i)n
15. a
3n
nϭ2
16–20

APPLICATIONS
OF THE IDENTITY THEOREM

(c) Generating function. Show that the generating
function of the Fibonacci numbers is f (z) ϭ
1>(1 Ϫ z Ϫ z 2); that is, if a power series (1) represents
this f (z), its coefficients must be the Fibonacci numbers
and conversely. Hint. Start from f (z)(1 Ϫ z Ϫ z 2) ϭ 1
and use Theorem 2.

State clearly and explicitly where and how you are using
Theorem 2.
16. Even functions. If f (z) in (2) is even (i.e.,
f (Ϫz) ϭ f (z)), show that an ϭ 0 for odd n. Give
examples.

15.4

Taylor and Maclaurin Series
The Taylor series3 of a function f (z), the complex analog of the real Taylor series is
ؕ

(1)

f (z) ϭ a an(z Ϫ z 0)n

where

an ϭ

nϭ1

1 (n)
f (z 0)
n!

or, by (1), Sec. 14.4,
(2)

an ϭ

2pi Ώ
1

C

f (z*)
(z* Ϫ z 0)nϩ1

dz*.

In (2) we integrate counterclockwise around a simple closed path C that contains z 0 in its
interior and is such that f (z) is analytic in a domain containing C and every point inside C.
A Maclaurin series3 is a Taylor series with center z 0 ϭ 0.
2
LEONARDO OF PISA, called FIBONACCI (ϭ son of Bonaccio), about 1180–1250, Italian mathematician,
credited with the first renaissance of mathematics on Christian soil.
3
BROOK TAYLOR (1685–1731), English mathematician who introduced real Taylor series. COLIN
MACLAURIN (1698–1746), Scots mathematician, professor at Edinburgh.

c15.qxd

11/1/10

6:41 PM

Page 691

SEC. 15.4 Taylor and Maclaurin Series

691

The remainder of the Taylor series (1) after the term an(z Ϫ z 0)n is
Rn(z) ϭ

(3)

(z Ϫ z 0)nϩ1
2pi

Ώ

C

f (z*)
(z* Ϫ z 0)nϩ1(z* Ϫ z)

dz*

(proof below). Writing out the corresponding partial sum of (1), we thus have
f (z) ϭ f (z 0) ϩ
(4)
ϩ

z Ϫ z0
1!

f r(z 0) ϩ

(z Ϫ z 0)n
n!

(z Ϫ z 0)2
2!

f s(z 0) ϩ Á

f (n)(z 0) ϩ Rn(z).

This is called Taylor’s formula with remainder.
We see that Taylor series are power series. From the last section we know that power
series represent analytic functions. And we now show that every analytic function can be
represented by power series, namely, by Taylor series (with various centers). This makes
Taylor series very important in complex analysis. Indeed, they are more fundamental in
complex analysis than their real counterparts are in calculus.
THEOREM 1

Taylor’s Theorem

Let f (z) be analytic in a domain D, and let z ϭ z 0 be any point in D. Then there
exists precisely one Taylor series (1) with center z 0 that represents f (z). This
representation is valid in the largest open disk with center z 0 in which f (z) is analytic.
The remainders Rn(z) of (1) can be represented in the form (3). The coefficients
satisfy the inequality
ƒ an ƒ Ϲ

(5)

M
rn

where M is the maximum of ƒ f (z) ƒ on a circle ƒ z Ϫ z 0 ƒ ϭ r in D whose interior is
also in D.
PROOF

The key tool is Cauchy’s integral formula in Sec. 14.3; writing z and z* instead of z 0 and
z (so that z* is the variable of integration), we have
(6)

f (z) ϭ

2pi Ώ

f (z*)

1

C

z* Ϫ z

dz*.

z lies inside C, for which we take a circle of radius r with center z 0 and interior in D
(Fig. 367). We develop 1>(z* Ϫ z) in (6) in powers of z Ϫ z 0. By a standard algebraic
manipulation (worth remembering!) we first have
(7)

1
1
ϭ
ϭ
z* Ϫ z
z* Ϫ z 0 Ϫ (z Ϫ z 0)

1
(z* Ϫ z 0) a1 Ϫ

z Ϫ z0
z* Ϫ z 0

b

.

c15.qxd

11/1/10

692

6:41 PM

Page 692

CHAP. 15 Power Series, Taylor Series
y
z*
z
z0

C

r

x

Fig. 367. Cauchy formula (6)

For later use we note that since z* is on C while z is inside C, we have
`

(7*)

z Ϫ z0

` Ͻ 1.

z* Ϫ z 0

(Fig. 367).

To (7) we now apply the sum formula for a finite geometric sum
1 Ϫ q nϩ1
1
q nϩ1
1 ϩ q ϩ Á ϩ qn ϭ
ϭ
Ϫ
1Ϫq
1Ϫq
1Ϫq

(8*)

(q

1),

which we use in the form (take the last term to the other side and interchange sides)
q nϩ1
1
ϭ 1 ϩ q ϩ Á ϩ qn ϩ
.
1Ϫq
1Ϫq

(8)

Applying this with q ϭ (z Ϫ z 0)>(z* Ϫ z 0) to the right side of (7), we get
1
z* Ϫ z

ϭ

1
z* Ϫ z 0

c1 ϩ
ϩ

z Ϫ z0
z* Ϫ z 0
1

a

ϩa

z Ϫ z0
z* Ϫ z 0

z Ϫ z0

z* Ϫ z z* Ϫ z 0

b ϩÁ ϩa

z Ϫ z0

2

z* Ϫ z 0

b d
n

nϩ1

b

.

We insert this into (6). Powers of z Ϫ z 0 do not depend on the variable of integration z*,
so that we may take them out from under the integral sign. This yields
f (z) ϭ

2pi Ώ
1

C

f (z*)
z Ϫ z0
dz* ϩ
z* Ϫ z 0
2pi
Áϩ

Ώ

C

f (z*)
dz* ϩ Á
(z* Ϫ z 0)2

(z Ϫ z 0)n
2pi

Ώ

C

f (z*)
(z* Ϫ z 0)nϩ1

dz* ϩ Rn(z)

with Rn(z) given by (3). The integrals are those in (2) related to the derivatives, so that
we have proved the Taylor formula (4).
Since analytic functions have derivatives of all orders, we can take n in (4) as large as
we please. If we let n approach infinity, we obtain (1). Clearly, (1) will converge and
represent f (z) if and only if
(9)

lim Rn(z) ϭ 0.

n :ϱ

c15.qxd

11/1/10

6:41 PM

Page 693

SEC. 15.4 Taylor and Maclaurin Series

693

We prove (9) as follows. Since z* lies on C, whereas z lies inside C (Fig. 367), we have
ƒ z* Ϫ z ƒ Ͼ 0. Since f (z) is analytic inside and on C, it is bounded, and so is the function
f (z*)>(z* Ϫ z), say,
`

f (z*)
z* Ϫ z

ෂ
` ϹM

for all z* on C. Also, C has the radius r ϭ ƒ z* Ϫ z 0 ƒ and the length 2pr. Hence by the
ML-inequality (Sec. 14.1) we obtain from (3)
ƒ Rn ƒ ϭ

ƒ z Ϫ z 0 ƒ nϩ1
2p

(10)

`

Ώ (z* Ϫ z )

f (z*)

C

0

(z* Ϫ z)

nϩ1

dz* `

ƒ z Ϫ z 0 ƒ nϩ1 ෂ 1
ෂ ` z Ϫ z0 `
Ϲ
M
2
p
r
ϭ
M
2p
r
r nϩ1

nϩ1

.

Now ƒ z Ϫ z 0 ƒ Ͻ r because z lies inside C. Thus ƒ z Ϫ z 0 ƒ >r Ͻ 1, so that the right side
approaches 0 as n : ϱ. This proves that the Taylor series converges and has the sum f (z).
Uniqueness follows from Theorem 2 in the last section. Finally, (5) follows from an in
(1) and the Cauchy inequality in Sec. 14.4. This proves Taylor’s theorem.
᭿
Accuracy of Approximation. We can achieve any preassinged accuracy in approximating f (z) by a partial sum of (1) by choosing n large enough. This is the practical use
of formula (9).
Singularity, Radius of Convergence. On the circle of convergence of (1) there is at
least one singular point of f (z), that is, a point z ϭ c at which f (z) is not analytic
(but such that every disk with center c contains points at which f (z) is analytic). We
also say that f (z) is singular at c or has a singularity at c. Hence the radius of convergence R of (1) is usually equal to the distance from z 0 to the nearest singular point
of f (z).
(Sometimes R can be greater than that distance: Ln z is singular on the negative real
axis, whose distance from z 0 ϭ Ϫ1 ϩ i is 1, but the Taylor series of Ln z with center
z 0 ϭ Ϫ1 ϩ i has radius of convergence 12.)

Power Series as Taylor Series
Taylor series are power series—of course! Conversely, we have
THEOREM 2

Relation to the Previous Section

A power series with a nonzero radius of convergence is the Taylor series of its sum.

PROOF

Given the power series
f (z) ϭ a0 ϩ a1(z Ϫ z 0) ϩ a2(z Ϫ z 0)2 ϩ a3(z Ϫ z 0)3 ϩ Á .

c15.qxd

11/1/10

6:41 PM

694

Page 694

CHAP. 15 Power Series, Taylor Series

Then f (z 0) ϭ a0. By Theorem 5 in Sec. 15.3 we obtain
f r(z) ϭ a1 ϩ 2a2(z Ϫ z 0) ϩ 3a3(z Ϫ z 0)2 ϩ Á ,

thus

f r(z 0) ϭ a1

f s(z) ϭ 2a2 ϩ 3 # 2(z Ϫ z 0) ϩ Á ,

thus

f s(z 0) ϭ 2!a2

and in general f (n)(z 0) ϭ n!an. With these coefficients the given series becomes the Taylor
series of f (z) with center z 0.
᭿
Comparison with Real Functions. One surprising property of complex analytic
functions is that they have derivatives of all orders, and now we have discovered the other
surprising property that they can always be represented by power series of the form (1).
This is not true in general for real functions; there are real functions that have derivatives
of all orders but cannot be represented by a power series. (Example: f (x) ϭ exp (Ϫ1>x 2)
if x 0 and f (0) ϭ 0; this function cannot be represented by a Maclaurin series in an
open disk with center 0 because all its derivatives at 0 are zero.)

Important Special Taylor Series
These are as in calculus, with x replaced by complex z. Can you see why? (Answer. The
coefficient formulas are the same.)
EXAMPLE 1

Geometric Series
Let f (z) ϭ 1>(1 Ϫ z). Then we have f (n)(z) ϭ n!>(1 Ϫ z)nϩ1, f (n)(0) ϭ n!. Hence the Maclaurin expansion of
1>(1 Ϫ z) is the geometric series
1

ؕ

ϭ a zn ϭ 1 ϩ z ϩ z2 ϩ Á
1 Ϫ z nϭ0

(11)

f (z) is singular at z ϭ 1; this point lies on the circle of convergence.

EXAMPLE 2

( ƒ z ƒ Ͻ 1).

᭿

Exponential Function
We know that the exponential function ez (Sec. 13.5) is analytic for all z, and (ez) r ϭ ez. Hence from (1) with
z 0 ϭ 0 we obtain the Maclaurin series
ؕ
zn
z2
ez ϭ a
ϭ1ϩzϩ
ϩÁ.
n!
2!
nϭ0

(12)

This series is also obtained if we replace x in the familiar Maclaurin series of ex by z.
Furthermore, by setting z ϭ iy in (12) and separating the series into the real and imaginary parts (see Theorem
2, Sec. 15.1) we obtain
ؕ

eiy ϭ a
nϭ0

ؕ
ؕ
y 2k
y 2kϩ1
ϭ a (Ϫ1)k
ϩ i a (Ϫ1)k
.
n!
(2k)!
(2k ϩ 1)!
kϭ0
kϭ0

(iy)n

Since the series on the right are the familiar Maclaurin series of the real functions cos y and sin y, this shows
that we have rediscovered the Euler formula
(13)

eiy ϭ cos y ϩ i sin y.

Indeed, one may use (12) for defining ez and derive from (12) the basic properties of ez. For instance, the
᭿
differentiation formula (ez) r ϭ ez follows readily from (12) by termwise differentiation.

c15.qxd

11/1/10

6:41 PM

Page 695

SEC. 15.4 Taylor and Maclaurin Series
EXAMPLE 3

695

Trigonometric and Hyperbolic Functions
By substituting (12) into (1) of Sec. 13.6 we obtain
ؕ
z 2n
z2
z4
cos z ϭ a (Ϫ1)n
ϭ1Ϫ
ϩ
Ϫϩ Á
(2n)!
2!
4!
nϭ0

(14)

ؕ
z 2nϩ1
z3
z5
ϭzϪ
ϩ
Ϫϩ Á.
sin z ϭ a (Ϫ1)n
(2n ϩ 1)!
3!
5!
nϭ0

When z ϭ x these are the familiar Maclaurin series of the real functions cos x and sin x. Similarly, by substituting
(12) into (11), Sec. 13.6, we obtain
ؕ
z 2n
z2
z4
ϭ1ϩ
ϩ
ϩÁ
cosh z ϭ a
(2n)!
2!
4!
nϭ0

(15)

EXAMPLE 4

ؕ
z 2nϩ1
z3
z5
sinh z ϭ a
ϭzϩ
ϩ
ϩÁ.
(2n ϩ 1)!
3!
5!
nϭ0

᭿

Logarithm
From (1) it follows that

Ln (1 ϩ z) ϭ z Ϫ

(16)

z2
2

ϩ

z3

Ϫϩ Á

3

( ƒ z ƒ Ͻ 1).

Replacing z by Ϫz and multiplying both sides by Ϫ1, we get
ϪLn (1 Ϫ z) ϭ Ln

(17)

1
1Ϫz

ϭzϩ

z2
2

ϩ

z3
3

ϩÁ

( ƒ z ƒ Ͻ 1).

By adding both series we obtain
(18)

Ln

1ϩz
1Ϫz

ϭ 2 az ϩ

z3
3

ϩ

z5
5

ϩÁb

( ƒ z ƒ Ͻ 1).

᭿

Practical Methods
The following examples show ways of obtaining Taylor series more quickly than by the
use of the coefficient formulas. Regardless of the method used, the result will be the same.
This follows from the uniqueness (see Theorem 1).
EXAMPLE 5

Substitution
Find the Maclaurin series of f (z) ϭ 1>(1 ϩ z 2).

Solution. By substituting Ϫz 2 for z in (11) we obtain
(19)

1
1ϩz

2

ϭ

1
1 Ϫ (Ϫz )
2

ؕ

ؕ

nϭ0

nϭ0

ϭ a (Ϫz 2)n ϭ a (Ϫ1)nz 2n ϭ 1 Ϫ z 2 ϩ z 4 Ϫ z 6 ϩ Á

( ƒ z ƒ Ͻ 1).

᭿

c15.qxd

11/1/10

6:41 PM

696
EXAMPLE 6

Page 696

CHAP. 15 Power Series, Taylor Series
Integration
Find the Maclaurin series of f (z) ϭ arctan z.

Solution. We have f r(z) ϭ 1>(1 ϩ z 2). Integrating (19) term by term and using f (0) ϭ 0 we get
ؕ
(Ϫ1)n 2nϩ1
z3
z5
arctan z ϭ a
z
ϭzϪ
ϩ
Ϫϩ Á
2n ϩ 1
3
5
nϭ0

( ƒ z ƒ Ͻ 1);

this series represents the principal value of w ϭ u ϩ iv ϭ arctan z defined as that value for which
ƒ u ƒ Ͻ p>2.
᭿

EXAMPLE 7

Development by Using the Geometric Series
Develop 1>(c Ϫ z) in powers of z Ϫ z 0, where c Ϫ z 0

0.

Solution. This was done in the proof of Theorem 1, where c ϭ z*. The beginning was simple algebra and
then the use of (11) with z replaced by (z Ϫ z 0)>(c Ϫ z 0):
1
1
ϭ
ϭ
cϪz
c Ϫ z 0 Ϫ (z Ϫ z 0)

ϭ

1
(c Ϫ z 0) a1 Ϫ

z Ϫ z0
b
c Ϫ z0

ϭ

ؕ
z Ϫ z0 n
1
a
b
c Ϫ z0 a c Ϫ z0
nϭ0

z Ϫ z0
z Ϫ z0 2
1
a1 ϩ
ϩa
b ϩ Á b.
c Ϫ z0
c Ϫ z0
c Ϫ z0

This series converges for
`

EXAMPLE 8

z Ϫ z0
` Ͻ 1,
c Ϫ z0

ƒ z Ϫ z0 ƒ Ͻ ƒ c Ϫ z0 ƒ .

that is,

᭿

Binomial Series, Reduction by Partial Fractions
Find the Taylor series of the following function with center z 0 ϭ 1.
f (z) ϭ

2z 2 ϩ 9z ϩ 5
z 3 ϩ z 2 Ϫ 8z Ϫ 12

Solution. We develop f (z) in partial fractions and the first fraction in a binomial series
1
(1 ϩ z)

m

(20)

ϭ 1 Ϫ mz ϩ

ؕ
Ϫm n
ϭ (1 ϩ z)؊m ϭ a a
bz
n
nϭ0

m(m ϩ 1)
2!

z2 Ϫ

m(m ϩ 1)(m ϩ 2)
3!

z3 ϩ Á

with m ϭ 2 and the second fraction in a geometric series, and then add the two series term by term. This gives
f (z) ϭ
ϭ

1
2
1
2
1
1
1
ϩ
ϭ
Ϫ
ϭ a
bϪ
1
(z ϩ 2)2
[3 ϩ (z Ϫ 1)]2
1 Ϫ 12 (z Ϫ 1)
zϪ3
2 Ϫ (z Ϫ 1)
9 [1 ϩ 3 (z Ϫ 1)]2
n

n

ؕ
ؕ
zϪ1
zϪ1
(Ϫ1)n(n ϩ 1)
Ϫ2
1
a
b
a
b
Ϫ
a
b
ϭ
Ϫ n d (z Ϫ 1)n
c
a n
a
a
9 nϭ0
3
2
2
3nϩ2
nϭ0
nϭ0

1

ϭϪ

ؕ

8
9

Ϫ

31
54

(z Ϫ 1) Ϫ

23
108

(z Ϫ 1)2 Ϫ

275
1944

(z Ϫ 1)3 Ϫ Á .

We see that the first series converges for ƒ z Ϫ 1 ƒ Ͻ 3 and the second for ƒ z Ϫ 1 ƒ Ͻ 2. This had to be expected
because 1>(z ϩ 2)2 is singular at Ϫ2 and 2>(z Ϫ 3) at 3, and these points have distance 3 and 2, respectively,
᭿
from the center z 0 ϭ 1. Hence the whole series converges for ƒ z Ϫ 1 ƒ Ͻ 2.

c15.qxd

11/1/10

6:41 PM

Page 697

SEC. 15.4 Taylor and Maclaurin Series

697

PROBLEM SET 15.4
1. Calculus. Which of the series in this section have you
discussed in calculus? What is new?
2. On Examples 5 and 6. Give all the details in the
derivation of the series in those examples.

MACLAURIN SERIES

3–10

Find the Maclaurin series and its radius of convergence.
zϩ2
3. sin 2z 2
4.
1 Ϫ z2
1
1
5.
6.
1 ϩ 3iz
2 ϩ z4
7.
9.

cos2 12 z
z

Ύ

0

10. exp (z 2)

Ύ

12. C(z) ϭ

sin t 2 dt
2

1p

Ύ

e؊t dt
2

14. Si(z) ϭ

0

Ύ

z

cos t 2 dt

Ύ

z

0

E2

sec z ϭ E 0 Ϫ

2!

z2 ϩ

E4
4!

sin t
dt
t

z4 Ϫ ϩ Á

defines the Euler numbers E 2n. Show that E 0 ϭ 1,
E 2 ϭ Ϫ1, E 4 ϭ 5, E 6 ϭ Ϫ61. Write a program that
computes the E 2n from the coefficient formula in (1)
or extracts them as a list from the series. (For tables
see Ref. [GenRef1], p. 810, listed in App. 1.)
(b) Bernoulli numbers. The Maclaurin series
(22)

B2 2 B3 3
z
ϭ 1 ϩ B1z ϩ
z ϩ
z ϩÁ
e Ϫ1
2!
3!
z

4

2i
e2iz Ϫ 1

Ϫ

4i
e4iz Ϫ 1

Ϫi

22n(22n Ϫ 1)
(2n)!

nϭ1

3

15. CAS Project. sec, tan. (a) Euler numbers. The
Maclaurin series
(21)

tan z ϭ

B2n z 2n؊1.

16. Inverse sine. Developing 1> 21 Ϫ z 2 and integrating,
show that

0

z

B3 ϭ 0,

1 Á
B5 ϭ 0, B6 ϭ 42
, .

ϭ a (Ϫ1)n؊1

exp (Ϫt 2) dt

HIGHER TRANSCENDENTAL
FUNCTIONS

z

B4 ϭ

1
Ϫ 30
,

ؕ

0

0

13. erf z ϭ

(24)

z

Ύ

B1 ϭ Ϫ 12 , B2 ϭ 16 ,

Write a program for computing Bn.
(c) Tangent. Using (1), (2), Sec. 13.6, and (22), show
that tan z has the following Maclaurin series and
calculate from it a table of B0, Á , B20:

8. sin z
2

Find the Maclaurin series by termwise integrating the
integrand. (The integrals cannot be evaluated by the usual
methods of calculus. They define the error function erf z,
sine integral Si(z), and Fresnel integrals4 S(z) and C(z),
which occur in statistics, heat conduction, optics, and other
applications. These are special so-called higher transcendental functions.)
11. S(z) ϭ

(23)

2

Ϫt
exp a
b dt
2

11–14

defines the Bernoulli numbers Bn. Using undetermined
coefficients, show that

1 z
1
arcsin z ϭ z ϩ a b
ϩa
2 3
2
ϩa

1
2

# 3 z5
# 4b 5

# 3 # 5 z7
Á
# 4 # 6 b 7 ϩ ( ƒ z ƒ Ͻ 1).

Show that this series represents the principal value of
arcsin z (defined in Team Project 30, Sec. 13.7).
17. TEAM PROJECT. Properties from Maclaurin
Series. Clearly, from series we can compute function
values. In this project we show that properties of
functions can often be discovered from their Taylor or
Maclaurin series. Using suitable series, prove the
following.
(a) The formulas for the derivatives of ez, cos z, sin z,
cosh z, sinh z. and Ln (1 ϩ z)
(b) 12 (eiz ϩ e؊iz) ϭ cos z
(c) sin z 0 for all pure imaginary z ϭ iy 0
18–25

TAYLOR SERIES

Find the Taylor series with center
convergence.
18. 1>z, z 0 ϭ i
19.
20. cos2 z, z 0 ϭ p>2
21.
22. cosh (z Ϫ pi), z 0 ϭ pi
23. 1>(z ϩ i)2, z 0 ϭ i
24.
25. sinh (2z Ϫ i), z 0 ϭ i>2

z 0 and its radius of
1>(1 Ϫ z), z 0 ϭ i
sin z, z 0 ϭ p>2
ez(z؊2), z 0 ϭ 1

AUGUSTIN FRESNEL (1788–1827), French physicist and engineer, known for his work in optics.

c15.qxd

11/1/10

6:41 PM

698

15.5

Page 698

CHAP. 15 Power Series, Taylor Series

Uniform Convergence.

Optional

We know that power series are absolutely convergent (Sec. 15.2, Theorem 1) and, as
another basic property, we now show that they are uniformly convergent. Since uniform
convergence is of general importance, for instance, in connection with termwise integration
of series, we shall discuss it quite thoroughly.
To define uniform convergence, we consider a series whose terms are any complex
functions f0(z), f1(z), Á
ؕ

Á.
a fm(z) ϭ f0(z) ϩ f1(z) ϩ f2(z) ϩ

(1)

mϭ0

(This includes power series as a special case in which fm(z) ϭ am(z Ϫ z 0)m.) We assume
that the series (1) converges for all z in some region G. We call its sum s (z) and its nth
partial sum sn(z); thus
sn(z) ϭ f0(z) ϩ f1(z) ϩ Á ϩ fn(z).
Convergence in G means the following. If we pick a z ϭ z 1 in G, then, by the definition
of convergence at z 1, for given P Ͼ 0 we can find an N1(P) such that
ƒ s (z1) Ϫ sn(z1) ƒ Ͻ P

for all n Ͼ N1(P).

If we pick a z 2 in G, keeping P as before, we can find an N2(P) such that
ƒ s (z 2) Ϫ sn(z 2) ƒ Ͻ P

for all n Ͼ N2(P),

and so on. Hence, given an P Ͼ 0, to each z in G there corresponds a number Nz(P). This
number tells us how many terms we need (what sn we need) at a z to make ƒ s (z) Ϫ sn(z) ƒ
smaller than P. Thus this number Nz(P) measures the speed of convergence.
Small Nz(P) means rapid convergence, large Nz(P) means slow convergence at the point
z considered. Now, if we can find an N (P) larger than all these Nz(P) for all z in G, we
say that the convergence of the series (1) in G is uniform. Hence this basic concept is
defined as follows.
DEFINITION

Uniform Convergence

A series (1) with sum s (z) is called uniformly convergent in a region G if for every
P Ͼ 0 we can find an N ϭ N (P), not depending on z, such that
ƒ s (z) Ϫ sn(z) ƒ Ͻ P

for all n Ͼ N (P) and all z in G.

Uniformity of convergence is thus a property that always refers to an infinite set in
the z-plane, that is, a set consisting of infinitely many points.
EXAMPLE 1

Geometric Series
Show that the geometric series 1 ϩ z ϩ z 2 ϩ Á is (a) uniformly convergent in any closed disk ƒ z ƒ Ϲ r Ͻ 1,
(b) not uniformly convergent in its whole disk of convergence ƒ z ƒ Ͻ 1.

c15.qxd

11/1/10

6:41 PM

Page 699

SEC. 15.5 Uniform Convergence. Optional

699

Solution. (a) For z in that closed disk we have ƒ 1 Ϫ z ƒ м 1 Ϫ r (sketch it). This implies that
1> ƒ 1 Ϫ z ƒ Ϲ 1>(1 Ϫ r). Hence (remember (8) in Sec. 15.4 with q ϭ z)
ؕ

ƒ s(z) Ϫ sn(z) ƒ ϭ 2

a

zm 2 ϭ 2

mϭnϩ1

z nϩ1
1Ϫz

2Ϲ

r nϩ1
1Ϫr

.

Since r Ͻ 1, we can make the right side as small as we want by choosing n large enough, and since the right
side does not depend on z (in the closed disk considered), this means that the convergence is uniform.
(b) For given real K (no matter how large) and n we can always find a z in the disk ƒ z ƒ Ͻ 1 such that
`

z nϩ1
1Ϫz

` ϭ

ƒ z ƒ nϩ1
ƒ1 Ϫ zƒ

Ͼ K,

simply by taking z close enough to 1. Hence no single N (P) will suffice to make ƒ s (z) Ϫ sn(z) ƒ smaller than a
given P Ͼ 0 throughout the whole disk. By definition, this shows that the convergence of the geometric series
in ƒ z ƒ Ͻ 1 is not uniform.
᭿

This example suggests that for a power series, the uniformity of convergence may at most
be disturbed near the circle of convergence. This is true:
THEOREM 1

Uniform Convergence of Power Series

A power series
ؕ

m
a am(z Ϫ z 0)

(2)

mϭ0

with a nonzero radius of convergence R is uniformly convergent in every circular
disk ƒ z Ϫ z 0 ƒ Ϲ r of radius r Ͻ R.

PROOF

For ƒ z Ϫ z 0 ƒ Ϲ r and any positive integers n and p we have
(3) ƒ anϩ1(z Ϫ z 0)nϩ1 ϩ Á ϩ anϩp(z Ϫ z 0)nϩp ƒ Ϲ ƒ anϩ1 ƒ r nϩ1 ϩ Á ϩ ƒ anϩp ƒ r nϩp.
Now (2) converges absolutely if ƒ z Ϫ z 0 ƒ ϭ r Ͻ R (by Theorem 1 in Sec. 15.2). Hence
it follows from the Cauchy convergence principle (Sec. 15.1) that, an P Ͼ 0 being given,
we can find an N (P) such that
ƒ anϩ1 ƒ r nϩ1 ϩ Á ϩ ƒ anϩp ƒ r nϩp Ͻ P

for n Ͼ N (P) and

p ϭ 1, 2, Á .

From this and (3) we obtain
ƒ anϩ1(z Ϫ z 0)nϩ1 ϩ Á ϩ anϩp(z Ϫ z 0)nϩp ƒ Ͻ P
for all z in the disk ƒ z Ϫ z 0 ƒ Ϲ r, every n Ͼ N (P), and every p ϭ 1, 2, Á . Since N (P) is
independent of z, this shows uniform convergence, and the theorem is proved.
᭿
Thus we have established uniform convergence of power series, the basic concern of this
section. We now shift from power series to arbitary series of variable terms and examine
uniform convergence in this more general setting. This will give a deeper understanding
of uniform convergence.

c15.qxd

11/1/10

6:41 PM

700

Page 700

CHAP. 15 Power Series, Taylor Series

Properties of Uniformly Convergent Series
Uniform convergence derives its main importance from two facts:
1. If a series of continuous terms is uniformly convergent, its sum is also continuous
(Theorem 2, below).
2. Under the same assumptions, termwise integration is permissible (Theorem 3).
This raises two questions:
1. How can a converging series of continuous terms manage to have a discontinuous
sum? (Example 2)
2. How can something go wrong in termwise integration? (Example 3)
Another natural question is:
3. What is the relation between absolute convergence and uniform convergence? The
surprising answer: none. (Example 5)
These are the ideas we shall discuss.
If we add finitely many continuous functions, we get a continuous function as their sum.
Example 2 will show that this is no longer true for an infinite series, even if it converges
absolutely. However, if it converges uniformly, this cannot happen, as follows.
THEOREM 2

Continuity of the Sum

Let the series
ؕ

Á
a fm(z) ϭ f0(z) ϩ f1(z) ϩ
mϭ0

be uniformly convergent in a region G. Let F (z) be its sum. Then if each term fm(z)
is continuous at a point z 1 in G, the function F (z) is continuous at z1.

PROOF

Let sn(z) be the nth partial sum of the series and Rn(z) the corresponding remainder:
sn ϭ f0 ϩ f1 ϩ Á ϩ fn,

Rn ϭ fnϩ1 ϩ fnϩ2 ϩ Á .

Since the series converges uniformly, for a given P Ͼ 0 we can find an N ϭ N (P) such that
ƒ RN(z) ƒ Ͻ

P
3

for all z in G.

Since sN(z) is a sum of finitely many functions that are continuous at z 1, this sum is
continuous at z 1. Therefore, we can find a d Ͼ 0 such that
ƒ sN(z) Ϫ sN(z1) ƒ Ͻ

P
3

for all z in G for which ƒ z Ϫ z1 ƒ Ͻ d.

Using F ϭ sN ϩ RN and the triangle inequality (Sec. 13.2), for these z we thus obtain
ƒ F(z) Ϫ F (z 1) ƒ ϭ ƒ sN(z) ϩ RN(z) Ϫ [sN(z 1) ϩ RN(z 1)] ƒ
Ϲ ƒ sN(z) Ϫ sN(z1) ƒ ϩ ƒ RN(z) ƒ ϩ ƒ RN(z1) ƒ Ͻ

P
P
P
ϩ ϩ ϭ P.
3
3
3

This implies that F (z) is continuous at z1, and the theorem is proved.

᭿

c15.qxd

11/1/10

6:41 PM

Page 701

SEC. 15.5 Uniform Convergence. Optional
EXAMPLE 2

701

Series of Continuous Terms with a Discontinuous Sum
Consider the series
x2

x2 ϩ

1ϩx

2

ϩ

x2
(1 ϩ x )

2 2

x2

ϩ

ϩÁ

(1 ϩ x 2)3

(x real).

This is a geometric series with q ϭ 1>(1 ϩ x 2) times a factor x 2. Its nth partial sum is
sn(x) ϭ x 2 c 1 ϩ

1
1ϩx

2

1

ϩ

(1 ϩ x )

2 2

1

ϩÁ ϩ

(1 ϩ x 2)n

d.

We now use the trick by which one finds the sum of a geometric series, namely, we multiply sn(x) by
Ϫq ϭ Ϫ1>(1 ϩ x 2),
Ϫ

1
1ϩx

2

sn(x) ϭ Ϫx 2 c

1
1ϩx

ϩÁϩ

2

1
(1 ϩ x )

2 n

ϩ

1
(1 ϩ x 2)nϩ1

d.

Adding this to the previous formula, simplifying on the left, and canceling most terms on the right, we obtain
x2
1 ϩ x2

sn(x) ϭ x 2 c 1 Ϫ

1
(1 ϩ x 2)nϩ1

d,

thus
sn(x) ϭ 1 ϩ x 2 Ϫ

1
(1 ϩ x 2)n

.

The exciting Fig. 368 “explains” what is going on. We see that if x

0, the sum is

s (x) ϭ lim sn(x) ϭ 1 ϩ x ,
2

n:ϱ

but for x ϭ 0 we have sn(0) ϭ 1 Ϫ 1 ϭ 0 for all n, hence s (0) ϭ 0. So we have the surprising fact that the sum
is discontinuous (at x ϭ 0), although all the terms are continuous and the series converges even absolutely (its
terms are nonnegative, thus equal to their absolute value!).
Theorem 2 now tells us that the convergence cannot be uniform in an interval containing x ϭ 0. We can also
verify this directly. Indeed, for x 0 the remainder has the absolute value
ƒ Rn(x) ƒ ϭ ƒ s (x) Ϫ sn(x) ƒ ϭ

1
(1 ϩ x 2)n

and we see that for a given P (Ͻ1) we cannot find an N depending only on P such that ƒ Rn ƒ Ͻ P for all n Ͼ N(P)
and all x, say, in the interval 0 Ϲ x Ϲ 1.
᭿
y
2

s

s
s4

1.5
s64
s16

–1

0

s1

1

x

Fig. 368. Partial sums in Example 2

Termwise Integration
This is our second topic in connection with uniform convergence, and we begin with an
example to become aware of the danger of just blindly integrating term-by-term.

c15.qxd

11/1/10

6:41 PM

702

Page 702

CHAP. 15 Power Series, Taylor Series

EXAMPLE 3

Series for Which Termwise Integration Is Not Permissible
Let u m(x) ϭ mxe؊mx and consider the series
2

ؕ

a fm(x)

fm(x) ϭ u m(x) Ϫ u m؊1(x)

where

mϭ0

in the interval 0 Ϲ x Ϲ 1. The nth partial sum is
sn ϭ u 1 Ϫ u 0 ϩ u 2 Ϫ u 1 ϩ Á ϩ u n Ϫ u n؊1 ϭ u n Ϫ u 0 ϭ u n.
Hence the series has the sum F (x) ϭ lim sn(x) ϭ lim u n(x) ϭ 0
n:ϱ

(0 Ϲ x Ϲ 1). From this we obtain

n:ϱ

1

Ύ

F (x) dx ϭ 0.

0

On the other hand, by integrating term by term and using f1 ϩ f2 ϩ Á ϩ fn ϭ sn, we have
ؕ

Ύ

a

1

0

mϭ1

n

fm(x) dx ϭ lim a
n:ϱ

mϭ1

Ύ

1

fm(x) dx ϭ lim

n:ϱ

0

Ύ

1

sn(x) dx.

0

Now sn ϭ u n and the expression on the right becomes
lim

n:ϱ

Ύ

1

u n(x) dx ϭ lim

n:ϱ

0

Ύ

1

nxe؊nx dx ϭ lim
2

n:ϱ

0

1
2

(1 Ϫ e؊n) ϭ

1
2

,

but not 0. This shows that the series under consideration cannot be integrated term by term from x ϭ 0 to
x ϭ 1.
᭿

The series in Example 3 is not uniformly convergent in the interval of integration, and
we shall now prove that in the case of a uniformly convergent series of continuous
functions we may integrate term by term.
THEOREM 3

Termwise Integration

Let
ؕ

F (z) ϭ a fm(z) ϭ f0(z) ϩ f1(z) ϩ Á
mϭ0

be a uniformly convergent series of continuous functions in a region G. Let C be
any path in G. Then the series
ؕ

(4)

a
mϭ0

Ύf

m(z)

dz ϭ

C

is convergent and has the sum

Ύ f (z) dz ϩ Ύ f (z) dz ϩ Á
0

C

1

C

Ύ F (z) dz.
C

PROOF

From Theorem 2 it follows that F (z) is continuous. Let sn(z) be the nth partial sum of the
given series and Rn(z) the corresponding remainder. Then F ϭ sn ϩ Rn and by integration,

Ύ F (z) dz ϭ Ύ s (z) dz ϩ Ύ R (z) dz.
n

C

C

n

C

c15.qxd

11/1/10

6:41 PM

Page 703

SEC. 15.5 Uniform Convergence. Optional

703

Let L be the length of C. Since the given series converges uniformly, for every given
P Ͼ 0 we can find a number N such that ƒ Rn(z) ƒ Ͻ P>L for all n Ͼ N and all z in G. By
applying the ML-inequality (Sec. 14.1) we thus obtain
`

Ύ R (z) dz ` Ͻ LP L ϭ P

for all n Ͼ N.

n

C

Since Rn ϭ F Ϫ sn, this means that
`

Ύ F (z) dz Ϫ Ύ s (z) dz ` Ͻ P

for all n Ͼ N.

n

C

C

Hence, the series (4) converges and has the sum indicated in the theorem.

᭿

Theorems 2 and 3 characterize the two most important properties of uniformly convergent
series. Also, since differentiation and integration are inverse processes, Theorem 3 implies
THEOREM 4

Termwise Differentiation

Let the series f0(z) ϩ f1(z) ϩ f2(z) ϩ Á be convergent in a region G and let F (z)
be its sum. Suppose that the series f0r(z) ϩ f1r(z) ϩ f2r(z) ϩ Á converges uniformly
in G and its terms are continuous in G. Then
F r(z) ϭ f0r (z) ϩ f1r (z) ϩ f2r (z) ϩ Á

for all z in G.

Test for Uniform Convergence
Uniform convergence is usually proved by the following comparison test.
Weierstrass5 M-Test for Uniform Convergence

THEOREM 5

Consider a series of the form (1) in a region G of the z-plane. Suppose that one can
find a convergent series of constant terms,
(5)

M0 ϩ M1 ϩ M2 ϩ Á ,

such that ƒ fm(z) ƒ Ϲ M m for all z in G and every m ϭ 0, 1, Á . Then (1) is uniformly
convergent in G.

The simple proof is left to the student (Team Project 18).

5

KARL WEIERSTRASS (1815–1897), great German mathematician, who developed complex analysis based
on the concept of power series and residue integration. (See footnote in Section 13.4.) He put analysis on a
sound theoretical footing. His mathematical rigor is so legendary that one speaks Weierstrassian rigor. (See
paper by Birkhoff and Kreyszig, 1984 in footnote in Sec. 5.5; Kreyszig, E., On the Calculus, of Variations and
Its Major Influences on the Mathematics of the First Half of Our Century. Part II, American Mathematical
Monthly (1994), 101, No. 9, pp. 902–908). Weierstrass also made contributions to the calculus of variations,
approximation theory, and differential geometry. He obtained the concept of uniform convergence in 1841
(published 1894, sic!); the first publication on the concept was by G. G. STOKES (see Sec 10.9) in 1847.

c15.qxd

11/1/10

6:41 PM

704
EXAMPLE 4

Page 704

CHAP. 15 Power Series, Taylor Series
Weierstrass M-Test
Does the following series converge uniformly in the disk ƒ z ƒ Ϲ 1?
ؕ

zm ϩ 1

mϭ1

m 2 ϩ cosh m ƒ z ƒ

a

.

Solution. Uniform convergence follows by the Weierstrass M-test and the convergence of S1>m 2 (see
Sec. 15.1, in the proof of Theorem 8) because
`

zm ϩ 1
m 2 ϩ cosh m ƒ z ƒ

` Ϲ
Ϲ

ƒzƒm ϩ 1
m2
2
m2

᭿

.

No Relation Between Absolute
and Uniform Convergence
We finally show the surprising fact that there are series that converge absolutely but not
uniformly, and others that converge uniformly but not absolutely, so that there is no relation
between the two concepts.
EXAMPLE 5

No Relation Between Absolute and Uniform Convergence
The series in Example 2 converges absolutely but not uniformly, as we have shown. On the other hand, the series
ؕ

(Ϫ1)m؊1

mϭ1

x ϩm

a

2

1

ϭ

x ϩ1
2

Ϫ

1
x ϩ2
2

ϩ

1
x ϩ3
2

Ϫ ϩÁ

(x real)

converges uniformly on the whole real line but not absolutely.
Proof. By the familiar Leibniz test of calculus (see App. A3.3) the remainder Rn does not exceed its first
term in absolute value, since we have a series of alternating terms whose absolute values form a monotone
decreasing sequence with limit zero. Hence given P Ͼ 0, for all x we have
ƒ Rn(x) ƒ Ϲ

1
1
Ͻ ϽP
n
x2 ϩ n ϩ 1

if n Ͼ N(P) м

1
.
P

This proves uniform convergence, since N (P) does not depend on x.
The convergence is not absolute because for any fixed x we have
`

(Ϫ1)m؊1
x ϩm
2

where k is a suitable constant, and kS1>m diverges.

` ϭ

1

x ϩm
k
Ͼ
m
2

᭿

PROBLEM SET 15.5
1. CAS EXPERIMENT. Graphs of Partial Sums. (a)
Fig. 368. Produce this exciting figure using your CAS.
Add further curves, say, those of s256, s1024, etc. on the
same screen.

(b) Power series. Study the nonuniformity of convergence experimentally by graphing partial sums near
the endpoints of the convergence interval for real
z ϭ x.

c15.qxd

11/1/10

6:41 PM

Page 705

SEC. 15.5 Uniform Convergence. Optional

POWER SERIES

2–9

Where does the power series converge uniformly? Give
reason.
ؕ

n

nϩ2
2. a a
b zn
7n
Ϫ
3
nϭ0
1
3. a n (z ϩ i)2n
3
nϭ0
ؕ

3n(1 Ϫ i)n
(z Ϫ i)n
n!
nϭ0

4. a

ؕ
n
5. a a b (4z ϩ 2i)n
2
nϭ2
n

2

6. a 2 (tanh n ) z

(b) Termwise differentiation. Derive Theorem 4
from Theorem 3.
(c) Subregions. Prove that uniform convergence of a
series in a region G implies uniform convergence in
any portion of G. Is the converse true?
(d) Example 2. Find the precise region of convergence
of the series in Example 2 with x replaced by a complex
variable z.

ؕ

ؕ

705

ϱ
(e) Figure 369. Show that x 2 S mϭ1
(1 ϩ x 2)؊m ϭ 1
if x 0 and 0 if x ϭ 0. Verify by computation that the
partial sums s1, s2, s3 look as shown in Fig. 369.

y
1

2n

s
s3

nϭ0
ؕ

s2

1
7. a 2 az ϩ ib
2
nϭ1 n
n!

ؕ
3n
8. a
(z Ϫ 1)2n
n(n
ϩ
1)
nϭ1
ؕ

9. a
nϭ1

(Ϫ1)n
2nn 2

Prove that the series converges uniformly in the indicated
region.
ؕ z 2n
10. a
,
2n!
nϭ0
ؕ

11. a
nϭ1

zn
n

nϭ1
ؕ

13. a
nϭ1

nϭ0

n 3 cosh n ƒ z ƒ

,

ƒzƒ Ϲ 1

n

sin ƒ z ƒ
n2

, all z

zn
ƒ z ƒ 2n ϩ 1

ؕ (n!)2
15. a
z n,
(2n!)
nϭ0
ؕ tanhn ƒ z ƒ
16. a
,
n(n ϩ 1)
nϭ1
ؕ

pn

nϭ1

n4

17. a

HEAT EQUATION

2

ؕ

14. a

19–20

Show that (9) in Sec. 12.6 with coefficients (10) is a solution
of the heat equation for t Ͼ 0, assuming that f (x) is
continuous on the interval 0 Ϲ x Ϲ L and has one-sided
derivatives at all interior points of that interval. Proceed as
follows.

ƒ u n ƒ Ͻ Ke؊ln t0

zn

z 2n,

,

x

19. Show that ƒ Bn ƒ is bounded, say ƒ Bn ƒ Ͻ K for all n.
Conclude that

ƒzƒ Ϲ 1

,

2

ؕ

12. a

ƒ z ƒ Ϲ 1020

1

0

Fig. 369. Sum s and partial
sums in Team Project 18(e)

(z Ϫ 2i)n

UNIFORM CONVERGENCE

10–17

–1

s1

2 Ϲ ƒ z ƒ Ϲ 10
ƒzƒ Ϲ 3
all z

ƒ z ƒ Ϲ 0.56

18. TEAM PROJECT. Uniform Convergence.
(a) Weierstrass M-test. Give a proof.

t м t0 Ͼ 0

if

and, by the Weierstrass test, the series (9) converges
uniformly with respect to x and t for t м t 0, 0 Ϲ x Ϲ L.
Using Theorem 2, show that u (x, t) is continuous for
t м t 0 and thus satisfies the boundary conditions (2)
for t м t 0.

20. Show that ƒ 0u n>0t ƒ Ͻ l2nKe؊ln t0 if t м t 0 and the
series of the expressions on the right converges, by
the ratio test. Conclude from this, the Weierstrass
test, and Theorem 4 that the series (9) can be
differentiated term by term with respect to t and the
resulting series has the sum 0u>0t. Show that (9) can
be differentiated twice with respect to x and the
resulting series has the sum 0 2u>0x 2. Conclude from
this and the result to Prob. 19 that (9) is a solution
of the heat equation for all t м t 0. (The proof that (9)
satisfies the given initial condition can be found in
Ref. [C10] listed in App. 1.)
2

c15.qxd

11/1/10

6:41 PM

Page 706

706

CHAP. 15 Power Series, Taylor Series

CHAPTER 15 REVIEW QUESTIONS AND PROBLEMS
1. What is convergence test for series? State two tests from
memory. Give examples.
2. What is a power series? Why are these series very
important in complex analysis?
3. What is absolute convergence? Conditional convergence?
Uniform convergence?
4. What do you know about convergence of power series?
5. What is a Taylor series? Give some basic examples.
6. What do you know about adding and multiplying power
series?
7. Does every function have a Taylor series development?
Explain.
8. Can properties of functions be discovered from
Maclaurin series? Give examples.
9. What do you know about termwise integration of
series?
10. How did we obtain Taylor’s formula from Cauchy’s
formula?
11–15

RADIUS OF CONVERGENCE

Find the radius of convergence.
ؕ

11. a

nϩ1

2
nϭ2 n ϩ 1

(z ϩ 1)n

ؕ
nϭ2

n(n Ϫ 1)
3n

(Ϫ2)nϩ1

nϭ1

16–20

2n

zn

RADIUS OF CONVERGENCE

Find the radius of convergence. Try to identify the sum of
the series as a familiar function.
ؕ zn
16. a n

ؕ zn
17. a
zn
n!
nϭ0

nϭ1
ؕ
(Ϫ1)n
18. a
(pz)2nϩ1
(2n
ϩ
1)!
nϭ0
ؕ
zn
19. a
(2n)!
nϭ0

21–25

ؕ
zn
20. a
(3 ϩ 4i)n
nϭ0

MACLAURIN SERIES

Find the Maclaurin series and its radius of convergence.
Show details.
21. (sinh z 2)>z 2
22. 1>(1 Ϫ z)3
2
23. cos z
24. 1>(pz ϩ 1)
2
2
25. Ϫ(exp>(Ϫz ) Ϫ 1)>z
26–30

ؕ
4n
12. a
(z Ϫ pi)n
n
Ϫ
1
nϭ2

13. a

ؕ

15. a

TAYLOR SERIES

Find the Taylor series with the given point as center and its
radius of convergence.
26. z 4, i
27. cos z, 12 p
28. 1>z, 2i
29. Ln z, 3
30. ez, pi

(z Ϫ i)n

ؕ
n5
14. a
(z Ϫ 3i)2n
n!
nϭ1

SUMMARY OF CHAPTER

15

Power Series, Taylor Series
Sequences, series, and convergence tests are discussed in Sec. 15.1. A power series
is of the form (Sec. 15.2)
ؕ

(1)

n
2
Á;
a an(z Ϫ z 0) ϭ a0 ϩ a1(z Ϫ z 0) ϩ a2(z Ϫ z 0) ϩ
nϭ0

z 0 is its center. The series (1) converges for ƒ z Ϫ z 0 ƒ Ͻ R and diverges for
ƒ z Ϫ z 0 ƒ Ͼ R, where R is the radius of convergence. Some power series converge

c15.qxd

11/1/10

6:41 PM

Page 707

Summary of Chapter 15

707

for all z (then we write R ϭ ϱ). In exceptional cases a power series may converge
only at the center; such a series is practically useless. Also, R ϭ lim ƒ an>anϩ1 ƒ
if this limit exists. The series (1) converges absolutely (Sec. 15.2) and uniformly
(Sec. 15.5) in every closed disk ƒ z Ϫ z 0 ƒ Ϲ r Ͻ R (R Ͼ 0). It represents an analytic
function f (z) for ƒ z Ϫ z 0 ƒ Ͻ R. The derivatives f r(z), f s(z), Á are obtained by
termwise differentiation of (1), and these series have the same radius of convergence
R as (1). See Sec. 15.3.
Conversely, every analytic function f (z) can be represented by power series. These
Taylor series of f (z) are of the form (Sec. 15.4)
(2)

ؕ
1 (n)
f (z) ϭ a
f (z 0)(z Ϫ z 0)n
n!
nϭ0

( ƒ z Ϫ z 0) ƒ Ͻ R),

as in calculus. They converge for all z in the open disk with center z 0 and radius
generally equal to the distance from z 0 to the nearest singularity of f (z) (point at
which f (z) ceases to be analytic as defined in Sec. 15.4). If f (z) is entire (analytic
for all z; see Sec. 13.5), then (2) converges for all z. The functions ez, cos z, sin z,
etc. have Maclaurin series, that is, Taylor series with center 0, similar to those in
calculus (Sec. 15.4).

c16.qxd

11/1/10

6:57 PM

Page 708

CHAPTER

16

Laurent Series.
Residue Integration
The main purpose of this chapter is to learn about another powerful method for evaluating
complex integrals and certain real integrals. It is called residue integration. Recall that
the first method of evaluating complex integrals consisted of directly applying Cauchy’s
integral formula of Sec. 14.3. Then we learned about Taylor series (Chap. 15) and will
now generalize Taylor series. The beauty of residue integration, the second method of
integration, is that it brings together a lot of the previous material.
Laurent series generalize Taylor series. Indeed, whereas a Taylor series has positive
integer powers (and a constant term) and converges in a disk, a Laurent series (Sec. 16.1)
is a series of positive and negative integer powers of z Ϫ z 0 and converges in an annulus
(a circular ring) with center z 0. Hence, by a Laurent series, we can represent a given
function f (z) that is analytic in an annulus and may have singularities outside the ring as
well as in the “hole” of the annulus.
We know that for a given function the Taylor series with a given center z 0 is unique.
We shall see that, in contrast, a function f (z) can have several Laurent series with the
same center z 0 and valid in several concentric annuli. The most important of these series
is the one that converges for 0 Ͻ ƒ z Ϫ z 0 ƒ Ͻ R, that is, everywhere near the center z 0
except at z 0 itself, where z 0 is a singular point of f (z). The series (or finite sum) of the
negative powers of this Laurent series is called the principal part of the singularity of
f (z) at z 0, and is used to classify this singularity (Sec. 16.2). The coefficient of the power
1>(z Ϫ z 0) of this series is called the residue of f (z) at z 0. Residues are used in an elegant
and powerful integration method, called residue integration, for complex contour integrals
(Sec. 16.3) as well as for certain complicated real integrals (Sec. 16.4).
Prerequisite: Chaps. 13, 14, Sec. 15.2.
Sections that may be omitted in a shorter course: 16.2, 16.4.
References and Answers to Problems: App. 1 Part D, App. 2.

16.1

Laurent Series
Laurent series generalize Taylor series. If, in an application, we want to develop a function
f (z) in powers of z Ϫ z 0 when f (z) is singular at z 0 (as defined in Sec. 15.4), we cannot
use a Taylor series. Instead we can use a new kind of series, called Laurent series,1
1

PIERRE ALPHONSE LAURENT (1813–1854), French military engineer and mathematician, published the
theorem in 1843.

708

c16.qxd

11/1/10

6:57 PM

Page 709

SEC. 16.1 Laurent Series

709

consisting of positive integer powers of z Ϫ z 0 (and a constant) as well as negative integer
powers of z Ϫ z 0; this is the new feature.
Laurent series are also used for classifying singularities (Sec. 16.2) and in a powerful
integration method (“residue integration,” Sec. 16.3).
A Laurent series of f (z) converges in an annulus (in the “hole” of which f (z) may have
singularities), as follows.
THEOREM 1

Laurent’s Theorem

Let f (z) be analytic in a domain containing two concentric circles C1 and C2 with
center z 0 and the annulus between them (blue in Fig. 370). Then f (z) can be
represented by the Laurent series
ؕ
ؕ
bn
f (z) ϭ a an(z Ϫ z 0)n ϩ a
(z Ϫ z 0)n
nϭ0
nϭ1

ϭ a0 ϩ a1(z Ϫ z 0) ϩ a2(z Ϫ z 0)2 ϩ Á

(1)

Áϩ

b1
z Ϫ z0

ϩ

b2
(z Ϫ z 0)2

ϩÁ

consisting of nonnegative and negative powers. The coefficients of this Laurent series
are given by the integrals
(2) an ϭ

2pi Ώ
1

C

f (z*)
(z* Ϫ z 0)nϩ1

dz*,

bn ϭ

1
2pi

Ώ (z* Ϫ z )
0

n؊1

f (z*) dz*,

C

taken counterclockwise around any simple closed path C that lies in the annulus
and encircles the inner circle, as in Fig. 370. [The variable of integration is denoted
by z* since z is used in (1).]
This series converges and represents f (z) in the enlarged open annulus obtained
from the given annulus by continuously increasing the outer circle C1 and decreasing
C2 until each of the two circles reaches a point where f (z) is singular.
In the important special case that z 0 is the only singular point of f (z) inside C2,
this circle can be shrunk to the point z 0, giving convergence in a disk except at the
center. In this case the series (or finite sum) of the negative powers of (1) is called
the principal part of f (z) at z 0 [or of that Laurent series (1)].

C1
C

z0
C2

Fig. 370. Laurent’s theorem

c16.qxd

11/1/10

6:57 PM

710

Page 710

CHAP. 16 Laurent Series. Residue Integration

COMMENT. Obviously, instead of (1), (2) we may write (denoting bn by a؊n)
ؕ

f (z) ϭ a an(z Ϫ z 0)n

(1 r )

nϭ؊ؕ

where all the coefficients are now given by a single integral formula, namely,
an ϭ

(2 r )

1
2pi

Ώ (z* Ϫ z )
f (z*)
0

C

nϩ1

(n ϭ 0, Ϯ1, Ϯ2, Á ).

dz*

Let us now prove Laurent’s theorem.
PROOF

(a) The nonnegative powers are those of a Taylor series.
To see this, we use Cauchy’s integral formula (3) in Sec. 14.3 with z* (instead of z) as
the variable of integration and z instead of z 0. Let g(z) and h(z) denote the functions
represented by the two terms in (3), Sec. 14.3. Then
f (z) ϭ g(z) ϩ h(z) ϭ

(3)

2pi Ώ

f (z*)

1

C1

z* Ϫ z

dz* Ϫ

2pi Ώ

f (z*)

1

C2

z* Ϫ z

dz*.

Here z is any point in the given annulus and we integrate counterclockwise over both C1
and C2, so that the minus sign appears since in (3) of Sec. 14.3 the integration over C2
is taken clockwise. We transform each of these two integrals as in Sec. 15.4. The first
integral is precisely as in Sec. 15.4. Hence we get exactly the same result, namely, the
Taylor series of g(z),
g(z) ϭ

(4)

1
2pi

Ώ

ؕ

f (z*)

C1

dz* ϭ a an(z Ϫ z 0)n
z* Ϫ z
nϭ0

with coefficients [see (2), Sec. 15.4, counterclockwise integration]
an ϭ

(5)

Ώ
2pi
1

C1

f (z*)
(z* Ϫ z 0)nϩ1

dz*.

Here we can replace C1 by C (see Fig. 370), by the principle of deformation of path, since
z 0, the point where the integrand in (5) is not analytic, is not a point of the annulus. This
proves the formula for the an in (2).
(b) The negative powers in (1) and the formula for bn in (2) are obtained if we consider
h(z). It consists of the second integral times Ϫ1>(2pi) in (3). Since z lies in the annulus,
it lies in the exterior of the path C2. Hence the situation differs from that for the first
integral. The essential point is that instead of [see (7*) in Sec. 15.4]
(6)

(a)

`

z Ϫ z0
z* Ϫ z 0

` Ͻ1

we now have

(b)

`

z* Ϫ z 0
z Ϫ z0

` Ͻ 1.

Consequently, we must develop the expression 1>(z* Ϫ z) in the integrand of the second
integral in (3) in powers of (z* Ϫ z 0)>(z Ϫ z 0) (instead of the reciprocal of this) to get a
convergent series. We find

c16.qxd

11/1/10

6:57 PM

Page 711

SEC. 16.1 Laurent Series

711

1
1
ϭ
ϭ
z* Ϫ z
z* Ϫ z 0 Ϫ (z Ϫ z 0)

Ϫ1
z* Ϫ z 0
(z Ϫ z 0) a1 Ϫ z Ϫ z b
0

.

Compare this for a moment with (7) in Sec. 15.4, to really understand the difference. Then
go on and apply formula (8), Sec. 15.4, for a finite geometric sum, obtaining
2

n

1
1
z* Ϫ z 0
z* Ϫ z 0
z* Ϫ z 0
Áϩa
z* Ϫ z ϭ Ϫ z Ϫ z 0 e 1 ϩ z Ϫ z 0 ϩ a z Ϫ z 0 b ϩ
z Ϫ z0 b f
nϩ1

1
z* Ϫ z 0
Ϫ z Ϫ z* a z Ϫ z b
0

.

Multiplication by Ϫf (z*)>2pi and integration over C2 on both sides now yield
h(z) ϭ Ϫ

2pi Ώ
1

C2

ϭ

1
2pi

e

f (z*)
z* Ϫ z

1
z Ϫ z0

Ώ

dz*
f (z*) dz* ϩ

C2

1
ϩ
(z Ϫ z 0)n

Ώ

1
(z Ϫ z 0)2

Ώ

(z* Ϫ z 0) f (z*) dz* ϩ Á

C2

(z* Ϫ z 0)n؊1f (z*) dz*

C2

ϩ

1
(z Ϫ z 0)nϩ1

Ώ

C2

(z* Ϫ z 0)nf (z*) dz* f ϩ Rn*(z)

with the last term on the right given by
(7)

R*n(z) ϭ

1
2pi(z Ϫ z 0)nϩ1

Ώ

(z* Ϫ z 0)nϩ1

C2

z Ϫ z*

f (z*) dz*.

As before, we can integrate over C instead of C2 in the integrals on the right. We see that
on the right, the power 1>(z Ϫ z 0)n is multiplied by bn as given in (2). This establishes
Laurent’s theorem, provided
(8)

lim R n*(z) ϭ 0.

n:ϱ

(c) Convergence proof of (8). Very often (1) will have only finitely many negative powers.
Then there is nothing to be proved. Otherwise, we begin by noting that f (z*)>(z Ϫ z*) in (7)
is bounded in absolute value, say,
`

f (z*)
z Ϫ z*

ෂ
` ϽM

for all z* on C2

because f (z*) is analytic in the annulus and on C2, and z* lies on C2 and z outside, so
that z Ϫ z* 0. From this and the ML-inequality (Sec. 14.1) applied to (7) we get the
inequality (L ϭ 2pr2 ϭ length of C2, r2 ϭ ƒ z* Ϫ z 0 ƒ ϭ radius of C2 ϭ const)

c16.qxd

11/1/10

6:57 PM

712

Page 712

CHAP. 16 Laurent Series. Residue Integration

ƒ R*n(z) ƒ Ϲ

ෂ
nϩ1
ML
r2
nϩ1 ෂ
r
M
L
ϭ
a
b
.
2
2p ƒ z Ϫ z 0 ƒ nϩ1
2p ƒ z Ϫ z 0 ƒ
1

From (6b) we see that the expression on the right approaches zero as n approaches infinity.
This proves (8). The representation (1) with coefficients (2) is now established in the given
annulus.
(d) Convergence of (1) in the enlarged annulus. The first series in (1) is a Taylor
series [representing g(z)]; hence it converges in the disk D with center z 0 whose radius
equals the distance of the singularity (or singularities) closest to z 0. Also, g(z) must be
singular at all points outside C1 where f (z) is singular.
The second series in (1), representing h(z), is a power series in Z ϭ 1>(z Ϫ z 0). Let the
given annulus be r2 Ͻ ƒ z Ϫ z 0 ƒ Ͻ r1, where r1 and r2 are the radii of C1 and C2, respectively
(Fig. 370). This corresponds to 1>r2 Ͼ ƒ Z ƒ Ͼ 1>r1. Hence this power series in Z must
converge at least in the disk ƒ Z ƒ Ͻ 1>r2. This corresponds to the exterior ƒ z Ϫ z 0 ƒ Ͼ r2 of
C2, so that h(z) is analytic for all z outside C2. Also, h(z) must be singular inside C2
where f (z) is singular, and the series of the negative powers of (1) converges for all z
in the exterior E of the circle with center z 0 and radius equal to the maximum distance
from z 0 to the singularities of f (z) inside C2. The domain common to D and E is the
enlarged open annulus characterized near the end of Laurent’s theorem, whose proof
is now complete.
᭿
Uniqueness. The Laurent series of a given analytic function f (z) in its annulus of
convergence is unique (see Team Project 18). However, f (z) may have different Laurent
series in two annuli with the same center; see the examples below. The uniqueness is
essential. As for a Taylor series, to obtain the coefficients of Laurent series, we do not
generally use the integral formulas (2); instead, we use various other methods, some of
which we shall illustrate in our examples. If a Laurent series has been found by any such
process, the uniqueness guarantees that it must be the Laurent series of the given function
in the given annulus.
EXAMPLE 1

Use of Maclaurin Series
Find the Laurent series of z ؊5 sin z with center 0.

Solution. By (14), Sec. 15.4, we obtain
ؕ
(Ϫ1)n 2n؊4
1
1
1
1 2
z ؊5 sin z ϭ a
z
ϭ 4Ϫ 2ϩ
Ϫ
z ϩ Ϫ Á
(2n
ϩ
1)!
120
5040
z
6z
nϭ0

( ƒ z ƒ Ͼ 0).

Here the “annulus” of convergence is the whole complex plane without the origin and the principal part of the
series at 0 is z ؊4 Ϫ 16 z ؊2.
᭿

EXAMPLE 2

Substitution
Find the Laurent series of z 2e1>z with center 0.

Solution. From (12) in Sec. 15.4 with z replaced by 1>z we obtain a Laurent series whose principal part is
an infinite series,
z 2e1>z ϭ z 2 a1 ϩ

1
1!z

ϩ

1
2!z 2

1
1
1
ϩ
ϩ Á b ϭ z2 ϩ z ϩ ϩ
ϩÁ
2
3!z
4!z 2

( ƒ z ƒ Ͼ 0).

᭿

c16.qxd

11/1/10

6:57 PM

Page 713

SEC. 16.1 Laurent Series
EXAMPLE 3

713

Development of 1>(1 ؊ z)
Develop 1>(1 Ϫ z) (a) in nonnegative powers of z, (b) in negative powers of z.

Solution.

(b)

EXAMPLE 4

ؕ

1

(a)

ϭ a zn

1Ϫz
1

ϭ

1Ϫz

ؕ

Ϫ1
z(1 Ϫ z ؊1)

(valid if ƒ z ƒ Ͻ 1).

nϭ0

1

ϭϪa

z nϩ1

nϭ0

1

ϭϪ

z

Ϫ

1
z2

Ϫ Á

(valid if ƒ z ƒ Ͼ 1).

᭿

Laurent Expansions in Different Concentric Annuli
Find all Laurent series of 1>(z 3 Ϫ z 4) with center 0.

Solution. Multiplying by 1>z 3, we get from Example 3
(I)

ؕ
1
1
1
ϭ a z n؊3 ϭ 3 ϩ 2 ϩ ϩ 1 ϩ z ϩ Á
z
z
z Ϫz
z
nϭ0

1

3

ؕ

1

(II)

EXAMPLE 5

4

z Ϫz
3

4

1

ϭϪa

Use of Partial Fractions
Find all Taylor and Laurent series of f (z) ϭ

ϭϪ

z nϩ4

nϭ0

Ϫ2z ϩ 3
z 2 Ϫ 3z ϩ 2

1
z4

Ϫ

1
z5

Ϫ Á

(0 Ͻ ƒ z ƒ Ͻ 1),
( ƒ z ƒ Ͼ 1).

᭿

with center 0.

Solution. In terms of partial fractions,
f (z) ϭ Ϫ

1
zϪ1

Ϫ

1
zϪ2

.

(a) and (b) in Example 3 take care of the first fraction. For the second fraction,
(c)

(d)

Ϫ

Ϫ

1
zϪ2

1
zϪ2

ϭ

1
2 a1 Ϫ

ϭϪ

1
2

zb

ؕ
1
ϭ a nϩ1 z n
2
nϭ0

ؕ

1
z a1 Ϫ

2
b
z

ϭϪa
nϭ0

2n
z

nϩ1

(I) From (a) and (c), valid for ƒ z ƒ Ͻ 1 (see Fig. 371),
ؕ
1
3
5
9
f (z) ϭ a a1 ϩ nϩ1 b z n ϭ ϩ z ϩ z 2 ϩ Á .
2
4
8
2
nϭ0

y
III
II
I
1

2

x

Fig. 371. Regions of convergence in Example 5

( ƒ z ƒ Ͻ 2),

( ƒ z ƒ Ͼ 2).

c16.qxd

11/1/10

6:57 PM

714

Page 714

CHAP. 16 Laurent Series. Residue Integration
(II) From (c) and (b), valid for 1 Ͻ ƒ z ƒ Ͻ 2,
ؕ
ؕ
1
1
1
1
1
1
1
f (z) ϭ a nϩ1 z n Ϫ a nϩ1 ϭ ϩ z ϩ z 2 ϩ Á Ϫ Ϫ 2 Ϫ Á .
2
4
8
z
2
z
z
nϭ0
nϭ0

(III) From (d) and (b), valid for ƒ z ƒ Ͼ 2,
ؕ

f (z) ϭ Ϫ a (2n ϩ 1)
nϭ0

3
1
2
5
9
ϭϪ Ϫ 2 Ϫ 3 Ϫ 4 Ϫ Á.
z nϩ1
z
z
z
z

᭿

If f (z) in Laurent’s theorem is analytic inside C2, the coefficients bn in (2) are zero by
Cauchy’s integral theorem, so that the Laurent series reduces to a Taylor series. Examples
3(a) and 5(I) illustrate this.

PROBLEM SET 16.1
LAURENT SERIES NEAR A SINGULARITY
AT 0

1–8

Expand the function in a Laurent series that converges for
0 Ͻ ƒ z ƒ Ͻ R and determine the precise region of convergence. Show the details of your work.
1.

3.

cos z
z

exp z 2
z

5.

2.

4

4.

3

1
z Ϫz
2

6.

3

1
7. z cosh z
3

8.

exp (Ϫ1>z 2)
z2
sin pz
z2
sinh 2z
z2
z2 Ϫ z3

Find the Laurent series that converges for 0 Ͻ ƒ z Ϫ z 0 ƒ Ͻ R
and determine the precise region of convergence. Show details.
9.
11.
13.
15.
16.

ez
(z Ϫ 1)

2

z0 ϭ 1

,

z2

10.

,
(z Ϫ pi)4

z 0 ϭ pi

12.

1

,
2

z0 ϭ i

14.

,

z0 ϭ p

z 3(z Ϫ i)
cos z
(z Ϫ p)

2

sin z
1
4

(z Ϫ p)

3

,

z 0 ϭ 14 p

z 2 Ϫ 3i
(z Ϫ 3)

2

1
z (z Ϫ i)
2

Ύ

1
z2

ez

LAURENT SERIES NEAR A SINGULARITY
AT z0

9–16

17. CAS PROJECT. Partial Fractions. Write a program
for obtaining Laurent series by the use of partial
fractions. Using the program, verify the calculations in
Example 5 of the text. Apply the program to two other
functions of your choice.
18. TEAM PROJECT. Laurent Series. (a) Uniqueness.
Prove that the Laurent expansion of a given analytic
function in a given annulus is unique.
(b) Accumulation of singularities. Does tan (1>z)
have a Laurent series that converges in a region
0 Ͻ ƒ z ƒ Ͻ R? (Give a reason.)
(c) Integrals. Expand the following functions in a
Laurent series that converges for ƒ z ƒ Ͼ 0:

,

z0 ϭ 3

,

z0 ϭ i

eaz
, z0 ϭ b
zϪb

z

0

et Ϫ 1
dt,
t

1
z3

Ύ

z

0

sin t
dt.
t

19–25
TAYLOR AND LAURENT SERIES
Find all Taylor and Laurent series with center z 0. Determine
the precise regions of convergence. Show details.
19.

21.

22.
24.

25.

1
1Ϫz

2

sin z
z ϩ 12 p
1
z

,
2

z0 ϭ 0

,

z0 ϭ i

sinh z

23.

z0 ϭ 1

,

z 3 Ϫ 2iz 2
(z Ϫ i)2

z0 ϭ 1

z 0 ϭ Ϫ12 p

,

(z Ϫ 1)4

1
20. z ,

,

z0 ϭ i

z8
1 Ϫ z4

,

z0 ϭ 0

c16.qxd

11/1/10

6:57 PM

Page 715

SEC. 16.2 Singularities and Zeros. Infinity

16.2

715

Singularities and Zeros. Infinity
Roughly, a singular point of an analytic function f (z) is a z 0 at which f (z) ceases to be
analytic, and a zero is a z at which f (z) ϭ 0. Precise definitions follow below. In this
section we show that Laurent series can be used for classifying singularities and Taylor
series for discussing zeros.
Singularities were defined in Sec. 15.4, as we shall now recall and extend. We also
remember that, by definition, a function is a single-valued relation, as was emphasized
in Sec. 13.3.
We say that a function f (z) is singular or has a singularity at a point z ϭ z 0 if f (z) is not
analytic (perhaps not even defined) at z ϭ z 0, but every neighborhood of z ϭ z 0 contains
points at which f (z) is analytic. We also say that z ϭ z 0 is a singular point of f (z).
We call z ϭ z 0 an isolated singularity of f (z) if z ϭ z 0 has a neighborhood without
further singularities of f (z). Example: tan z has isolated singularities at Ϯp>2, Ϯ3p>2, etc.;
tan (1>z) has a nonisolated singularity at 0. (Explain!)
Isolated singularities of f (z) at z ϭ z 0 can be classified by the Laurent series
(1)

ؕ
ؕ
bn
f (z) ϭ a an(z Ϫ z 0)n ϩ a
(z Ϫ z 0)n
nϭ0
nϭ1

(Sec. 16.1)

valid in the immediate neighborhood of the singular point z ϭ z 0, except at z 0 itself, that
is, in a region of the form
0 Ͻ ƒ z Ϫ z 0 ƒ Ͻ R.
The sum of the first series is analytic at z ϭ z 0, as we know from the last section. The
second series, containing the negative powers, is called the principal part of (1), as we
remember from the last section. If it has only finitely many terms, it is of the form
b1
bm
ϩ Á ϩ
(z Ϫ z 0)m
z Ϫ z0

(2)

(bm

0).

Then the singularity of f (z) at z ϭ z 0 is called a pole, and m is called its order. Poles of
the first order are also known as simple poles.
If the principal part of (1) has infinitely many terms, we say that f (z) has at z ϭ z 0 an
isolated essential singularity.
We leave aside nonisolated singularities.
EXAMPLE 1

Poles. Essential Singularities
The function
f (z) ϭ

1
z(z Ϫ 2)

5

ϩ

3
(z Ϫ 2)2

has a simple pole at z ϭ 0 and a pole of fifth order at z ϭ 2. Examples of functions having an isolated essential
singularity at z ϭ 0 are
ؕ
1
1
1
e1>z ϭ a
ϭ1ϩ ϩ
ϩ Á
z
n!z n
2!z 2
nϭ0

c16.qxd

11/1/10

6:57 PM

716

Page 716

CHAP. 16 Laurent Series. Residue Integration
and
sin

ؕ
1
(Ϫ1)n
1
1
ϭ Ϫ
ϩ
Ϫϩ Á.
ϭ a
2nϩ1
3
5
z
z
(2n
ϩ
1)!z
3!z
5!z
nϭ0

1

Section 16.1 provides further examples. In that section, Example 1 shows that z ؊5 sin z has a fourth-order
pole at 0. Furthermore, Example 4 shows that 1>(z 3 Ϫ z 4) has a third-order pole at 0 and a Laurent series with
infinitely many negative powers. This is no contradiction, since this series is valid for ƒ z ƒ Ͼ 1; it merely tells
us that in classifying singularities it is quite important to consider the Laurent series valid in the immediate
neighborhood of a singular point. In Example 4 this is the series (I), which has three negative powers.
᭿

The classification of singularities into poles and essential singularities is not merely a formal
matter, because the behavior of an analytic function in a neighborhood of an essential
singularity is entirely different from that in the neighborhood of a pole.
EXAMPLE 2

Behavior Near a Pole
f (z) ϭ 1>z 2 has a pole at z ϭ 0, and ƒ f (z) ƒ : ϱ as z : 0 in any manner. This illustrates the following
theorem.
᭿

THEOREM 1

Poles

If f (z) is analytic and has a pole at z ϭ z 0, then ƒ f (z) ƒ : ϱ as z : z 0 in any manner.

The proof is left as an exercise (see Prob. 24).
EXAMPLE 3

Behavior Near an Essential Singularity
The function f (z) ϭ e1>z has an essential singularity at z ϭ 0. It has no limit for approach along the imaginary
axis; it becomes infinite if z : 0 through positive real values, but it approaches zero if z : 0 through negative real
values. It takes on any given value c ϭ c0eia 0 in an arbitrarily small P-neighborhood of z ϭ 0. To see the
latter, we set z ϭ reiu, and then obtain the following complex equation for r and u, which we must solve:
e1>z ϭ e(cos u؊i sin u)>r ϭ c0eia.
Equating the absolute values and the arguments, we have e(cos u)>r ϭ c0, that is
cos u ϭ r ln c0,

and

Ϫsin u ϭ ar

respectively. From these two equations and cos2 u ϩ sin2 u ϭ r 2(ln c0)2 ϩ a2r 2 ϭ 1 we obtain the formulas
r2 ϭ

1
(ln c0) ϩ a
2

2

and

tan u ϭ Ϫ

a
ln c0

.

Hence r can be made arbitrarily small by adding multiples of 2p to a, leaving c unaltered. This illustrates the
very famous Picard’s theorem (with z ϭ 0 as the exceptional value).
᭿

THEOREM 2

Picard’s Theorem

If f (z) is analytic and has an isolated essential singularity at a point z 0, it takes on
every value, with at most one exceptional value, in an arbitrarily small P-neighborhood
of z 0.
For the rather complicated proof, see Ref. [D4], vol. 2, p. 258. For historical information
on Picard, see footnote 9 in Problem Set 1.7.

c16.qxd

11/1/10

6:57 PM

Page 717

SEC. 16.2 Singularities and Zeros. Infinity

717

Removable Singularities. We say that a function f (z) has a removable singularity at
z ϭ z 0 if f (z) is not analytic at z ϭ z 0, but can be made analytic there by assigning a
suitable value f (z 0). Such singularities are of no interest since they can be removed as
just indicated. Example: f (z) ϭ (sin z)>z becomes analytic at z ϭ 0 if we define f (0) ϭ 1.

Zeros of Analytic Functions
A zero of an analytic function f (z) in a domain D is a z ϭ z 0 in D such that f (z 0) ϭ 0.
A zero has order n if not only f but also the derivatives f r , f s , Á , f (n؊1) are all 0 at z ϭ z 0
but f (n)(z 0) 0. A first-order zero is also called a simple zero. For a second-order zero,
f (z 0) ϭ f r(z 0) ϭ 0 but f s(z 0) 0. And so on.
EXAMPLE 4

Zeros
The function 1 ϩ z 2 has simple zeros at Ϯi. The function (1 Ϫ z 4)2 has second-order zeros at Ϯ1 and Ϯi. The
function (z Ϫ a)3 has a third-order zero at z ϭ a. The function ez has no zeros (see Sec. 13.5). The function sin z
has simple zeros at 0, Ϯp, Ϯ2p, Á , and sin2 z has second-order zeros at these points. The function 1 Ϫ cos z has
second-order zeros at 0, Ϯ2p, Ϯ4p, Á , and the function (1 Ϫ cos z)2 has fourth-order zeros at these points. ᭿

Taylor Series at a Zero. At an nth-order zero z ϭ z 0 of f (z), the derivatives f r(z 0), Á ,
f (n؊1)(z 0) are zero, by definition. Hence the first few coefficients a0, Á , an؊1 of the Taylor
series (1), Sec. 15.4, are zero, too, whereas an 0, so that this series takes the form
f (z) ϭ an(z Ϫ z 0)n ϩ anϩ1(z Ϫ z 0)nϩ1 ϩ Á

(3)

ϭ (z Ϫ z 0)n [an ϩ anϩ1(z Ϫ z 0) ϩ anϩ2(z Ϫ z 0)2 ϩ Á ]

(an

0).

This is characteristic of such a zero, because, if f (z) has such a Taylor series, it has an
nth-order zero at z ϭ z 0, as follows by differentiation.
Whereas nonisolated singularities may occur, for zeros we have
THEOREM 3

Zeros

The zeros of an analytic function f (z) ([ 0) are isolated; that is, each of them has
a neighborhood that contains no further zeros of f (z).

PROOF

The factor (z Ϫ z 0)n in (3) is zero only at z ϭ z 0. The power series in the brackets [ Á ]
represents an analytic function (by Theorem 5 in Sec. 15.3), call it g(z). Now
g(z 0) ϭ an 0, since an analytic function is continuous, and because of this continuity,
also g(z) 0 in some neighborhood of z ϭ z 0. Hence the same holds of f (z).
᭿
This theorem is illustrated by the functions in Example 4.
Poles are often caused by zeros in the denominator. (Example: tan z has poles where
cos z is zero.) This is a major reason for the importance of zeros. The key to the connection
is the following theorem, whose proof follows from (3) (see Team Project 12).

THEOREM 4

Poles and Zeros

Let f (z) be analytic at z ϭ z 0 and have a zero of nth order at z ϭ z 0. Then 1>f (z)
has a pole of nth order at z ϭ z 0; and so does h(z)>f (z), provided h(z) is analytic
at z ϭ z 0 and h(z 0) 0.

c16.qxd

11/1/10

6:57 PM

718

Page 718

CHAP. 16 Laurent Series. Residue Integration
N

P*

y

x
P

Fig. 372. Riemann sphere

Riemann Sphere. Point at Infinity
When we want to study complex functions for large ƒ z ƒ , the complex plane will generally
become rather inconvenient. Then it may be better to use a representation of complex numbers
on the so-called Riemann sphere. This is a sphere S of diameter 1 touching the complex
z-plane at z ϭ 0 (Fig. 372), and we let the image of a point P (a number z in the plane) be
the intersection P* of the segment PN with S, where N is the “North Pole” diametrically
opposite to the origin in the plane. Then to each z there corresponds a point on S.
Conversely, each point on S represents a complex number z, except for N, which does
not correspond to any point in the complex plane. This suggests that we introduce an
additional point, called the point at infinity and denoted ϱ (“infinity”) and let its image
be N. The complex plane together with ϱ is called the extended complex plane. The
complex plane is often called the finite complex plane, for distinction, or simply the
complex plane as before. The sphere S is called the Riemann sphere. The mapping of
the extended complex plane onto the sphere is known as a stereographic projection.
(What is the image of the Northern Hemisphere? Of the Western Hemisphere? Of a straight
line through the origin?)

Analytic or Singular at Infinity
If we want to investigate a function f (z) for large ƒ z ƒ , we may now set z ϭ 1>w and investigate
f (z) ϭ f (1>w) ϵ g(w) in a neighborhood of w ϭ 0. We define f (z) to be analytic or singular
at infinity if g(w) is analytic or singular, respectively, at w ϭ 0. We also define
(4)

g(0) ϭ lim g(w)
w:0

if this limit exists.
Furthermore, we say that f (z) has an nth-order zero at infinity if f (1>w) has such a zero
at w ϭ 0. Similarly for poles and essential singularities.
EXAMPLE 5

Functions Analytic or Singular at Infinity. Entire and Meromorphic Functions
The function f (z) ϭ 1>z 2 is analytic at ϱ since g(w) ϭ f (1>w) ϭ w 2 is analytic at w ϭ 0, and f (z) has a secondorder zero at ϱ. The function f (z) ϭ z 3 is singular at ϱ and has a third-order pole there since the function
g(w) ϭ f (1>w) ϭ 1>w 3 has such a pole at w ϭ 0. The function ez has an essential singularity at ϱ since e1>w
has such a singularity at w ϭ 0. Similarly, cos z and sin z have an essential singularity at ϱ.
Recall that an entire function is one that is analytic everywhere in the (finite) complex plane. Liouville’s
theorem (Sec. 14.4) tells us that the only bounded entire functions are the constants, hence any nonconstant
entire function must be unbounded. Hence it has a singularity at ϱ, a pole if it is a polynomial or an essential
singularity if it is not. The functions just considered are typical in this respect.

c16.qxd

11/1/10

6:57 PM

Page 719

SEC. 16.3 Residue Integration Method

719

An analytic function whose only singularities in the finite plane are poles is called a meromorphic function.
Examples are rational functions with nonconstant denominator, tan z, cot z, sec z, and csc z.
᭿

In this section we used Laurent series for investigating singularities. In the next section
we shall use these series for an elegant integration method.

PROBLEM SET 16.2
1–10
ZEROS
Determine the location and order of the zeros.
1. sin4 12 z
2. (z 4 Ϫ 81)3
3. (z ϩ 81i)4
4. tan2 2z
؊2
2
5. z sin pz
6. cosh4 z
4
2
7. z ϩ (1 Ϫ 8i) z Ϫ 8i
8. (sin z Ϫ 1)3
9. sin 2z cos 2z
10. (z 2 Ϫ 8)3(exp (z 2) Ϫ 1)
11. Zeros. If f (z) is analytic and has a zero of order n at
z ϭ z 0, show that f 2(z) has a zero of order 2n at z 0.
12. TEAM PROJECT. Zeros. (a) Derivative. Show that
if f (z) has a zero of order n Ͼ 1 at z ϭ z 0, then f r(z)
has a zero of order n Ϫ 1 at z 0.
(b) Poles and zeros. Prove Theorem 4.
(c) Isolated k-points. Show that the points at which
a nonconstant analytic function f (z) has a given value
k are isolated.
(d) Identical functions. If f1(z) and f2(z) are analytic
in a domain D and equal at a sequence of points z n in
D that converges in D, show that f1(z) ϵ f2(z) in D.

16.3

13–22

SINGULARITIES

Determine the location of the singularities, including those
at infinity. For poles also state the order. Give reasons.
zϩ1
z
1
ϩ
Ϫ
(z ϩ 2i)2
(z Ϫ i)2
zϪi
2
8
14. ez؊i ϩ
Ϫ
(z Ϫ i)3
zϪi
13.

15. z exp (1>(z Ϫ 1 Ϫ i)2)

1
b
zϪ1
20. 1>(cos z Ϫ sin z)
1>(ez Ϫ e2z)
22. (z Ϫ p)؊1 sin z
e1>(z؊1)>(ez Ϫ 1)
2
Essential singularity. Discuss e1>z in a similar way as
1>z
e is discussed in Example 3 of the text.
Poles. Verify Theorem 1 for f (z) ϭ z ؊3 Ϫ z ؊1. Prove
Theorem 1.
Riemann sphere. Assuming that we let the image of
the x-axis be the meridians 0° and 180°, describe and
sketch (or graph) the images of the following regions
on the Riemann sphere: (a) ƒ z ƒ Ͼ 100, (b) the lower
half-plane, (c) 12 Ϲ ƒ z ƒ Ϲ 2.

17. cot 4 z
19.
21.
23.
24.
25.

16. tan pz
18. z 3 exp a

Residue Integration Method
We now cover a second method of evaluating complex integrals. Recall that we solved
complex integrals directly by Cauchy’s integral formula in Sec. 14.3. In Chapter 15 we
learned about power series and especially Taylor series. We generalized Taylor series to
Laurent series (Sec. 16.1) and investigated singularities and zeroes of various functions
(Sec. 16.2). Our hard work has paid off and we see how much of the theoretical groundwork
comes together in evaluating complex integrals by the residue method.
The purpose of Cauchy’s residue integration method is the evaluation of integrals

Ώ f (z) dz
C

taken around a simple closed path C. The idea is as follows.
If f (z) is analytic everywhere on C and inside C, such an integral is zero by Cauchy’s
integral theorem (Sec. 14.2), and we are done.

c16.qxd

11/1/10

6:57 PM

720

Page 720

CHAP. 16 Laurent Series. Residue Integration

The situation changes if f (z) has a singularity at a point z ϭ z 0 inside C but is otherwise
analytic on C and inside C as before. Then f (z) has a Laurent series
ؕ
b1
b2
Á
f (z) ϭ a an(z Ϫ z 0)n ϩ
ϩ
2 ϩ
z
Ϫ
z
(z
Ϫ
z
)
0
0
nϭ0

that converges for all points near z ϭ z 0 (except at z ϭ z 0 itself), in some domain of the
form 0 Ͻ ƒ z Ϫ z 0 ƒ Ͻ R (sometimes called a deleted neighborhood, an old-fashioned term
that we shall not use). Now comes the key idea. The coefficient b1 of the first negative
power 1>(z Ϫ z 0) of this Laurent series is given by the integral formula (2) in Sec. 16.1
with n ϭ 1, namely,
b1 ϭ

1
2pi

Ώ f (z) dz.
C

Now, since we can obtain Laurent series by various methods, without using the integral
formulas for the coefficients (see the examples in Sec. 16.1), we can find b1 by one of
those methods and then use the formula for b1 for evaluating the integral, that is,

Ώ f (z) dz ϭ 2pib .

(1)

1

C

Here we integrate counterclockwise around a simple closed path C that contains z ϭ z 0
in its interior (but no other singular points of f (z) on or inside C!).
The coefficient b1 is called the residue of f (z) at z ϭ z 0 and we denote it by
b1 ϭ Res f (z).

(2)

EXAMPLE 1

zϭz0

Evaluation of an Integral by Means of a Residue
Integrate the function f (z) ϭ z ؊4 sin z counterclockwise around the unit circle C.

Solution. From (14) in Sec. 15.4 we obtain the Laurent series
f (z) ϭ

1
z3
sin z
1
1
ϩ
Ϫ
ϩϪ Á
ϭ 3Ϫ
4
3!z
5!
7!
z
z

which converges for ƒ z ƒ Ͼ 0 (that is, for all z 0). This series shows that f (z) has a pole of third order at z ϭ 0
and the residue b1 ϭ Ϫ13 !. From (1) we thus obtain the answer

Ώ

C

EXAMPLE 2

CAUTION!

sin z
pi
.
dz ϭ 2pib1 ϭ Ϫ
3
z4

᭿

Use the Right Laurent Series!

Integrate f (z) ϭ 1>(z 3 Ϫ z 4) clockwise around the circle C: ƒ z ƒ ϭ 12 .

Solution. z 3 Ϫ z 4 ϭ z 3(1 Ϫ z) shows that f (z) is singular at z ϭ 0 and z ϭ 1. Now z ϭ 1 lies outside C.
Hence it is of no interest here. So we need the residue of f (z) at 0. We find it from the Laurent series that
converges for 0 Ͻ ƒ z ƒ Ͻ 1. This is series (I) in Example 4, Sec. 16.1,
1
z3 Ϫ z4

ϭ

1
z3

ϩ

1
z2

ϩ

1
z

ϩ1ϩzϩ Á

(0 Ͻ ƒ z ƒ Ͻ 1).

c16.qxd

11/1/10

6:57 PM

Page 721

SEC. 16.3 Residue Integration Method

721

We see from it that this residue is 1. Clockwise integration thus yields

Ώ

dz

C

z3 Ϫ z4

ϭ Ϫ2pi Res f (z) ϭ Ϫ2pi.
zϭ0

CAUTION! Had we used the wrong series (II) in Example 4, Sec. 16.1,
1
z Ϫz
3

4

ϭϪ

1
z

4

Ϫ

1
z

5

Ϫ

1
z6

Ϫ Á

( ƒ z ƒ Ͼ 1),

᭿

we would have obtained the wrong answer, 0, because this series has no power 1>z.

Formulas for Residues
To calculate a residue at a pole, we need not produce a whole Laurent series, but, more
economically, we can derive formulas for residues once and for all.
Simple Poles at z 0.

A first formula for the residue at a simple pole is
Res f (z) ϭ b1 ϭ lim (z Ϫ z 0) f (z).

(3)

(Proof below).

z:z0

zϭz0

A second formula for the residue at a simple pole is
Res f (z) ϭ Res

(4)

zϭz0

zϭz0

p(z)
q(z)

ϭ

p(z 0)
.
q r (z 0)

(Proof below).

In (4) we assume that f (z) ϭ p(z)>q(z) with p(z 0) 0 and q(z) has a simple zero at z 0,
so that f (z) has a simple pole at z 0 by Theorem 4 in Sec. 16.2.
PROOF

We prove (3). For a simple pole at z ϭ z 0 the Laurent series (1), Sec. 16.1, is
b1
f (z) ϭ z Ϫ z ϩ a0 ϩ a1(z Ϫ z 0) ϩ a2(z Ϫ z 0)2 ϩ Á
0

(0 Ͻ ƒ z Ϫ z 0 ƒ Ͻ R).

Here b1 0. (Why?) Multiplying both sides by z Ϫ z 0 and then letting z : z 0, we obtain
the formula (3):
lim (z Ϫ z 0) f (z) ϭ b1 ϩ lim (z Ϫ z 0)[a0 ϩ a1(z Ϫ z 0) ϩ Á ] ϭ b1

z:z0

z:z0

where the last equality follows from continuity (Theorem 1, Sec. 15.3).
We prove (4). The Taylor series of q(z) at a simple zero z 0 is
q(z) ϭ (z Ϫ z 0)q r(z 0) ϩ

(z Ϫ z 0)2
2!

q s(z 0) ϩ Á .

Substituting this into f ϭ p>q and then f into (3) gives
Res f (z) ϭ lim (z Ϫ z 0)
zϭz0

z:z0

p(z)
q(z)

ϭ lim

z : z0

(z Ϫ z 0)p(z)
.
(z Ϫ z 0)[q r(z 0) ϩ (z Ϫ z 0)q s(z 0)>2 ϩ Á ]

z Ϫ z 0 cancels. By continuity, the limit of the denominator is q r(z 0) and (4) follows. ᭿

c16.qxd

11/1/10

6:57 PM

722

Page 722

CHAP. 16 Laurent Series. Residue Integration

EXAMPLE 3

Residue at a Simple Pole
f (z) ϭ (9z ϩ i)>(z 3 ϩ z) has a simple pole at i because z 2 ϩ 1 ϭ (z ϩ i)(z Ϫ i), and (3) gives the residue

Res
zϭi

9z ϩ i

ϭ lim (z Ϫ i)

z(z 2 ϩ 1)

z:i

9z ϩ i
z(z ϩ i)(z Ϫ i)

ϭ c

9z ϩ i
z(z ϩ i)

d

ϭ
zϭi

10i
Ϫ2

ϭ Ϫ5i.

By (4) with p(i) ϭ 9i ϩ i and q r(z) ϭ 3z 2 ϩ 1 we confirm the result,
9z ϩ i

Res

z(z 2 ϩ 1)

zϭi

ϭ c

9z ϩ i
3z 2 ϩ 1

d

ϭ
zϭi

10i
Ϫ2

᭿

ϭ Ϫ5i.

Poles of Any Order at z 0. The residue of f (z) at an mth-order pole at z 0 is

Res f (z) ϭ

(5)

zϭz0

1
d m؊1
lim e m؊1 c (z Ϫ z 0)mf (z) d f .
(m Ϫ 1)! z:z0 dz

In particular, for a second-order pole (m ϭ 2),
Res f (z) ϭ lim {[(z Ϫ z 0)2f (z)] r }.

(5*)

PROOF

z:z0

zϭz0

We prove (5). The Laurent series of f (z) converging near z 0 (except at z 0 itself) is (Sec. 16.2)
f (z) ϭ

where bm

bm
(z Ϫ z 0)

m

ϩ

bm؊1
(z Ϫ z 0)

m؊1

ϩ Á ϩ

b1
z Ϫ z0

ϩ a0 ϩ a1(z Ϫ z 0) ϩ Á

0. The residue wanted is b1. Multiplying both sides by (z Ϫ z 0)m gives

(z Ϫ z 0)mf (z) ϭ bm ϩ bm؊1(z Ϫ z 0) ϩ Á ϩ b1(z Ϫ z 0)m؊1 ϩ a0(z Ϫ z 0)m ϩ Á .
We see that b1 is now the coefficient of the power (z Ϫ z 0)m؊1of the power series of
g(z) ϭ (z Ϫ z 0)mf (z). Hence Taylor’s theorem (Sec. 15.4) gives (5):
b1 ϭ
ϭ
EXAMPLE 4

1
g (m؊1)(z 0)
(m Ϫ 1)!
1
d m؊1
[(z Ϫ z 0)mf (z)].
(m Ϫ 1)! dz m؊1

᭿

Residue at a Pole of Higher Order
f (z) ϭ 50z>(z 3 ϩ 2z 2 Ϫ 7z ϩ 4) has a pole of second order at z ϭ 1 because the denominator equals
(z ϩ 4)(z Ϫ 1)2 (verify!). From (5*) we obtain the residue
Res f (z) ϭ lim
zϭ1

z:1

d
dz

[(z Ϫ 1)2 f (z)] ϭ lim

z:1

d
dz

a

50z
zϩ4

bϭ

200
52

ϭ 8.

᭿

c16.qxd

11/1/10

6:57 PM

Page 723

SEC. 16.3 Residue Integration Method

723

Several Singularities Inside the Contour.
Residue Theorem
Residue integration can be extended from the case of a single singularity to the case of
several singularities within the contour C. This is the purpose of the residue theorem. The
extension is surprisingly simple.
THEOREM 1

Residue Theorem

Let f (z) be analytic inside a simple closed path C and on C, except for finitely many
singular points z 1, z 2, Á , z k inside C. Then the integral of f (z) taken counterclockwise
around C equals 2pi times the sum of the residues of f (z) at z 1, Á , z k :
k

Ώ f (z) dz ϭ 2pi a Res f (z).

(6)

jϭ1

C

PROOF

zϭzj

We enclose each of the singular points z j in a circle Cj with radius small enough that
those k circles and C are all separated (Fig. 373 where k ϭ 3). Then f (z) is analytic in the
multiply connected domain D bounded by C and C1, Á , Ck and on the entire boundary
of D. From Cauchy’s integral theorem we thus have
(7)

Ώ f (z) dz ϩ Ώ f (z) dz ϩ Ώ

f (z) dz ϩ Á ϩ

Ώ f (z) dz ϭ 0,
Ck

C2

C1

C

the integral along C being taken counterclockwise and the other integrals clockwise (as in
Figs. 354 and 355, Sec. 14.2). We take the integrals over C1, Á , Ck to the right and
compensate the resulting minus sign by reversing the sense of integration. Thus,
(8)

Ώ f (z) dz ϭ Ώ
C

f (z) dz ϩ

Ώ

f (z) dz ϩ Á ϩ

C2

C1

Ώ

f (z) dz

Ck

where all the integrals are now taken counterclockwise. By (1) and (2),

Ώ f (z) dz ϭ 2pi Res f (z),

j ϭ 1, Á , k,

zϭzj

Cj

᭿

so that (8) gives (6) and the residue theorem is proved.

z2
z1

z3
C

Fig. 373. Residue theorem

c16.qxd

11/1/10

6:57 PM

724

Page 724

CHAP. 16 Laurent Series. Residue Integration

This important theorem has various applications in connection with complex and real integrals.
Let us first consider some complex integrals. (Real integrals follow in the next section.)
EXAMPLE 5

Integration by the Residue Theorem. Several Contours
Evaluate the following integral counterclockwise around any simple closed path such that (a) 0 and 1 are inside
C, (b) 0 is inside, 1 outside, (c) 1 is inside, 0 outside, (d) 0 and 1 are outside.
4 Ϫ 3z

Ώ

dz

z2 Ϫ z

C

Solution. The integrand has simple poles at 0 and 1, with residues [by (3)]
Res
zϭ0

4 Ϫ 3z
4 Ϫ 3z
ϭ c
ϭ Ϫ4,
z(z Ϫ 1)
z Ϫ 1 d zϭ0

4 Ϫ 3z
4 Ϫ 3z
ϭ c
ϭ 1.
d
z
z(z Ϫ 1)
zϭ1

Res
zϭ1

[Confirm this by (4).] Answer: (a) 2pi(Ϫ4 ϩ 1) ϭ Ϫ6pi, (b) Ϫ8pi, (c) 2pi, (d) 0.

EXAMPLE 6

᭿

Another Application of the Residue Theorem
Integrate (tan z)>(z 2 Ϫ 1) counterclockwise around the circle C: ƒ z ƒ ϭ 32 .

Solution. tan z is not analytic at Ϯp>2, Ϯ3p>2, Á , but all these points lie outside the contour C. Because
of the denominator z 2 Ϫ 1 ϭ (z Ϫ 1)(z ϩ 1) the given function has simple poles at Ϯ1. We thus obtain from
(4) and the residue theorem

Ώ

C

tan z
z2 Ϫ 1

dz ϭ 2pi aRes

tan z
z2 Ϫ 1

zϭ1

ϭ 2pi a

tan z
2z

`

ϩ
zϭ1

tan z

ϩ Res

z2 Ϫ 1

zϭ؊1

tan z
2z

`

zϭ؊1

b

b

᭿

ϭ 2pi tan 1 ϭ 9.7855i.

EXAMPLE 7

Poles and Essential Singularities
Evaluate the following integral, where C is the ellipse 9x 2 ϩ y 2 ϭ 9 (counterclockwise, sketch it).

Ώ az
C

zepz
4

Ϫ 16

ϩ zep>z b dz.

Solution. Since z 4 Ϫ 16 ϭ 0 at Ϯ2i and Ϯ2, the first term of the integrand has simple poles at Ϯ2i inside
C, with residues [by (4); note that e2pi ϭ 1]

zepz

Res

z Ϫ 16
4

zϭ2i

Res

ϭ c

zepz

zϭ؊2i

z Ϫ 16
4

zepz
4z 3

ϭ c

zepz
4z 3

d

ϭϪ
zϭ2i

d

1
16

ϭϪ
zϭ؊2i

,

1
16

and simple poles at Ϯ2, which lie outside C, so that they are of no interest here. The second term of the integrand
has an essential singularity at 0, with residue p2>2 as obtained from
zep>z ϭ z a1 ϩ

p
z

ϩ

p2
2!z

2

ϩ

p3

p2 # 1 Á
ϩ
ϩÁbϭzϩpϩ
z
3!z
2
3

1
1
Ϫ 16
ϩ 12 p2) ϭ p(p2 Ϫ 14 )i ϭ 30.221i by the residue theorem.
Answer: 2pi(Ϫ 16

( ƒ z ƒ Ͼ 0).

᭿

c16.qxd

11/1/10

6:57 PM

Page 725

SEC. 16.4 Residue Integration of Real Integrals

725

PROBLEM SET 16.3
1. Verify the calculations in Example 3 and find the other
residues.
2. Verify the calculations in Example 4 and find the other
residue.
3–12
RESIDUES
Find all the singularities in the finite plane and the
corresponding residues. Show the details.
3.
5.

sin 2z
z

4.

6

8

11.

8.

1
1 Ϫ ez
ez

10.

17.

18.

cos z
19.

20.

(z 2 Ϫ 1)2
z4

21.

z Ϫ iz ϩ 2

13. CAS PROJECT. Residue at a Pole. Write a program
for calculating the residue at a pole of any order in the
finite plane. Use it for solving Probs. 5–10.

Ώ

C

15.

z 2 Ϫ 4z Ϫ 5

Ώ tan 2pz dz,

sinh z
dz, C: ƒ z Ϫ 2i ƒ ϭ 2
2z Ϫ i

Ώ

(z ϩ 1)3

dz

cos pz

Ώ

z 2 sin z

z5

23.

Ώ

C

24.
dz, C: ƒ z Ϫ 2 Ϫ i ƒ ϭ 3.2

Ώ

C: ƒ z Ϫ 0.2 ƒ ϭ 0.2

Ώ

C

C: ƒ z ƒ ϭ 12

dz, C the unit circle

30z 2 Ϫ 23z ϩ 5
(2z Ϫ 1)2(3z Ϫ 1)
exp (Ϫz 2)
sin 4z

C

25.

C: ƒ z Ϫ i ƒ ϭ 3

,

dz,

4z 2 Ϫ 1

C: ƒ z Ϫ 1 ƒ ϭ 2

dz,

2

Ώ

C

14–25
RESIDUE INTEGRATION
Evaluate (counterclockwise). Show the details.
14.

Ώ

zϩ1
4

C

22.

z Ϫ 23

z Ϫ 2z 3

C

2

C: the unit circle

Ώ

C

p

dz,

ez
dz, C: ƒ z Ϫ pi>2 ƒ ϭ 4.5
cos z

C

z4

1>z

Ώ

C

12. e1>(1؊z)

(z Ϫ pi)3

Ώe
C

6. tan z

1 ϩ z2

7. cot pz
9.

16.

dz,

C: ƒ z ƒ ϭ 1.5

z cosh pz
z ϩ 13z 2 ϩ 36
4

dz, C the unit circle

dz,

ƒzƒ ϭ p

C

16.4

Residue Integration of Real Integrals
Surprisingly, residue integration can also be used to evaluate certain classes of complicated
real integrals. This shows an advantage of complex analysis over real analysis or calculus.

Integrals of Rational Functions of cos ␪ and sin ␪
We first consider integrals of the type
(1)

Jϭ

Ύ

2p

0

F(cos u, sin u) du

c16.qxd

11/1/10

6:57 PM

726

Page 726

CHAP. 16 Laurent Series. Residue Integration

where F(cos u, sin u) is a real rational function of cos u and sin u [for example, (sin2 u)>
(5 Ϫ 4 cos u)] and is finite (does not become infinite) on the interval of integration. Setting
eiu ϭ z, we obtain
cos u ϭ

1 iu
1
1
(e ϩ e؊iu) ϭ az ϩ b
z
2
2

sin u ϭ

1 iu
1
1
(e Ϫ e؊iu) ϭ az Ϫ b .
z
2i
2i

(2)

Since F is rational in cos u and sin u, Eq. (2) shows that F is now a rational function of
z, say, f (z). Since dz>du ϭ ieiu, we have du ϭ dz>iz and the given integral takes the form

Ώ

Jϭ

(3)

f (z)

C

dz
iz

and, as u ranges from 0 to 2p in (1), the variable z ϭ eiu ranges counterclockwise once
around the unit circle ƒ z ƒ ϭ 1. (Review Sec. 13.5 if necessary.)
EXAMPLE 1

An Integral of the Type (1)
Show by the present method that

Ύ

2p

du

ϭ 2p.

12 Ϫ cos u

0

Solution. We use cos u ϭ 12 (z ϩ 1>z) and du ϭ dz>iz. Then the integral becomes

Ώ

dz>iz

C

12 Ϫ

1
2

az ϩ

1
b
z

ϭ

Ώ

dz
i
Ϫ (z 2 Ϫ 212z ϩ 1)
2

C

ϭϪ

Ώ

2
i

C

dz
(z Ϫ 12 Ϫ 1)(z Ϫ 12 ϩ 1)

.

We see that the integrand has a simple pole at z1 ϭ 12 ϩ 1 outside the unit circle C, so that it is of no interest
here, and another simple pole at z 2 ϭ 12 Ϫ 1 (where z Ϫ 12 ϩ 1 ϭ 0) inside C with residue [by (3), Sec. 16.3]
Res
zϭz2

1
(z Ϫ 12 Ϫ 1)(z Ϫ 12 ϩ 1)

ϭ c

1
z Ϫ 12 Ϫ 1

d

zϭ 12؊1

1
ϭϪ .
2
Answer: 2pi(Ϫ2>i)(Ϫ12 ) ϭ 2p. (Here Ϫ2>i is the factor in front of the last integral.)

᭿

As another large class, let us consider real integrals of the form

Ύ

(4)

ؕ

f (x) dx.

؊ؕ

Such an integral, whose interval of integration is not finite is called an improper integral,
and it has the meaning
(5 r )

Ύ

ؕ

؊ؕ

f (x) dx ϭ lim

a:؊ϱ

Ύ

0

a

f (x) dx ϩ lim

b:ϱ

Ύ

b

0

f (x) dx.

c16.qxd

11/1/10

6:57 PM

Page 727

SEC. 16.4 Residue Integration of Real Integrals

727

If both limits exist, we may couple the two independent passages to Ϫϱ and ϱ, and write

Ύ

(5)

ؕ

f (x) dx ϭ lim

R:ؕ

؊ؕ

Ύ

R

f (x) dx.

؊R

The limit in (5) is called the Cauchy principal value of the integral. It is written
pr. v.

Ύ

ؕ

f (x) dx.

؊ؕ

It may exist even if the limits in (5 r ) do not. Example:
lim

R:ؕ

Ύ

R

x dx ϭ lim a
R:ؕ

؊R

b

R2
R2
b ϭ 0,
Ϫ
2
2

but

lim

b :ؕ

Ύ x dx ϭ ϱ.
0

We assume that the function f (x) in (4) is a real rational function whose denominator
is different from zero for all real x and is of degree at least two units higher than the
degree of the numerator. Then the limits in (5 r ) exist, and we may start from (5). We
consider the corresponding contour integral

Ώ f (z) dz

(5*)

C

around a path C in Fig. 374. Since f (x) is rational, f (z) has finitely many poles in the
upper half-plane, and if we choose R large enough, then C encloses all these poles. By
the residue theorem we then obtain

Ώ

f (z) dz ϭ

C

Ύ

f (z) dz ϩ

S

Ύ

R

f (x) dx ϭ 2pi a Res f (z)

؊R

where the sum consists of all the residues of f (z) at the points in the upper half-plane at
which f (z) has a pole. From this we have
(6)

Ύ

R

f (x) dx ϭ 2pi a Res f (z) Ϫ

؊R

Ύ f (z) dz.
S

We prove that, if R : ϱ, the value of the integral over the semicircle S approaches
zero. If we set z ϭ Reiu, then S is represented by R ϭ const, and as z ranges along S, the
variable u ranges from 0 to p. Since, by assumption, the degree of the denominator of
f (z) is at least two units higher than the degree of the numerator, we have
ƒ f (z) ƒ Ͻ

k

( ƒ z ƒ ϭ R Ͼ R0)

ƒzƒ2

y

S

–R

R

x

Fig. 374. Path C of the contour integral in (5*)

c16.qxd

11/1/10

6:57 PM

728

Page 728

CHAP. 16 Laurent Series. Residue Integration

for sufficiently large constants k and R0. By the ML-inequality in Sec. 14.1,

Ύ f (z) dz ` Ͻ Rk

`

pR ϭ

2

S

kp

(R Ͼ R0).

R

Hence, as R approaches infinity, the value of the integral over S approaches zero, and (5)
and (6) yield the result

Ύ

(7)

ؕ

f (x) dx ϭ 2pi a Res f (z)

؊ؕ

where we sum over all the residues of f (z) at the poles of f (z) in the upper half-plane.
EXAMPLE 2

An Improper Integral from 0 to ؕ
Using (7), show that

Ύ

ؕ

0

dx
1 ϩ x4

ϭ

p
2 12

.

y
z2

z1

x
z3

z4

Fig. 375. Example 2

Solution. Indeed, f (z) ϭ 1>(1 ϩ z 4) has four simple poles at the points (make a sketch)
z 1 ϭ epi>4,

z 3 ϭ e؊3pi>4,

z 2 ϭ e3pi>4,

z 4 ϭ e؊pi>4.

The first two of these poles lie in the upper half-plane (Fig. 375). From (4) in the last section we find the residues
1

Res f (z) ϭ c
zϭz1

(1 ϩ z 4) r

Res f (z) ϭ c

(1 ϩ z 4) r

zϭz2

1

d

zϭz1

ϭ c

4z 3

d

zϭz2

ϭ c

1
1
1
ϭ e؊9pi>4 ϭ e ؊ pi>4.
d
4z 3 zϭz2 4
4

1

d

ϭ
zϭz1

1
4

e؊3pi>4 ϭ Ϫ

1
4

epi>4.

(Here we used epi ϭ Ϫ1 and e؊2pi ϭ 1.) By (1) in Sec. 13.6 and (7) in this section,

Ύ

ؕ

؊ϱ

dx
1 ϩ x4

ϭϪ

2pi
4

(epi>4 Ϫ e؊pi>4) ϭ Ϫ

2pi #
p
p
p
2i sin ϭ p sin ϭ
.
12
4
4
4

c16.qxd

11/1/10

6:57 PM

Page 729

SEC. 16.4 Residue Integration of Real Integrals

729

Since 1>(1 ϩ x 4) is an even function, we thus obtain, as asserted,

Ύ

ؕ

dx
1ϩx

0

4

ϭ

2 Ύ
1

ؕ

؊ؕ

dx
1ϩx

4

ϭ

p
2 12

᭿

.

Fourier Integrals
The method of evaluating (4) by creating a closed contour (Fig. 374) and “blowing it up”
extends to integrals
(8)

Ύ

ؕ

f (x) cos sx dx

Ύ

and

؊ϱ

ؕ

f (x) sin sx dx

(s real)

؊ϱ

as they occur in connection with the Fourier integral (Sec. 11.7).
If f (x) is a rational function satisfying the assumption on the degree as for (4), we may
consider the corresponding integral

Ώ f (z)e

isz

(s real and positive)

dz

C

over the contour C in Fig. 374. Instead of (7) we now get

Ύ

(9)

ؕ

f (x)eisx dx ϭ 2pi a Res [ f (z)eisz ]

(s Ͼ 0)

؊ؕ

where we sum the residues of f (z)eisz at its poles in the upper half-plane. Equating the
real and the imaginary parts on both sides of (9), we have

Ύ

ؕ

f (x) cos sx dx ϭ Ϫ2p a Im Res [ f (z)eisz ],

؊ؕ
ؕ

(10)

Ύ

(s Ͼ 0)
f (x) sin sx dx ϭ 2p a Re Res [ f (z)eisz ].

؊ؕ

To establish (9), we must show [as for (4)] that the value of the integral over the
semicircle S in Fig. 374 approaches 0 as R : ϱ. Now s Ͼ 0 and S lies in the upper halfplane y м 0. Hence
ƒ eisz ƒ ϭ ƒ eis(xϩiy) ƒ ϭ ƒ eisx ƒ ƒ e؊sy ƒ ϭ 1 # e؊sy Ϲ 1

(s Ͼ 0, y м 0).

From this we obtain the inequality ƒ f (z)eisz ƒ ϭ ƒ f (z) ƒ ƒ eisz ƒ Ϲ ƒ f (z) ƒ (s Ͼ 0, y м 0). This
reduces our present problem to that for (4). Continuing as before gives (9) and (10).
᭿
EXAMPLE 3

An Application of (10)
Show that

Ύ

ؕ

cos sx

؊ؕ k ϩ x

2

2

dx ϭ

p
k

e؊ks,

Ύ

ؕ

sin sx

2
2
؊ؕ k ϩ x

dx ϭ 0

(s Ͼ 0, k Ͼ 0).

c16.qxd

11/1/10

730

6:57 PM

Page 730

CHAP. 16 Laurent Series. Residue Integration

Solution. In fact, eisz>(k 2 ϩ z 2) has only one pole in the upper half-plane, namely, a simple pole at z ϭ ik,
and from (4) in Sec. 16.3 we obtain

eisz

Res

k ϩz
2

zϭik

eisz

ϭ c

2

2z

d

ϭ
zϭik

e؊ks
2ik

.

Thus

Ύ

ؕ

eisx
k ϩx
2

؊ؕ

2

dx ϭ 2pi

e؊ks
2ik

ϭ

p
k

e؊ks.

Since eisx ϭ cos sx ϩ i sin sx, this yields the above results [see also (15) in Sec. 11.7.]

᭿

Another Kind of Improper Integral
We consider an improper integral

Ύ

(11)

B

f (x) dx

A

whose integrand becomes infinite at a point a in the interval of integration,
lim ƒ f (x) ƒ ϭ ϱ.

x:a

By definition, this integral (11) means
(12)

Ύ

B

f (x) dx ϭ lim

P:0

A

Ύ

a؊P

f (x) dx ϩ lim

h:0

A

Ύ

B

f (x) dx

aϩh

where both P and h approach zero independently and through positive values. It may
happen that neither of these two limits exists if P and h go to 0 independently, but the
limit
lim c
P:0

(13)

Ύ

a؊P

f (x) dx ϩ

A

Ύ

B

f (x) dx d

aϩP

exists. This is called the Cauchy principal value of the integral. It is written
pr. v.

Ύ

B

f (x) dx.

A

For example,
pr. v.

Ύ

1

؊1

dx
x

3

ϭ lim c
P:0

Ύ

؊P

؊1

dx
x

3

1

ϩ

Ύ xdx d ϭ 0;
3

P

the principal value exists, although the integral itself has no meaning.
In the case of simple poles on the real axis we shall obtain a formula for the principal
value of an integral from Ϫϱ to ϱ. This formula will result from the following theorem.

c16.qxd

11/1/10

6:57 PM

Page 731

SEC. 16.4 Residue Integration of Real Integrals

THEOREM 1

731

Simple Poles on the Real Axis

If f (z) has a simple pole at z ϭ a on the real axis, then (Fig. 376)
lim

r:0

Ύ f (z) dz ϭ pi Res f (z).
zϭa

C2

C2

a–r

a

a+r

x

Fig. 376. Theorem 1

PROOF

By the definition of a simple pole (Sec. 16.2) the integrand f (z) has for 0 Ͻ ƒ z Ϫ a ƒ Ͻ R
the Laurent series
b1
f (z) ϭ z Ϫ a ϩ g(z),

b1 ϭ Res f (z).
zϭa

Here g(z) is analytic on the semicircle of integration (Fig. 376)
C2: z ϭ a ϩ reiu,

0ϹuϹp

and for all z between C2 and the x-axis, and thus bounded on C2, say, ƒ g(z) ƒ Ϲ M. By
integration,

Ύ

C2

f (z) dz ϭ

Ύ

p

0

b1
re

iu

ireiu du ϩ

Ύ

g(z) dz ϭ b1pi ϩ

C2

Ύ

g(z) dz.

C2

The second integral on the right cannot exceed Mpr in absolute value, by the
ML-inequality (Sec. 14.1), and ML ϭ Mpr : 0 as r : 0.
᭿
Figure 377 shows the idea of applying Theorem 1 to obtain the principal value of the
integral of a rational function f (x) from Ϫϱ to ϱ . For sufficiently large R the integral over
the entire contour in Fig. 377 has the value J given by 2pi times the sum of the residues
of f (z) at the singularities in the upper half-plane. We assume that f (x) satisfies the degree
condition imposed in connection with (4). Then the value of the integral over the large
S
C2

–R

a–r

a

a+r R

Fig. 377. Application of Theorem 1

c16.qxd

11/1/10

6:57 PM

732

Page 732

CHAP. 16 Laurent Series. Residue Integration

semicircle S approaches 0 as R : ϱ. For r : 0 the integral over C2 (clockwise!)
approaches the value
K ϭ Ϫpi Res f (z)
zϭa

by Theorem 1. Together this shows that the principal value P of the integral from Ϫϱ to
ϱ plus K equals J; hence P ϭ J Ϫ K ϭ J ϩ pi Reszϭa f (z). If f (z) has several simple
poles on the real axis, then K will be Ϫpi times the sum of the corresponding residues.
Hence the desired formula is

(14)

pr. v.

Ύ

ؕ

f (x) dx ϭ 2pi a Res f (z) ϩ pi a Res f (z)

؊ؕ

where the first sum extends over all poles in the upper half-plane and the second over all
poles on the real axis, the latter being simple by assumption.
EXAMPLE 4

Poles on the Real Axis
Find the principal value

pr. v.

Ύ

ؕ

dx

2
2
؊ؕ (x Ϫ 3x ϩ 2)(x ϩ 1)

.

Solution. Since
x 2 Ϫ 3x ϩ 2 ϭ (x Ϫ 1)(x Ϫ 2),
the integrand f (x), considered for complex z, has simple poles at

z ϭ 1,

Res f (z) ϭ c
zϭ1

1
(z Ϫ 2)(z 2 ϩ 1)
1

ϭϪ
z ϭ 2,

Res f (z) ϭ c
zϭ2

ϭ
z ϭ i,

1
5

Res f (z) ϭ c
zϭi

ϭ

2

d

zϭ1

d

zϭ2

,
1

(z Ϫ 1)(z 2 ϩ 1)
,
1

(z Ϫ 3z ϩ 2)(z ϩ i)
2

1
6 ϩ 2i

ϭ

3Ϫi
20

d

zϭi

,

and at z ϭ Ϫi in the lower half-plane, which is of no interest here. From (14) we get the answer
pr. v.

Ύ

ؕ

؊ؕ

dx
3Ϫi
1
1
p
ϭ 2pi a
b ϩ pi aϪ ϩ b ϭ
.
(x 2 Ϫ 3x ϩ 2)(x 2 ϩ 1)
20
2
5
10

᭿

More integrals of the kind considered in this section are included in the problem set. Try
also your CAS, which may sometimes give you false results on complex integrals.

c16.qxd

11/1/10

6:57 PM

Page 733

Chaper 16 Review Questions and Problems

733

PROBLEM SET 16.4
1–9
INTEGRALS INVOLVING COSINE AND SINE
Evaluate the following integrals and show the details of
your work.
1.

Ύ

p

Ύ

2p

Ύ

2p

0

3.

2 du
k Ϫ cos u

0

5.

0

7.

Ύ

2p

0

9.

Ύ

2p

0

2.

Ύ

p

Ύ

2p

Ύ

2p

0

1 ϩ sin u
du
3 ϩ cos u

4.

cos u
du
5 Ϫ 4 cos u
a
du
a Ϫ sin u

6.

0

8.

Ύ

2p

0

22.

du
p ϩ 3 cos u

0

2

1 ϩ 4 cos u
du
17 Ϫ 8 cos u

12.

14.

Ύ

1
du
8 Ϫ 2 sin u

Ύ

ؕ

18.

Ύ

ؕ

Ύ

ؕ

x2 ϩ 1
x4 ϩ 1

Ύ
Ύ

ؕ

cos 4x
x ϩ 5x ϩ 4
4

2

x

3
؊ؕ 8 Ϫ x

Ύ

ؕ

17.

Ύ

ؕ

19.

x

2
2
؊ؕ (x ϩ 1)(x ϩ 4)

Ύ

ؕ

Ύ

ؕ

؊ؕ

dx

dx

2 2
؊ؕ (1 ϩ x )

؊ؕ

cos 2x

ؕ

13.

15.

dx

dx
2
2
؊ؕ (x ϩ 1)

؊ؕ

20.

dx

2
2
؊ؕ (x Ϫ 2x ϩ 5)

23.

Ύ

Ύ

x2
x6 ϩ 1
sin 3x
x4 ϩ 1

ؕ

؊ؕ

dx
x Ϫ ix
2

IMPROPER INTEGRALS:
POLES ON THE REAL AXIS

25.

Ύ

ؕ

dx

؊ؕ x Ϫ 1

4

ؕ

؊ؕ

11.

2 3
؊ؕ (1 ϩ x )

؊ؕ

16.

dx

Ύ

ؕ

dx

Find the Cauchy principal value (showing details):

sin2 u
du
5 Ϫ 4 cos u

IMPROPER INTEGRALS:
INFINITE INTERVAL OF INTEGRATION

ؕ

sin x

2
؊ؕ (x Ϫ 1)(x ϩ 4)

23–26

Evaluate the following integrals and show details of your
work.
10.

Ύ

ؕ

؊ؕ

cos u
du
13 Ϫ 12 cos 2u

10–22

21.

xϩ5
x Ϫx
3

dx

24.

Ύ

26.

Ύ

ؕ

dx

4
2
؊ؕ x ϩ 3x Ϫ 4
ؕ

؊ؕ

x2
x Ϫ1
4

dx

27. CAS EXPERIMENT. Simple Poles on the Real
ؕ
f (x) dx,
Axis. Experiment with integrals ͐؊ؕ
؊1
Á
f (x) ϭ [(x Ϫ a1)(x Ϫ a2) (x Ϫ ak)] , aj real and
all different, k Ͼ 1. Conjecture that the principal value
of these integrals is 0. Try to prove this for a special
k, say, k ϭ 3. For general k.
28. TEAM PROJECT. Comments on Real Integrals.
(a) Formula (10) follows from (9). Give the details.
(b) Use of auxiliary results. Integrating e؊z around
the boundary C of the rectangle with vertices Ϫa, a,
a ϩ ib, Ϫa ϩ ib, letting a : ϱ, and using
2

dx

Ύ

dx

ؕ

0

e؊x dx ϭ
2

1p
,
2

show that
dx

dx
x Ϫ1
4

dx

Ύ

ؕ

0

e؊x cos 2bx dx ϭ
2

1p ؊b2
e .
2

(This integral is needed in heat conduction in Sec.
12.7.)
(c) Inspection. Solve Probs. 13 and 17 without
calculation.

CHAPTER 16 REVIEW QUESTIONS AND PROBLEMS
1. What is a Laurent series? Its principal part? Its use?
Give simple examples.
2. What kind of singularities did we discuss? Give definitions and examples.
3. What is the residue? Its role in integration? Explain
methods to obtain it.

4. Can the residue at a singularity be zero? At a simple
pole? Give reason.
5. State the residue theorem and the idea of its proof from
memory.
6. How did we evaluate real integrals by residue integration?
How did we obtain the closed paths needed?

c16.qxd

11/1/10

6:57 PM

734

Page 734

CHAP. 16 Laurent Series. Residue Integration

7. What are improper integrals? Their principal value?
Why did they occur in this chapter?
8. What do you know about zeros of analytic functions?
Give examples.
9. What is the extended complex plane? The Riemann
sphere R? Sketch z ϭ 1 ϩ i on R.
10. What is an entire function? Can it be analytic at
infinity? Explain the definitions.

COMPLEX INTEGRALS

11–18

Integrate counterclockwise around C. Show the details.
11.

sin 3z
z2

12. e
13.

14.

15.

2>z

18. cot 4z, C: ƒ z ƒ ϭ 34

REAL INTEGRALS
19–25
Evaluate by the methods of this chapter. Show details.
19.

Ύ

2p

Ύ

2p

0

C: ƒ z ƒ ϭ p

,

21.

,

5z

15z ϩ 9

, C: ƒ z ƒ ϭ 4
z 3 Ϫ 9z
cos z
17. n , n ϭ 0, 1, 2, Á , C: ƒ z ƒ ϭ 1
z
16.

C: ƒ z Ϫ 1 Ϫ i ƒ ϭ 2

0

3

z ϩ4
2

5z 3
z ϩ4
2

,

C: ƒ z ƒ ϭ 3

22.

,

C: ƒ z Ϫ i ƒ ϭ pi>2

24.

25z 2
(z Ϫ 5)2

,

Ύ

du
13 Ϫ 5 sin u

20.

0

sin u
du
3 ϩ cos u

sin u
du
34 Ϫ 16 sin u

ؕ

dx

23.

4
؊ؕ 1 ϩ 4x

Ύ

Ύ

2p

ؕ

dx

25.

2
؊ؕ x Ϫ 4ix

Ύ

ؕ

x

2 2
؊ؕ (1 ϩ x )

Ύ

ؕ

cos x

2
؊ؕ x ϩ 1

dx

dx

C: ƒ z Ϫ 5 ƒ ϭ 1

SUMMARY OF CHAPTER

16

Laurent Series. Residue Integration
A Laurent series is a series of the form
(1)

ؕ
ؕ
bn
f (z) ϭ a an(z Ϫ z 0)n ϩ a
(z Ϫ z 0)n
nϭ0
nϭ1

(Sec. 16.1)

or, more briefly written [but this means the same as (1)!]
ؕ

(1*)

f (z) ϭ a an(z Ϫ z 0)n,
nϭ؊ؕ

an ϭ

Ώ
2pi
1

C

f (z*)
(z* Ϫ z 0)nϩ1

dz*

where n ϭ 0, Ϯ1, Ϯ2, Á . This series converges in an open annulus (ring) A with
center z 0. In A the function f (z) is analytic. At points not in A it may have
singularities. The first series in (1) is a power series. In a given annulus, a Laurent
series of f (z) is unique, but f (z) may have different Laurent series in different annuli
with the same center.
Of particular importance is the Laurent series (1) that converges in a neighborhood
of z 0 except at z 0 itself, say, for 0 Ͻ ƒ z Ϫ z 0 ƒ Ͻ R (R Ͼ 0, suitable). The series

c16.qxd

11/1/10

6:57 PM

Page 735

Summary of Chapter 16

735

(or finite sum) of the negative powers in this Laurent series is called the principal
part of f (z) at z 0. The coefficient b1 of 1>(z Ϫ z 0) in this series is called the residue
of f (z) at z 0 and is given by [see (1) and (1*)]
(2) b1 ϭ Res f (z) ϭ
z:z0

1
2pi

Ώ f (z*) dz*.

Thus

C

Ώ f (z*) dz* ϭ 2pi Res f (z).
zϭz0

C

b1 can be used for integration as shown in (2) because it can be found from
Res f (z) ϭ

(3)

zϭz0

1
d m؊1
lim ¢ m؊1 [(z Ϫ z 0)mf (z)]≤ ,
(m Ϫ 1)! z:z0 dz

(Sec. 16.3),

provided f (z) has at z 0 a pole of order m; by definition this means that principal
part has 1>(z Ϫ z 0)m as its highest negative power. Thus for a simple pole (m ϭ 1),
Res f (z) ϭ lim (z Ϫ z 0)f (z);
zϭz0

z:z0

also,

Res
zϭz0

p(z)
q(z)

ϭ

p(z 0)
q r(z 0)

.

If the principal part is an infinite series, the singularity of f (z) at z 0 is called an
essential singularity (Sec. 16.2).
Section 16.2 also discusses the extended complex plane, that is, the complex plane
with an improper point ϱ (“infinity”) attached.
Residue integration may also be used to evaluate certain classes of complicated
real integrals (Sec. 16.4).

c17.qxd

11/1/10

7:35 PM

Page 736

CHAPTER

17

Conformal Mapping
Conformal mappings are invaluable to the engineer and physicist as an aid in solving
problems in potential theory. They are a standard method for solving boundary value
problems in two-dimensional potential theory and yield rich applications in electrostatics,
heat flow, and fluid flow, as we shall see in Chapter 18.
The main feature of conformal mappings is that they are angle-preserving (except at
some critical points) and allow a geometric approach to complex analysis. More details
are as follows. Consider a complex function w ϭ f (z) defined in a domain D of the z–plane;
then to each point in D there corresponds a point in the w-plane. In this way we obtain a
mapping of D onto the range of values of f (z) in the w-plane. In Sec. 17.1 we show that
if f (z) is an analytic function, then the mapping given by w ϭ f (z) is a conformal mapping,
that is, it preserves angles, except at points where the derivative f r(z) is zero. (Such points
are called critical points.)
Conformality appeared early in the history of construction of maps of the globe.
Such maps can be either “conformal,” that is, give directions correctly, or “equiareal,”
that is, give areas correctly except for a scale factor. However, the maps will always
be distorted because they cannot have both properties, as can be proven, see [GenRef8]
in App. 1. The designer of accurate maps then has to select which distortion to take
into account.
Our study of conformality is similar to the approach used in calculus where we study
properties of real functions y ϭ f (x) and graph them. Here we study the properties of conformal
mappings (Secs. 17.1–17.4) to get a deeper understanding of the properties of functions, most
notably the ones discussed in Chap. 13. Chapter 17 ends with an introduction to Riemann
surfaces, an ingenious geometric way of dealing with multivalued complex functions such as
w ϭ sqrt (z) and w ϭ ln z.
So far we have covered two main approaches to solving problems in complex analysis.
The first one was solving complex integrals by Cauchy’s integral formula and was broadly
covered by material in Chaps. 13 and 14. The second approach was to use Laurent series
and solve complex integrals by residue integration in Chaps. 15 and 16. Now, in Chaps. 17
and 18, we develop a third approach, that is, the geometric approach of conformal mapping
to solve boundary value problems in complex analysis.
Prerequisite: Chap. 13.
Sections that may be omitted in a shorter course: 17.3 and 17.5.
References and Answers to Problems: App. 1 Part D, App. 2.

736

c17.qxd

11/1/10

7:35 PM

Page 737

SEC. 17.1 Geometry of Analytic Functions: Conformal Mapping

17.1

737

Geometry of Analytic Functions:
Conformal Mapping
We shall see that conformal mappings are those mappings that preserve angles, except at
critical points, and that these mappings are defined by analytic functions. A critical point
occurs wherever the derivative of such a function is zero. To arrive at these results, we
have to define terms more precisely.
A complex function
w ϭ f (z) ϭ u(x, y) ϩ iv(x, y)

(1)

(z ϭ x ϩ iy)

of a complex variable z gives a mapping of its domain of definition D in the complex
z-plane into the complex w-plane or onto its range of values in that plane.1 For any point z 0
in D the point w0 ϭ f (z 0) is called the image of z 0 with respect to f. More generally, for
the points of a curve C in D the image points form the image of C; similarly for other
point sets in D. Also, instead of the mapping by a function w ϭ f (z) we shall say more
briefly the mapping w ‫ ؍‬f (z).
EXAMPLE 1

Mapping w ‫ ؍‬f (x) ‫ ؍‬z2
Using polar forms z ϭ reiu and w ϭ Rei␾, we have w ϭ z 2 ϭ r 2e2iu. Comparing moduli and arguments gives
R ϭ r 2 and ␾ ϭ 2u. Hence circles r ϭ r0 are mapped onto circles R ϭ r 20 and rays u ϭ u0 onto rays ␾ ϭ 2u0 .
Figure 378 shows this for the region 1 Ϲ ƒ z ƒ Ϲ 32 , p>6 Ϲ u Ϲ p>3, which is mapped onto the region
1 Ϲ ƒ w ƒ Ϲ 94 , p>3 Ϲ u Ϲ 2p>3.
In Cartesian coordinates we have z ϭ x ϩ iy and
u ϭ Re (z 2) ϭ x 2 Ϫ y 2,

v ϭ Im (z 2) ϭ 2xy.

Hence vertical lines x ϭ c ϭ const are mapped onto u ϭ c2 Ϫ y 2, v ϭ 2cy. From this we can eliminate y. We
obtain y 2 ϭ c2 Ϫ u and v2 ϭ 4c2y 2. Together,
v2 ϭ 4c2(c2 Ϫ u)

(Fig. 379).

These parabolas open to the left. Similarly, horizontal lines y ϭ k ϭ const are mapped onto parabolas opening
to the right,
(Fig. 379). ᭿
v2 ϭ 4k 2(k 2 ϩ u)
v

y
2
1
0
0

1
(z-plane)

2

x

–4

–3

–2

–1

0

1

2

3

4

u

(w-plane)

Fig. 378. Mapping w ϭ z 2. Lines ͉z͉ ϭ const, arg z ϭ const and their images in the w-plane
1
The general terminology is as follows. A mapping of a set A into a set B is called surjective or a mapping of
A onto B if every element of B is the image of at least one element of A. It is called injective or one-to-one if
different elements of A have different images in B. Finally, it is called bijective if it is both surjective and injective.

c17.qxd

11/1/10

7:35 PM

738

Page 738

CHAP. 17 Conformal Mapping
v
y=2

y=1
4
y=1

2

2

y=0
–5

5

u

–2

x=1
2

–4
x=1

x=3

2 x=2

Fig. 379. Images of x ϭ const, y ϭ const under w ϭ z 2

Conformal Mapping
A mapping w ϭ f (z) is called conformal if it preserves angles between oriented curves in
magnitude as well as in sense. Figure 380 shows what this means. The angle a (0 Ϲ a Ϲ p)
between two intersecting curves C1 and C2 is defined to be the angle between their oriented
tangents at the intersection point z 0. And conformality means that the images C *1 and C *2
of C1 and C2 make the same angle as the curves themselves in both magnitude and direction.
THEOREM 1

Conformality of Mapping by Analytic Functions

The mapping w ϭ f (z) by an analytic function f is conformal, except at critical
points, that is, points at which the derivative f r is zero.
PROOF

w ϭ z 2 has a critical point at z ϭ 0, where f r(z) ϭ 2z ϭ 0 and the angles are doubled (see
Fig. 378), so that conformality fails.
The idea of proof is to consider a curve
C: z(t) ϭ x(t) ϩ iy(t)

(2)

in the domain of f (z) and to show that w ϭ f (z) rotates all tangents at a point z 0 (where
f r(z 0) 0) through the same angle. Now zؒ(t) ϭ dz>dt ϭ xؒ(t) ϩ iyؒ (t) is tangent to C in
(2) because this is the limit of (z1 Ϫ z 0)>¢t (which has the direction of the secant z1 Ϫ z 0
C2
C2*
z0

α
α
C1

(z-plane)

f(z0)

C1*
(w-plane)

Fig. 380. Curves C1 and C2 and their respective images
C *1 and C *2 under a conformal mapping w ϭ ƒ(z)

c17.qxd

11/1/10

7:35 PM

Page 739

SEC. 17.1 Geometry of Analytic Functions: Conformal Mapping

739

in Fig. 381) as z1 approaches z 0 along C. The image C * of C is w ϭ f (z(t)). By the chain
rule, wؒ ϭ f r(z(t))zؒ(t). Hence the tangent direction of C * is given by the argument (use (9)
in Sec. 13.2)
arg wؒ ϭ arg f r ϩ arg zؒ

(3)

where arg zؒ gives the tangent direction of C. This shows that the mapping rotates all
directions at a point z 0 in the domain of analyticity of f through the same angle arg f r(z 0),
which exists as long as f r(z 0) 0. But this means conformality, as Fig. 381 illustrates
for an angle a between two curves, whose images C *1 and C *2 make the same angle (because
of the rotation).
᭿

z1 = z(t0 + Δt)

z(t0 )

z0 = z(t0 )
Curve C
Tangent

Fig. 381. Secant and tangent of the curve C

In the remainder of this section and in the next ones we shall consider various conformal
mappings that are of practical interest, for instance, in modeling potential problems.
EXAMPLE 2

Conformality of w ‫ ؍‬z n
The mapping w ϭ z n, n ϭ 2, 3, Á , is conformal, except at z ϭ 0, where w r ϭ nz n؊1 ϭ 0. For n ϭ 2 this is
shown in Fig. 378; we see that at 0 the angles are doubled. For general n the angles at 0 are multiplied by a
factor n under the mapping. Hence the sector 0 Ϲ u Ϲ p>n is mapped by z n onto the upper half-plane v м 0
(Fig. 382).
᭿
y

v

π /n
x

u

Fig. 382. Mapping by w ϭ z

EXAMPLE 3

n

Mapping w ‫ ؍‬z ؉ 1/z. Joukowski Airfoil
In terms of polar coordinates this mapping is
w ϭ u ϩ iv ϭ r (cos u ϩ i sin u) ϩ

1
(cos u Ϫ i sin u).
r

By separating the real and imaginary parts we thus obtain
u ϭ a cos u,

v ϭ b sin u

where

aϭrϩ

1
,
r

bϭrϪ

1
.
r

Hence circles ƒ z ƒ ϭ r ϭ const 1 are mapped onto ellipses x 2>a 2 ϩ y 2>b 2 ϭ 1. The circle r ϭ 1 is mapped
onto the segment Ϫ2 Ϲ u Ϲ 2 of the u-axis. See Fig. 383.

11/1/10

7:35 PM

740

Page 740

CHAP. 17 Conformal Mapping
y

v

x

1 2

–2

u

2

Fig. 383. Example 3
Now the derivative of w is
wr ϭ 1 Ϫ

1
z

2

ϭ

(z ϩ 1)(z Ϫ 1)
z2

which is 0 at z ϭ Ϯ1. These are the points at which the mapping is not conformal. The two circles in Fig. 384
pass through z ϭ Ϫ1. The larger is mapped onto a Joukowski airfoil. The dashed circle passes through both Ϫ1
and 1 and is mapped onto a curved segment.
Another interesting application of w ϭ z ϩ 1>z (the flow around a cylinder) will be considered in Sec. 18.4. ᭿
y

v
C

1

–1

x

u

2

–2

Fig. 384. Joukowski airfoil

Conformality of w ‫ ؍‬ez
From (10) in Sec. 13.5 we have ƒ ez ƒ ϭ ex and Arg z ϭ y. Hence ez maps a vertical straight line x ϭ x 0 ϭ const
onto the circle ƒ w ƒ ϭ ex0 and a horizontal straight line y ϭ y0 ϭ const onto the ray arg w ϭ y0. The rectangle
in Fig. 385 is mapped onto a region bounded by circles and rays as shown.
The fundamental region Ϫp Ͻ Arg z Ϲ p of ez in the z-plane is mapped bijectively and conformally onto
the entire w-plane without the origin w ϭ 0 (because ez ϭ 0 for no z). Figure 386 shows that the upper half
0 Ͻ y Ϲ p of the fundamental region is mapped onto the upper half-plane 0 Ͻ arg w Ϲ p, the left half being
mapped inside the unit disk ƒ w ƒ Ϲ 1 and the right half outside (why?).
᭿
0

v
3

1.

EXAMPLE 4

φ=

c17.qxd

y
1
0.5

D

C

A

B

0
0

1

C*
2
1
x

–3

–2

–1

.5

0
φ=
B*
D*
A*

0

Fig. 385. Mapping by w ϭ e

1

2

3

u

z

v
y
π

0

x

–1

(z-plane)

0
(w-plane)

Fig. 386. Mapping by w ϭ e z

1

u

c17.qxd

11/1/10

7:35 PM

Page 741

SEC. 17.1 Geometry of Analytic Functions: Conformal Mapping
EXAMPLE 5

741

Principle of Inverse Mapping. Mapping w ‫ ؍‬Ln z
Principle. The mapping by the inverse z ϭ f ؊1 (w) of w ϭ f (z) is obtained by interchanging the roles of the
z-plane and the w-plane in the mapping by w ϭ f (z).
Now the principal value w ϭ f (z) ϭ Ln z of the natural logarithm has the inverse z ϭ f ؊1 (w) ϭ ew. From
Example 4 (with the notations z and w interchanged!) we know that f ؊1 (w) ϭ ew maps the fundamental region
of the exponential function onto the z-plane without z ϭ 0 (because ew 0 for every w). Hence w ϭ f (z) ϭ Ln z
maps the z-plane without the origin and cut along the negative real axis (where u ϭ Im Ln z jumps by 2p)
conformally onto the horizontal strip Ϫp Ͻ v Ϲ p of the w-plane, where w ϭ u ϩ iv.
Since the mapping w ϭ Ln z ϩ 2pi differs from w ϭ Ln z by the translation 2pi (vertically upward), this
function maps the z-plane (cut as before and 0 omitted) onto the strip p Ͻ v Ϲ 3p. Similarly for each of the
infinitely many mappings w ϭ ln z ϭ Ln z Ϯ 2npi (n ϭ 0, 1, 2, Á ). The corresponding horizontal strips of width
2p (images of the z-plane under these mappings) together cover the whole w-plane without overlapping.
᭿

Magnification Ratio.

By the definition of the derivative we have
lim `

(4)

z:z0

f (z) Ϫ f (z 0)
z Ϫ z 0 ` ϭ ƒ f r(z 0) ƒ .

Therefore, the mapping w ϭ f (z) magnifies (or shortens) the lengths of short lines by
approximately the factor ƒ f r(z 0) ƒ . The image of a small figure conforms to the original
figure in the sense that it has approximately the same shape. However, since f r(z) varies
from point to point, a large figure may have an image whose shape is quite different from
that of the original figure.
More on the Condition f ؅(z) 0. From (4) in Sec. 13.4 and the Cauchy–Riemann
equations we obtain
(5 r )

ƒ f r(z) ƒ 2 ϭ `

0u
0v 2
0u 2
0v 2
0u 0v
0u 0v
ϩi
` ϭa b ϩa b ϭ
Ϫ
0x
0x
0x
0x
0x 0y
0y 0x

that is,

(5)

0u
0x
ƒ f r(z) ƒ 2 ϭ 4
0v
0x

0u
0(u, v)
0y
4ϭ
.
0(x, y)
0v
0y

This determinant is the so-called Jacobian (Sec. 10.3) of the transformation w ϭ f (z)
written in real form u ϭ u(x, y), v ϭ v(x, y). Hence f r(z 0) 0 implies that the Jacobian
is not 0 at z 0. This condition is sufficient that the mapping w ϭ f (z) in a sufficiently small
neighborhood of z 0 is one-to-one or injective (different points have different images). See
Ref. [GenRef4] in App. 1.

PROBLEM SET 17.1
1. On Fig. 378. One “rectangle” and its image are colored.
Identify the images for the other “rectangles.”
2. On Example 1. Verify all calculations.
3. Mapping w ‫ ؍‬z 3. Draw an analog of Fig. 378 for
w ϭ z 3.

4. Conformality. Why do the images of the straight lines
x ϭ const and y ϭ const under a mapping by an
analytic function intersect at right angles? Same
question for the curves ƒ z ƒ ϭ const and arg z ϭ const.
Are there exceptional points?

c17.qxd

11/1/10

7:35 PM

742

Page 742

CHAP. 17 Conformal Mapping

5. Experiment on w ‫ ؍‬z . Find out whether w ϭ z preserves angles in size as well as in sense. Try to prove
your result.
6–9
MAPPING OF CURVES
Find and sketch or graph the images of the given curves
under the given mapping.
6. x ϭ 1, 2, 3, 4, y ϭ 1, 2, 3, 4, w ϭ z 2
7. Rotation. Curves as in Prob. 6, w ϭ iz
8. Reflection in the unit circle. ƒ z ƒ ϭ 13 , 12 , 1, 2, 3,
Arg z ϭ 0, Ϯp>4, Ϯp>2, Ϯ3p>2
9. Translation. Curves as in Prob. 6, w ϭ z ϩ 2 ϩ i
10. CAS EXPERIMENT. Orthogonal Nets. Graph the
orthogonal net of the two families of level curves
Re f (z) ϭ const and Im f (z) ϭ const, where (a) f (z) ϭ z4,
(b) f (z) ϭ 1>z, (c) f (z) ϭ 1>z 2, (d) f (z) ϭ (z ϩ i)>
(1 ϩ iz). Why do these curves generally intersect at
right angles? In your work, experiment to get the best
possible graphs. Also do the same for other functions
of your own choice. Observe and record shortcomings
of your CAS and means to overcome such deficiencies.
11–20
MAPPING OF REGIONS
Sketch or graph the given region and its image under the
given mapping.
11. ƒ z ƒ Ϲ 12 , Ϫp>8 Ͻ Arg z Ͻ p>8, w ϭ z 2
12. 1 Ͻ ƒ z ƒ Ͻ 3, 0 Ͻ Arg z Ͻ p>2, w ϭ z 3
13. 2 Ϲ Im z Ϲ 5, w ϭ iz
14. x м 1, w ϭ 1>z
15. ƒ z Ϫ 12 ƒ Ϲ 12 , w ϭ 1>z
16. ƒ z ƒ Ͻ 12 , Im z Ͼ 0, w ϭ 1>z
17. ϪLn 2 Ϲ x Ϲ Ln 4, w ϭ ez
18. Ϫ1 Ϲ x Ϲ 2, Ϫp Ͻ y Ͻ p, w ϭ ez

17.2

19. 1 Ͻ ƒ z ƒ Ͻ 4, p>4 Ͻ u Ϲ 3p>4, w ϭ Ln z
20. 21 Ϲ ƒ z ƒ Ϲ 1, 0 Ϲ u Ͻ p>2, w ϭ Ln z
21–26
FAILURE OF CONFORMALITY
Find all points at which the mapping is not conformal. Give
reason.
21. A cubic polynomial
22. z 2 ϩ 1>z 2
z ϩ 12
23. 2
4z ϩ 2
exp (z 5 Ϫ 80z)
cosh z
sin pz
Magnification of Angles. Let f (z) be analytic at z 0.
Suppose that f r(z 0) ϭ 0, Á , f (k؊1) (z 0) ϭ 0. Then the
mapping w ϭ f (z) magnifies angles with vertex at z 0 by
a factor k. Illustrate this with examples for k ϭ 2, 3, 4.
28. Prove the statement in Prob. 27 for general k ϭ 1,
2, Á . Hint. Use the Taylor series.
24.
25.
26.
27.

MAGNIFICATION RATIO, JACOBIAN
Find the magnification ratio M. Describe what it tells
you about the mapping. Where is M ϭ 1? Find the
Jacobian J.
29–35

w ϭ 12 z 2
w ϭ z3
w ϭ 1>z
w ϭ 1>z 2
w ϭ ez
zϩ1
34. w ϭ
2z Ϫ 2
29.
30.
31.
32.
33.

35. w ϭ Ln z

Linear Fractional Transformations
(Möbius Transformations)
Conformal mappings can help in modeling and solving boundary value problems by first
mapping regions conformally onto another. We shall explain this for standard regions
(disks, half-planes, strips) in the next section. For this it is useful to know properties of
special basic mappings. Accordingly, let us begin with the following very important class.
The next two sections discuss linear fractional transformations. The reason for our
thorough study is that such transformations are useful in modeling and solving boundary
value problems, as we shall see in Chapter 18. The task is to get a good grasp of which

c17.qxd

11/1/10

7:35 PM

Page 743

SEC. 17.2 Linear Fractional Transformations (Möbius Transformations)

743

conformal mappings map certain regions conformally onto each other, such as, say
mapping a disk onto a half-plane (Sec. 17.3) and so forth. Indeed, the first step in the
modeling process of solving boundary value problems is to identify the correct conformal
mapping that is related to the “geometry” of the boundary value problem.
The following class of conformal mappings is very important. Linear fractional
transformations (or Möbius transformations) are mappings
wϭ

(1)

az ϩ b
cz ϩ d

(ad Ϫ bc

0)

where a, b, c, d are complex or real numbers. Differentiation gives
wr ϭ

(2)

a(cz ϩ d) Ϫ c(az ϩ b)
(cz ϩ d)

2

ϭ

ad Ϫ bc
(cz ϩ d)2

.

This motivates our requirement ad Ϫ bc 0. It implies conformality for all z and excludes
the totally uninteresting case w r ϵ 0 once and for all. Special cases of (1) are

(3)

EXAMPLE 1

wϭzϩb

(Translations)

w ϭ az with ƒ a ƒ ϭ 1

(Rotations)

w ϭ az ϩ b

(Linear transformations)

w ϭ 1>z

(Inversion in the unit circle).

Properties of the Inversion w ‫ ؍‬1/z (Fig. 387)
In polar forms z ϭ reiu and w ϭ Rei␾ the inversion w ϭ 1>z is
Rei␾ ϭ

1
reiu

ϭ

1
r

e؊iu

Rϭ

and gives

1
r

␾ ϭ Ϫu.

,

Hence the unit circle ƒ z ƒ ϭ r ϭ 1 is mapped onto the unit circle ƒ w ƒ ϭ R ϭ 1; w ϭ ei␾ ϭ e؊iu. For a general
z the image w ϭ 1>z can be found geometrically by marking ƒ w ƒ ϭ R ϭ 1>r on the segment from 0 to z and
then reflecting the mark in the real axis. (Make a sketch.)
Figure 387 shows that w ϭ 1>z maps horizontal and vertical straight lines onto circles or straight lines. Even
the following is true.
w ϭ 1>z maps every straight line or circle onto a circle or straight line.

v

y

y = – 12

2
x = – 12

1

x = 12

1

y=0
–2

–1

1

2

x

–2

–1

–1

1
–1

y=

1
2

–2

x=0

Fig. 387. Mapping (Inversion) w ϭ 1>z

2

u

c17.qxd

11/1/10

7:35 PM

744

Page 744

CHAP. 17 Conformal Mapping
Proof. Every straight line or circle in the z-plane can be written
A (x 2 ϩ y 2) ϩ Bx ϩ Cy ϩ D ϭ 0
A ϭ 0 gives a straight line and A

0 a circle. In terms of z and
Azz ϩ B

(A, B C, D real).

z this equation becomes

zϪz
zϩz
ϩ D ϭ 0.
ϩC
2
2i

Now w ϭ 1>z. Substitution of z ϭ 1>w and multiplication by ww gives the equation
AϩB

wϩw
2

ϩC

wϪw
2i

ϩ Dww ϭ 0

or, in terms of u and v,
A ϩ Bu Ϫ Cv ϩ D(u 2 ϩ v2) ϭ 0.
This represents a circle (if D

0) or a straight line (if D ϭ 0) in the w-plane.

᭿

The proof in this example suggests the use of z and z instead of x and y, a general principle
that is often quite useful in practice.
Surprisingly, every linear fractional transformation has the property just proved:
THEOREM 1

Circles and Straight Lines

Every linear fractional transformation (1) maps the totality of circles and straight
lines in the z-plane onto the totality of circles and straight lines in the w-plane.

PROOF

This is trivial for a translation or rotation, fairly obvious for a uniform expansion or
contraction, and true for w ϭ 1>z, as just proved. Hence it also holds for composites of
these special mappings. Now comes the key idea of the proof: represent (1) in terms of
these special mappings. When c ϭ 0, this is easy. When c 0, the representation is
1
a
w ϭ K cz ϩ d ϩ c

where

KϭϪ

ad Ϫ bc
.
c

This can be verified by substituting K, taking the common denominator and simplifying;
this yields (1). We can now set
w1 ϭ cz,

w2 ϭ w1 ϩ d,

1
w3 ϭ w ,
2

w4 ϭ Kw3 ,

and see from the previous formula that then w ϭ w4 ϩ a>c. This tells us that (1) is indeed
᭿
a composite of those special mappings and completes the proof.

Extended Complex Plane
The extended complex plane (the complex plane together with the point ϱ in Sec. 16.2)
can now be motivated even more naturally by linear fractional transformations as follows.
To each z for which cz ϩ d 0 there corresponds a unique w in (1). Now let c 0.
Then for z ϭ Ϫd>c we have cz ϩ d ϭ 0, so that no w corresponds to this z. This suggests
that we let w ϭ ϱ be the image of z ϭ Ϫd>c.

c17.qxd

11/1/10

7:35 PM

Page 745

SEC. 17.2 Linear Fractional Transformations (Möbius Transformations)

745

Also, the inverse mapping of (1) is obtained by solving (1) for z; this gives again a
linear fractional transformation
zϭ

(4)

dw Ϫ b
.
Ϫcw ϩ a

When c 0, then cw Ϫ a ϭ 0 for w ϭ a>c, and we let a>c be the image of z ϭ ϱ. With
these settings, the linear fractional transformation (1) is now a one-to-one mapping of the
extended z-plane onto the extended w-plane. We also say that every linear fractional
transformation maps “the extended complex plane in a one-to-one manner onto itself.”
Our discussion suggests the following.
General Remark. If z ϭ ϱ, then the right side of (1) becomes the meaningless expression
(a # ϱ ϩ b)>(c # ϱ ϩ d). We assign to it the value w ϭ a>c if c 0 and w ϭ ϱ if c ϭ 0.

Fixed Points
Fixed points of a mapping w ϭ f (z) are points that are mapped onto themselves, are “kept
fixed” under the mapping. Thus they are obtained from
w ϭ f (z) ϭ z.
The identity mapping w ϭ z has every point as a fixed point. The mapping w ϭ z has
infinitely many fixed points, w ϭ 1>z has two, a rotation has one, and a translation none
in the finite plane. (Find them in each case.) For (1), the fixed-point condition w ϭ z is
zϭ

(5)

az ϩ b
,
cz ϩ d

thus

cz 2 Ϫ (a Ϫ d)z Ϫ b ϭ 0.

For c 0 this is a quadratic equation in z whose coefficients all vanish if and only if the
mapping is the identity mapping w ϭ z (in this case, a ϭ d 0, b ϭ c ϭ 0). Hence we have
THEOREM 2

Fixed Points

A linear fractional transformation, not the identity, has at most two fixed points. If
a linear fractional transformation is known to have three or more fixed points, it must
be the identity mapping w ϭ z.

To make our present general discussion of linear fractional transformations even more
useful from a practical point of view, we extend it by further facts and typical examples,
in the problem set as well as in the next section.

PROBLEM SET 17.2
1. Verify the calculations in the proof of Theorem 1,
including those for the case c ϭ 0.
2. Composition of LFTs. Show that substituting a linear
fractional transformation (LFT) into an LFT gives
an LFT.

3. Matrices. If you are familiar with 2 ϫ 2 matrices,
prove that the coefficient matrices of (1) and (4) are
inverses of each other, provided that ad Ϫ bc ϭ 1, and
that the composition of LFTs corresponds to the
multiplication of the coefficient matrices.

c17.qxd

11/1/10

7:35 PM

746

Page 746

CHAP. 17 Conformal Mapping

4. Fig. 387. Find the image of x ϭ k ϭ const under
w ϭ 1>z. Hint. Use formulas similar to those in
Example 1.
5. Inverse. Derive (4) from (1) and conversely.
6. Fixed points. Find the fixed points mentioned in the
text before formula (5).
7–10
INVERSE
Find the inverse z ϭ z(w). Check by solving z(w) for w.
i
7. w ϭ
2z Ϫ 1
zϪi
8. w ϭ
zϩi

11.
12.
13.
14.

w ϭ (a ϩ ib)z 2
w ϭ z Ϫ 3i
w ϭ 16z 5
w ϭ az ϩ b

15. w ϭ

iz ϩ 4
2z Ϫ 5i

16. w ϭ

aiz Ϫ 1
,
z ϩ ai

a

1

17–20
FIXED POINTS
Find all LFTs with fixed point(s).
17. z ϭ 0
18. z ϭ Ϯ1
19. z ϭ Ϯi
20. Without any fixed points

zϪi
9. w ϭ
3iz ϩ 4
z Ϫ 12 i
10. w ϭ 1
Ϫ2 iz Ϫ 1

17.3

11–16
FIXED POINTS
Find the fixed points.

Special Linear Fractional Transformations
We continue our study of linear fractional transformations. We shall identify linear fractional
transformations
wϭ

(1)

az ϩ b
cz ϩ d

(ad Ϫ bc

0)

that map certain standard domains onto others. Theorem 1 (below) will give us a tool for
constructing desired linear fractional transformations.
A mapping (1) is determined by a, b, c, d, actually by the ratios of three of these constants
to the fourth because we can drop or introduce a common factor. This makes it plausible
that three conditions determine a unique mapping (1):
THEOREM 1

Three Points and Their Images Given

Three given distinct points z1, z 2, z 3 can always be mapped onto three prescribed
distinct points w1, w2, w3 by one, and only one, linear fractional transformation
w ϭ f (z). This mapping is given implicitly by the equation
(2)

w Ϫ w1 w2 Ϫ w3
z Ϫ z1 z 2 Ϫ z 3
#
#
w Ϫ w3 w2 Ϫ w1 ϭ z Ϫ z 3 z 2 Ϫ z1 .

(If one of these points is the point ϱ , the quotient of the two differences containing
this point must be replaced by 1.)

PROOF

Equation (2) is of the form F(w) ϭ G(z) with linear fractional F and G. Hence
w ϭ F ؊1(G(z)) ϭ f (z), where F ؊1 is the inverse of F and is linear fractional (see (4) in

c17.qxd

11/1/10

7:35 PM

Page 747

SEC. 17.3 Special Linear Fractional Transformations

747

Sec. 17.2) and so is the composite F ؊1(G (z)) (by Prob. 2 in Sec. 17.2), that is, w ϭ f (z)
is linear fractional. Now if in (2) we set w ϭ w1, w2, w3 on the left and z ϭ z1, z 2, z 3 on
the right, we see that
F(w1) ϭ 0,

F(w2) ϭ 1,

F(w3) ϭ ϱ

G(z1) ϭ 0,

G(z 2) ϭ 1,

G(z 3) ϭ ϱ.

؊1

From the first column, F(w1) ϭ G(z1), thus w1 ϭ F (G(z 1)) ϭ f (z1). Similarly, w2 ϭ f (z 2),
w3 ϭ f (z 3). This proves the existence of the desired linear fractional transformation.
To prove uniqueness, let w ϭ g(z) be a linear fractional transformation, which also
maps z j onto wj, j ϭ 1, 2, 3. Thus wj ϭ g(z j). Hence g ؊1(wj) ϭ z j, where wj ϭ f (z j).
Together, g؊1( f (z j)) ϭ z j, a mapping with the three fixed points z 1, z 2, z 3. By Theorem 2
in Sec. 17.2, this is the identity mapping, g؊1( f (z)) ϭ z for all z. Thus f (z) ϭ g(z) for all
z, the uniqueness.
The last statement of Theorem 1 follows from the General Remark in Sec. 17.2. ᭿

Mapping of Standard Domains by Theorem 1
Using Theorem 1, we can now find linear fractional transformations of some practically
useful domains (here called “standard domains”) according to the following principle.
Principle. Prescribe three boundary points z1, z 2, z 3 of the domain D in the z-plane.
Choose their images w1, w2, w3 on the boundary of the image D* of D in the w-plane.
Obtain the mapping from (2). Make sure that D is mapped onto D*, not onto its
complement. In the latter case, interchange two w-points. (Why does this help?)
v
y=5
y=1
x=2

x = –2

y=1
2

x = –1

x=1
u

1

y=0
x=1

x = –1

2

2

x=0

Fig. 388.

EXAMPLE 1

Linear fractional transformation in Example 1

Mapping of a Half-Plane onto a Disk (Fig. 388)
Find the linear fractional transformation (1) that maps z1 ϭ Ϫ1, z 2 ϭ 0, z 3 ϭ 1 onto w1 ϭ Ϫ1, w2 ϭ Ϫi,
w3 ϭ 1, respectively.

Solution. From (2) we obtain
w Ϫ (Ϫ1) # Ϫi Ϫ 1
z Ϫ (Ϫ1) # 0 Ϫ 1
ϭ
,
wϪ1
Ϫi Ϫ (Ϫ1)
zϪ1
0 Ϫ (Ϫ1)

c17.qxd

11/1/10

7:35 PM

748

Page 748

CHAP. 17 Conformal Mapping
thus
wϭ

zϪi
Ϫiz ϩ 1

.

Let us show that we can determine the specific properties of such a mapping without much calculation. For
z ϭ x we have w ϭ (x Ϫ i)>(Ϫix ϩ 1), thus ƒ w ƒ ϭ 1, so that the x-axis maps onto the unit circle. Since z ϭ i
gives w ϭ 0, the upper half-plane maps onto the interior of that circle and the lower half-plane onto the exterior.
z ϭ 0, i, ϱ go onto w ϭ Ϫi, 0, i, so that the positive imaginary axis maps onto the segment S: u ϭ 0, Ϫ1 Ϲ v Ϲ 1.
The vertical lines x ϭ const map onto circles (by Theorem 1, Sec. 17.2) through w ϭ i (the image of z ϭ ϱ ) and
perpendicular to ƒ w ƒ ϭ 1 (by conformality; see Fig. 388). Similarly, the horizontal lines y ϭ const map onto
circles through w ϭ i and perpendicular to S (by conformality). Figure 388 gives these circles for y м 0, and for
y Ͻ 0 they lie outside the unit disk shown.
᭿

EXAMPLE 2

Occurrence of ؕ
Determine the linear fractional transformation that maps z 1 ϭ 0, z 2 ϭ 1, z 3 ϭ ϱ onto w1 ϭ Ϫ1, w2 ϭ Ϫi,
w3 ϭ 1, respectively.

Solution. From (2) we obtain the desired mapping
wϭ

zϪi
zϩi

.

This is sometimes called the Cayley transformation.2 In this case, (2) gave at first the quotient (1 Ϫ ϱ)>(z Ϫ ϱ),
which we had to replace by 1.
᭿

EXAMPLE 3

Mapping of a Disk onto a Half-Plane
Find the linear fractional transformation that maps z 1 ϭ Ϫ1, z 2 ϭ i, z 3 ϭ 1 onto w1 ϭ 0, w2 ϭ i, w3 ϭ ϱ,
respectively, such that the unit disk is mapped onto the right half-plane. (Sketch disk and half-plane.)

Solution. From (2) we obtain, after replacing (i Ϫ ϱ)>(w Ϫ ϱ) by 1,
wϭϪ

zϩ1
zϪ1

.

᭿

Mapping half-planes onto half-planes is another task of practical interest. For instance,
we may wish to map the upper half-plane y м 0 onto the upper half-plane v м 0. Then
the x-axis must be mapped onto the u-axis.
EXAMPLE 4

Mapping of a Half-Plane onto a Half-Plane
Find the linear fractional transformation that maps z1 ϭ Ϫ2, z 2 ϭ 0, z 3 ϭ 2 onto w1 ϭ ϱ, w2 ϭ 14 , w3 ϭ 38 ,
respectively.

Solution. You may verify that (2) gives the mapping function
wϭ
What is the image of the x-axis? Of the y-axis?

zϩ1
2z ϩ 4

᭿

Mappings of disks onto disks is a third class of practical problems. We may readily
verify that the unit disk in the z-plane is mapped onto the unit disk in the w-plane by the
following function, which maps z 0 onto the center w ϭ 0.

2
ARTHUR CAYLEY (1821–1895), English mathematician and professor at Cambridge, is known for his
important work in algebra, matrix theory, and differential equations.

c17.qxd

11/1/10

7:35 PM

Page 749

SEC. 17.3 Special Linear Fractional Transformations

wϭ

(3)

749

z Ϫ z0
cz Ϫ 1

c ϭ z 0,

,

ƒ z 0 ƒ Ͻ 1.

To see this, take ƒ z ƒ ϭ 1, obtaining, with c ϭ z 0 as in (3),
ƒ z Ϫ z0 ƒ ϭ ƒ z Ϫ c ƒ
ϭ ƒzƒ ƒz Ϫ cƒ
ϭ ƒ zz Ϫ cz ƒ ϭ ƒ 1 Ϫ cz ƒ ϭ ƒ cz Ϫ 1 ƒ .
Hence
ƒ w ƒ ϭ ƒ z Ϫ z 0 ƒ > ƒ cz Ϫ 1 ƒ ϭ 1
from (3), so that ƒ z ƒ ϭ 1 maps onto ƒ w ƒ ϭ 1, as claimed, with z 0 going onto 0, as the
numerator in (3) shows.
Formula (3) is illustrated by the following example. Another interesting case will be
given in Prob. 17 of Sec. 18.2.
EXAMPLE 5

Mapping of the Unit Disk onto the Unit Disk
Taking z 0 ϭ 12 in (3), we obtain (verify!)
wϭ

2z Ϫ 1

(Fig. 389).

zϪ2

y

᭿

v
x=1

y=–1
2

2

x=0
x=–1
2

y=0
B

A
1 x

A*

B*
1

2

u

y=1
2

Fig. 389. Mapping in Example 5

EXAMPLE 6

Mapping of an Angular Region onto the Unit Disk
Certain mapping problems can be solved by combining linear fractional transformations with others. For instance,
to map the angular region D: Ϫp>6 Ϲ arg z Ϲ p>6 (Fig. 390) onto the unit disk ƒ w ƒ Ϲ 1, we may map D by
Z ϭ z 3 onto the right Z-half-plane and then the latter onto the disk ƒ w ƒ Ϲ 1 by
wϭi

ZϪ1
Zϩ1

,

combined

wϭi

z3 Ϫ 1
z3 ϩ 1

.

᭿

c17.qxd

11/1/10

7:35 PM

750

Page 750

CHAP. 17 Conformal Mapping

π /6

( z-plane)

(Z-plane)

(w-plane)

Fig. 390. Mapping in Example 6

This is the end of our discussion of linear fractional transformations. In the next section
we turn to conformal mappings by other analytic functions (sine, cosine, etc.).

PROBLEM SET 17.3
1. CAS EXPERIMENT. Linear Fractional Transformations (LFTs). (a) Graph typical regions (squares,
disks, etc.) and their images under the LFTs in
Examples 1–5 of the text.
(b) Make an experimental study of the continuous
dependence of LFTs on their coefficients. For instance,
change the LFT in Example 4 continuously and graph
the changing image of a fixed region (applying animation
if available).
2. Inverse. Find the inverse of the mapping in Example 1.
Show that under that inverse the lines x ϭ const are
the images of circles in the w-plane with centers on the
line v ϭ 1.
3. Inverse. If w ϭ f (z) is any transformation that has an
inverse, prove the (trivial!) fact that f and its inverse
have the same fixed points.
4. Obtain the mapping in Example 1 of this section from
Prob. 18 in Problem Set 17.2.
5. Derive the mapping in Example 2 from (2).
6. Derive the mapping in Example 4 from (2). Find its
inverse and the fixed points.
7. Verify the formula for disks.

17.4

LFTs FROM THREE POINTS AND IMAGES
8–16
Find the LFT that maps the given three points onto the three
given points in the respective order.
8. 0, 1, 2 onto 1, 12 , 13
9. 1, i, Ϫ1 onto i, Ϫ1, Ϫi
10. 0, Ϫi, i onto Ϫ1, 0, ϱ
11. Ϫ1, 0, 1 onto Ϫi, Ϫ1, i
12. 0, 2i, Ϫ2i onto Ϫ1, 0, ϱ
13. 0, 1, ϱ onto ϱ, 1, 0
14. Ϫ1, 0, 1 onto 1, 1 ϩ i, 1 ϩ 2i
15. 1, i, 2 onto 0, Ϫi Ϫ 1, Ϫ12
16. Ϫ32 , 0, 1 onto 0, 32 , 1
17. Find an LFT that maps ƒ z ƒ Ϲ 1 onto ƒ w ƒ Ϲ 1 so that
z ϭ i>2 is mapped onto w ϭ 0. Sketch the images of
the lines x ϭ const and y ϭ const.
18. Find all LFTs w(z) that map the x-axis onto the u-axis.
19. Find an analytic function w ϭ f (z) that maps the region
0 Ϲ arg z Ϲ p>4 onto the unit disk ƒ w ƒ Ϲ 1.
20. Find an analytic function that maps the second quadrant
of the z-plane onto the interior of the unit circle in the
w-plane.

Conformal Mapping by Other Functions
We shall now cover mappings by trigonometric and hyperbolic analytic functions. So far
we have covered the mappings by z n and ez (Sec. 17.1) as well as linear fractional
transformations (Secs. 17.2 and 17.3).
Sine Function. Figure 391 shows the mapping by
(1)

w ϭ u ϩ iv ϭ sin z ϭ sin x cosh y ϩ i cos x sinh y

(Sec. 13.6).

c17.qxd

11/1/10

7:35 PM

Page 751

SEC. 17.4 Conformal Mapping by Other Functions

751
v

y

2
1

1

π x
2

–π

2

–1

u

1
–1

–1

–2
(z-plane)

(w-plane)

Fig. 391. Mapping w ϭ u ϩ iv ϭ sin z

Hence
u ϭ sin x cosh y,

(2)

v ϭ cos x sinh y.

Since sin z is periodic with period 2p, the mapping is certainly not one-to-one if we
consider it in the full z-plane. We restrict z to the vertical strip S: Ϫ12 p Ϲ x Ϲ 12 p in
Fig. 391. Since f r(z) ϭ cos z ϭ 0 at z ϭ Ϯ12 p, the mapping is not conformal at these two
critical points. We claim that the rectangular net of straight lines x ϭ const and y ϭ const
in Fig. 391 is mapped onto a net in the w-plane consisting of hyperbolas (the images of
the vertical lines x ϭ const) and ellipses (the images of the horizontal lines y ϭ const)
intersecting the hyperbolas at right angles (conformality!). Corresponding calculations are
simple. From (2) and the relations sin2 x ϩ cos2 x ϭ 1 and cosh2 y Ϫ sinh2 y ϭ 1 we
obtain
u2
sin2 x

Ϫ

u2
cosh2 y

v2
cos2 x

ϩ

ϭ cosh2 y Ϫ sinh2 y ϭ 1

(Hyperbolas)

ϭ sin2 x ϩ cos2 x ϭ 1

(Ellipses).

v2
sinh2 y

Exceptions are the vertical lines x ϭ Ϫ 12 px ϭ 12 p, which are “folded” onto u Ϲ Ϫ1 and
u м 1 (v ϭ 0), respectively.
Figure 392 illustrates this further. The upper and lower sides of the rectangle are mapped
onto semi-ellipses and the vertical sides onto Ϫcosh 1 Ϲ u Ϲ Ϫ1 and 1 Ϲ u Ϲ cosh 1
(v ϭ 0), respectively. An application to a potential problem will be given in Prob. 3 of
Sec. 18.2.

y
C

1

D

A

–π

π
2

2

E

v
B

–1

x

C*
E*

D*
–1

F

Fig. 392. Mapping by w ϭ sin z

A*

1

B*
F* u

c17.qxd

11/1/10

752

7:35 PM

Page 752

CHAP. 17 Conformal Mapping

The mapping w ϭ cos z could be discussed independently, but since

Cosine Function.

w ϭ cos z ϭ sin (z ϩ 12 p),

(3)

we see at once that this is the same mapping as sin z preceded by a translation to the right
through 12 p units.
Hyperbolic Sine.

Since
w ϭ sinh z ϭ Ϫi sin (iz),

(4)

the mapping is a counterclockwise rotation Z ϭ iz through 12 p (i.e., 90°), followed by the
sine mapping Z * ϭ sin Z, followed by a clockwise 90°-rotation w ϭ ϪiZ *.
Hyperbolic Cosine.

This function
w ϭ cosh z ϭ cos (iz)

(5)

defines a mapping that is a rotation Z ϭ iz followed by the mapping w ϭ cos Z.
Figure 393 shows the mapping of a semi-infinite strip onto a half-plane by w ϭ cosh z.
Since cosh 0 ϭ 1, the point z ϭ 0 is mapped onto w ϭ 1. For real z ϭ x м 0, cosh z is
real and increases with increasing x in a monotone fashion, starting from 1. Hence the
positive x-axis is mapped onto the portion u м 1 of the u-axis.
For pure imaginary z ϭ iy we have cosh iy ϭ cos y. Hence the left boundary of the strip
is mapped onto the segment 1 м u м Ϫ1 of the u-axis, the point z ϭ pi corresponding to
w ϭ cosh ip ϭ cos p ϭ Ϫ1.
On the upper boundary of the strip, y ϭ p, and since sin p ϭ 0 and cos p ϭ Ϫ1, it
follows that this part of the boundary is mapped onto the portion u Ϲ Ϫ1 of the u-axis.
Hence the boundary of the strip is mapped onto the u-axis. It is not difficult to see that
the interior of the strip is mapped onto the upper half of the w-plane, and the mapping is
one-to-one.
This mapping in Fig. 393 has applications in potential theory, as we shall see in Prob. 12
of Sec. 18.3.
y
π

B
v
B*

A

x

–1

A*
0

1

u

Fig. 393. Mapping by w ϭ cosh z

Tangent Function. Figure 394 shows the mapping of a vertical infinite strip onto the
unit circle by w ϭ tan z, accomplished in three steps as suggested by the representation
(Sec. 13.6)
w ϭ tan z ϭ

sin z
(eiz Ϫ e؊iz)>i
(e2iz Ϫ 1)>i
ϭ
ϭ
.
iz
؊iz
cos z
e ϩe
e2iz ϩ 1

c17.qxd

11/1/10

7:35 PM

Page 753

SEC. 17.4 Conformal Mapping by Other Functions

753

Hence if we set Z ϭ e2iz and use 1>i ϭ Ϫi, we have
w ϭ tan z ϭ ϪiW,

(6)

Wϭ

ZϪ1
,
Zϩ1

Z ϭ e2iz.

We now see that w ϭ tan z is a linear fractional transformation preceded by an exponential
mapping (see Sec. 17.1) and followed by a clockwise rotation through an angle 12 p(90°).
The strip is S : Ϫ 14 p Ͻ x Ͻ 14 p, and we show that it is mapped onto the unit disk in
the w-plane. Since Z ϭ e2iz ϭ e؊2yϩ2ix, we see from (10) in Sec. 13.5 that ƒ Z ƒ ϭ e؊2y,
Arg Z ϭ 2x. Hence the vertical lines x ϭ Ϫp>4, 0, p>4 are mapped onto the rays
Arg Z ϭ Ϫp>2, 0, p>2, respectively. Hence S is mapped onto the right Z-half-plane. Also
ƒ Z ƒ ϭ e؊2y Ͻ 1 if y Ͼ 0 and ƒ Z ƒ Ͼ 1 if y Ͻ 0. Hence the upper half of S is mapped inside
the unit circle ƒ Z ƒ ϭ 1 and the lower half of S outside ƒ Z ƒ ϭ 1, as shown in Fig. 394.
Now comes the linear fractional transformation in (6), which we denote by g(Z):
W ϭ g(Z) ϭ

(7)

ZϪ1
.
Zϩ1

For real Z this is real. Hence the real Z-axis is mapped onto the real W-axis. Furthermore,
the imaginary Z-axis is mapped onto the unit circle ƒ W ƒ ϭ 1 because for pure imaginary
Z ϭ iY we get from (7)
ƒ W ƒ ϭ ƒ g(iY) ƒ ϭ `

iY Ϫ 1
` ϭ 1.
iY ϩ 1

The right Z-half-plane is mapped inside this unit circle ƒ W ƒ ϭ 1, not outside, because
Z ϭ 1 has its image g(1) ϭ 0 inside that circle. Finally, the unit circle ƒ Z ƒ ϭ 1 is mapped
onto the imaginary W-axis, because this circle is Z ϭ ei␾, so that (7) gives a pure imaginary
expression, namely,
g(ei␾) ϭ

ei␾ Ϫ 1
ei␾>2 Ϫ e؊i␾>2
i sin (␾>2)
ϭ
ϭ
.
i␾
i␾>2
؊i␾>2
cos (␾>2)
e ϩ1
e
ϩe

From the W-plane we get to the w-plane simply by a clockwise rotation through p>2; see (6).
Together we have shown that w ϭ tan z maps S: Ϫp>4 Ͻ Re z Ͻ p>4 onto the unit
disk ƒ w ƒ Ͻ 1, with the four quarters of S mapped as indicated in Fig. 394. This mapping
is conformal and one-to-one.
y

v

x

(z-plane)

u

(Z-plane)

(W-plane)

Fig. 394. Mapping by w ϭ tan z

(w-plane)

c17.qxd

11/1/10

7:35 PM

754

Page 754

CHAP. 17 Conformal Mapping

PROBLEM SET 17.4
CONFORMAL MAPPING w ‫ ؍‬ez
1. Find the image of x ϭ c ϭ const, Ϫp Ͻ y Ϲ p, under
w ϭ ez.
2. Find the image of y ϭ k ϭ const, Ϫϱ Ͻ x Ϲ ϱ, under
w ϭ ez.
3–7
Find and sketch the image of the given region
under w ϭ ez.
3. Ϫ12 Ϲ x Ϲ 12 , Ϫp Ϲ y Ϲ p
4. 0 Ͻ x Ͻ 1, 12 Ͻ y Ͻ 1
5. Ϫϱ Ͻ x Ͻ ϱ, 0 Ϲ y Ϲ 2 p
6. 0 Ͻ x Ͻ ϱ, 0 Ͻ y Ͻ p>2
7. 0 Ͻ x Ͻ 1, 0 Ͻ y Ͻ p
8. CAS EXPERIMENT. Conformal Mapping. If your
CAS can do conformal mapping, use it to solve Prob. 7.
Then increase y beyond p, say, to 50 p or 100 p. State
what you expected. See what you get as the image.
Explain.

CONFORMAL MAPPING w ‫ ؍‬sin z
9. Find the points at which w ϭ sin z is not conformal.
10. Sketch or graph the images of the lines x ϭ 0, Ϯp>6,
Ϯp>3, Ϯp>2 under the mapping w ϭ sin z.
11–14
Find and sketch or graph the image of the given
region under w ϭ sin z.

0 Ͻ x Ͻ p>2, 0 Ͻ y Ͻ 2
Ϫp>4 Ͻ x Ͻ p>4, 0 Ͻ y Ͻ 1
0 Ͻ x Ͻ 2p, 1 Ͻ y Ͻ 3
0 Ͻ x Ͻ p>6, Ϫϱ Ͻ y Ͻ ϱ
Describe the mapping w ϭ cosh z in terms of the mapping w ϭ sin z and rotations and translations.
16. Find all points at which the mapping w ϭ cosh 2pz is
not conformal.
11.
12.
13.
14.
15.

17.5

Riemann Surfaces.

17. Find an analytic function that maps the region R
bounded by the positive x- and y-semi-axes and the
hyperbola xy ϭ p in the first quadrant onto the upper
half-plane. Hint. First map R onto a horizontal strip.

CONFORMAL MAPPING w ‫ ؍‬cos z
18. Find the images of the lines y ϭ k ϭ const under the
mapping w ϭ cos z.
19. Find the images of the lines x ϭ c ϭ const under the
mapping w ϭ cos z.
20–23
Find and sketch or graph the image of the given
region under the mapping w ϭ cos z.
0 Ͻ x Ͻ 2p, 12 Ͻ y Ͻ 1
0 Ͻ x Ͻ p>2, 0 Ͻ y Ͻ 2 directly and from Prob. 11
Ϫ1 Ͻ x Ͻ 1, 0 Ϲ y Ϲ 1
p Ͻ x Ͻ 2p, y Ͻ 0
Find and sketch the image of the region 2 Ϲ ƒ z ƒ Ϲ 3,
p>4 Ϲ u Ϲ p>2 under the mapping w ϭ Ln z.
zϪ1
25. Show that w ϭ Ln
maps the upper half-plane
zϩ1
onto the horizontal strip 0 Ϲ Im w Ϲ p as shown in
the figure.
20.
21.
22.
23.
24.

A

B

(∞)

–1

C

D

0
1
(z-plane)

E
(∞)

πi
C*
D*(∞)

E* = A*

B*(∞)

0
(w-plane)

Problem 25

Optional

One of the simplest but most ingeneous ideas in complex analysis is that of Riemann
surfaces. They allow multivalued relations, such as w ϭ 1z or w ϭ ln z, to become
single-valued and therefore functions in the usual sense. This works because the Riemann
surfaces consist of several sheets that are connected at certain points (called branch points).
Thus w ϭ 1z will need two sheets, being single-valued on each sheet. How many sheets
do you think w ϭ ln z needs? Can you guess, by recalling Sec. 13.7? (The answer will
be given at the end of this section). Let us start our systematic discussion.
The mapping given by
(1)

w ϭ u ϩ iv ϭ z 2

(Sec. 17.1)

c17.qxd

11/1/10

7:36 PM

Page 755

SEC. 17.5 Riemann Surfaces. Optional

755

is conformal, except at z ϭ 0, where w r ϭ 2z ϭ 0, At z ϭ 0, angles are doubled under
the mapping. Thus the right z-half-plane (including the positive y-axis) is mapped onto
the full w-plane, cut along the negative half of the u-axis; this mapping is one-to-one.
Similarly for the left z-half-plane (including the negative y-axis). Hence the image of the
full z-plane under w ϭ z 2 “covers the w-plane twice” in the sense that every w 0 is the
image of two z-points; if z1 is one, the other is Ϫz1. For example, z ϭ i and Ϫi are both
mapped onto w ϭ Ϫ1.
Now comes the crucial idea. We place those two copies of the cut w-plane upon each
other so that the upper sheet is the image of the right half z-plane R and the lower sheet
is the image of the left half z-plane L. We join the two sheets crosswise along the cuts
(along the negative u-axis) so that if z moves from R to L, its image can move from the
upper to the lower sheet. The two origins are fastened together because w ϭ 0 is the image
of just one z-point, z ϭ 0. The surface obtained is called a Riemann surface (Fig. 395a).
w ϭ 0 is called a “winding point” or branch point. w ϭ z 2 maps the full z-plane onto
this surface in a one-to-one manner.
By interchanging the roles of the variables z and w it follows that the double-valued
relation
w ϭ 1z

(2)

(Sec. 13.2)

becomes single-valued on the Riemann surface in Fig. 395a, that is, a function in the usual
sense. We can let the upper sheet correspond to the principal value of 1z. Its image is
the right w-half-plane. The other sheet is then mapped onto the left w-half-plane.

(a) Riemann surface of

z

(b) Riemann surface of

3

z

Fig. 395. Riemann surfaces
3
Similarly, the triple-valued relation w ϭ 1
z becomes single-valued on the three-sheeted
Riemann surface in Fig. 395b, which also has a branch point at z ϭ 0.
The infinitely many-valued natural logarithm (Sec. 13.7)

w ϭ ln z ϭ Ln z ϩ 2npi

(n ϭ 0, Ϯ1, Ϯ2, Á )

becomes single-valued on a Riemann surface consisting of infinitely many sheets,
w ϭ Ln z corresponds to one of them. This sheet is cut along the negative x-axis and the
upper edge of the slit is joined to the lower edge of the next sheet, which corresponds to
the argument p Ͻ u Ϲ 3p, that is, to
w ϭ Ln z ϩ 2pi.
The principal value Ln z maps its sheet onto the horizontal strip Ϫp Ͻ v Ϲ p. The
function w ϭ Ln z ϩ 2pi maps its sheet onto the neighboring strip p Ͻ v Ϲ 3p, and so
on. The mapping of the points z 0 of the Riemann surface onto the points of the w-plane
is one-to-one. See also Example 5 in Sec. 17.1.

c17.qxd

11/1/10

756

7:36 PM

Page 756

CHAP. 17 Conformal Mapping

PROBLEM SET 17.5
1. If z moves from z ϭ 14 twice around the circle ƒ z ƒ ϭ 14 ,
what does w ϭ 1z do?
2. Show that the Riemann surface of w ϭ
1(z Ϫ 1)(z Ϫ 2) has branch points at 1 and 2 sheets,
which we may cut and join crosswise from 1 to 2.
Hint. Introduce polar coordinates z Ϫ 1 ϭ r1eiu1 and
z Ϫ 2 ϭ r2eiu2, so that w ϭ 1r1r2 ei(u1ϩu2)>2.
3. Make a sketch, similar to Fig. 395, of the Riemann
4
surface of w ϭ 1
z ϩ 1.

4–10

RIEMANN SURFACES

Find the branch points and the number of sheets of the
Riemann surface.
4. 1iz Ϫ 2 ϩ i

3
5. z 2 ϩ 2
4z ϩ i

6. ln (6z Ϫ 2i)

7. 2 z Ϫ z 0

8. e 1z, 2ez

9. 2z 3 ϩ z

n

10. 2(4 Ϫ z 2)(1 Ϫ z 2)

CHAPTER 17 REVIEW QUESTIONS AND PROBLEMS
1. What is a conformal mapping? Why does it occur in
complex analysis?
2. At what points are w ϭ z 5 Ϫ z and w ϭ cos (pz 2) not
conformal?
3. What happens to angles at z 0 under a mapping w ϭ f (z)
if f r(z 0) ϭ 0, f s(z 0) ϭ 0, f t(z 0) 0?
4. What is a linear fractional transformation? What can
you do with it? List special cases.
5. What is the extended complex plane? Ways of introducing it?
6. What is a fixed point of a mapping? Its role in this
chapter? Give examples.
7. How would you find the image of x ϭ Re z ϭ 1 under
w ϭ iz, z 2, ez, 1>z?
8. Can you remember mapping properties of w ϭ ln z?
9. What mapping gave the Joukowski airfoil? Explain
details.
10. What is a Riemann surface? Its motivation? Its simplest
example.
11–16
MAPPING w ‫ ؍‬z2
Find and sketch the image of the given region or curve
under w ϭ z 2.
11. 1 Ͻ ƒ z ƒ Ͻ 2, ƒ arg z ƒ Ͻ p>8
12. 1> 1p Ͻ ƒ z ƒ Ͻ 1p, 0 Ͻ arg z Ͻ p>2
13. Ϫ4 Ͻ xy Ͻ 4
14. 0 Ͻ y Ͻ 2
15. x ϭ Ϫ1, 1
16. y ϭ Ϫ2, 2
17–22
MAPPING w ‫ ؍‬1/z
Find and sketch the image of the given region or curve
under w ϭ 1>z.
17. ƒ z ƒ Ͻ 1
18. ƒ z ƒ Ͻ 1, 0 Ͻ arg z Ͻ p>2
19. 2 Ͻ ƒ z ƒ Ͻ 3, y Ͼ 0 20. 0 Ϲ arg z Ϲ p>4

21. (x Ϫ 12 )2 ϩ y 2 ϭ 14 , y Ͼ 0
22. z ϭ 1 ϩ iy (Ϫϱ Ͻ y Ͻ ϱ)
23–28

LINEAR FRACTIONAL
TRANSFORMATIONS (LFTs)

Find the LFT that maps
23. Ϫ1, 0, 1 onto 4 ϩ 3i, 5i>2, 4 Ϫ 3i, respectively
24. 0, 2, 4 onto ϱ, 12 , 14 , respectively
25. 1, i, Ϫi onto i, Ϫ1, 1, respectively
26. 0, 1, 2 onto 2i, 1 ϩ 2i, 2 ϩ 2i, respectively
27. 0, 1, ϱ onto ϱ, 1, 0, respectively
28. Ϫ1, Ϫi, i onto 1 Ϫ i, 2, 0, respectively
29–34
FIXED POINTS
Find the fixed points of the mapping
29. w ϭ (2 ϩ i)z
30. w ϭ z 4 ϩ z Ϫ 64
31. w ϭ (3z ϩ 2)>(z Ϫ 1) 32. (2iz Ϫ 1)>(z ϩ 2i)
33. w ϭ z 5 ϩ 10z 3 ϩ 10z
34. w ϭ (iz ϩ 5)>(5z ϩ i)
35–40
GIVEN REGIONS
Find an analytic function w ϭ f (z) that maps
35. The infinite strip 0 Ͻ y Ͻ p>4 onto the upper halfplane v Ͼ 0.
36. The quarter-disk ƒ z ƒ Ͻ 1, x Ͼ 0, y Ͼ 0 onto the exterior
of the unit circle ƒ w ƒ ϭ 1.
37. The sector 0 Ͻ arg z Ͻ p>2 onto the region u Ͻ 1.
38. The interior of the unit circle ƒ z ƒ ϭ 1 onto the exterior
of the circle ƒ w ϩ 2 ƒ ϭ 2.
39. The region x Ͼ 0, y Ͼ 0, xy Ͻ c onto the strip 0 Ͻ
v Ͻ 1.
40. The semi-disk ƒ z ƒ Ͻ 2, y Ͼ 0 onto the exterior of the
circle ƒ w Ϫ p ƒ ϭ p.

c17.qxd

11/1/10

7:36 PM

Page 757

Summary of Chapter 17

757

SUMMARY OF CHAPTER

17

Conformal Mapping
A complex function w ϭ f (z) gives a mapping of its domain of definition in the
complex z-plane onto its range of values in the complex w-plane. If f (z) is analytic,
this mapping is conformal, that is, angle-preserving: the images of any two
intersecting curves make the same angle of intersection, in both magnitude and sense,
as the curves themselves (Sec. 17.1). Exceptions are the points at which f r(z) ϭ 0
(“critical points,” e.g. z ϭ 0 for w ϭ z 2).
For mapping properties of ez, cos z, sin z etc. see Secs. 17.1 and 17.4.
Linear fractional transformations, also called Möbius transformations
(1)

wϭ

az ϩ b
cz ϩ d

(Secs. 17.2, 17.3)

(ad Ϫ bc 0) map the extended complex plane (Sec. 17.2) onto itself. They solve
the problems of mapping half-planes onto half-planes or disks, and disks onto disks
or half-planes. Prescribing the images of three points determines (1) uniquely.
Riemann surfaces (Sec. 17.5) consist of several sheets connected at certain points
called branch points. On them, multivalued relations become single-valued, that is,
functions in the usual sense. Examples. For w ϭ 1z we need two sheets (with branch
point 0) since this relation is doubly-valued. For w ϭ ln z we need infinitely many
sheets since this relation is infinitely many-valued (see Sec. 13.7).

c18.qxd

11/2/10

6:54 PM

Page 758

CHAPTER

18

Complex Analysis
and Potential Theory
In Chapter 17 we developed the geometric approach of conformal mapping. This meant
that, for a complex analytic function w ϭ f (z) defined in a domain D of the z-plane, we
associated with each point in D a corresponding point in the w-plane. This gave us a
conformal mapping (angle-preserving), except at critical points where f r(z) ϭ 0.
Now, in this chapter, we shall apply conformal mappings to potential problems. This
will lead to boundary value problems and many engineering applications in electrostatics,
heat flow, and fluid flow. More details are as follows.
Recall that Laplace’s equation ٌ2 £ ϭ 0 is one of the most important PDEs in
engineering mathematics because it occurs in gravitation (Secs. 9.7, 12.11), electrostatics
(Sec. 9.7), steady-state heat conduction (Sec. 12.5), incompressible fluid flow, and other
areas. The theory of this equation is called potential theory (although “potential” is also
used in a more general sense in connection with gradients (see Sec. 9.7)). Because we
want to treat this equation with complex analytic methods, we restrict our discussion to
the “two-dimensional case.” Then £ depends only on two Cartesian coordinates x and y,
and Laplace’s equation becomes
ٌ2 £ ϭ £ xx ϩ £ yy ϭ 0.
An important idea then is that its solutions £ are closely related to complex analytic
functions £ ϩ i° as shown in Sec. 13.4. (Remark: We use the notation £ ϩ i° to free
u and v, which will be needed in conformal mapping u ϩ iv.) This important relation is
the main reason for using complex analysis in problems of physics and engineering.
We shall examine this connection between Laplace’s equation and complex analytic
functions and illustrate it by modeling applications from electrostatics (Secs. 18.1,
18.2), heat conduction (Sec. 18.3), and hydrodynamics (Sec. 18.4). This in turn will
lead to boundary value problems in two-dimensional potential theory. As a result,
some of the functions of Chap. 17 will be used to transform complicated regions into
simpler ones.
Section 18.5 will derive the important Poisson formula for potentials in a circular disk.
Section 18.6 will deal with harmonic functions, which, as you recall, are solutions of
Laplace’s equation and have continuous second partial derivatives. In that section we will
show how results on analytic functions can be used to characterize properties of harmonic
functions.
Prerequisite: Chaps. 13, 14, 17.
References and Answers to Problems: App. 1 Part D, App. 2.

758

c18.qxd

11/2/10

6:55 PM

Page 759

SEC. 18.1 Electrostatic Fields

18.1

759

Electrostatic Fields
The electrical force of attraction or repulsion between charged particles is governed by
Coulomb’s law (see Sec. 9.7). This force is the gradient of a function £, called the
electrostatic potential. At any points free of charges, £ is a solution of Laplace’s equation
ٌ2 £ ϭ 0.
The surfaces £ ϭ const are called equipotential surfaces. At each point P at which
the gradient of £ is not the zero vector, it is perpendicular to the surface £ ϭ const
through P; that is, the electrical force has the direction perpendicular to the equipotential
surface. (See also Secs. 9.7 and 12.11.)
The problems we shall discuss in this entire chapter are two-dimensional (for the
reason just given in the chapter opening), that is, they model physical systems that lie
in three-dimensional space (of course!), but are such that the potential £ is independent
of one of the space coordinates, so that £ depends only on two coordinates, which we
call x and y. Then Laplace’s equation becomes

y

x

ٌ2 £ ϭ

(1)

Fig. 396. Potential
in Example 1

EXAMPLE 1

02£
0x 2

ϩ

02£
0y 2

ϭ 0.

Equipotential surfaces now appear as equipotential lines (curves) in the xy-plane.
Let us illustrate these ideas by a few simple examples.
Potential Between Parallel Plates
Find the potential £ of the field between two parallel conducting plates extending to infinity (Fig. 396), which
are kept at potentials £ 1 and £ 2, respectively.

Solution. From the shape of the plates it follows that £ depends only on x, and Laplace’s equation becomes
£ s ϭ 0. By integrating twice we obtain £ ϭ ax ϩ b, where the constants a and b are determined by the given

boundary values of £ on the plates. For example, if the plates correspond to x ϭ Ϫ1 and x ϭ 1, the solution is
£(x) ϭ 12 (£ 2 Ϫ £ 1)x ϩ 12 (£ 2 ϩ £ 1) .

᭿

The equipotential surfaces are parallel planes.

EXAMPLE 2

Potential Between Coaxial Cylinders
Find the potential £ between two coaxial conducting cylinders extending to infinity on both ends (Fig. 397)
and kept at potentials £ 1 and £ 2, respectively.

Solution. Here £ depends only on r ϭ 2x 2 ϩ y 2, for reasons of symmetry, and Laplace’s equation
r 2u rr ϩ ru r ϩ u uu ϭ 0 [(5), Sec. 12.10] with u uu ϭ 0 and u ϭ £ becomes r £ s ϩ £ r ϭ 0. By separating
variables and integrating we obtain
1
£s
ϭϪ ,
£r
r

ln £ r ϭ Ϫln r ϩ ෂ
a,

£r ϭ

a
,
r

£ ϭ a ln r ϩ b

and a and b are determined by the given values of £ on the cylinders. Although no infinitely extended conductors
exist, the field in our idealized conductor will approximate the field in a long finite conductor in that part which
is far away from the two ends of the cylinders.
᭿

c18.qxd

11/2/10

6:55 PM

Page 760

760

CHAP. 18 Complex Analysis and Potential Theory
y

x

Fig. 397. Potential in Example 2

EXAMPLE 3
y

Potential in an Angular Region
Find the potential £ between the conducting plates in Fig. 398, which are kept at potentials £ 1 (the lower plate)
and £ 2, and make an angle a, where 0 Ͻ a Ϲ p. (In the figure we have a ϭ 120° ϭ 2p>3.)

Solution. u ϭ Arg z (z ϭ x ϩ iy

0) is constant on rays u ϭ const. It is harmonic since it is the imaginary
part of an analytic function, Ln z (Sec. 13.7). Hence the solution is
£(x, y) ϭ a ϩ b Arg z

x

with a and b determined from the two boundary conditions (given values on the plates)
a ϩ b(Ϫ12 a) ϭ £ 1,

a ϩ b(12 a) ϭ £ 2 .

Thus a ϭ (£ 2 ϩ £ 1)>2, b ϭ (£ 2 Ϫ £ 1)>a. The answer is

Fig. 398. Potential
in Example 3

£(x, y) ϭ

1
1
(£ 2 ϩ £ 1) ϩ (£ 2 Ϫ £ 1) u,
2
a

y
u ϭ arctan .
x

᭿

Complex Potential
Let £(x, y) be harmonic in some domain D and °(x, y) a harmonic conjugate of £ in D.
(Note the change of notation from u and v of Sec. 13.4 to £ and ° . From the next section
on, we had to free u and v for use in conformal mapping. Then
(2)

F(z) ϭ £(x, y) ϩ i°(x, y)

is an analytic function of z ϭ x ϩ iy. This function F is called the complex potential
corresponding to the real potential £. Recall from Sec. 13.4 that for given £, a conjugate
° is uniquely determined except for an additive real constant. Hence we may say the
complex potential, without causing misunderstandings.
The use of F has two advantages, a technical one and a physical one. Technically, F
is easier to handle than real or imaginary parts, in connection with methods of complex
analysis. Physically, ° has a meaning. By conformality, the curves ° ϭ const intersect
the equipotential lines £ ϭ const in the xy-plane at right angles [except where F r(z) ϭ 0].
Hence they have the direction of the electrical force and, therefore, are called lines
of force. They are the paths of moving charged particles (electrons in an electron
microscope, etc.).

c18.qxd

11/2/10

6:55 PM

Page 761

SEC. 18.1 Electrostatic Fields
EXAMPLE 4

761

Complex Potential
In Example 1, a conjugate is ° ϭ ay. It follows that the complex potential is
F(z) ϭ az ϩ b ϭ ax ϩ b ϩ iay,

᭿

and the lines of force are horizontal straight lines y ϭ const parallel to the x-axis.

EXAMPLE 5

Complex Potential
In Example 2 we have £ ϭ a ln r ϩ b ϭ a ln ƒ z ƒ ϩ b. A conjugate is ° ϭ a Arg z. Hence the complex
potential is
F(z) ϭ a Ln z ϩ b
and the lines of force are straight lines through the origin. F(z) may also be interpreted as the complex potential
of a source line (a wire perpendicular to the xy-plane) whose trace in the xy-plane is the origin.
᭿

EXAMPLE 6

Complex Potential
In Example 3 we get F(z) by noting that i Ln z ϭ i ln ƒ z ƒ Ϫ Arg z, multiplying this by Ϫb, and adding a:
F(z) ϭ a Ϫ ib Ln z ϭ a ϩ b Arg z Ϫ ib ln ƒ z ƒ .
We see from this that the lines of force are concentric circles ƒ z ƒ ϭ const. Can you sketch them?

᭿

Superposition
More complicated potentials can often be obtained by superposition.
EXAMPLE 7

Potential of a Pair of Source Lines (a Pair of Charged Wires)
Determine the potential of a pair of oppositely charged source lines of the same strength at the points z ϭ c and
z ϭ Ϫc on the real axis.

Solution. From Examples 2 and 5 it follows that the potential of each of the source lines is
£ 1 ϭ K ln ƒ z Ϫ c ƒ

and

£ 2 ϭ ϪK ln ƒ z ϩ c ƒ ,

respectively. Here the real constant K measures the strength (amount of charge). These are the real parts of the
complex potentials
F1(z) ϭ K Ln (z Ϫ c)

and

F2(z) ϭ ϪK Ln (z ϩ c).

Hence the complex potential of the combination of the two source lines is
(3)

F(z) ϭ F1(z) ϩ F2(z) ϭ K [Ln (z Ϫ c) Ϫ Ln (z ϩ c)] .

The equipotential lines are the curves
£ ϭ Re F(z) ϭ K ln `

zϪc
zϩc

` ϭ const,

thus

`

zϪc
zϩc

These are circles, as you may show by direct calculation. The lines of force are
° ϭ Im F(z) ϭ K[Arg (z Ϫ c) Ϫ Arg (z ϩ c)] ϭ const.
We write this briefly (Fig. 399)
° ϭ K(u1 Ϫ u2) ϭ const.

` ϭ const.

c18.qxd

11/2/10

762

6:55 PM

Page 762

CHAP. 18 Complex Analysis and Potential Theory
Now u1 Ϫ u2 is the angle between the line segments from z to c and Ϫc (Fig. 399). Hence the lines of force
are the curves along each of which the line segment S: Ϫc Ϲ x Ϲ c appears under a constant angle. These curves
are the totality of circular arcs over S, as is (or should be) known from elementary geometry. Hence the lines
of force are circles. Figure 400 shows some of them together with some equipotential lines.
In addition to the interpretation as the potential of two source lines, this potential could also be thought of as
the potential between two circular cylinders whose axes are parallel but do not coincide, or as the potential
between two equal cylinders that lie outside each other, or as the potential between a cylinder and a plane wall.
Explain this using Fig. 400.
᭿

The idea of the complex potential as just explained is the key to a close relation of potential
theory to complex analysis and will recur in heat flow and fluid flow.

z

θ 1 – θ2

θ2
–c

θ1
c

x

Fig. 400. Equipotential lines and lines
of force (dashed) in Example 7

Fig. 399. Arguments in Example 7

PROBLEM SET 18.1
1–4

COAXIAL CYLINDERS

Find and sketch the potential between two coaxial cylinders
of radii r1 and r2 having potential U1 and U2, respectively.
1. r1 ϭ 2.5 mm, r2 ϭ 4.0 cm, U1 ϭ 0 V,
U2 ϭ 220 V
2. r1 ϭ 1 cm, r2 ϭ 2 cm, U1 ϭ 400 V, U2 ϭ 0 V
3. r1 ϭ 10 cm, r2 ϭ 1 m, U1 ϭ 10 kV,
U2 ϭ Ϫ10 kV
4. If r1 ϭ 2 cm, r2 ϭ 6 cm and U1 ϭ 300 V, U2 ϭ 100 V,
respectively, is the potential at r ϭ 4 cm equal to
200 V? Less? More? Answer without calculation. Then
calculate and explain.
5–7

PARALLEL PLATES

Find and sketch the potential between the parallel plates
having potentials U1 and U2. Find the complex potential.
5. Plates at x 1 ϭ Ϫ5 cm, x 2 ϭ 5 cm, potentials U1 ϭ
250 V, U2 ϭ 500 V, respectively.
6. Plates at y ϭ x and y ϭ x ϩ k, potentials U1 ϭ 0 V,
U2 ϭ 220 V, respectively.
7. Plates at x 1 ϭ 12 cm, x 2 ϭ 24 cm, potentials U1 ϭ
20 kV, U2 ϭ 8 kV, respectively.
8. CAS EXPERIMENT. Complex Potentials. Graph
the equipotential lines and lines of force in (a)–(d) (four

graphs, Re F(z) and Im F(z) on the same axes). Then
explore further complex potentials of your choice with
the purpose of discovering configurations that might
be of practical interest.
(a) F(z) ϭ z 2
(b) F(z) ϭ iz 2
(c) F(z) ϭ 1>z
(d) F(z) ϭ i>z
9. Argument. Show that £ ϭ u> p ϭ (1> p) arctan ( y>x)
is harmonic in the upper half-plane and satisfies the
boundary condition £(x, 0) ϭ 1 if x Ͻ 0 and 0 if
x Ͼ 0, and the corresponding complex potential is
F(z) ϭ Ϫ(i> p) Ln z.
10. Conformal mapping. Map the upper z-half-plane
onto ƒ w ƒ Ϲ 1 so that 0, ϱ, Ϫ1 are mapped onto 1, i, Ϫi,
respectively. What are the boundary conditions on
ƒ w ƒ ϭ 1 resulting from the potential in Prob. 9? What
is the potential at w ϭ 0?
11. Text Example 7. Verify, by calculation, that the equipotential lines are circles.
12–15

OTHER CONFIGURATIONS

12. Find and sketch the potential between the axes
(potential 500 V) and the hyperbola xy ϭ 4 (potential
100 V).

c18.qxd

11/2/10

6:55 PM

Page 763

SEC. 18.2 Use of Conformal Mapping. Modeling

763

13. Arccos. Show that F(z) ϭ arccos z (defined in Problem
Set 13.7) gives the potential of a slit in Fig. 401.

y

y

y

x

1

–1

1

–1

1

x

x

Fig. 402. Other apertures
Fig. 401. Slit
14. Arccos. Show that F(z) in Prob. 13 gives the potentials
in Fig. 402.

18.2

15. Sector. Find the real and complex potentials in the
sector Ϫp>6 Ϲ u Ϲ p>6 between the boundary u ϭ
Ϯp>6, kept at 0 V, and the curve x 3 Ϫ 3xy 2 ϭ 1, kept
at 220 V.

Use of Conformal Mapping. Modeling
We have just explored the close relation between potential theory and complex analysis.
This relationship is so close because complex potentials can be modeled in complex
analysis. In this section we shall explore the close relation that results from the use of
conformal mapping in modeling and solving boundary value problems for the Laplace
equation. The process consists of finding a solution of the equation in some domain,
assuming given values on the boundary (Dirichlet problem, see also Sec. 12.6). The key
idea is then to use conformal mapping to map a given domain onto one for which the
solution is known or can be found more easily. This solution thus obtained is then mapped
back to the given domain. The reason this approach works is due to Theorem 1, which
asserts that harmonic functions remain harmonic under conformal mapping:

THEOREM 1

Harmonic Functions Under Conformal Mapping

Let £* be harmonic in a domain D* in the w-plane. Suppose that w ϭ u ϩ iv ϭ f (z)
is analytic in a domain D in the z-plane and maps D conformally onto D*. Then
the function
(1)

£(x, y) ϭ £* (u(x, y), v(x, y))

is harmonic in D.

PROOF

The composite of analytic functions is analytic, as follows from the chain rule. Hence, taking
a harmonic conjugate °*(u, v) of £*, as defined in Sec. 13.4, and forming the analytic
function F*(w) ϭ £*(u, v) ϩ i°*(u, v) we conclude that F(z) ϭ F*( f (z)) is analytic in D.
Hence its real part £(x, y) ϭ Re F(z) is harmonic in D. This completes the proof.
We mention without proof that if D* is simply connected (Sec. 14.2), then a harmonic
conjugate of £ * exists. Another proof of Theorem 1 without the use of a harmonic
conjugate is given in App. 4.
᭿

c18.qxd

11/2/10

6:55 PM

764
EXAMPLE 1

Page 764

CHAP. 18 Complex Analysis and Potential Theory
Potential Between Noncoaxial Cylinders
Model the electrostatic potential between the cylinders C1: ƒ z ƒ ϭ 1 and C2: ƒ z Ϫ 25 ƒ ϭ 25 in Fig. 403. Then give
the solution for the case that C1 is grounded, U1 ϭ 0 V, and C2 has the potential U2 ϭ 110 V.
We map the unit disk ƒ z ƒ ϭ 1 onto the unit disk ƒ w ƒ ϭ 1 in such a way that C2 is mapped onto
some cylinder C2*: ƒ w ƒ ϭ r0. By (3), Sec. 17.3, a linear fractional transformation mapping the unit disk onto the
unit disk is

Solution.

wϭ

(2)

zϪb
bz Ϫ 1

y

v
U1 = 0

C1

U1 = 0

U2 = 110 V
C2

U2 = 110 V

r0

x

u

Fig. 403. Example 1: z-plane

Fig. 404. Example 1: w-plane

where we have chosen b ϭ z 0 real without restriction. z 0 is of no immediate help here because centers of circles
do not map onto centers of the images, in general. However, we now have two free constants b and r0 and shall
succeed by imposing two reasonable conditions, namely, that 0 and 54 (Fig. 403) should be mapped onto r0 and
Ϫr0 (Fig. 404), respectively. This gives by (2)
r0 ϭ

0Ϫb
0Ϫ1

ϭ b,

Ϫr0 ϭ

and with this,

4
5

Ϫb

4b>5 Ϫ 1

ϭ

4
5

Ϫ r0

4r0>5 Ϫ 1

,

a quadratic equation in r0 with solutions r0 ϭ 2 (no good because r0 Ͻ 1) and r0 ϭ 12 . Hence our mapping
function (2) with b ϭ 12 becomes that in Example 5 of Sec. 17.3,
(3)

w ϭ f (z) ϭ

2z Ϫ 1

.

zϪ2

From Example 5 in Sec. 18.1, writing w for z we have as the complex potential in the w-plane the function
F*(w) ϭ a Ln w ϩ k and from this the real potential
£ * (u, v) ϭ Re F* (w) ϭ a ln ƒ w ƒ ϩ k .
This is our model. We now determine a and k from the boundary conditions. If ƒ w ƒ ϭ 1, then £* ϭ a ln 1 ϩ k ϭ 0,
hence k ϭ 0. If ƒ w ƒ ϭ r0 ϭ 12 , then £* ϭ a ln (12 ) ϭ 110, hence a ϭ 110>ln (12 ) ϭ Ϫ158.7. Substitution of (3)
now gives the desired solution in the given domain in the z-plane
F(z) ϭ F* ( f (z)) ϭ a Ln

2z Ϫ 1
zϪ2

.

The real potential is
£(x, y) ϭ Re F(z) ϭ a ln `

2z Ϫ 1
zϪ2

`,

a ϭ Ϫ158.7.

Can we “see” this result? Well, £(x, y) ϭ const if and only if ƒ (2z Ϫ 1)>(z Ϫ 2) ƒ ϭ const, that is, ƒ w ƒ ϭ const
by (2) with b ϭ 12 . These circles are images of circles in the z-plane because the inverse of a linear fractional
transformation is linear fractional (see (4), Sec. 17.2), and any such mapping maps circles onto circles (or straight
lines), by Theorem 1 in Sec. 17.2. Similarly for the rays arg w ϭ const. Hence the equipotential lines
£(x, y) ϭ const are circles, and the lines of force are circular arcs (dashed in Fig. 404). These two families of
᭿
curves intersect orthogonally, that is, at right angles, as shown in Fig. 404.

c18.qxd

11/2/10

6:55 PM

Page 765

SEC. 18.2 Use of Conformal Mapping. Modeling
EXAMPLE 2

765

Potential Between Two Semicircular Plates
Model the potential between two semicircular plates P1 and P2 in Fig. 405 having potentials Ϫ3000 V and
3000 V, respectively. Use Example 3 in Sec. 18.1 and conformal mapping.

Solution. Step 1. We map the unit disk in Fig. 405 onto the right half of the w-plane (Fig. 406) by using
the linear fractional transformation in Example 3, Sec. 17.3:
w ϭ f (z) ϭ

1ϩz
1Ϫz

.
v

y

3 kV
2

2 kV
1 kV

P2: 3 kV

1

0

0
–1

u
x

–2
–3

–1 kV
P1: –3 kV

–3 kV

–2 kV

Fig. 406. Example 2: w-plane

Fig. 405. Example 2: z-plane

The boundary ƒ z ƒ ϭ 1 is mapped onto the boundary u ϭ 0 (the v-axis), with z ϭ Ϫ1, i, 1 going onto w ϭ 0, i, ϱ,
respectively, and z ϭ Ϫi onto w ϭ Ϫi. Hence the upper semicircle of ƒ z ƒ ϭ 1 is mapped onto the upper half,
and the lower semicircle onto the lower half of the v-axis, so that the boundary conditions in the w-plane are
as indicated in Fig. 406.
Step 2. We determine the potential £* (u, v) in the right half-plane of the w-plane. Example 3 in Sec. 18.1 with
a ϭ p, U1 ϭ Ϫ3000, and U2 ϭ 3000 [with £* (u, v) instead of £(x, y)] yields
£*(u, v) ϭ

6000

p

␸ ϭ arctan

␸,

v
.
u

On the positive half of the imaginary axis (␸ ϭ p>2), this equals 3000 and on the negative half Ϫ3000, as it
should be. £* is the real part of the complex potential
F*(w) ϭ Ϫ

6000 i

p

Ln w.

Step 3. We substitute the mapping function into F* to get the complex potential F(z) in Fig. 405 in the form
F(z) ϭ F*( f (z)) ϭ Ϫ

6000 i

p

Ln

1ϩz
.
1Ϫz

The real part of this is the potential we wanted to determine:
£(x, y) ϭ Re F(z) ϭ

6000

1ϩz

6000

1ϩz

p Im Ln 1 Ϫ z ϭ p Arg 1 Ϫ z .

As in Example 1 we conclude that the equipotential lines £(x, y) ϭ const are circular arcs because they
correspond to Arg [(1 ϩ z)>(1 Ϫ z)] ϭ const, hence to Arg w ϭ const. Also, Arg w ϭ const are rays from 0
to ϱ, the images of z ϭ Ϫ1 and z ϭ 1, respectively. Hence the equipotential lines all have Ϫ1 and 1 (the
points where the boundary potential jumps) as their endpoints (Fig. 405). The lines of force are circular arcs,
too, and since they must be orthogonal to the equipotential lines, their centers can be obtained as intersections
of tangents to the unit circle with the x-axis, (Explain!)
᭿

Further examples can easily be constructed. Just take any mapping w ϭ f (z) in Chap. 17,
a domain D in the z-plane, its image D* in the w-plane, and a potential £* in D*. Then (1)
gives a potential in D. Make up some examples of your own, involving, for instance,
linear fractional transformations.

c18.qxd

11/2/10

766

6:55 PM

Page 766

CHAP. 18 Complex Analysis and Potential Theory

Basic Comment on Modeling
We formulated the examples in this section as models on the electrostatic potential. It is
quite important to realize that this is accidental. We could equally well have phrased
everything in terms of (time-independent) heat flow; then instead of voltages we would
have had temperatures, the equipotential lines would have become isotherms (ϭ lines of
constant temperature), and the lines of the electrical force would have become lines along
which heat flows from higher to lower temperatures (more on this in the next section).
Or we could have talked about fluid flow; then the electrostatic lines of force would have
become streamlines (more on this in Sec. 18.4). What we again see here is the unifying
power of mathematics: different phenomena and systems from different areas in physics
having the same types of model can be treated by the same mathematical methods. What
differs from area to area is just the kinds of problems that are of practical interest.

PROBLEM SET 18.2
1. Derivation of (3) from (2). Verify the steps.
2. Second proof. Give the details of the steps given on
p. A93 of the book. What is the point of that proof?
3–5

APPLICATION OF THEOREM 1

3. Find the potential £ in the region R in the first quadrant
of the z-plane bounded by the axes (having potential
U1) and the hyperbola y ϭ 1>x (having potential U2)
by mapping R onto a suitable infinite strip. Show that
£ is harmonic. What are its boundary values?
4. Let £* ϭ 4uv, w ϭ f (z) ϭ ez, and D: x Ͻ 0,
0 Ͻ y Ͻ p. Find £. What are its boundary values?
5. CAS PROJECT. Graphing Potential Fields.
Graph equipotential lines (a) in Example 1 of the text,
(b) if the complex potential is F(z) ϭ z 2, iz 2, ez.
(c) Graph the equipotential surfaces for F(z) ϭ Ln z as
cylinders in space.
6. Apply Theorem 1 to £* (u, v) ϭ u 2 Ϫ v2, w ϭ
f (z) ϭ ez, and any domain D, showing that the resulting
potential £ is harmonic.
7. Rectangle, sin z. Let D: 0 Ϲ x Ϲ 12 p, 0 Ϲ y Ϲ 1; D*
the image of D under w ϭ sin z; and £ * ϭ u 2 Ϫ v2.
What is the corresponding potential £ in D? What are
its boundary values? Sketch D and D*.
8. Conjugate potential. What happens in Prob. 7 if you
replace the potential by its conjugate harmonic?
9. Translation. What happens in Prob. 7 if we replace
sin z by cos z ϭ sin (z ϩ 12 p)?
10. Noncoaxial Cylinders. Find the potential between
the cylinders C1: ƒ z ƒ ϭ 1 (potential U1 ϭ 0) and
C2: ƒ z Ϫ c ƒ ϭ c (potential U2 ϭ 220 V), where
0 Ͻ c Ͻ 12 . Sketch or graph equipotential lines and
their orthogonal trajectories for c ϭ 14 . Can you guess
how the graph changes if you increase c (Ͻ 12 )?

11. On Example 2. Verify the calculations.
12. Show that in Example 2 the y-axis is mapped onto the
unit circle in the w-plane.
13. At z ϭ Ϯ1 in Fig. 405 the tangents to the equipotential
lines as shown make equal angles. Why?
14. Figure 405 gives the impression that the potential on
the y-axis changes more rapidly near 0 than near Ϯi.
Can you verify this?
15. Angular region. By applying a suitable conformal
mapping, obtain from Fig. 406 the potential £ in the
sector Ϫ14 p Ͻ Arg z Ͻ 14 p such that £ ϭ Ϫ3 kV if
Arg z ϭ Ϫ14 p and £ ϭ 3 kV if Arg z ϭ 14 p.
16. Solve Prob. 15 if the sector is Ϫ 18 p Ͻ Arg z Ͻ 18 p.
17. Another extension of Example 2. Find the linear
fractional transformation z ϭ g (Z ) that maps ƒ Z ƒ Ϲ 1
onto ƒ z ƒ Ϲ 1 with Z ϭ i>2 being mapped onto z ϭ 0.
Show that Z1 ϭ 0.6 ϩ 0.8i is mapped onto z ϭ Ϫ1
and Z2 ϭ Ϫ0.6 ϩ 0.8i onto z ϭ 1, so that the
equipotential lines of Example 2 look in ƒ Z ƒ Ϲ 1 as
shown in Fig. 407.
Y
Z2

–3 kV
Z1
0
1
X
2

3 kV

Fig. 407. Problem 17

c18.qxd

11/2/10

6:55 PM

Page 767

SEC. 18.3 Heat Problems

767

18. The equipotential lines in Prob. 17 are circles. Why?
19. Jump on the boundary. Find the complex and real
potentials in the upper half-plane with boundary values
5 kV if x Ͻ 2 and 0 if x Ͼ 2 on the x-axis.

18.3

20. Jumps. Do the same task as in Prob. 19 if the boundary
values on the x-axis are V0 when Ϫa Ͻ x Ͻ a and 0
elsewhere.

Heat Problems
Heat conduction in a body of homogeneous material is modeled by the heat equation
Tt ϭ c2ٌ2T
where the function T is temperature, Tt ϭ 0T>0t, t is time, and c2 is a positive constant
(specific to the material of the body; see Sec. 12.6).
Now if a heat flow problem is steady, that is, independent of time, we have Tt ϭ 0. If
it is also two-dimensional, then the heat equation reduces to
(1)

ٌ2T ϭ Txx ϩ Tyy ϭ 0,

which is the two-dimensional Laplace equation. Thus we have shown that we can model
a two-dimensional steady heat flow problem by Laplace’s equation.
Furthermore we can treat this heat flow problem by methods of complex analysis, since
T (or T (x, y)) is the real part of the complex heat potential
F(z) ϭ T (x, y) ϩ i°(x, y).
We call T (x, y) the heat potential. The curves T (x, y) ϭ const are called isotherms, which
means lines of constant temperature. The curves ° (x, y) ϭ const are called heat flow
lines because heat flows along them from higher temperatures to lower temperatures.
It follows that all the examples considered so far (Secs. 18.1, 18.2) can now be
reinterpreted as problems on heat flow. The electrostatic equipotential lines £(x, y) ϭ const
now become isotherms T (x, y) ϭ const, and the lines of electrical force become lines of
heat flow, as in the following two problems.
EXAMPLE 1

Temperature Between Parallel Plates
Find the temperature between two parallel plates x ϭ 0 and x ϭ d in Fig. 408 having temperatures 0 and 100°C,
respectively.

Solution. As in Example 1 of Sec. 18.1 we conclude that T (x, y) ϭ ax ϩ b. From the boundary conditions,
b ϭ 0 and a ϭ 100>d. The answer is

T (x, y) ϭ

100
d

x [°C].

The corresponding complex potential is F(z) ϭ (100>d)z. Heat flows horizontally in the negative x-direction
along the lines y ϭ const.
᭿

EXAMPLE 2

Temperature Distribution Between a Wire and a Cylinder
Find the temperature field around a long thin wire of radius r1 ϭ 1 mm that is electrically heated to T1 ϭ 500°F
and is surrounded by a circular cylinder of radius r2 ϭ 100 mm, which is kept at temperature T2 ϭ 60°F by
cooling it with air. See Fig. 409. (The wire is at the origin of the coordinate system.)

11/2/10

6:55 PM

768

Page 768

CHAP. 18 Complex Analysis and Potential Theory

Solution. T depends only on r, for reasons of symmetry. Hence, as in Sec. 18.1 (Example 2),
T (x, y) ϭ a ln r ϩ b.
The boundary conditions are
T1 ϭ 500 ϭ a ln 1 ϩ b,

T2 ϭ 60 ϭ a ln 100 ϩ b.

Hence b ϭ 500 (since ln 1 ϭ 0) and a ϭ (60 Ϫ b)>ln 100 ϭ Ϫ95.54. The answer is
T (x, y) ϭ 500 Ϫ 95.54 ln r [°F].
The isotherms are concentric circles. Heat flows from the wire radially outward to the cylinder. Sketch T as a
function of r. Does it look physically reasonable?
᭿
y
y

y

Insulated

x

x

T = 20°C

T = 100°C

T = 60°F

T = 0°C

c18.qxd

T = 50°C

x

1

0

Fig. 409. Example 2

Fig. 408. Example 1

Fig. 410. Example 3

Mathematically the calculations remain the same in the transition to another field of
application. Physically, new problems may arise, with boundary conditions that would
make no sense physically or would be of no practical interest. This is illustrated by the
next two examples.
EXAMPLE 3

A Mixed Boundary Value Problem
Find the temperature distribution in the region in Fig. 410 (cross section of a solid quarter-cylinder), whose
vertical portion of the boundary is at 20°C, the horizontal portion at 50°C, and the circular portion is insulated.

Solution. The insulated portion of the boundary must be a heat flow line, since, by the insulation, heat is
prevented from crossing such a curve, hence heat must flow along the curve. Thus the isotherms must meet
such a curve at right angles. Since T is constant along an isotherm, this means that
(2)

0T
ϭ0
0n

along an insulated portion of the boundary.

Here 0T>0n is the normal derivative of T, that is, the directional derivative (Sec. 9.7) in the direction normal
(perpendicular) to the insulated boundary. Such a problem in which T is prescribed on one portion of the boundary
and 0T>0n on the other portion is called a mixed boundary value problem.
In our case, the normal direction to the insulated circular boundary curve is the radial direction toward the
origin. Hence (2) becomes 0T> 0r ϭ 0, meaning that along this curve the solution must not depend on r. Now
Arg z ϭ u satisfies (1), as well as this condition, and is constant (0 and p>2) on the straight portions of the
boundary. Hence the solution is of the form
T (x, y) ϭ au ϩ b.
The boundary conditions yield a # p>2 ϩ b ϭ 20 and a # 0 ϩ b ϭ 50. This gives
T (x, y) ϭ 50 Ϫ

60

p

u,

u ϭ arctan

y
x

.

11/2/10

6:55 PM

Page 769

SEC. 18.3 Heat Problems

769

The isotherms are portions of rays u ϭ const. Heat flows from the x-axis along circles r ϭ const (dashed in
Fig. 410) to the y-axis.
᭿
y

T* = 20°C

v

T* = 0°C

c18.qxd

T = 0°C

–1

Insulated

1

T = 20°C

x

π
– __
2

_π_
2

u

Fig. 412. Example 4: w-plane

Fig. 411. Example 4: z-plane

EXAMPLE 4

Insulated

Another Mixed Boundary Value Problem in Heat Conduction
Find the temperature field in the upper half-plane when the x-axis is kept at T ϭ 0°C for x Ͻ Ϫ1, is insulated
for Ϫ1 Ͻ x Ͻ 1, and is kept at T ϭ 20°C for x Ͼ 1 (Fig. 411).

Solution. We map the half-plane in Fig. 411 onto the vertical strip in Fig. 412, find the temperature T * (u, v)
there, and map it back to get the temperature T (x, y) in the half-plane.
The idea of using that strip is suggested by Fig. 391 in Sec. 17.4 with the roles of z ϭ x ϩ iy and w ϭ u ϩ iv
interchanged. The figure shows that z ϭ sin w maps our present strip onto our half-plane in Fig. 411. Hence the
inverse function
w ϭ f (z) ϭ arcsin z
maps that half-plane onto the strip in the w-plane. This is the mapping function that we need according to
Theorem 1 in Sec. 18.2.
The insulated segment Ϫ1 Ͻ x Ͻ 1 on the x-axis maps onto the segment Ϫp>2 Ͻ u Ͻ p>2 on the u-axis.
The rest of the x-axis maps onto the two vertical boundary portions u ϭ Ϫp>2 and p>2, v Ͼ 0, of the strip.
This gives the transformed boundary conditions in Fig. 412 for T * (u, v), where on the insulated horizontal
boundary, 0T *>0n ϭ 0T *> 0v ϭ 0 because v is a coordinate normal to that segment.
Similarly to Example 1 we obtain
T *(u, v) ϭ 10 ϩ

20

p

u

which satisfies all the boundary conditions. This is the real part of the complex potential F *(w) ϭ 10 ϩ (20> p) w.
Hence the complex potential in the z-plane is
F(z) ϭ F * ( f (z)) ϭ 10 ϩ

20

p

arcsin z

and T (x, y) ϭ Re F(z) is the solution. The isotherms are u ϭ const in the strip and the hyperbolas in the z-plane,
perpendicular to which heat flows along the dashed ellipses from the 20°-portion to the cooler 0°-portion of the
boundary, a physically very reasonable result.
᭿

Sections 18.3 and 18.5 show some of the usefulness of conformal mappings and complex
potentials. Furthermore, complex potential models fluid flow in Sec. 18.4.

PROBLEM SET 18.3
1. Parallel plates. Find the temperature between the
plates y ϭ 0 and y ϭ d kept at 20 and 100°C, respectively. (i) Proceed directly. (ii) Use Example 1 and a
suitable mapping.

2. Infinite plate. Find the temperature and the complex
potential in an infinite plate with edges y ϭ x Ϫ 4 and
y ϭ x ϩ 4 kept at Ϫ20 and 40°C, respectively (Fig. 413).
In what case will this be an approximate model?

11/2/10

6:55 PM

Page 770

770

CHAP. 18 Complex Analysis and Potential Theory
y

10.

40
°C

y

T

=

a

–4

x

4

b

T = T3

T = T2

x

T = T1

y

T

=

20
°C

11.

–1
T =0

Fig. 413. Problem 2: Infinite plate

T = 100°C
x

T = 0°C

Hint. Apply w ϭ cosh z to Prob. 11.
14.

100°C

1

°C

1
T = 100°C T = 0°C

x

Insulated

T = 0°C

x

T

T

=

y
T = 200°C

y
0°C

13.

20

T = 200°C

T = 0°C

π

0

Find the temperature distribution T (x, y) and the complex
potential F(z) in the given thin metal plate whose faces
are insulated and whose edges are kept at the indicated
temperatures or are insulated as shown.
4. y
5.

x

T =0

y

TEMPERATURE T (x, y) IN PLATES

4–18

1
T = 100°C

12.
3. CAS PROJECT. Isotherms. Graph isotherms and
lines of heat flow in Examples 2–4. Can you see from
the graphs where the heat flow is very rapid?

=

45°

15.

C

0°

y = 10/x

Insulated
60

45°

C

0°

–2

T

=

=

T

x

T = 200°C

°C

45°

x

T = –20°C

16.

y

–20

x

T = 0°C

8.

a

T* = T1

x

y

a

x

17. First quadrant of the z-plane with y-axis kept at 100°C,
the segment 0 Ͻ x Ͻ 1 of the x-axis insulated and the
x-axis for x Ͼ 1 kept at 200°C. Hint. Use Example 4.
18. Figure 410, T (0, y) ϭ Ϫ30°C, T (x, 0) ϭ 100°C
19. Interpretation. Formulate Prob. 11 in terms of electrostatics.

b
T* = T1

Insulated

x
T = 20°C

T* = T2

T=0

y = √3 x
T = T1

v

9.

50
0°C

T = T2

50
0

60°

T=

7.
°C

6.
T=

c18.qxd

T =0

x

20. Interpretation. Interpret Prob. 17 in Sec. 18.2 as a heat
problem, with boundary temperatures, say, 10°C on the
upper part and 200°C on the lower.

c18.qxd

11/2/10

6:55 PM

Page 771

SEC. 18.4 Fluid Flow

18.4

771

Fluid Flow
Laplace’s equation also plays a basic role in hydrodynamics, in steady nonviscous fluid
flow under physical conditions discussed later in this section. For methods of complex
analysis to be applicable, our problems will be two-dimensional, so that the velocity vector
V by which the motion of the fluid can be given depends only on two space variables x
and y, and the motion is the same in all planes parallel to the xy-plane.
Then we can use for the velocity vector V a complex function
V ϭ V1 ϩ iV2

(1)

giving the magnitude ƒ V ƒ and direction Arg V of the velocity at each point z ϭ x ϩ iy.
Here V1 and V2 are the components of the velocity in the x and y directions. V is tangential
to the path of the moving particles, called a streamline of the motion (Fig. 414).
We show that under suitable assumptions (explained in detail following the examples),
for a given flow there exists an analytic function
F(z) ϭ £(x, y) ϩ i°(x, y),

(2)

called the complex potential of the flow, such that the streamlines are given by
°(x, y) ϭ const, and the velocity vector or, briefly, the velocity is given by
V ϭ V1 ϩ iV2 ϭ F r(z)

(3)

y
V

V2

V1
Streamline
x

Fig. 414. Velocity

where the bar denotes the complex conjugate. ° is called the stream function. The
function £ is called the velocity potential. The curves £(x, y) ϭ const are called
equipotential lines. The velocity vector V is the gradient of £; by definition, this
means that
(4)

V1 ϭ

0£
,
0x

V2 ϭ

0£
.
0y

Indeed, for F ϭ £ ϩ i°, Eq. (4) in Sec. 13.4 is F r ϭ £ x ϩ i° x with ° x ϭ Ϫ£ y by
the second Cauchy–Riemann equation. Together we obtain (3):
F r(z) ϭ £ x Ϫ i° x ϭ £ x ϩ i£ y ϭ V1 ϩ iV2 ϭ V.

c18.qxd

11/2/10

6:55 PM

772

Page 772

CHAP. 18 Complex Analysis and Potential Theory

Furthermore, since F(z) is analytic, £ and ° satisfy Laplace’s equation
(5)

ٌ2 £ ϭ

02£
0x 2

02£

ϩ

0y 2

ϭ 0,

ٌ2 ° ϭ

02°
0x 2

ϩ

02°
0y 2

ϭ 0.

Whereas in electrostatics the boundaries (conducting plates) are equipotential lines, in
fluid flow the boundaries across which fluid cannot flow must be streamlines. Hence in
fluid flow the stream function is of particular importance.
Before discussing the conditions for the validity of the statements involving (2)–(5), let
us consider two flows of practical interest, so that we first see what is going on from a
practical point of view. Further flows are included in the problem set.
EXAMPLE 1

Flow Around a Corner
The complex potential F(z) ϭ z 2 ϭ x 2 Ϫ y 2 ϩ 2ixy models a flow with
Equipotential lines

£ ϭ x 2 Ϫ y 2 ϭ const

(Hyperbolas)

Streamlines

° ϭ 2xy ϭ const

(Hyperbolas).

From (3) we obtain the velocity vector
V ϭ 2z ϭ 2 (x Ϫ iy),

that is,

V1 ϭ 2x,

V2 ϭ Ϫ2y.

The speed (magnitude of the velocity) is
ƒV ƒ ϭ 2V 21 ϩ V 22 ϭ 22x 2 ϩ y 2.
The flow may be interpreted as the flow in a channel bounded by the positive coordinates axes and a hyperbola,
say, xy ϭ 1 (Fig. 415). We note that the speed along a streamline S has a minimum at the point P where the
cross section of the channel is large.
᭿
y
S

P
x

0

Fig. 415. Flow around a corner (Example 1)

EXAMPLE 2

Flow Around a Cylinder
Consider the complex potential
F(z) ϭ £(x, y) ϩ i°(x, y) ϭ z ϩ

1
.
z

Using the polar form z ϭ reiu, we obtain
F(z) ϭ reiu ϩ

1 ؊iu
1
1
e
ϭ ar ϩ b cos u ϩ i ar Ϫ b sin u.
r
r
r

Hence the streamlines are
°(x, y) ϭ ar Ϫ

1
b sin u ϭ const.
r

c18.qxd

11/2/10

6:55 PM

Page 773

SEC. 18.4 Fluid Flow

773
In particular, °(x, y) ϭ 0 gives r Ϫ 1>r ϭ 0 or sin u ϭ 0. Hence this streamline consists of the unit circle
(r ϭ 1>r gives r ϭ 1) and the x-axis (u ϭ 0 and u ϭ p). For large ƒ z ƒ the term 1>z in F(z) is small in absolute
value, so that for these z the flow is nearly uniform and parallel to the x-axis. Hence we can interpret this as a
flow around a long circular cylinder of unit radius that is perpendicular to the z-plane and intersects it in the
unit circle ƒ z ƒ ϭ 1 and whose axis corresponds to z ϭ 0.
The flow has two stagnation points (that is, points at which the velocity V is zero), at z ϭ Ϯ1. This follows
from (3) and
F r(z) ϭ 1 Ϫ

1
z2

,

z 2 Ϫ 1 ϭ 0.

hence

(See Fig. 416.)

᭿

y

x

Fig. 416. Flow around a cylinder (Example 2)

Assumptions and Theory Underlying (2)–(5)
THEOREM 1

Complex Potential of a Flow

If the domain of flow is simply connected and the flow is irrotational and
incompressible, then the statements involving (2)–(5) hold. In particular, then the
flow has a complex potential F(z), which is an analytic function. (Explanation of
terms below.)
PROOF

We prove this theorem, along with a discussion of basic concepts related to fluid flow.
(a) First Assumption: Irrotational. Let C be any smooth curve in the z-plane given
by z(s) ϭ x(s) ϩ iy(s), where s is the arc length of C. Let the real variable Vt be the
component of the velocity V tangent to C (Fig. 417). Then the value of the real line
integral

Ύ V ds

(6)

t

C

y
Vt
α
C
V
x

Fig. 417. Tangential component of the
velocity with respect to a curve C

c18.qxd

11/2/10

774

6:55 PM

Page 774

CHAP. 18 Complex Analysis and Potential Theory

taken along C in the sense of increasing s is called the circulation of the fluid along C,
a name that will be motivated as we proceed in this proof. Dividing the circulation by the
length of C, we obtain the mean velocity1 of the flow along the curve C. Now
Vt ϭ ƒ V ƒ cos a

(Fig. 417).

Hence Vt is the dot product (Sec. 9.2) of V and the tangent vector dz>ds of C (Sec. 17.1);
thus in (6),
Vt ds ϭ aV1

dx
dy
ϩ V2 b ds ϭ V1 dx ϩ V2 dy.
ds
ds

The circulation (6) along C now becomes

Ύ V ds ϭ Ύ (V dx ϩ V dy).

(7)

t

1

C

2

C

As the next idea, let C be a closed curve satisfying the assumption as in Green’s theorem
(Sec. 10.4), and let C be the boundary of a simply connected domain D. Suppose further
that V has continuous partial derivatives in a domain containing D and C. Then we can
use Green’s theorem to represent the circulation around C by a double integral,

Ώ (V dx ϩ V dy) ϭ Ύ Ύ a 0x Ϫ
0V2

(8)

1

2

C

0V1
0y

b dx dy.

D

The integrand of this double integral is called the vorticity of the flow. The vorticity
divided by 2 is called the rotation
v(x, y) ϭ

(9)

0V1
1 0V2
Ϫ
b.
a
2 0x
0y

We assume the flow to be irrotational, that is, v(x, y) ϵ 0 throughout the flow; thus,
0V2

(10)

0x

Ϫ

0V1
0y

ϭ 0.

To understand the physical meaning of vorticity and rotation, take for C in (8) a circle.
Let r be the radius of C. Then the circulation divided by the length 2pr of C is the mean

1

Definitions:

b

f (x) dx ϭ mean value of f on the interval a Ϲ x Ϲ b,
bϪa Ύ
1

a

1
L
1
A

Ύ f (s) ds ϭ mean value of f on C

(L ϭ length of C ),

C

Ύ Ύ f (x, y) dx dy ϭ mean value of f on D
D

(A ϭ area of D).

c18.qxd

11/2/10

6:55 PM

Page 775

SEC. 18.4 Fluid Flow

775

velocity of the fluid along C. Hence by dividing this by r we obtain the mean angular
velocity v0 of the fluid about the center of the circle:
v0 ϭ

1
2pr 2

Ύ Ύ a 0V0x Ϫ 0V0y b dx dy ϭ p1r Ύ Ύ v(x, y) dx dy.
2

1

2

D

D

If we now let r : 0, the limit of v0 is the value of v at the center of C. Hence v(x, y) is
the limiting angular velocity of a circular element of the fluid as the circle shrinks to the
point (x, y). Roughly speaking, if a spherical element of the fluid were suddenly solidified
and the surrounding fluid simultaneously annihilated, the element would rotate with the
angular velocity v.
(b) Second Assumption: Incompressible. Our second assumption is that the fluid is
incompressible. (Fluids include liquids, which are incompressible, and gases, such as air,
which are compressible.) Then

(11)

0V1

ϩ

0x

0V2
0y

ϭ0

in every region that is free of sources or sinks, that is, points at which fluid is produced
or disappears, respectively. The expression in (11) is called the divergence of V and is
denoted by div V. (See also (7) in Sec. 9.8.)
(c) Complex Velocity Potential. If the domain D of the flow is simply connected
(Sec. 14.2) and the flow is irrotational, then (10) implies that the line integral (7) is
independent of path in D (by Theorem 3 in Sec. 10.2, where F1 ϭ V1, F2 ϭ V2, F3 ϭ 0,
and z is the third coordinate in space and has nothing to do with our present z). Hence if
we integrate from a fixed point (a, b) in D to a variable point (x, y) in D, the integral
becomes a function of the point (x, y), say, £(x, y):

(12)

£(x, y) ϭ

Ύ

(x,y)

(V1 dx ϩ V2 dy).

(a,b)

We claim that the flow has a velocity potential £, which is given by (12). To prove
this, all we have to do is to show that (4) holds. Now since the integral (7) is
independent of path, V1 dx ϩ V2 dy is exact (Sec. 10.2), namely, the differential of £,
that is,
V1 dx ϩ V2 dy ϭ

0£
0£
dx ϩ
dy.
0x
0y

From this we see that V1 ϭ 0£>0x and V2 ϭ 0£>0y, which gives (4).
That £ is harmonic follows at once by substituting (4) into (11), which gives the first
Laplace equation in (5).
We finally take a harmonic conjugate ° of £. Then the other equation in (5) holds.
Also, since the second partial derivatives of £ and ° are continuous, we see that the
complex function
F(z) ϭ £(x, y) ϩ i°(x, y)

c18.qxd

11/2/10

776

6:55 PM

Page 776

CHAP. 18 Complex Analysis and Potential Theory

is analytic in D. Since the curves °(x, y) ϭ const are perpendicular to the equipotential
curves £(x, y) ϭ const (except where F r(z) ϭ 0), we conclude that °(x, y) ϭ const are
the streamlines. Hence ° is the stream function and F(z) is the complex potential of the
flow. This completes the proof of Theorem 1 as well as our discussion of the important
role of complex analysis in compressible fluid flow.
᭿

PROBLEM SET 18.4
1. Differentiability. Under what condition on the velocity
vector V in (1) will F(z) in (2) be analytic?
2. Corner flow. Along what curves will the speed in
Example 1 be constant? Is this obvious from Fig. 415?
3. Cylinder. Guess from physics and from Fig. 416 where
on the y-axis the speed is maximum. Then calculate.
4. Cylinder. Calculate the speed along the cylinder wall
in Fig. 416, also confirming the answer to Prob. 3.
5. Irrotational flow. Show that the flow in Example 2 is
irrotational.
6. Extension of Example 1. Sketch or graph and
interpret the flow in Example 1 on the whole upper
half-plane.
7. Parallel flow. Sketch and interpret the flow with
complex potential F(z) ϭ z.
8. Parallel flow. What is the complex potential of an
upward parallel flow of speed K Ͼ 0 in the direction
of y ϭ x? Sketch the flow.
9. Corner. What F(z) would be suitable in Example 1 if
the angle of the corner were p>4 instead of p>2?
10. Corner. Show that F(z) ϭ iz 2 also models a flow
around a corner. Sketch streamlines and equipotential
lines. Find V.
11. What flow do you obtain from F(z) ϭ ϪiKz, K positive
real?
12. Conformal mapping. Obtain the flow in Example 1
from that in Prob. 11 by a suitable conformal mapping.
13. 60؇- Sector. What F(z) would be suitable in Example 1
if the angle at the corner were p>3?
14. Sketch or graph streamlines and equipotential lines
of F(z) ϭ iz 3. Find V. Find all points at which V is
horizontal.
15. Change F(z) in Example 2 slightly to obtain a flow
around a cylinder of radius r0 that gives the flow in
Example 2 if r0 : 1.
16. Cylinder. What happens in Example 2 if you replace
z by z 2? Sketch and interpret the resulting flow in the
first quadrant.
17. Elliptic cylinder. Show that F(z) ϭ arccos z gives
confocal ellipses as streamlines, with foci at z ϭ Ϯ1,

and that the flow circulates around an elliptic cylinder
or a plate (the segment from Ϫ1 to 1 in Fig. 418).

–1

1

Fig. 418. Flow around a plate in Prob. 17.
18. Aperture. Show that F(z) ϭ arccosh z gives confocal
hyperbolas as streamlines, with foci at z ϭ Ϯ1, and the
flow may be interpreted as a flow through an aperture
(Fig. 419).

–1

1

Fig. 419. Flow through an aperture in Prob. 18.
19. Potential F(z) ϭ 1>z. Show that the streamlines of
F(z) ϭ 1>z and circles through the origin with centers
on the y-axis.
20. TEAM PROJECT. Role of the Natural Logarithm
in Modeling Flows. (a) Basic flows: Source and sink.
Show that F(z) ϭ (c>2p) ln z with constant positive
real c gives a flow directed radially outward (Fig. 420),
so that F models a point source at z ϭ 0 (that is, a
source line x ϭ 0, y ϭ 0 in space) at which fluid is
produced. c is called the strength or discharge of the
source. If c is negative real, show that the flow is
directed radially inward, so that F models a sink at
z ϭ 0, a point at which fluid disappears. Note that
z ϭ 0 is the singular point of F(z).

c18.qxd

11/2/10

6:55 PM

Page 777

SEC. 18.5 Poisson’s Integral Formula for Potentials

777

y

zϭ

x

ϪK 2
iK
Ϯ
ϩ 1;
B 16p2
4p

if K ϭ 0 they are at Ϯ1; as K increases they move up
on the unit circle until they unite at z ϭ i (K ϭ 4p, see
Fig. 422), and if K Ͼ 4p they lie on the imaginary axis
(one lies in the field of flow and the other one lies inside
the cylinder and has no physical meaning).

Fig. 420. Point source
(b) Basic flows: Vortex. Show that F(z) ϭ Ϫ(Ki>2p)
ln z with positive real K gives a flow circulating counterclockwise around z ϭ 0 (Fig. 421). z ϭ 0 is called a
vortex. Note that each time we travel around the vortex,
the potential increases by K.
(c) Addition of flows. Show that addition of the
velocity vectors of two flows gives a flow whose
complex potential is obtained by adding the complex
potentials of those flows.

K=0

K = 4π

y

x

K = 2.8π

Fig. 421. Vortex flow
(d) Source and sink combined. Find the complex
potentials of a flow with a source of strength 1 at z ϭ Ϫa
and of a flow with a sink of strength 1 at z ϭ a. Add
both and sketch or graph the streamlines. Show that for
small ƒ a ƒ these lines look similar to those in Prob. 19.
(e) Flow with circulation around a cylinder. Add
the potential in (b) to that in Example 2. Show that this
gives a flow for which the cylinder wall ƒ z ƒ ϭ 1 is a
streamline. Find the speed and show that the stagnation
points are

18.5

K = 6π

Fig. 422. Flow around a cylinder without circulation
(K ‫ ؍‬0) and with circulation

Poisson’s Integral Formula for Potentials
So far in this chapter we have seen powerful methods based on conformal mappings and
complex potentials. They were used for modeling and solving two-dimensional potential
problems and demonstrated the importance of complex analysis.
Now we introduce a further method that results from complex integration. It will yield
the very important Poisson integral formula (5) for potentials in a standard domain

c18.qxd

11/2/10

778

6:55 PM

Page 778

CHAP. 18 Complex Analysis and Potential Theory

(a circular disk). In addition, from (5), we will derive a useful series (7) for these potentials.
This allows us to solve problems for disks and then map solutions conformally onto other
domains.

Derivation of Poisson’s Integral Formula
Poisson’s formula will follow from Cauchy’s integral formula (Sec. 14.3)
F(z) ϭ

(1)

Ώ

1
2pi

C

F(z*)
dz*.
z* Ϫ z

Here C is the circle z* ϭ Re (counterclockwise, 0 Ϲ a Ϲ 2p), and we assume that F(z*)
is analytic in a domain containing C and its full interior. Since dz* ϭ iReia da ϭ iz* da,
we obtain from (1)
ia

F(z) ϭ

(2)

1
2p

Ύ

2p

F(z*)

0

z*
da
z* Ϫ z

(z* ϭ Reia, z ϭ reiu).

Now comes a little trick. If instead of z inside C we take a Z outside C, the integrals (1)
and (2) are zero by Cauchy’s integral theorem (Sec. 14.2). We choose Z ϭ z*z*>z ϭ R2>z,
which is outside C because ƒ Z ƒ ϭ R2> ƒ z ƒ ϭ R2>r Ͼ R. From (2) we thus have
0ϭ

1
2p

Ύ

2p

F(z*)

0

z*
1
da ϭ
z* Ϫ Z
2p

Ύ

2p

F(z*)

0

z*
da
z*z*
z* Ϫ
z

and by straightforward simplification of the last expression on the right,
0ϭ

1
2p

Ύ

2p

F(z*)

0

z
da.
z Ϫ z*

We subtract this from (2) and use the following formula that you can verify by direct
calculation (zz* cancels):
z*
z
z*z* Ϫ zz
.
Ϫ
ϭ
z* Ϫ z
z Ϫ z*
(z* Ϫ z)(z* Ϫ z)

(3)
We then have
(4)

F(z) ϭ

1
2p

Ύ

2p

F(z*)

0

z*z* Ϫ zz
da.
(z* Ϫ z)( z* Ϫ z )

From the polar representations of z and z* we see that the quotient in the integrand is real
and equal to
R2 Ϫ r 2
(Reia Ϫ reiu)(Re؊ia Ϫ re؊iu)

ϭ

R2 Ϫ r 2
R2 Ϫ 2Rr cos (u Ϫ a) ϩ r 2

.

c18.qxd

11/2/10

6:55 PM

Page 779

SEC. 18.5 Poisson’s Integral Formula for Potentials

779

We now write F(z) ϭ £(r, u) ϩ i°(r, u) and take the real part on both sides of (4). Then
we obtain Poisson’s integral formula2

(5)

1
£(r, u) ϭ
2p

Ύ

2p

£(R, a)

0

R2 Ϫ r 2
da.
R Ϫ 2Rr cos (u Ϫ a) ϩ r 2
2

This formula represents the harmonic function £ in the disk ƒ z ƒ Ϲ R in terms of its values
£(R, a) on the boundary (the circle) ƒ z ƒ ϭ R.
Formula (5) is still valid if the boundary function £(R, a) is merely piecewise
continuous (as is practically often the case; see Figs. 405 and 406 in Sec. 18.2 for an
example). Then (5) gives a function harmonic in the open disk, and on the circle ƒ z ƒ ϭ R
equal to the given boundary function, except at points where the latter is discontinuous.
A proof can be found in Ref. [D1] in App. 1.

Series for Potentials in Disks
From (5) we may obtain an important series development of £ in terms of simple harmonic
functions. We remember that the quotient in the integrand of (5) was derived from (3).
We claim that the right side of (3) is the real part of
(z* ϩ z)(z* Ϫ z)
z*z* Ϫ zz Ϫ z*z ϩ zz*
z* ϩ z
ϭ
ϭ
.
z* Ϫ z
(z* Ϫ z)(z* Ϫ z)
ƒ z* Ϫ z ƒ 2
Indeed, the last denominator is real and so is z*z* Ϫ zz in the numerator, whereas
Ϫz*z ϩ zz* ϭ 2i Im (zz*) in the numerator is pure imaginary. This verifies our claim.
Now by the use of the geometric series we obtain (develop the denominator)
(6)

ؕ
ؕ
1 ϩ (z>z*)
z
z* ϩ z
z n
z n
ϭ
ϭ a1 ϩ b a a b ϭ 1 ϩ 2 a a b .
z* Ϫ z
1 Ϫ (z>z*)
z* nϭ0 z*
z*
nϭ1

Since z ϭ reiu and z* ϭ Reia, we have
Re c a

z n
rn
r n
b d ϭ Re c n einue؊ina d ϭ a b cos (nu Ϫ na).
z*
R
R

On the right, cos (nu Ϫ na) ϭ cos nu cos na ϩ sin nu sin na. Hence from (6) we obtain
Re
(6*)

2

ؕ
z* ϩ z
z n
ϭ 1 ϩ 2 a Re a b
z* Ϫ z
z*
nϭ1
ؕ
r n
ϭ 1 ϩ 2 a a b (cos nu cos na ϩ sin nu sin na).
R
nϭ1

SIMÉON DENIS POISSON (1781–1840), French mathematician and physicist, professor in Paris from 1809.
His work includes potential theory, partial differential equations (Poisson equation, Sec. 12.1), and probability
(Sec. 24.7).

11/2/10

6:55 PM

780

Page 780

CHAP. 18 Complex Analysis and Potential Theory

This expression is equal to the quotient in (5), as we have mentioned before, and
by inserting it into (5) and integrating term by term with respect to a from 0 to 2p
we obtain
ؕ
r n
£(r, u) ϭ a0 ϩ a a b (an cos nu ϩ bn sin nu)
R
nϭ1

(7)

where the coefficients are [the 2 in (6*) cancels the 2 in 1>(2p) in (5)]
a0 ϭ

Ύ

1
2p

2p

an ϭ

£(R, a) da,

0

Ύ

1

p

2p

£(R, a) cos na da,

0

n ϭ 1, 2, Á ,

(8)
1
bn ϭ p

Ύ

2p

£(R, a) sin na da,

0

the Fourier coefficients of £(R, a); see Sec. 11.1. Now, for r ϭ R, the series (7) becomes
the Fourier series of £(R, a). Hence the representation (7) will be valid whenever the
given £(R, a) on the boundary can be represented by a Fourier series.
Dirichlet Problem for the Unit Disk
Find the electrostatic potential £(r, u) in the unit disk r Ͻ 1 having the boundary values
Ϫa> p
£(1, a) ϭ b
a> p

if Ϫp Ͻ a Ͻ 0

(Fig. 423).

0ϽaϽp

if

Solution. Since £(1, a) is even, bn ϭ 0, and from (8) we obtain a0 ϭ 12 and
an ϭ

1

cϪ

p

Ύ

0

a

p
؊p

cos na da ϩ

Ύ

p

0

a

p

cos na da d ϭ

2
(cos np Ϫ 1).
n 2p2

Hence, an ϭ Ϫ4>(n p ) if n is odd, an ϭ 0 if n ϭ 2, 4, Á , and the potential is
2

2

£(r, u) ϭ

1
2

Ϫ

4
2

p

c r cos u ϩ

r3
3

2

cos 3u ϩ

r5
52

cos 5u ϩ Á d .

᭿

Figure 424 shows the unit disk and some of the equipotential lines (curves £ ϭ const).

y

8

0.

9

2

0.

7

=

0.

0.6
0.

Φ

0.4

EXAMPLE 1

0.
3

c18.qxd

1

0.

Φ(1, α)

x

1

–π

0

π

α

Fig. 423. Boundary values in Example 1

Fig. 424. Potential in Example 1

c18.qxd

11/19/10

8:22 PM

Page 781

SEC. 18.6 General Properties of Harmonic Functions

781

PROBLEM SET 18.5
1. Give the details of the derivation of the series (7) from
the Poisson formula (5).
2. Verify (3).
3. Show that each term of (7) is a harmonic function in
the disk r Ͻ R.
4. Why does the series in Example 1 reduce to a cosine
series?
5–18

HARMONIC FUNCTIONS IN A DISK

Using (7), find the potential £(r, u) in the unit disk r Ͻ 1
having the given boundary values £(1, u). Using the sum
of the first few terms of the series, compute some values
of £ and sketch a figure of the equipotential lines.
5. £(1, u) ϭ 32 sin 3u
6. £(1, u) ϭ 5 Ϫ cos 2u
7. £(1, u) ϭ a cos 4u
2

8. £(1, u) ϭ 4 sin3 u
9. £(1, u) ϭ 8 sin4 u
10. £(1, u) ϭ 16 cos3 2u
11. £(1, u) ϭ u> p if Ϫp Ͻ u Ͻ p
12. £(1, u) ϭ k if 0 Ͻ u Ͻ p and 0 otherwise

16. £(1, u) ϭ b

uϩp

if Ϫp Ͻ u Ͻ 0

uϪp

if

0ϽuϽp

17. £(1, u) ϭ u2> p2 if Ϫp Ͻ u Ͻ p
18. £(1, u) ϭ b

0

if Ϫp Ͻ u Ͻ 0

u

if

0ϽuϽp

19. CAS EXPERIMENT. Series (7). Write a program for
series developments (7). Experiment on accuracy by
computing values from partial sums and comparing them
with values that you obtain from your CAS graph. Do
this (a) for Example 1 and Fig. 424, (b) for £ in Prob. 11
(which is discontinuous on the boundary!), (c) for a £
of your choice with continuous boundary values, and
(d) for £ with discontinuous boundary values.
20. TEAM PROJECT. Potential in a Disk. (a) Mean
value property. Show that the value of a harmonic
function £ at the center of a circle C equals the mean
of the value of £ on C (see Sec. 18.4, footnote 1, for
definitions of mean values).
(b) Separation of variables. Show that the terms of
(7) appear as solutions in separating the Laplace
equation in polar coordinates.

13. £(1, u) ϭ u if Ϫ12 p Ͻ u Ͻ 12 p and 0 otherwise

(c) Harmonic conjugate. Find a series for a harmonic
conjugate ° of £ from (7). Hint. Use the Cauchy–
Riemann equations.

15. £(1, u) ϭ 1 if Ϫ 12 p Ͻ u Ͻ 12 p and 0 otherwise

(d) Power series. Find a series for F(z) ϭ £ ϩ i°.

14. £(1, u) ϭ ƒ u ƒ > p if Ϫp Ͻ u Ͻ p

18.6

General Properties of Harmonic Functions.
Uniqueness Theorem for the Dirichlet Problem
Recall from Sec. 10.8 that harmonic functions are solutions to Laplace’s equation and
their second-order partial derivatives are continuous. In this section we explore how
general properties of harmonic functions often can be obtained from properties of analytic
functions. This can frequently be done in a simple fashion. Specifically, important mean
value properties of harmonic functions follow readily from those of analytic functions.
The details are as follows.

THEOREM 1

Mean Value Property of Analytic Functions

Let F(z) be analytic in a simply connected domain D. Then the value of F(z) at a point
z 0 in D is equal to the mean value of F(z) on any circle in D with center at z 0.

c18.qxd

11/2/10

6:55 PM

782

Page 782

CHAP. 18 Complex Analysis and Potential Theory

PROOF

In Cauchy’s integral formula (Sec. 14.3)

Ώ

1
F(z 0) ϭ 2pi

(1)

C

we choose for C the circle z ϭ z 0 ϩ re
(1) becomes
1
2p

F(z 0) ϭ

(2)

in D. Then z Ϫ z 0 ϭ reia, dz ϭ ireia da, and

ia

Ύ

F(z)
z Ϫ z 0 dz

2p

F(z 0 ϩ reia) da.

0

The right side is the mean value of F on the circle (ϭ value of the integral divided by the
᭿
length 2p of the interval of integration). This proves the theorem.
For harmonic functions, Theorem 1 implies
THEOREM 2

Two Mean Value Properties of Harmonic Functions

Let £(x, y) be harmonic in a simply connected domain D. Then the value of £(x, y)
at a point (x 0, y0) in D is equal to the mean value of £(x, y) on any circle in D with
center at (x 0, y0). This value is also equal to the mean value of £(x, y) on any
circular disk in D with center (x 0, y0). [See footnote 1 in Sec. 18.4.]

PROOF

The first part of the theorem follows from (2) by taking the real parts on both sides,
£(x 0, y0) ϭ Re F(x 0 ϩ iy0) ϭ

1
2p

Ύ

2p

£(x 0 ϩ r cos a, y0 ϩ r sin a) da.

0

The second part of the theorem follows by integrating this formula over r from 0 to r0 (the
radius of the disk) and dividing by r 20>2,
(3)

£(x 0, y0) ϭ

1

pr 20

r0

Ύ Ύ
0

2p

£(x 0 ϩ r cos a, y0 ϩ r sin a)r da dr.

0

The right side is the indicated mean value (integral divided by the area of the region of
integration).
᭿
Returning to analytic functions, we state and prove another famous consequence of Cauchy’s
integral formula. The proof is indirect and shows quite a nice idea of applying the MLinequality. (A bounded region is a region that lies entirely in some circle about the origin.)
THEOREM 3

Maximum Modulus Theorem for Analytic Functions

Let F(z) be analytic and nonconstant in a domain containing a bounded region R
and its boundary. Then the absolute value ƒ F(z) ƒ cannot have a maximum at an
interior point of R. Consequently, the maximum of ƒ F(z) ƒ is taken on the boundary
of R. If F(z) 0 in R, the same is true with respect to the minimum of ƒ F(z) ƒ .

c18.qxd

11/2/10

6:55 PM

Page 783

SEC. 18.6 General Properties of Harmonic Functions

PROOF

783

We assume that ƒ F(z) ƒ has a maximum at an interior point z 0 of R and show that this
leads to a contradiction. Let ƒ F(z 0) ƒ ϭ M be this maximum. Since F(z) is not constant,
ƒ F(z) ƒ is not constant, as follows from Example 3 in Sec. 13.4. Consequently, we can find
a circle C of radius r with center at z 0 such that the interior of C is in R and ƒ F(z) ƒ is
smaller than M at some point P of C. Since ƒ F(z) ƒ is continuous, it will be smaller than
M on an arc C1 of C that contains P (see Fig. 425), say,
ƒ F(z) ƒ Ϲ M Ϫ k (k Ͼ 0)

for all z on C1.

Let C1 have the length L 1. Then the complementary arc C2 of C has the length 2pr Ϫ L 1.
We now apply the ML-inequality (Sec. 14.1) to (1) and note that ƒ z Ϫ z 0 ƒ ϭ r. We then
obtain (using straightforward calculation in the second line of the formula)
1
M ϭ ƒ F(z 0) ƒ Ϲ 2p `

Ύ

C1

F(z)
1
z Ϫ z 0 dz ` ϩ 2p `

Ύ

C2

F(z)
z Ϫ z 0 dz `

1 MϪk
1 M
kL
Ϲ 2p a r b L 1 ϩ 2p a r b (2pr Ϫ L 1) ϭ M Ϫ 2p1r Ͻ M
that is, M Ͻ M, which is impossible. Hence our assumption is false and the first statement
is proved.
Next we prove the second statement. If F(z) 0 in R, then 1>F(z) is analytic in R.
From the statement already proved it follows that the maximum of 1> ƒ F(z) ƒ lies on the
boundary of R. But this maximum corresponds to the minimum of ƒ F(z) ƒ . This completes
the proof.
᭿

P
C1

z0

C2

Fig. 425. Proof of Theorem 3

This theorem has several fundamental consequences for harmonic functions, as follows.

THEOREM 4

Harmonic Functions

Let £(x, y) be harmonic in a domain containing a simply connected bounded region
R and its boundary curve C. Then:
(I) (Maximum principle) If £(x, y) is not constant, it has neither a maximum
nor a minimum in R. Consequently, the maximum and the minimum are taken on
the boundary of R.
(II) If £(x, y) is constant on C, then £(x, y) is a constant.
(III) If h(x, y) is harmonic in R and on C and if h(x, y) ϭ £(x, y) on C, then
h(x, y) ϭ £(x, y) everywhere in R.

c18.qxd

11/2/10

6:55 PM

784

Page 784

CHAP. 18 Complex Analysis and Potential Theory

PROOF

(I) Let °(x, y) be a conjugate harmonic function of £(x, y) in R. Then the complex
function F(z) ϭ £(x, y) ϩ i°(x, y) is analytic in R, and so is G (z) ϭ eF(z). Its absolute
value is
ƒ G (z) ƒ ϭ eRe F(z) ϭ e£(x, y).
From Theorem 3 it follows that ƒ G (z) ƒ cannot have a maximum at an interior point of R.
Since e£ is a monotone increasing function of the real variable £, the statement about
the maximum of £ follows. From this, the statement about the minimum follows by
replacing £ by Ϫ£.
(II) By (I) the function £(x, y) takes its maximum and its minimum on C. Thus, if
£(x, y) is constant on C, its minimum must equal its maximum, so that £(x, y) must be
a constant.
(III) If h and £ are harmonic in R and on C, then h Ϫ £ is also harmonic in R and
on C, and by assumption, h Ϫ £ ϭ 0 everywhere on C. By (II) we thus have h Ϫ £ ϭ 0
᭿
everywhere in R, and (III) is proved.
The last statement of Theorem 4 is very important. It means that a harmonic function is
uniquely determined in R by its values on the boundary of R. Usually, £(x, y) is required
to be harmonic in R and continuous on the boundary of R, that is,
lim £(x, y) ϭ £(x 0, y0), where (x 0, y0) is on the boundary and (x, y) is in R.

x:x0
y:y0

Under these assumptions the maximum principle (I) is still applicable. The problem of
determining £(x, y) when the boundary values are given is called the Dirichlet problem
for the Laplace equation in two variables, as we know. From (III) we thus have, as a
highlight of our discussion,
THEOREM 5

Uniqueness Theorem for the Dirichlet Problem

If for a given region and given boundary values the Dirichlet problem for the Laplace
equation in two variables has a solution, the solution is unique.

PROBLEM SET 18.6
PROBLEMS RELATED TO THEOREMS 1 AND 2
1–4
Verify Theorem 1 for the given F(z), z 0, and
circle of radius 1.
(z ϩ 1)3, z 0 ϭ 52
2z 4, z 0 ϭ Ϫ2
(3z Ϫ 2)2, z 0 ϭ 4
(z Ϫ 1)؊2, z 0 ϭ Ϫ1
Integrate ƒ z ƒ around the unit circle. Does the result
contradict Theorem 1?
6. Derive the first statement in Theorem 2 from Poisson’s
integral formula.
1.
2.
3.
4.
5.

7–9
Verify (3) in Theorem 2 for the given £(x, y),
(x 0, y0), and circle of radius 1.
(x Ϫ 1)( y Ϫ 1), (2, Ϫ2)
x 2 Ϫ y 2, (3, 8)
x ϩ y ϩ xy, (1, 1)
Verify the calculations involving the inequalities in the
proof of Theorem 3.
11. CAS EXPERIMENT. Graphing Potentials. Graph
the potentials in Probs. 7 and 9 and for two other
functions of your choice as surfaces over a rectangle
in the xy-plane. Find the locations of the maxima and
minima by inspecting these graphs.
7.
8.
9.
10.

c18.qxd

11/2/10

6:55 PM

Page 785

Chapter 18 Review Questions and Problems
12. TEAM PROJECT. Maximum Modulus of Analytic
Functions. (a) Verify Theorem 3 for (i) F(z) ϭ z 2 and
the rectangle 1 Ϲ x Ϲ 5, 2 Ϲ y Ϲ 4, (ii) F(z) ϭ sin z
and the unit disk, and (iii) F(z) ϭ ez and any bounded
domain.
(b) F(z) ϭ 1 ϩ ƒ z ƒ is not zero in the disk ƒ z ƒ Ϲ 2 and
has a minimum at an interior point. Does this contradict
Theorem 3?
(c) F(x) ϭ sin x (x real) has a maximum 1 at p>2.
Why can this not be a maximum of ƒ F(z) ƒ ϭ ƒ sin z ƒ in
a domain containing z ϭ p>2?
(d) If F(z) is analytic and not constant in the closed
unit disk D: ƒ z ƒ Ϲ 1 and ƒ F(z) ƒ ϭ c ϭ const on the unit
circle, show that F(z) must have a zero in D.
13–17

MAXIMUM MODULUS

Find the location and size of the maximum of ƒ F(z) ƒ in the
unit disk ƒ z ƒ Ϲ 1.
13. F(z) ϭ cos z

785
14. F(z) ϭ exp z 2
15. F(z) ϭ sinh 2z
16. F(z) ϭ az ϩ b (a, b complex, a

0)

17. F(z) ϭ 2z Ϫ 2
2

18. Verify the maximum principle for £(x, y) ϭ ex sin y
and the rectangle a Ϲ x Ϲ b, 0 Ϲ y Ϲ 2p.
19. Harmonic conjugate. Do £ and a harmonic conjugate
° in a region R have their maximum at the same point
of R?
20. Conformal mapping. Find the location (u 1, v1) of the
maximum of £* ϭ eu cos v in R*: ƒ w ƒ Ϲ 1, v м 0,
where w ϭ u ϩ iv. Find the region R that is mapped
onto R* by w ϭ f (z) ϭ z 2. Find the potential in R
resulting from £* and the location (x 1, y1) of the
maximum. Is (u 1, v1) the image of (x 1, y1)? If so, is
this just by chance?

CHAPTER 18 REVIEW QUESTIONS AND PROBLEMS
1. Why can potential problems be modeled and solved by
methods of complex analysis? For what dimensions?
2. What parts of complex analysis are mainly of interest
to the engineer and physicist?
3. What is a harmonic function? A harmonic conjugate?
4. What areas of physics did we consider? Could you
think of others?
5. Give some examples of potential problems considered
in this chapter. Make a list of corresponding functions.
6. What does the complex potential give physically?
7. Write a short essay on the various assumptions made
in fluid flow in this chapter.
8. Explain the use of conformal mapping in potential
theory.
9. State the maximum modulus theorem and mean value
theorems for harmonic functions.
10. State Poisson’s integral formula. Derive it from Cauchy’s
formula.
11. Find the potential and the complex potential between
the plates y ϭ x and y ϭ x ϩ 10 kept at 10 V and 110 V,
respectively.
12. Find the potential and complex potential between the
coaxial cylinders of axis 0 (hence the vertical axis
in space) and radii r1 ϭ 1 cm, r2 ϭ 10 cm, kept at
potential U1 ϭ 200 V and U2 ϭ 2 kV, respectively.
13. Do the task in Prob. 12 if U1 ϭ 220 V and the outer
cylinder is grounded, U2 ϭ 0.

14. If plates at x 1 ϭ 1 and x 2 ϭ 10 are kept at potentials
U1 ϭ 200 V, U2 ϭ 2 kV, is the potential at x ϭ 5
larger or smaller than the potential at r ϭ 5 in Prob. 12?
No calculation. Give reason.
15. Make a list of important potential functions, with
applications, from memory.
16. Find the equipotential lines of F(z) ϭ i Ln z.
17. Find the potential in the first quadrant of the xy-plane if
the x-axis has potential 2 kV and the y-axis is grounded.
18. Find the potential in the angular region between the
plates Arg z ϭ p>6 kept at 800 V and Arg z ϭ p>3
kept at 600 V.
19. Find the temperature T in the upper half-plane if, on
the x-axis, T ϭ 30°C for x Ͼ 1 and Ϫ30°C for x Ͻ 1.
20. Interpret Prob. 18 as an electrostatic problem. What are
the lines of electric force?
21. Find the streamlines and the velocity for the complex
potential F(z) ϭ (1 ϩ i)z. Describe the flow.
22. Describe the streamlines for F(z) ϭ 12 z 2 ϩ z.
23. Show that the isotherms of F(z) ϭ Ϫiz 2 ϩ z are
hyperbolas.
24. State the theorem on the behavior of harmonic
functions under conformal mapping. Verify it for
£* ϭ eu sin v and w ϭ u ϩ iv ϭ z 2.
25. Find V in Prob. 22 and verify that it gives vectors
tangent to the streamlines.

c18.qxd

11/2/10

786

6:55 PM

Page 786

CHAP. 18 Complex Analysis and Potential Theory

SUMMARY OF CHAPTER

18

Complex Analysis and Potential Theory
Potential theory is the theory of solutions of Laplace’s equation
(1)

ٌ2 £ ϭ 0.

Solutions whose second partial derivatives are continuous are called harmonic
functions. Equation (1) is the most important PDE in physics, where it is of interest
in two and three dimensions. It appears in electrostatics (Sec. 18.1), steady-state
heat problems (Sec. 18.3), fluid flow (Sec. 18.4), gravity, etc. Whereas the threedimensional case requires other methods (see Chap. 12), two-dimensional potential
theory can be handled by complex analysis, since the real and imaginary parts of
an analytic function are harmonic (Sec. 13.4). They remain harmonic under
conformal mapping (Sec. 18.2), so that conformal mapping becomes a powerful
tool in solving boundary value problems for (1), as is illustrated in this chapter.
With a real potential £ in (1) we can associate a complex potential
(2)

F(z) ϭ £ ϩ i°

(Sec. 18.1).

Then both families of curves £ ϭ const and ° ϭ const have a physical meaning.
In electrostatics, they are equipotential lines and lines of electrical force (Sec. 18.1).
In heat problems, they are isotherms (curves of constant temperature) and lines of
heat flow (Sec. 18.3). In fluid flow, they are equipotential lines of the velocity
potential and streamlines (Sec. 18.4).
For the disk, the solution of the Dirichlet problem is given by the Poisson formula
(Sec. 18.5) or by a series that on the boundary circle becomes the Fourier series of
the given boundary values (Sec. 18.5).
Harmonic functions, like analytic functions, have a number of general properties;
particularly important are the mean value property and the maximum modulus
property (Sec. 18.6), which implies the uniqueness of the solution of the Dirichlet
problem (Theorem 5 in Sec. 18.6).

c19-a.qxd

11/9/10

3:39 PM

Page 787

PART

E

Numeric
Analysis
Software
CHAPTER
CHAPTER
CHAPTER

(p. 788–789)
1 9 Numerics in General
2 0 Numeric Linear Algebra
21
Numerics for ODEs and PDEs
Numeric analysis or briefly numerics continues to be one of the fastest growing areas
of engineering mathematics. This is a natural trend with the ever greater availability of
computing power and global Internet use. Indeed, good software implementation of
numerical methods are readily available. Take a look at the updated list of Software
starting on p. 788. It contains software for purchase (commercial software) and software
for free download (public-domain software). For convenience, we provide Internet
addresses and phone numbers. The software list includes computer algebra systems
(CASs), such as Maple and Mathematica, along with the Maple Computer Guide, 10th
ed., and Mathematica Computer Guide, 10th ed., by E. Kreyszig and E. J. Norminton
related to this text that teach you stepwise how to use these computer algebra systems and
with complete engineering examples drawn from the text. Furthermore, there is scientific
software, such as IMSL, LAPACK (free download), and scientific calculators with graphic
capabilities such as TI-Nspire. Note that, although we have listed frequently used quality
software, this list is by no means complete.
In your career as an engineer, appplied mathematician, or scientist you are likely to use
commercially available software or proprietary software, owned by the company you work
for, that uses numeric methods to solve engineering problems, such as modeling chemical or
biological processes, planning ecologically sound heating systems, or computing trajectories
of spacecraft or satellites. For example, one of the collaborators of this book (Herbert Kreyszig)
used proprietary software to determine the value of bonds, which amounted to solving higher
degree polynomial equations, using numeric methods discussed in Sec. 19.2.
787

c19-a.qxd

11/2/10

788

8:20 PM

Page 788

PART E Numeric Analysis

However, the availability of quality software does not alleviate your effort and
responsibility to first understand these numerical methods. Your effort will pay off
because, with your mathematical expertise in numerics, you will be able to plan your
solution approach, judiciously select and use the appropriate software, judge the quality
of software, and, perhaps, even write your own numerics software.
Numerics extends your ability to solve problems that are either difficult or impossible
to solve analytically. For example, certain integrals such as error function [see App. 3,
formula (35)] or large eigenvalue problems that generate high-degree characteristic
polynomials cannot be solved analytically. Numerics is also used to construct approximating
polynomials through data points that were obtained from some experiments.
Part E is designed to give you a solid background in numerics. We present many numeric
methods as algorithms, which give these methods in detailed steps suitable for software
implementation on your computer, CAS, or programmable calculator. The first chapter,
Chap. 19, covers three main areas. These are general numerics (floating point, rounding errors,
etc.), solving equations of the form f (x) ϭ 0 (using Newton’s method and other methods),
interpolation along with methods of numeric integration that make use of it, and differentiation.
Chapter 20 covers the essentials of numeric linear algebra. The chapter breaks into two
parts: solving linear systems of equations by methods of Gauss, Doolittle, Cholesky, etc.
and solving eigenvalue problems numerically. Chapter 21 again has two themes: solving
ordinary differential equations and systems of ordinary differential equations as well as
solving partial differential equations.
Numerics is a very active area of research as new methods are invented, existing methods
improved and adapted, and old methods—impractical in precomputer times—are
rediscovered. A main goal in these activities is the development of well-structured
software. And in large-scale work—millions of equations or steps of iterations—even
small algorithmic improvements may have a large significant effect on computing time,
storage demand, accuracy, and stability.
Remark on Software Use. Part E is designed in such a way as to allow compelete flexibility
on the use of CASs, software, or graphing calculators. The computational requirements
range from very little use to heavy use. The choice of computer use is at the discretion
of the professor. The material and problem sets (except where clearly indicated such as
in CAS Projects, CAS Problems, or CAS Experiments, which can be omitted without loss
of continuity) do not require the use of a CAS or software. A scientific calculator perhaps
with graphing capabilities is all that is required.

Software
See also http://www.wiley.com/college/kreyszig/
The following list will help you if you wish to find software. You may also obtain information
on known and new software from websites such as Dr. Dobb’s Portal, from articles published
by the American Mathematical Society (see also its website at www.ams.org), the Society
for Industrial and Applied Mathematics (SIAM, at www.siam.org), the Association for
Computing Machinery (ACM, at www.acm.org), or the Institute of Electrical and Electronics
Engineers (IEEE, at www.ieee.org). Consult also your library, computer science department,
or mathematics department.

c19-a.qxd

11/2/10

8:20 PM

Page 789

PART E Numeric Analysis

789

TI-Nspire. Includes TI-Nspire CAS and programmable graphic calculators. Texas Instruments, Inc., Dallas, TX. Telephone: 1-800-842-2737 or (972) 917-8324; website at
www.education.ti.com.
EISPACK. See LAPACK.
GAMS (Guide to Available Mathematical Software). Website at http://gams.nist.gov.
Online cross-index of software development by NIST.
IMSL (International Mathematical and Statistical Library). Visual Numerics, Inc.,
Houston, TX. Telephone: 1-800-222-4675 or (713) 784-3131; website at www.vni.com.
Mathematical and statistical FORTRAN routines with graphics.
LAPACK. FORTRAN 77 routines for linear algebra. This software package supersedes
LINPACK and EISPACK. You can download the routines from www.netlib.org/lapack.
The LAPACK User’s Guide is available at www.netlib.org.
LINPACK see LAPACK
Maple. Waterloo Maple, Inc., Waterloo, ON, Canada. Telephone: 1-800-267-6583 or
(519) 747-2373; website at www.maplesoft.com.
Maple Computer Guide. For Advanced Engineering Mathematics, 10th edition. By
E. Kreyszig and E. J. Norminton. John Wiley and Sons, Inc., Hoboken, NJ. Telephone:
1-800-225-5945 or (201) 748-6000.
Mathcad. Parametric Technology Corp. (PTC), Needham, MA. Website at www.ptc.com.
Mathematica. Wolfram Research, Inc., Champaign, IL. Telephone: 1-800-965-3726 or
(217) 398-0700; website at www.wolfram.com.
Mathematica Computer Guide. For Advanced Engineering Mathematics, 10th edition.
By E. Kreyszig and E. J. Norminton. John Wiley and Sons, Inc., Hoboken, NJ. Telephone:
1-800-225-5945 or (201) 748-6000.
Matlab. The MathWorks, Inc., Natick, MA. Telephone: (508) 647-7000; website at
www.mathworks.com.
NAG. Numerical Algorithms Group, Inc., Lisle, IL. Telephone: (630) 971-2337; website
at www.nag.com. Numeric routines in FORTRAN 77, FORTRAN 90, and C.
NETLIB. Extensive library of public-domain software. See at www.netlib.org.
NIST. National Institute of Standards and Technology, Gaithersburg, MD. Telephone:
(301) 975-6478; website at www.nist.gov. For Mathematical and Computational Science
Division telephone: (301) 975-3800. See also http://math.nist.gov.
Numerical Recipes. Cambridge University Press, New York, NY. Telephone: 1-800-2214512 or (212) 924-3900; website at www.cambridge.org/us. Book, 3rd ed. (in Cϩϩ) see
App. 1, Ref. [E25]; source code on CD ROM in Cϩϩ, which also contains old source code
(but not text) for (out of print) 2nd ed. C, FORTRAN 77, FORTRAN 90 as well as source
code for (out of print) 1st ed. To order, call office at West Nyack, NY, at 1-800-872-7423
or (845) 353-7500 or online at www.nr.com.
FURTHER SOFTWARE IN STATISTICS. See Part G.

c19-a.qxd

11/2/10

8:20 PM

Page 790

CHAPTER

19

Numerics in General
Numeric analysis or briefly numerics has a distinct flavor that is different from basic
calculus, from solving ODEs algebraically, or from other (nonnumeric) areas. Whereas in
calculus and in ODEs there were very few choices on how to solve the problem and your
answer was an algebraic answer, in numerics you have many more choices and your
answers are given as tables of values (numbers) or graphs. You have to make judicous
choices as to what numeric method or algorithm you want to use, how accurate you need
your result to be, with what value (starting value) do you want to begin your computation,
and others. This chapter is designed to provide a good transition from the algebraic type
of mathematics to the numeric type of mathematics.
We begin with the general concepts such as floating point, roundoff errors, and general
numeric errors and their propagation. This is followed in Sec. 19.2 by the important topic
of solving equations of the type f (x) ϭ 0 by various numeric methods, including the famous
Newton method. Section 19.3 introduces interpolation methods. These are methods that
construct new (unknown) function values from known function values. The knowledge
gained in Sec. 19.3 is applied to spline interpolation (Sec. 19.4) and is useful for understanding numeric integration and differentiation covered in the last section.
Numerics provides an invaluable extension to the knowledge base of the problemsolving engineer. Many problems have no solution formula (think of a complicated integral
or a polynomial of high degree or the interpolation of values obtained by measurements).
In other cases a complicated solution formula may exist but may be practically useless.
It is for these kinds of problems that a numerical method may generate a good answer.
Thus, it is very important that the applied mathematician, engineer, physicist, or scientist
becomes familiar with the essentials of numerics and its ideas, such as estimation of errors,
order of convergence, numerical methods expressed in algorithms, and is also informed
about the important numeric methods.
Prerequisite: Elementary calculus.
References and Answers to Problems: App. 1 Part E, App. 2.

19.1

Introduction
As an engineer or physicist you may deal with problems in elasticity and need to solve
an equation such as x cosh x ϭ 1 or a more difficult problem of finding the roots of a
higher order polynomial. Or you encounter an integral such as

Ύ

1

0

790

exp (Ϫx 2) dx

c19-a.qxd

11/2/10

8:20 PM

Page 791

SEC. 19.1 Introduction

791

[see App. 3, formula (35)] that you cannot solve by elementary calculus. Such problems,
which are difficult or impossible to solve algebraically, arise frequently in applications.
They call for numeric methods, that is, systematic methods that are suitable for solving,
numerically, the problems on computers or calculators. Such solutions result in tables of
numbers, graphical representation (figures), or both. Typical numeric methods are iterative
in nature and, for a well-choosen problem and a good starting value, will frequently
converge to a desired answer. The evolution from a given problem that you observed in
an experimental lab or in an industrial setting (in engineering, physics, biology, chemistry,
economics, etc.) to an approximation suitable for numerics to a final answer usually
requires the following steps.
1. Modeling. We set up a mathematical model of our problem, such as an integral, a
system of equations, or a differential equation.
2. Choosing a numeric method and parameters (e.g., step size), perhaps with a
preliminary error estimation.
3. Programming. We use the algorithm to write a corresponding program in a CAS,
such as Maple, Mathematica, Matlab, or Mathcad, or, say, in Java, C or C ϩϩ, or
FORTRAN, selecting suitable routines from a software system as needed.
4. Doing the computation.
5. Interpreting the results in physical or other terms, also deciding to rerun if further
results are needed.
Steps 1 and 2 are related. A slight change of the model may often admit of a more efficient
method. To choose methods, we must first get to know them. Chapters 19–21 contain efficient
algorithms for the most important classes of problems occurring frequently in practice.
In Step 3 the program consists of the given data and a sequence of instructions to be
executed by the computer in a certain order for producing the answer in numeric or graphic
form.
To create a good understanding of the nature of numeric work, we continue in this
section with some simple general remarks.

Floating-Point Form of Numbers
We know that in decimal notation, every real number is represented by a finite or an
infinite sequence of decimal digits. Now most computers have two ways of representing
numbers, called fixed point and floating point. In a fixed-point system all numbers are
given with a fixed number of decimals after the decimal point; for example, numbers
given with 3 decimals are 62.358, 0.014, 1.000. In a text we would write, say, 3 decimals
as 3D. Fixed-point representations are impractical in most scientific computations because
of their limited range (explain!) and will not concern us.
In a floating-point system we write, for instance,
0.6247 ؒ 103,

0.1735 ؒ 10؊13,

Ϫ0.2000 ؒ 10؊1

or sometimes also
6.247 ؒ 102,

1.735 ؒ 10؊14,

Ϫ2.000 ؒ 10؊2.

We see that in this system the number of significant digits is kept fixed, whereas the decimal
point is “floating.” Here, a significant digit of a number c is any given digit of c, except

c19-a.qxd

11/2/10

792

8:20 PM

Page 792

CHAP. 19 Numerics in General

possibly for zeros to the left of the first nonzero digit; these zeros serve only to fix the
position of the decimal point. (Thus any other zero is a significant digit of c.) For instance,
13600, 1.3600,

0.0013600

all have 5 significant digits. In a text we indicate, say, 5 significant digits, by 5S.
The use of exponents permits us to represent very large and very small numbers. Indeed,
theoretically any nonzero number a can be written as
(1)

a ϭ Ϯm ؒ 10n,

0.1 Ϲ ƒ m ƒ Ͻ 1,

n integer.

On modern computers, which use binary (base 2) numbers, m is limited to k binary digits (e.g.,
k ϭ 8) and n is limited (see below), giving representations (for finitely many numbers only!)
(2)

a ϭ Ϯm ؒ 2n,

m ϭ 0.d1d2 Á dk,

d1 Ͼ 0.

These numbers a are called k-digit binary machine numbers. Their fractional part m
(or m) is called the mantissa. This is not identical with “mantissa” as used for logarithms.
n is called the exponent of a.
It is important to realize that there are only finitely many machine numbers and that
they become less and less “dense” with increasing a. For instance, there are as many
numbers between 2 and 4 as there are between 1024 and 2048. Why?
The smallest positive machine number eps with 1 ϩ eps Ͼ 1 is called the machine
accuracy. It is important to realize that there are no numbers in the intervals [1, 1 ϩ eps],
[2, 2 ϩ 2 ؒ eps], Á , [1024, 1024 ϩ 1024 ؒ eps], Á . This means that, if the mathematical
answer to a computation would be 1024 ϩ 1024 ؒ eps>2, the computer result will be either
1024 or 1024 ؒ eps so it is impossible to achieve greater accuracy.
Underflow and Overflow. The range of exponents that a typical computer can handle
is very large. The IEEE (Institute of Electrical and Electronic Engineers) floating-point
standard for single precision is from 2؊126 to 2128 (1.175 ϫ 10؊38 to 3.403 ϫ 1038) and
for double precision it is from 2؊1022 to 21024 (2.225 ϫ 10؊308 to 1.798 ϫ 10308).
As a minor technicality, to avoid storing a minus in the exponent, the ranges are shifted
from [Ϫ126, 128] by adding 126 (for double precision 1022). Note that shifted exponents
of 255 and 1047 are used for some special cases such as representing infinity.
If, in a computation a number outside that range occurs, this is called underflow when
the number is smaller and overflow when it is larger. In the case of underflow, the result
is usually set to zero and computation continues. Overflow might cause the computer to
halt. Standard codes (by IMSL, NAG, etc.) are written to avoid overflow. Error messages
on overflow may then indicate programming errors (incorrect input data, etc.). From here
on, we will be discussing the decimal results that we obtain from our computations.

Roundoff
An error is caused by chopping (ϭ discarding all digits from some decimal on) or rounding.
This error is called roundoff error, regardless of whether we chop or round. The rule for
rounding off a number to k decimals is as follows. (The rule for rounding off to k significant
digits is the same, with “decimal” replaced by “significant digit.”)
Roundoff Rule. To round a number x to k decimals, and 5 ؒ 10؊(kϩ1) to x and chop the
digits after the (k ϩ 1)st digit.

c19-a.qxd

11/2/10

8:20 PM

Page 793

SEC. 19.1 Introduction
EXAMPLE 1

793

Roundoff Rule
Round the number 1.23454621 to (a) 2 decimals, (b) 3 decimals, (c) 4 decimals, (d) 5 decimals, and (e) 6 decimals.

Solution. (a) For 2 decimals we add 5 ؒ 10؊(kϩ1) ϭ 5 ؒ 10؊3 ϭ 0.005 to the given number, that is,

1.2345621 ϩ 0.005 ϭ 1.23 954621. Then we chop off the digits “954621” after the space or equivalently
1.23954621 Ϫ 0.00954621 ϭ 1.23.
(b) 1.23454621 ϩ 0.0005 ϭ 1.235 04621, so that for 3 decimals we get 1.234.
(c) 1.23459621 after chopping give us 1.2345 (4 decimals).
(d) 1.23455121 yields 1.23455 (5 decimals).
(e) 1.23454671 yields 1.234546 (6 decimals).
Can you round the number to 7 decimals?
᭿

Chopping is not recommended because the corresponding error can be larger than that
in rounding. (Nevertheless, some computers use it because it is simpler and faster. On the
other hand, some computers and calculators improve accuracy of results by doing
intermediate calculations using one or more extra digits, called guarding digits.)
Error in Rounding. Let a ϭ fl (a) in (2) be the floating-point computer approximation of
a in (1) obtained by rounding, where fl suggests floating. Then the roundoff rule gives (by
dropping exponents) ƒ m Ϫ m ƒ Ϲ 12 ؒ 10؊k. Since ƒ m ƒ м 0.1, this implies (when a 0)
(3)

`

mϪm
1
aϪa
1؊k
a ` Ϸ ` m ` Ϲ 2 ؒ 10 .

The right side u ϭ 12 ؒ 101؊k is called the rounding unit. If we write a ϭ a(1 ϩ d), we
have by algebra (a Ϫ a)>a ϭ d, hence ƒ d ƒ Ϲ u by (3). This shows that the rounding unit
u is an error bound in rounding.
Rounding errors may ruin a computation completely, even a small computation. In
general, these errors become the more dangerous the more arithmetic operations (perhaps
several millions!) we have to perform. It is therefore important to analyze computational
programs for expected rounding errors and to find an arrangement of the computations
such that the effect of rounding errors is as small as possible.
As mentioned, the arithmetic in a computer is not exact and causes further errors;
however, these will not be relevant to our discussion.
Accuracy in Tables. Although available software has rendered various tables of function
values superfluous, some tables (of higher functions, of coefficients of integration
formulas, etc.) will still remain in occasional use. If a table shows k significant digits, it
is conventionally assumed that any value ෂ
a in the table deviates from the exact value a
by at most Ϯ12 unit of the kth digit.

Loss of Significant Digits
This means that a result of a calculation has fewer correct digits than the numbers from
which it was obtained. This happens if we subtract two numbers of about the same size,
for example, 0.1439 Ϫ 0.1426 (“subtractive cancellation”). It may occur in simple
problems, but it can be avoided in most cases by simple changes of the algorithm—if one
is aware of it! Let us illustrate this with the following basic problem.
EXAMPLE 2

Quadratic Equation. Loss of Significant Digits
Find the roots of the equation
x 2 ϩ 40x ϩ 2 ϭ 0,
using 4 significant digits (abbreviated 4S) in the computation.

c19-a.qxd

11/2/10

794

8:20 PM

Page 794

CHAP. 19 Numerics in General

Solution. A formula for the roots x 1, x 2 of a quadratic equation ax 2 ϩ bx ϩ c ϭ 0 is
(4)

x1 ϭ

1
2a

(Ϫb ϩ 2b 2 Ϫ 4ac),

x2 ϭ

1
2a

(Ϫb Ϫ 2b 2 Ϫ 4ac).

Furthermore, since x 1x 2 ϭ c>a, another formula for those roots
x1 ϭ

(5)

c
,
ax 2

x 2 as in (4).

We see that this avoids cancellation in x 1 for positive b.
If b Ͻ 0, calculate x1 from (4) and then x 2 ϭ c>(ax 1).
For x 2 ϩ 40x ϩ 2 ϭ 0 we obtain from (4) x ϭ Ϫ20 Ϯ 1398 ϭ Ϫ20 Ϯ 19.95, hence x 2 ϭ Ϫ20.00 Ϫ 19.95,
involving no difficulty, and x1 ϭ Ϫ20.00 ϩ 19.95 ϭ Ϫ0.05, a poor value involving loss of digits by subtractive
cancellation.
In contrast, (5) gives x1 ϭ 2.000>(Ϫ39.95) ϭ Ϫ0.05006, the absolute value of the error being less than one
unit of the last digit, as a computation with more digits shows. The 10S-value is Ϫ0.05006265674.
᭿

Errors of Numeric Results
Final results of computations of unknown quantities generally are approximations; that
is, they are not exact but involve errors. Such an error may result from a combination
of the following effects. Roundoff errors result from rounding, as discussed above.
Experimental errors are errors of given data (probably arising from measurements).
Truncating errors result from truncating (prematurely breaking off), for instance, if we
replace a Taylor series with the sum of its first few terms. These errors depend on the
computational method used and must be dealt with individually for each method.
[“Truncating” is sometimes used as a term for chopping off (see before), a terminology
that is not recommended.]
Formulas for Errors. If ෂ
a is an approximate value of a quantity whose exact value is
a, we call the difference
PϭaϪෂ
a

(6)
the error of ෂ
a . Hence
(6*)

aϭෂ
a ϩ P,

True value ϭ Approximation ϩ Error.

For instance, if ෂ
a ϭ 10.5 is an approximation of a ϭ 10.2, its error is P ϭ Ϫ0.3. The
error of an approximation ෂ
a ϭ 1.60 of a ϭ 1.82 is P ϭ 0.22.
CAUTION! In the literature ƒ a Ϫ ෂ
a ƒ (“absolute error”) or ෂ
a Ϫ a are sometimes also
used as definitions of error.
The relative error Pr of ෂ
a is defined by
(7)

Pr ϭ

P
aϪෂ
a
Error
ϭ
ϭ
a
a
True value

(a

0).

This looks useless because a is unknown. But if ƒ P ƒ is much less than ƒ ෂ
a ƒ , then we can
use ෂ
a instead of a and get
(7 r )

P
Pr Ϸ ෂ .
a

c19-a.qxd

11/2/10

8:20 PM

Page 795

SEC. 19.1 Introduction

795

This still looks problematic because P is unknown—if it were known, we could get
aϭෂ
a ϩ P from (6) and we would be done. But what one often can obtain in practice is
a , that is, a number b such that
an error bound for ෂ
ƒ P ƒ Ϲ b,

hence

ƒaϪෂ
a ƒ Ϲ b.

a the unknown a can at most lie. Similarly,
This tells us how far away from our computed ෂ
for the relative error, an error bound is a number br such that
ƒ Pr ƒ Ϲ br,

hence

`

aϪෂ
a
a ` Ϲ br .

Error Propagation
This is an important matter. It refers to how errors at the beginning and in later steps
(roundoff, for example) propagate into the computation and affect accuracy, sometimes
very drastically. We state here what happens to error bounds. Namely, bounds for the
error add under addition and subtraction, whereas bounds for the relative error add under
multiplication and division. You do well to keep this in mind.
THEOREM 1

Error Propagation

(a) In addition and subtraction, a bound for the error of the results is given by
the sum of the error bounds for the terms.
(b) In multiplication and division, an error bound for the relative error of the
results is given (approximately) by the sum of the bounds for the relative errors
of the given numbers.

PROOF

x ϩ Px, y ϭ ෂ
y ϩ Py, ƒ Px ƒ Ϲ bx, ƒ Py ƒ Ϲ by. Then for the
(a) We use the notations x ϭ ෂ
error P of the difference we obtain
ƒ P ƒ ϭ ƒ x Ϫ y Ϫ (xෂ Ϫ ෂ
y) ƒ
ϭ ƒx Ϫ ෂ
x Ϫ (y Ϫ ෂ
y) ƒ
ϭ ƒ Px Ϫ Py ƒ Ϲ ƒ Px ƒ ϩ ƒ Py ƒ Ϲ bx ϩ by .
The proof for the sum is similar and is left to the student.
xෂ
y we get from the relative errors Prx and Pry of ෂ
x, ෂ
y
(b) For the relative error Pr of ෂ
and bounds brx, bry
ƒ Pr ƒ ϭ `
Ϸ `

xy Ϫ ෂ
xෂ
y
xy Ϫ (x Ϫ Px)(y Ϫ Py)
Px y ϩ Py x Ϫ PxPy
` ϭ `
` ϭ `
`
xy
xy
xy
Py
Px
Px y ϩ Py x
`
Ϲ
`
`
ϩ
`
xy
x
y ` ϭ ƒ Prx ƒ ϩ ƒ Pry ƒ Ϲ brx ϩ bry.

This proof shows what “approximately” means: we neglected PxPy as small in absolute
value compared to ƒ Px ƒ and ƒ Py ƒ . The proof for the quotient is similar but slightly more
tricky (see Prob. 13).
᭿

c19-a.qxd

11/2/10

796

8:20 PM

Page 796

CHAP. 19 Numerics in General

Basic Error Principle
Every numeric method should be accompanied by an error estimate. If such a formula is
lacking, is extremely complicated, or is impractical because it involves information (for
instance, on derivatives) that is not available, the following may help.
Error Estimation by Comparison. Do a calculation twice with different accuracy.
a2 Ϫ ෂ
a 1 of the results ෂ
a 1, ෂ
a 2 as a (perhaps crude) estimate of the
Regard the difference ෂ
ෂ
ෂ
a 2 ϩ P2 by formula (4*). This implies
error P1 of the inferior result a 1. Indeed, a 1 ϩ P1 ϭ ෂ
ෂ
a2 Ϫ ෂ
a 1 ϭ P1 Ϫ P2 Ϸ P1 because ෂ
a 2 is generally more accurate than ෂ
a 1, so that ƒ P2 ƒ is
small compared to ƒ P1 ƒ .

Algorithm. Stability
Numeric methods can be formulated as algorithms. An algorithm is a step-by-step
procedure that states a numeric method in a form (a “pseudocode”) understandable to
humans. (See Table 19.1 to see what an algorithm looks like.) The algorithm is then used
to write a program in a programming language that the computer can understand so that
it can execute the numeric method. Important algorithms follow in the next sections. For
routine tasks your CAS or some other software system may contain programs that you
can use or include as parts of larger programs of your own.
Stability. To be useful, an algorithm should be stable; that is, small changes in the initial
data should cause only small changes in the final results. However, if small changes in the
initial data can produce large changes in the final results, we call the algorithm unstable.
This “numeric instability,” which in most cases can be avoided by choosing a better
algorithm, must be distinguished from “mathematical instability” of a problem, which is
called “ill-conditioning,” a concept we discuss in the next section.
Some algorithms are stable only for certain initial data, so that one must be careful in
such a case.

PROBLEM SET 19.1
1. Floating point. Write 84.175, Ϫ528.685, 0.000924138,
and Ϫ362005 in floating-point form, rounded to 5S
(5 significant digits).
2. Write Ϫ76.437125, 60100, and Ϫ0.00001 in floatingpoint form, rounded to 4S.
3. Small differences of large numbers may be particularly strongly affected by rounding errors. Illustrate
this by computing 0.81534>(35 ؒ 724 Ϫ 35.596) as
given with 5S, then rounding stepwise to 4S, 3S, and 2S,
where “stepwise” means round the rounded numbers, not
the given ones.
4. Order of terms, in adding with a fixed number of
digits, will generally affect the sum. Give an example.
Find empirically a rule for the best order.

5. Rounding and adding. Let a1, Á , an be numbers with
aj correctly rounded to Sj digits. In calculating the sum
a1 ϩ Á ϩ an, retaining S ϭ min Sj significant digits,
is it essential that we first add and then round the result
or that we first round each number to S significant digits
and then add?
6. Nested form. Evaluate
f (x) ϭ x 3 Ϫ 7.5x 2 ϩ 11.2x ϩ 2.8
ϭ ((x Ϫ 7.5)x ϩ 11.2)x ϩ 2.8
at x ϭ 3.94 using 3S arithmetic and rounding, in both
of the given forms. The latter, called the nested form,
is usually preferable since it minimizes the number of
operations and thus the effect of rounding.

c19-a.qxd

11/10/10

2:34 AM

Page 797

SEC. 19.1 Introduction

797

7. Quadratic equation. Solve x 2 Ϫ 30x ϩ 1 ϭ 0 by (4)
and by (5), using 6S in the computation. Compare and
comment.

22. Convert (0.59375)10 to (0.10011)2 by successive
multiplication by 2 and dropping (removing) the integer
parts, which give the binary digits c1, c2, Á :

8. Solve x 2 Ϫ 40x ϩ 2 ϭ 0, using 4S-computation.
c1 ϭ
c2 ϭ
c3 ϭ
c4 ϭ
c5 ϭ

9. Do the computations in Prob. 7 with 4S and 2S.
10. Instability. For small ƒ a ƒ the equation (x Ϫ k)2 ϭ a
has nearly a double root. Why do these roots show
instability?
11. Theorems on errors. Prove Theorem 1(a) for addition.
12. Overflow and underflow can sometimes be avoided
by simple changes in a formula. Explain this in terms
of 2x 2 ϩ y 2 ϭ x21 ϩ (y>x)2 with x 2 м y 2 and x so
large that x 2 would cause overflow. Invent examples
of your own.
13. Division. Prove Theorem 1(b) for division.
14. Loss of digits. Square root. Compute 2x 2 ϩ 4 Ϫ 2
with 6S arithmetic for x ϭ 0.001 (a) as given and
(b) from x 2>( 2x 2 ϩ 4 ϩ 2) (derive!).

15. Logarithm. Compute ln a Ϫ ln b with 6S arithmetic
for a ϭ 4.00000 and b ϭ 3.99900 (a) as given and
(b) from ln (a>b).
16. Cosine. Compute 1 Ϫ cos x with 6S arithmetic for
x ϭ 0.02 (a) as given and (b) by 2 sin2 12 x (derive!).
17. Discuss the numeric use of (12) in App. A3.1 for
cos v Ϫ cos u when u Ϸ v.

18. Quotient near 0>0. (a) Compute (1 Ϫ cos x)>sin x with
6S arithmetic for x ϭ 0.005. (b) Looking at Prob. 16,
find a much better formula.
19. Exponential function. Calculate 1>e ϭ 0.367879 (6S)
from the partial sums of 5–10 terms of the Maclaurin
series (a) of e؊x with x ϭ 1, (b) of ex with x ϭ 1 and
then taking the reciprocal. Which is more accurate?
20. Compute e؊10 with 6S arithmetic in two ways (as in
Prob. 19).
21. Binary conversion. Show that
23 ϭ 20 # 101 ϩ 3 # 100 ϭ 16 ϩ 4 ϩ 2 ϩ 1
ϭ 24 ϩ 22 ϩ 21 ϩ 20 ϭ (1 0 1 1 1.)2
can be obtained by the division algorithm
2 23

Remainder 1 ϭ c0

2 11

1 ϭ c1

2 5

1 ϭ c2

2 2
0

0 ϭ c3
1 ϭ c4

0
1
0
0
1
1

.59375 ؒ 2
.1875 ؒ 2
.375 ؒ 2
.75 ؒ 2
.5 ؒ 2
.0

23. Show that 0.1 is not a binary machine number.
24. Prove that any binary machine number has a finite
decimal representation. Is the converse true?
25. CAS

EXPERIMENT.

x ϭ 0.1 ϭ

Approximations.

Obtain

ؕ

3
؊4m
from Prob. 23. Which machine
a 2
2 mϭ1

number (partial sum) Sn will first have the value 0.1
to 30 decimal digits?
26. CAS EXPERIMENT. Integration from Calculus.
Integrating by parts, show that In ϭ ͐1 exx n dx ϭ
0
e Ϫ nIn؊1, I0 ϭ e Ϫ 1. (a) Compute In, n ϭ 0, Á ,
using 4S arithmetic, obtaining I8 ϭ Ϫ3.906. Why is
this nonsense? Why is the error so large?
(b) Experiment in (a) with the number of digits k Ͼ 4.
As you increase k, will the first negative value n ϭ N
occur earlier or later? Find an empirical formula for
N ϭ N (k).
27. Backward Recursion. In Prob. 26. Using ex Ͻ e
(0 Ͻ x Ͻ 1), conclude that ƒ In ƒ Ϲ e>(n ϩ 1) : 0 as
n : ϱ. Solve the iteration formula for In؊1 ϭ
(e Ϫ In)>n, start from I15 Ϸ 0 and compute 4S values
of I14, I13, Á , I1.
28. Harmonic series. 1 ϩ 12 ϩ 13 ϩ Á diverges. Is the
same true for the corresponding series of computer
numbers?
29. Approximations of p ‫ ؍‬3.14159265358979 Á are
22>7 and 355>113. Determine the corresponding errors
and relative errors to 3 significant digits.
30. Compute p by Machin’s approximation 16 arctan
1
(15) Ϫ 4 arctan (239
) to 10S (which are correct). [In
1986, D. H. Bailey (NASA Ames Research Center,
Moffett Field, CA 94035) computed almost 30 million
decimals of p on a CRAY-2 in less than 30 hrs. The
race for more and more decimals is continuing. See the
Internet under pi.]

c19-a.qxd

11/2/10

8:20 PM

798

19.2

Page 798

CHAP. 19 Numerics in General

Solution of Equations by Iteration
For each of the remaining sections of this chapter, we select basic kinds of problems and
discuss numeric methods on how to solve them. The reader will learn about a variety of
important problems and become familiar with ways of thinking in numerical analysis.
Perhaps the easiest conceptual problem is to find solutions of a single equation
(1)

f (x) ϭ 0,

where f is a given function. A solution of (1) is a number x ϭ s such that f (s) ϭ 0. Here,
s suggests “solution,” but we shall also use other letters.
It is interesting to note that the task of solving (1) is a question made for numeric
algorithms, as in general there are no direct formulas, except in a few simple cases.
Examples of single equations are x 3 ϩ x ϭ 1, sin x ϭ 0.5x, tan x ϭ x, cosh x ϭ sec x,
cosh x cos x ϭ Ϫ1, which can all be written in the form of (1). The first of the five equations
is an algebraic equation because the corresponding f is a polynomial. In this case the
solutions are called roots of the equation and the solution process is called finding roots. The
other equations are transcendental equations because they involve transcendental functions.
There are a very large number of applications in engineering, where we have to solve a
single equation (1). You have seen such applications when solving characteristic equations
in Chaps. 2, 4, and 8; partial fractions in Chap. 6; residue integration in Chap. 16, finding
eigenvalues in Chap. 12, and finding zeros of Bessel functions, also in Chap. 12. Moreover,
methods of finding roots are very important in areas outside of classical engineering. For
example, in finance, the problem of determining how much a bond is worth amounts to
solving an algebraic equation.
To solve (1) when there is no formula for the exact solution available, we can use an
approximation method, such as an iteration method. This is a method in which we start from
an initial guess x 0 (which may be poor) and compute step by step (in general better and better)
approximations x1, x2, Á of an unknown solution of (1). We discuss three such methods that
are of particular practical importance and mention two others in the problem set.
It is very important that the reader understand these methods and their underlying ideas.
The reader will then be able to select judiciously the appropriate software from among
different software packages that employ variations of such methods and not just treat the
software programs as “black boxes.”
In general, iteration methods are easy to program because the computational operations
are the same in each step—just the data change from step to step—and, more importantly,
if in a concrete case a method converges, it is stable in general (see Sec. 19.1).

Fixed-Point Iteration for Solving Equations f (x) ϭ 0
Note: Our present use of the word “fixed point” has absolutely nothing to do with that in
the last section.
By some algebraic steps we transform (1) into the form
(2)

x ϭ g(x).

Then we choose an x 0 and compute x1 ϭ g(x 0), x 2 ϭ g(x1), and in general
(3)

x nϩ1 ϭ g(x n)

(n ϭ 0, 1, Á ).

c19-a.qxd

11/2/10

8:20 PM

Page 799

SEC. 19.2 Solution of Equations by Iteration

799

A solution of (2) is called a fixed point of g, motivating the name of the method. This is a
solution of (1), since from x ϭ g(x) we can return to the original form f (x) ϭ 0. From (1)
we may get several different forms of (2). The behavior of corresponding iterative sequences
x 0, x 1, Á may differ, in particular, with respect to their speed of convergence. Indeed, some
of them may not converge at all. Let us illustrate these facts with a simple example.
EXAMPLE 1

An Iteration Process (Fixed-Point Iteration)
Set up an iteration process for the equation f (x) ϭ x 2 Ϫ 3x ϩ 1 ϭ 0. Since we know the solutions
x ϭ 1.5 Ϯ 11.25,

thus

2.618034

and

0.381966,

we can watch the behavior of the error as the iteration proceeds.

Solution. The equation may be written
x ϭ g1(x) ϭ 13 (x 2 ϩ 1) ,

(4a)

x nϩ1 ϭ 13 (x 2n ϩ 1) .

thus

If we choose x 0 ϭ 1, we obtain the sequence (Fig. 426a; computed with 6S and then rounded)
x 0 ϭ 1.000,

x 1 ϭ 0.667,

x 2 ϭ 0.481,

x 3 ϭ 0.411,

x 4 ϭ 0.390, Á

which seems to approach the smaller solution. If we choose x 0 ϭ 2, the situation is similar. If we choose
x 0 ϭ 3, we obtain the sequence (Fig. 426a, upper part)
x 0 ϭ 3.000,

x 1 ϭ 3.333,

x 2 ϭ 4.037,

x 3 ϭ 5.766,

x 4 ϭ 11.415, Á

which diverges.
Our equation may also be written (divide by x)
x ϭ g2 (x) ϭ 3 Ϫ

(4b)

1
,
x

thus

x nϩ1 ϭ 3 Ϫ

1
,
xn

and if we choose x 0 ϭ 1, we obtain the sequence (Fig. 426b)
x 0 ϭ 1.000,

x 1 ϭ 2.000,

x 2 ϭ 2.500,

x 3 ϭ 2.600,

x 4 ϭ 2.615, Á

which seems to approach the larger solution. Similarly, if we choose x 0 ϭ 3, we obtain the sequence
(Fig. 426b)
x 0 ϭ 3.000,

x 1 ϭ 2.667,

x 2 ϭ 2.625,

5

x 3 ϭ 2.619,

x 4 ϭ 2.618, Á .

5
g1(x)

g2(x)

0
0

x
(a)

5

0

0

x
(b)

Fig. 426. Example 1, iterations (4a) and (4b)

5

c19-a.qxd

11/2/10

8:20 PM

800

Page 800

CHAP. 19 Numerics in General
Our figures show the following. In the lower part of Fig. 426a the slope of g1(x) is less than the slope of y ϭ x,
which is 1, thus ƒ g1r (x) ƒ Ͻ 1, and we seem to have convergence. In the upper part, g1(x) is steeper (g1r (x) Ͼ 1)
and we have divergence. In Fig. 426b the slope of g2(x) is less near the intersection point (x ϭ 2.618, fixed
point of g2, solution of f (x) ϭ 0), and both sequences seem to converge. From all this we conclude that
convergence seems to depend on the fact that, in a neighborhood of a solution, the curve of g(x) is less steep
than the straight line y ϭ x, and we shall now see that this condition ƒ g r(x) ƒ Ͻ 1 (ϭ slope of y ϭ x) is sufficient
for convergence.
᭿

An iteration process defined by (3) is called convergent for an x 0 if the corresponding
sequence x 0, x 1, Á is convergent.
A sufficient condition for convergence is given in the following theorem, which has
various practical applications.
THEOREM 1

Convergence of Fixed-Point Iteration

Let x ϭ s be a solution of x ϭ g(x) and suppose that g has a continuous derivative
in some interval J containing s. Then, if ƒ g r(x) ƒ Ϲ K Ͻ 1 in J, the iteration process
defined by (3) converges for any x 0 in J. The limit of the sequence {x n} is s.

PROOF

By the mean value theorem of differential calculus there is a t between x and s such that
g(x) Ϫ g(s) ϭ g r(t)(x Ϫ s)

(x in J).

Since g(s) ϭ s and x 1 ϭ g(x 0), x 2 ϭ g(x 1), Á , we obtain from this and the condition on
ƒ g r(x) ƒ in the theorem
ƒ x n Ϫ s ƒ ϭ ƒ g(x n؊1) Ϫ g(s) ƒ ϭ ƒ g r (t) ƒ ƒ x n؊1 Ϫ s ƒ Ϲ K ƒ x n؊1 Ϫ s ƒ .
Applying this inequality n times, for n, n Ϫ 1, Á , 1 gives
ƒ x n Ϫ s ƒ Ϲ K ƒ x n؊1 Ϫ s ƒ Ϲ K 2 ƒ x n؊2 Ϫ s ƒ Ϲ Á Ϲ K n ƒ x 0 Ϫ s ƒ .
Since K Ͻ 1, we have K n : 0; hence ƒ x n Ϫ s ƒ : 0 as n : ϱ.

᭿

We mention that a function g satisfying the condition in Theorem 1 is called a contraction
because ƒ g(x) Ϫ g(v) ƒ Ϲ K ƒ x Ϫ v ƒ , where K Ͻ 1. Furthermore, K gives information on
the speed of convergence. For instance, if K ϭ 0.5, then the accuracy increases by at least
2 digits in only 7 steps because 0.57 Ͻ 0.01.
EXAMPLE 2

An Iteration Process. Illustration of Theorem 1
Find a solution of f (x) ϭ x 3 ϩ x Ϫ 1 ϭ 0 by iteration.

Solution. A sketch shows that a solution lies near x ϭ 1. (a) We may write the equation as (x 2 ϩ 1)x ϭ 1 or
x ϭ g1 (x) ϭ

1
1ϩx

2

,

so that

x nϩ1 ϭ

1
1ϩ

x 2n

.

Also

ƒ g1r (x) ƒ ϭ

2ƒxƒ
(1 ϩ x 2)2

Ͻ1

for any x because 4x 2>(1 ϩ x 2)4 ϭ 4x 2>(1 ϩ 4x 2 ϩ Á ) Ͻ 1, so that by Theorem 1 we have convergence for
any x 0 . Choosing x 0 ϭ 1, we obtain (Fig. 427)
x 1 ϭ 0.500, x 2 ϭ 0.800, x 3 ϭ 0.610, x 4 ϭ 0.729, x 5 ϭ 0.653, x 6 ϭ 0.701, Á .
The solution exact to 6D is s ϭ 0.682328.

c19-a.qxd

11/2/10

8:20 PM

Page 801

SEC. 19.2 Solution of Equations by Iteration

801

(b) The given equation may also be written
x ϭ g2 (x) ϭ 1 Ϫ x 3.

ƒ g2r (x) ƒ ϭ 3x 2

Then

and this is greater than 1 near the solution, so that we cannot apply Theorem 1 and assert convergence. Try
x 0 ϭ 1, x 0 ϭ 0.5, x 0 ϭ 2 and see what happens.
The example shows that the transformation of a given f (x) ϭ 0 into the form x ϭ g(x) with g satisfying
ƒ g r(x) Ϲ K Ͻ 1 may need some experimentation.
᭿

1.0

g1(x)
x2

0.5

y

x1

y = f(x)

f(x0)
0
0

β
0.5

1.0

x2

x

x1

x0

x

Fig. 428. Newton’s method

Fig. 427. Iteration in Example 2

Newton’s Method for Solving Equations f (x) ϭ 0
Newton’s method, also known as Newton–Raphson’s method,1 is another iteration
method for solving equations f (x) ϭ 0, where f is assumed to have a continuous derivative f r .
The method is commonly used because of its simplicity and great speed.
The underlying idea is that we approximate the graph of f by suitable tangents. Using
an approximate value x 0 obtained from the graph of f, we let x 1 be the point of intersection
of the x-axis and the tangent to the curve of f at x 0 (see Fig. 428). Then
tan b ϭ f r(x 0) ϭ

f (x 0)
x0 Ϫ x1

,

hence

x1 ϭ x0 Ϫ

f (x 0)
f r(x 0)

.

In the second step we compute x 2 ϭ x 1 Ϫ f (x 1)>f r(x 1), in the third step x 3 from x 2 again
by the same formula, and so on. We thus have the algorithm shown in Table 19.1. Formula
(5) in this algorithm can also be obtained if we algebraically solve Taylor’s formula
(5*)

f (x nϩ1) Ϸ f (x n) ϩ (x nϩ1 Ϫ x n) f r(x n) ϭ 0.

1
JOSEPH RAPHSON (1648–1715), English mathematician who published a method similar to Newton’s
method. For historical details, see Ref. [GenRef2], p. 203, listed in App. 1.

c19-a.qxd

11/2/10

802

8:20 PM

Page 802

CHAP. 19 Numerics in General
Table 19.1 Newton’s Method for Solving Equations ƒ(x) ‫ ؍‬0

ALGORITHM NEWTON ( f, f r, x 0, P, N)
This algorithm computes a solution of ƒ(x) ϭ 0 given an initial approximation x0 (starting
value of the iteration). Here the function ƒ(x) is continuous and has a continuous
derivative ƒЈ(x).
INPUT: ƒ, ƒЈ, initial approximation x0, tolerance ⑀ Ͼ 0, maximum number of
iterations N.
OUTPUT: Approximate solution xn (n Ϲ N) or message of failure.
For n ϭ 0, 1, 2, • • • , N Ϫ 1 do:
1

Compute ƒЈ(xn).

2

If ƒЈ(xn) ϭ 0 then OUTPUT “Failure.” Stop.
[Procedure completed unsuccessfully]

3

Else compute

(5)

x nϩ1 ϭ x n Ϫ

f (x n)
f r(x n)

.

If ƒ x nϩ1 Ϫ x n ƒ Ϲ P ƒ x nϩ1 ƒ then OUTPUT x nϩ1. Stop.

4

[Procedure completed successfully]
End
5

OUTPUT “Failure”. Stop.
[Procedure completed unsuccessfully after N iterations]

End NEWTON

If it happens that f r(x n) ϭ 0 for some n (see line 2 of the algorithm), then try another
starting value x 0. Line 3 is the heart of Newton’s method.
The inequality in line 4 is a termination criterion. If the sequence of the x n converges
and the criterion holds, we have reached the desired accuracy and stop. Note that this is just
a form of the relative error test. It ensures that the result has the desired number of significant
digits. If ƒ x nϩ1 ƒ ϭ 0, the condition is satisfied if and only if x nϩ1 ϭ x n ϭ 0, otherwise
ƒ x nϩ1 Ϫ x n ƒ must be sufficiently small. The factor ƒ x nϩ1 ƒ is needed in the case of zeros
of very small (or very large) absolute value because of the high density (or of the scarcity)
of machine numbers for those x.
WARNING! The criterion by itself does not imply convergence. Example. The
harmonic series diverges, although its partial sums x n ϭ S n
kϭ1 1/k satisfy the criterion
because lim (x nϩ1 Ϫ x n) ϭ lim (1>(n ϩ 1)) ϭ 0.

c19-a.qxd

11/2/10

8:20 PM

Page 803

SEC. 19.2 Solution of Equations by Iteration

803

Line 5 gives another termination criterion and is needed because Newton’s method may
diverge or, due to a poor choice of x 0, may not reach the desired accuracy by a reasonable
number of iterations. Then we may try another x 0. If f (x) ϭ 0 has more than one solution,
different choices of x 0 may produce different solutions. Also, an iterative sequence may
sometimes converge to a solution different from the expected one.
EXAMPLE 3

Square Root
Set up a Newton iteration for computing the square root x of a given positive number c and apply it to c ϭ 2.

Solution. We have x ϭ 1c, hence f (x) ϭ x 2 Ϫ c ϭ 0, f r(x) ϭ 2x, and (5) takes the form
x nϩ1 ϭ x n Ϫ

x 2n Ϫ c
1
c
ϭ ax n ϩ
b.
2x n
2
xn

For c ϭ 2, choosing x 0 ϭ 1, we obtain
x 1 ϭ 1.500000,

x 2 ϭ 1.416667,

x 3 ϭ 1.414216,

x 4 ϭ 1.414214, Á .

᭿

x 4 is exact to 6D.

EXAMPLE 4

Iteration for a Transcendental Equation
Find the positive solution of 2 sin x ϭ x.

Solution. Setting f (x) ϭ x Ϫ 2 sin x, we have f r(x) ϭ 1 Ϫ 2 cos x, and (5) gives
x nϩ1 ϭ x n Ϫ

x n Ϫ 2 sin x n
1 Ϫ 2 cos x n

ϭ

2(sin x n Ϫ x n cos x n)
1 Ϫ 2 cos x n

n

xn

Nn

Dn

xnϩ1

0
1
2
3

2.00000
1.90100
1.89552
1.89550

3.48318
3.12470
3.10500
3.10493

1.83229
1.64847
1.63809
1.63806

1.90100
1.89552
1.89550
1.89549

ϭ

Nn
Dn

.

From the graph of f we conclude that the solution is near x 0 ϭ 2. We compute:
x 4 ϭ 1.89549 is exact to 5D since the solution to 6D is 1.895494.

EXAMPLE 5

᭿

Newton’s Method Applied to an Algebraic Equation
Apply Newton’s method to the equation f (x) ϭ x 3 ϩ x Ϫ 1 ϭ 0.

Solution. From (5) we have
x nϩ1 ϭ x n Ϫ

x 3n ϩ x n Ϫ 1
3x 2n ϩ 1

ϭ

2x 3n ϩ 1
3x 2n ϩ 1

.

Starting from x 0 ϭ 1, we obtain
x 1 ϭ 0.750000, x 2 ϭ 0.686047, x 3 ϭ 0.682340, x 4 ϭ 0.682328, Á
where x 4 has the error Ϫ1 ؒ 10؊6. A comparison with Example 2 shows that the present convergence is much
᭿
more rapid. This may motivate the concept of the order of an iteration process, to be discussed next.

c19-a.qxd

11/2/10

8:20 PM

804

Page 804

CHAP. 19 Numerics in General

Order of an Iteration Method.
Speed of Convergence
The quality of an iteration method may be characterized by the speed of convergence, as
follows.
Let x nϩ1 ϭ g(x n) define an iteration method, and let x n approximate a solution s of
x ϭ g(x). Then x n ϭ s Ϫ Pn, where Pn is the error of x n. Suppose that g is differentiable
a number of times, so that the Taylor formula gives
x nϩ1 ϭ g(x n) ϭ g(s) ϩ g r(s)(x n Ϫ s) ϩ 12 g s(s)(x n Ϫ s)2 ϩ Á

(6)

ϭ g(s) Ϫ g r(s)Pn ϩ 12 g s(s)P2n ϩ Á .

The exponent of Pn in the first nonvanishing term after g(s) is called the order of the
iteration process defined by g. The order measures the speed of convergence.
To see this, subtract g(s) ϭ s on both sides of (6). Then on the left you get x nϩ1 Ϫ s ϭ
ϪPnϩ1, where Pnϩ1 is the error of x nϩ1 . And on the right the remaining expression equals
approximately its first nonzero term because ƒ Pn ƒ is small in the case of convergence.
Thus
(a) Pnϩ1 Ϸ ϩg r(s)Pn

(7)

(b) Pnϩ1 Ϸ

Ϫ12 g

in the case of first order,

(s)P2n

s

in the case of second order,

etc.

Thus if Pn ϭ 10؊k in some step, then for second order, Pnϩ1 ϭ c ؒ (10؊k)2 ϭ c ؒ 10؊2k,
so that the number of significant digits is about doubled in each step.

Convergence of Newton’s Method
In Newton’s method, g(x) ϭ x Ϫ f (x)>f r(x). By differentiation,
g r(x) ϭ 1 Ϫ

f r(x)2 Ϫ f (x)f s(x)
f r(x)2

(8)
ϭ

f (x) f s(x)

.

f r(x)2

Since f (s) ϭ 0, this shows that also g r(s) ϭ 0. Hence Newton’s method is at least of second
order. If we differentiate again and set x ϭ s, we find that
(8*)

g s(s) ϭ

f s(s)
f r(s)

which will not be zero in general. This proves

THEOREM 2

Second-Order Convergence of Newton’s Method

If f (x) is three times differentiable and f r and f s are not zero at a solution s of
f (x) ϭ 0, then for x 0 sufficiently close to s, Newton’s method is of second order.

c19-a.qxd

11/2/10

8:20 PM

Page 805

SEC. 19.2 Solution of Equations by Iteration

805

Comments. For Newton’s method, (7b) becomes, by (8*),
Pnϩ1 Ϸ Ϫ

(9)

f s(s)
2f r(s)

P2n.

For the rapid convergence of the method indicated in Theorem 2 it is important that s be
a simple zero of f (x) (thus f r(s) 0) and that x 0 be close to s, because in Taylor’s formula
we took only the linear term [see (5*)], assuming the quadratic term to be negligibly small.
(With a bad x 0 the method may even diverge!)
EXAMPLE 6

Prior Error Estimate of the Number of Newton Iteration Steps
Use x 0 ϭ 2 and x 1 ϭ 1.901 in Example 4 for estimating how many iteration steps we need to produce the
solution to 5D-accuracy. This is an a priori estimate or prior estimate because we can compute it after only
one iteration, prior to further iterations.

Solution. We have f (x) ϭ x Ϫ 2 sin x ϭ 0. Differentiation gives
f s(s)
2 f r (s)

Ϸ

f s(x 1)
2 f r(x 1)

ϭ

2 sin x 1
2(1 Ϫ 2 cos x 1)

Ϸ 0.57.

Hence (9) gives
ƒ Pnϩ1 ƒ Ϸ 0.57P2n Ϸ 0.57(0.57P2n؊1)2 ϭ 0.573P4n؊1 Ϸ Á Ϸ 0.57MPMϩ1
Ϲ 5 ؒ 10؊6
0
where M ϭ 2n ϩ 2n؊1 ϩ Á ϩ 2 ϩ 1 ϭ 2nϩ1 Ϫ 1. We show below that P0 Ϸ Ϫ0.11. Consequently, our
condition becomes
0.57M0.11Mϩ1 Ϲ 5 ؒ 10؊6.
Hence n ϭ 2 is the smallest possible n, according to this crude estimate, in good agreement with Example 4.
P0 Ϸ Ϫ0.11 is obtained from P1 Ϫ P0 ϭ (P1 Ϫ s) Ϫ (P0 Ϫ s) ϭ Ϫx 1 ϩ x 0 Ϸ 0.10, hence P1 ϭ P0 ϩ 0.10 Ϸ
Ϫ0.57P20 or 0.57P20 ϩ P0 ϩ 0.10 Ϸ 0, which gives P0 Ϸ Ϫ0.11.
᭿

Difficulties in Newton’s Method. Difficulties may arise if ƒ f r(x) ƒ is very small near a
solution s of f (x) ϭ 0. For instance, let s be a zero of f (x) of second or higher order. Then
Newton’s method converges only linearly, as is shown by an application of l’Hopital’s rule
to (8). Geometrically, small ƒ f r(x) ƒ means that the tangent of f (x) near s almost coincides
with the x-axis (so that double precision may be needed to get f (x) and f r(x) accurately
enough). Then for values x ϭ ෂs far away from s we can still have small function values
R (ෂs ) ϭ f (ෂs ).
In this case we call the equation f (x) ϭ 0 ill-conditioned. R (ෂs ) is called the residual of
f (x) ϭ 0 at ෂ
s . Thus a small residual guarantees a small error of ෂs only if the equation is
not ill-conditioned.
EXAMPLE 7

An Ill-Conditioned Equation
f (x) ϭ x 5 ϩ 10؊4x ϭ 0 is ill-conditioned, x ϭ 0 is a solution. f r(0) ϭ 10؊4 is small. At ෂs ϭ 0.1 the residual
f (0.1) ϭ 2 ؒ 10؊5 is small, but the error Ϫ0.1 is larger in absolute value by a factor 5000. Invent a more drastic
example of your own.
᭿

Secant Method for Solving f (x) ϭ 0
Newton’s method is very powerful but has the disadvantage that the derivative f r may
sometimes be a far more difficult expression than f itself and its evaluation therefore

c19-a.qxd

11/2/10

8:20 PM

806

Page 806

CHAP. 19 Numerics in General

computationally expensive. This situation suggests the idea of replacing the derivative
with the difference quotient
f r(x n) Ϸ

f (x n) Ϫ f (x n؊1)
x n Ϫ x n؊1 .

Then instead of (5) we have the formula of the popular secant method
y

y = f(x)

Secant
Pn –1

Pn

s

xn +1

xn

x

xn –1

Fig. 429. Secant method

x nϩ1 ϭ x n Ϫ f (x n)

(10)

x n Ϫ x n؊1
f (x n) Ϫ f (x n؊1)

.

Geometrically, we intersect the x-axis at x nϩ1 with the secant of f (x) passing through
PnϪ1 and Pn in Fig. 429. We need two starting values x 0 and x 1. Evaluation of derivatives
is now avoided. It can be shown that convergence is superlinear (that is, more rapid than
linear, ƒ Pnϩ1 ƒ Ϸ const # ƒ Pn ƒ 1.62; see [E5] in App. 1), almost quadratic like Newton’s
method. The algorithm is similar to that of Newton’s method, as the student may show.
CAUTION! It is not good to write (10) as
x nϩ1 ϭ

x n؊1 f (x n) Ϫ x n f (x n؊1)
f (x n) Ϫ f (x n؊1)

,

because this may lead to loss of significant digits if x n and x n؊1 are about equal. (Can
you see this from the formula?)
EXAMPLE 8

Secant Method
Find the positive solution of f (x) ϭ x Ϫ 2 sin x ϭ 0 by the secant method, starting from x 0 ϭ 2, x 1 ϭ 1.9.

Solution. Here, (10) is
x nϩ1 ϭ x n Ϫ

(x n Ϫ 2 sin x n)(x n Ϫ x n؊1)
x n Ϫ x n؊1 ϩ 2(sin x n؊1 Ϫ sin x n)

ϭ xn Ϫ

Nn
Dn

.

Numeric values are:

n

xn؊1

xn

Nn

Dn

xnϩ1 Ϫ xn

1
2
3

2.000000
1.900000
1.895747

1.900000
1.895747
1.895494

Ϫ0.000740
Ϫ0.000002
0

Ϫ0.174005
Ϫ0.006986

Ϫ0.004253
Ϫ0.000252
0

x 3 ϭ 1.895494 is exact to 6D. See Example 4.

᭿

c19-a.qxd

11/2/10

8:20 PM

Page 807

SEC. 19.2 Solution of Equations by Iteration

807

Summary of Methods. The methods for computing solutions s of f (x) ϭ 0 with given
continuous (or differentiable) f (x) start with an initial approximation x 0 of s and generate
a sequence x 1, x 2, Á by iteration. Fixed-point methods solve f (x) ϭ 0 written as
x ϭ g(x), so that s is a fixed point of g, that is, s ϭ g(s). For g(x) ϭ x Ϫ f (x)>f r(x) this is
Newton’s method, which, for good x 0 and simple zeros, converges quadratically (and for
multiple zeros linearly). From Newton’s method the secant method follows by replacing
f r(x) by a difference quotient. The bisection method and the method of false position in
Problem Set 19.2 always converge, but often slowly.

PROBLEM SET 19.2
1–13
FIXED-POINT ITERATION
Solve by fixed-point iteration and answer related
questions where indicated. Show details.
1. Monotone sequence. Why is the sequence in Example 1
monotone? Why not in Example 2?
2. Do the iterations (b) in Example 2. Sketch a figure
similar to Fig. 427. Explain what happens.
3. f ϭ x Ϫ 0.5 cos x ϭ 0, x 0 ϭ 1. Sketch a figure.
4. f ϭ x Ϫ cosec x the zero near x ϭ 1.
5. Sketch f (x) ϭ x 3 Ϫ 5.00x 2 ϩ 1.01x ϩ 1.88, showing
roots near Ϯ1 and 5. Write x ϭ g(x) ϭ (5.00x 2 Ϫ
1.01x ϩ 1.88)>x 2. Find a root by starting from x 0 ϭ
5, 4, 1, Ϫ1. Explain the (perhaps unexpected) results.
6. Find a form x ϭ g(x) of f (x) ϭ 0 in Prob. 5 that yields
convergence to the root near x ϭ 1.
7. Find the smallest positive solution of sin x ϭ e؊x.
8. Solve x 4 Ϫ x Ϫ 0.12 ϭ 0 by starting from x 0 ϭ 1.
9. Find the negative solution of x 4 Ϫ x Ϫ 0.12 ϭ 0.
10. Elasticity. Solve x cosh x ϭ 1. (Similar equations
appear in vibrations of beams; see Problem Set 12.3.)
11. Drumhead. Bessel functions. A partial sum of the
Maclaurin series of J0(x) (Sec. 5.5) is f (x) ϭ 1 Ϫ 14 x 2 ϩ
1 4
1
6
64 x Ϫ 2304 x . Conclude from a sketch that f (x) ϭ 0
near x ϭ 2. Write f (x) ϭ 0 as x ϭ g(x) (by dividing f (x)
by 14 x and taking the resulting x-term to the other side).
Find the zero. (See Sec. 12.10 for the importance of these
zeros.)
12. CAS EXPERIMENT. Convergence. Let f (x) ϭ x 3 ϩ
2x 2 Ϫ 3x Ϫ 4 ϭ 0. Write this as x ϭ g(x), for g choosing (1) (x 3 Ϫ f )1>3, (2) (x 2 Ϫ 12 f )1>2, (3) x ϩ 13 f,
(4) (x 3 Ϫ f )>x 2, (5) (2x 2 Ϫ f )>(2x), and (6) x Ϫ f>f r
and in each case x 0 ϭ 1.5. Find out about convergence
and divergence and the number of steps to reach 6Svalues of a root.
13. Existence of fixed point. Prove that if g is continuous
in a closed interval I and its range lies in I, then the
equation x ϭ g(x) has at least one solution in I. Illustrate
that it may have more than one solution in I.

14–23
NEWTON’S METHOD
Apply Newton’s method (6S-accuracy). First sketch the
function(s) to see what is going on.
14. Cube root. Design a Newton iteration. Compute
3
27, x 0 ϭ 2.
15. f ϭ 2x Ϫ cos x, x 0 ϭ 1. Compare with Prob. 3.
16. What happens in Prob. 15 for any other x 0?
17. Dependence on x0. Solve Prob. 5 by Newton’s method
with x 0 ϭ 5, 4, 1, Ϫ3. Explain the result.
18. Legendre polynomials. Find the largest root of
the Legendre polynomial P5 (x) given by P5 (x) ϭ
1
5
3
8 (63x Ϫ 70x ϩ 15x) (Sec. 5.3) (to be needed in
Gauss integration in Sec. 19.5) (a) by Newton’s
method, (b) from a quadratic equation.
19. Associated Legendre functions. Find the smallest posi4
2
tive zero of P 24 ϭ (1 Ϫ x 2)P4s ϭ 15
2 (Ϫ7x ϩ 8x Ϫ 1)
(Sec. 5.3) (a) by Newton’s method, (b) exactly, by
solving a quadratic equation.
20. x ϩ ln x ϭ 2, x 0 ϭ 2
21. f ϭ x 3 Ϫ 5x ϩ 3 ϭ 0, x 0 ϭ 2, 0, Ϫ2
22. Heating, cooling. At what time x (4S-accuracy only) will
the processes governed by f1(x) ϭ 100(1 Ϫ e؊0.2x) and
f2 (x) ϭ 40e؊0.01x reach the same temperature? Also
find the latter.
23. Vibrating beam. Find the solution of cos x cosh x ϭ 1
near x ϭ 32 p. (This determines a frequency of a
vibrating beam; see Problem Set 12.3.)
24. Method of False Position (Regula falsi). Figure 430
shows the idea. We assume that f is continuous. We
compute the x-intercept c0 of the line through
(a0, f (a0)), (b0, f (b0)). If f (c0) ϭ 0, we are done. If
f (a0) f (c0) Ͻ 0 (as in Fig. 430), we set a1 ϭ a0, b1 ϭ c0
and repeat to get c1, etc. If f (a0)f (c0) Ͼ 0, then
f (c0) f (b0) Ͻ 0 and we set a1 ϭ c0, b1 ϭ b0, etc.
(a) Algorithm. Show that
c0 ϭ

a0 f (b0) Ϫ b0 f (a0)

f (b0) Ϫ f (a0)
and write an algorithm for the method.

c19-a.qxd

11/2/10

8:20 PM

808

Page 808

CHAP. 19 Numerics in General
must be 0 somewhere on [a, b]. The solution is found
by repeated bisection of the interval and in each iteration
picking that half which also satisfies that sign condition.
(a) Algorithm. Write an algorithm for the method.
(b) Comparison. Solve x ϭ cos x by Newton’s method
and by bisection. Compare.
(c) Solve e؊x ϭ ln x and ex ϩ x 4 ϩ x ϭ 2 by bisection.

y

y = f(x)

a0
c1

c0

x

b0

26–29
Fig. 430. Method of false position

(b) Solve x 4 ϭ 2, cos x ϭ 1x, and x ϩ ln x ϭ 2, with
a ϭ 1, b ϭ 2.
25. TEAM PROJECT. Bisection Method. This simple but
slowly convergent method for finding a solution of
f (x) ϭ 0 with continuous f is based on the intermediate
value theorem, which states that if a continuous function
f has opposite signs at some x ϭ a and x ϭ b (Ͼ a), that
is, either f (a) Ͻ 0, f (b) Ͼ 0 or f (a) Ͼ 0, f (b) Ͻ 0, then f

19.3

SECANT METHOD

Solve, using x 0 and x 1 as indicated:
26.
27.
28.
29.
30.

e؊x Ϫ tan x ϭ 0, x 0 ϭ 1, x 1 ϭ 0.7
Prob. 21, x 0 ϭ 1.0, x 1 ϭ 2.0
x ϭ cos x, x 0 ϭ 0.5, x 1 ϭ 1
sin x ϭ cot x, x 0 ϭ 1, x 1 ϭ 0.5
WRITING PROJECT. Solution of Equations.
Compare the methods in this section and problem set,
discussing advantages and disadvantages in terms of
examples of your own. No proofs, just motivations and
ideas.

Interpolation
We are given the values of a function f (x) at different points x 0, x 1, Á , x n. We want to
find approximate values of the function f (x) for “new” x’s that lie between these points
for which the function values are given. This process is called interpolation. The student
should pay close attention to this section as interpolation forms the underlying foundation
for both Secs. 19.4 and 19.5. Indeed, interpolation allows us to develop formulas for
numeric integration and differentiation as shown in Sec. 19.5.
Continuing our discussion, we write these given values of a function f in the form
Á,
f0 ϭ f (x 0),
f1 ϭ f (x 1),
fn ϭ f (x n)
or as ordered pairs
(x 0, f0),

(x 1, f1),

Á , (x n, fn).

Where do these given function values come from? They may come from a “mathematical”
function, such as a logarithm or a Bessel function. More frequently, they may be measured
or automatically recorded values of an “empirical” function, such as air resistance of a
car or an airplane at different speeds. Other examples of functions that are “empirical”
are the yield of a chemical process at different temperatures or the size of the U.S.
population as it appears from censuses taken at 10-year intervals.
A standard idea in interpolation now is to find a polynomial pn (x) of degree n (or less)
that assumes the given values; thus
(1)

pn(x0) ϭ f0,

pn(x1) ϭ f1,

Á,

pn(xn) ϭ fn .

We call this pn an interpolation polynomial and x 0, Á , x n the nodes. And if f (x) is a
mathematical function, we call pn an approximation of f (or a polynomial approximation,
because there are other kinds of approximations, as we shall see later). We use pn to get
(approximate) values of f for x’s between x 0 and x n (“interpolation”) or sometimes outside
this interval x 0 Ϲ x Ϲ x n (“extrapolation”).

c19-a.qxd

11/2/10

8:20 PM

Page 809

SEC. 19.3 Interpolation

809

Motivation. Polynomials are convenient to work with because we can readily differentiate
and integrate them, again obtaining polynomials. Moreover, they approximate continuous
functions with any desired accuracy. That is, for any continuous f (x) on an interval
J: a Ϲ x Ϲ b and error bound b Ͼ 0, there is a polynomial pn (x) (of sufficiently high
degree n) such that
ƒ f (x) Ϫ pn(x) ƒ Ͻ b

for all x on J.

This is the famous Weierstrass approximation theorem (for a proof see Ref. [GenRef7],
App. 1).
Existence and Uniqueness. Note that the interpolation polynomial pn satisfying (1) for
given data exists and we shall give formulas for it below. Furthermore, pn is unique:
Indeed, if another polynomial qn also satisfies qn(x 0) ϭ f0, Á , qn(x n) ϭ fn, then
pn(x) Ϫ qn(x) ϭ 0 at x 0, Á , x n, but a polynomial pn Ϫ qn of degree n (or less) with n ϩ 1
roots must be identically zero, as we know from algebra; thus pn(x) ϭ qn(x) for all x, which
means uniqueness.
᭿
How Do We Find pn? We shall explain several standard methods that give us pn. By
the uniqueness proof above, we know that, for given data, the different methods must give
us the same polynomial. However, the polynomials may be expressed in different forms
suitable for different purposes.

Lagrange Interpolation
Given (x 0, f0), (x 1, f1), Á , (x n, fn) with arbitrarily spaced x j, Lagrange had the idea of
multiplying each fj by a polynomial that is 1 at x j and 0 at the other n nodes and then
taking the sum of these n ϩ 1 polynomials. Clearly, this gives the unique interpolation
polynomial of degree n or less. Beginning with the simplest case, let us see how this
works.
Linear interpolation is interpolation by the straight line through (x 0, f0), (x 1, f1); see
Fig. 431. Thus the linear Lagrange polynomial p1 is a sum p1 ϭ L 0 f0 ϩ L 1 f1 with L 0
the linear polynomial that is 1 at x 0 and 0 at x 1; similarly, L 1 is 0 at x 0 and 1 at x 1.
Obviously,
xϪx
L 0(x) ϭ x Ϫ x1 ,
0
1

xϪx
L 1(x) ϭ x Ϫ x0 .
1
0

This gives the linear Lagrange polynomial
(2)

x Ϫ x1
x Ϫ x0
p1(x) ϭ L 0(x) f0 ϩ L 1(x) f1 ϭ x Ϫ x ؒ f0 ϩ x Ϫ x ؒ f1 .
0
1
1
0
y

Error

p1(x)

f0
x0

x

f1

y = f(x)

x1

Fig. 431. Linear Interpolation

x

c19-a.qxd

11/2/10

8:20 PM

810
EXAMPLE 1

Page 810

CHAP. 19 Numerics in General
Linear Lagrange Interpolation
Compute a 4D-value of ln 9.2 from ln 9.0 ϭ 2.1972, ln 9.5 ϭ 2.2513 by linear Lagrange interpolation and
determine the error, using ln 9.2 ϭ 2.2192 (4D).

Solution. x 0 ϭ 9.0, x 1 ϭ 9.5, f0 ϭ ln 9.0, f1 ϭ ln 9.5. Ln (2) we need
L 0(x) ϭ

x Ϫ 9.5
ϭ Ϫ2.0(x Ϫ 9.5),
Ϫ0.5

L 1(x) ϭ

x Ϫ 9.0
ϭ 2.0(x Ϫ 9.0),
0.5

L 0(9.2) ϭ Ϫ2.0(Ϫ0.3) ϭ 0.6
L 1(9.2) ϭ 2 ؒ 0.2 ϭ 0.4

(see Fig. 432) and obtain the answer
ln 9.2 Ϸ p1(9.2) ϭ L 0(9.2) f0 ϩ L 1(9.2) f1 ϭ 0.6 ؒ 2.1972 ϩ 0.4 ؒ 2.2513 ϭ 2.2188.
The error is P ϭ a Ϫ aෂ ϭ 2.2192 Ϫ 2.2188 ϭ 0.0004. Hence linear interpolation is not sufficient here to get
4D accuracy; it would suffice for 3D accuracy.
᭿
y

L0

L1

1

0

9 9.2 9.5

10

11

x

Fig. 432. L0 and L1 in Example 1

Quadratic interpolation is interpolation of given (x 0, f0), (x 1, f1), (x 2, f2) by a seconddegree polynomial p2(x), which by Lagrange’s idea is
p2(x) ϭ L 0(x)f0 ϩ L 1(x)f1 ϩ L 2(x)f2

(3a)

with L 0(x 0) ϭ 1, L 1(x 1) ϭ 1, L 2(x 2) ϭ 1, and L 0(x 1) ϭ L 0(x 2) ϭ 0, etc. We claim that

(3b)

L 0(x) ϭ

l 0(x)
(x Ϫ x 1)(x Ϫ x 2)
ϭ
l 0(x 0)
(x 0 Ϫ x 1)(x 0 Ϫ x 2)

L 1(x) ϭ

l 1(x)
(x Ϫ x 0)(x Ϫ x 2)
ϭ
l 1(x 1)
(x 1 Ϫ x 0)(x 1 Ϫ x 2)

L 2(x) ϭ

l 2(x)
(x Ϫ x 0)(x Ϫ x 1)
ϭ
.
l 2(x 2)
(x 2 Ϫ x 0)(x 2 Ϫ x 1)

How did we get this? Well, the numerator makes L k(x j) ϭ 0 if j
makes L k (x k) ϭ 1 because it equals the numerator at x ϭ x k .
EXAMPLE 2

k. And the denominator

Quadratic Lagrange Interpolation
Compute ln 9.2 by (3) from the data in Example 1 and the additional third value ln 11.0 ϭ 2.3979.

Solution. In (3),
L 0(x) ϭ

(x Ϫ 9.5)(x Ϫ 11.0)
ϭ x 2 Ϫ 20.5x ϩ 104.5,
(9.0 Ϫ 9.5)(9.0 Ϫ 11.0)

L 1(x) ϭ

1
(x Ϫ 9.0)(x Ϫ 11.0)
ϭϪ
(x 2 Ϫ 20x ϩ 99),
(9.5 Ϫ 9.0)(9.5 Ϫ 11.0)
0.75

L 1(9.2) ϭ 0.4800,

L 2(x) ϭ

1
(x Ϫ 9.0)(x Ϫ 9.5)
ϭ (x 2 Ϫ 18.5x ϩ 85.5),
(11.0 Ϫ 9.0)(11.0 Ϫ 9.5)
3

L 2(9.2) ϭ Ϫ0.0200,

L 0(9.2) ϭ 0.5400,

c19-a.qxd

11/2/10

8:20 PM

Page 811

SEC. 19.3 Interpolation

811

(see Fig. 433), so that (3a) gives, exact to 4D,
ln 9.2 Ϸ p2(9.2) ϭ 0.5400 ؒ 2.1972 ϩ 0.4800 ؒ 2.2513 Ϫ 0.0200 ؒ 2.3979 ϭ 2.2192 .
y

L0

᭿

L2

1

0

9

9.5

10

11

x

L1

Fig. 433. L0, L1, L2 in Example 2

General Lagrange Interpolation Polynomial.

(4a)

For general n we obtain

n
n
l k(x)
f (x) Ϸ pn(x) ϭ a L k(x) fk ϭ a
fk
l
k (x k)
kϭ0
kϭ0

where L k(x k) ϭ 1 and L k is 0 at the other nodes, and the L k are independent of the function
f to be interpolated. We get (4a) if we take
l 0(x) ϭ (x Ϫ x 1)(x Ϫ x 2) Á (x Ϫ x n),
(4b)

l k(x) ϭ (x Ϫ x 0) Á (x Ϫ x k؊1)(x Ϫ x kϩ1) Á (x Ϫ x n),

0 Ͻ k Ͻ n,

l n(x) ϭ (x Ϫ x 0)(x Ϫ x 1) Á (x Ϫ x n؊1).
We can easily see that pn (x k) ϭ fk. Indeed, inspection of (4b) shows that l k (x j) ϭ 0 if
j k, so that for x ϭ x k, the sum in (4a) reduces to the single term (l k(x k)>l k(x k)) fk ϭ fk.
Error Estimate. If f is itself a polynomial of degree n (or less), it must coincide
with pn because the n ϩ 1 data (x 0, f0), Á , (x n, fn) determine a polynomial uniquely,
so the error is zero. Now the special f has its (n ϩ 1)st derivative identically zero. This
makes it plausible that for a general f its (n ϩ 1)st derivative f (nϩ1) should measure the
error
Pn(x) ϭ f (x) Ϫ pn(x).
It can be shown that this is true if f (nϩ1) exists and is continuous. Then, with a suitable
t between x 0 and x n (or between x 0, x n, and x if we extrapolate),

(5)

Pn(x) ϭ f (x) Ϫ pn(x) ϭ (x Ϫ x 0)(x Ϫ x 1) Á (x Ϫ x n)

f (nϩ1)(t)
(n ϩ 1)!

.

Thus ƒ Pn(x) ƒ is 0 at the nodes and small near them, because of continuity. The product
(x Ϫ x 0) Á (x Ϫ x n) is large for x away from the nodes. This makes extrapolation risky.
And interpolation at an x will be best if we choose nodes on both sides of that x. Also,
we get error bounds by taking the smallest and the largest value of f (nϩ1)(t) in (5) on the
interval x 0 Ϲ t Ϲ x n (or on the interval also containing x if we extrapolate).

c19-a.qxd

11/2/10

8:20 PM

812

Page 812

CHAP. 19 Numerics in General

Most importantly, since pn is unique, as we have shown, we have
THEOREM 1

Error of Interpolation

Formula (5) gives the error for any polynomial interpolation method if f (x) has a
continuous (n ϩ 1)st derivative.

Practical error estimate. If the derivative in (5) is difficult or impossible to obtain, apply
the Error Principle (Sec. 19.1), that is, take another node and the Lagrange polynomial
pnϩ1(x) and regard pnϩ1(x) Ϫ pn(x) as a (crude) error estimate for pn(x).
EXAMPLE 3

Error Estimate (5) of Linear Interpolation. Damage by Roundoff. Error Principle
Estimate the error in Example 1 first by (5) directly and then by the Error Principle (Sec. 19.1).

Solution. (A) Estimation by (5). We have n ϭ 1, f (t) ϭ ln t, f r(t) ϭ 1>t, f s(t) ϭ Ϫ1>t 2. Hence
P1(x) ϭ (x Ϫ 9.0)(x Ϫ 9.5)

(Ϫ1)
2t 2

,

thus

P1(9.2) ϭ

0.03
t2

.

t ϭ 0.9 gives the maximum 0.03>92 ϭ 0.00037 and t ϭ 9.5 gives the minimum 0.03>9.52 ϭ 0.00033, so that
we get 0.00033 Ϲ P1 (9.2) Ϲ 0.00037, or better, 0.00038 because 0.3>81 ϭ 0.003703 Á .
But the error 0.0004 in Example 1 disagrees, and we can learn something! Repetition of the computation there
with 5D instead of 4D gives
ln 9.2 Ϸ p1(9.2) ϭ 0.6 ؒ 2.19722 ϩ 0.4 ؒ 2.25129 ϭ 2.21885
with an actual error P ϭ 2.21920 Ϫ 2.21885 ϭ 0.00035, which lies nicely near the middle between our two
error bounds.
This shows that the discrepancy (0.0004 vs. 0.00035) was caused by rounding, which is not taken into account
in (5).
(B) Estimation by the Error Principle. We calculate p1(9.2) ϭ 2.21885 as before and then p2(9.2) as in
Example 2 but with 5D, obtaining
p2(9.2) ϭ 0.54 ؒ 2.19722 ϩ 0.48 ؒ 2.25129 Ϫ 0.02 ؒ 2.39790 ϭ 2.21916.
The difference p2(9.2) Ϫ p1(9.2) ϭ 0.00031 is the approximate error of p1(9.2) that we wanted to obtain; this
᭿
is an approximation of the actual error 0.00035 given above.

Newton’s Divided Difference Interpolation
For given data (x 0, f0), Á , (x n, fn) the interpolation polynomial pn(x) satisfying (1) is
unique, as we have shown. But for different purposes we may use pn(x) in different forms.
Lagrange’s form just discussed is useful for deriving formulas in numeric differentiation
(approximation formulas for derivatives) and integration (Sec. 19.5).
Practically more important are Newton’s forms of pn(x), which we shall also use for solving
ODEs (in Sec. 21.2). They involve fewer arithmetic operations than Lagrange’s form.
Moreover, it often happens that we have to increase the degree n to reach a required accuracy.
Then in Newton’s forms we can use all the previous work and just add another term, a
possibility without counterpart for Lagrange’s form. This also simplifies the application of
the Error Principle (used in Example 3 for Lagrange). The details of these ideas are as follows.
Let pn؊1(x) be the (n Ϫ 1)st Newton polynomial (whose form we shall determine);
thus pn؊1(x 0) ϭ f0, pn؊1(x 1) ϭ f1, Á , pn؊1(x n؊1) ϭ fn؊1. Furthermore, let us write the
nth Newton polynomial as
(6)

pn(x) ϭ pn؊1(x) ϩ gn(x);

c19-a.qxd

11/2/10

8:20 PM

Page 813

SEC. 19.3 Interpolation

813

hence
gn(x) ϭ pn(x) Ϫ pn؊1(x).

(6 r )

Here gn(x) is to be determined so that pn(x 0) ϭ f0, pn(x 1) ϭ f1, Á , pn(x n) ϭ fn.
Since pn and pn؊1 agree at x 0, Á , x n؊1, we see that gn is zero there. Also, gn will
generally be a polynomial of nth degree because so is pn, whereas pn؊1 can be of degree
n Ϫ 1 at most. Hence gn must be of the form
gn(x) ϭ an(x Ϫ x 0)(x Ϫ x 1) Á (x Ϫ x n؊1).

(6 s )

We determine the constant an. For this we set x ϭ x n and solve (6 s ) algebraically for an.
Replacing gn(x n) according to (6 r ) and using pn(x n) ϭ fn, we see that this gives
an ϭ

(7)

fn Ϫ pn؊1(x n)
.
(x n Ϫ x 0)(x n Ϫ x 1) Á (x n Ϫ x n؊1)

We write ak instead of an and show that ak equals the kth divided difference, recursively
denoted and defined as follows:
f1 Ϫ f0
a1 ϭ f [x 0, x 1] ϭ x Ϫ x
1
0
a2 ϭ f [x 0, x 1, x 2] ϭ

f [x 1, x 2] Ϫ f [x 0, x 1]
x2 Ϫ x0

and in general
ak ϭ f [x 0, Á , x k] ϭ

(8)

f [x 1, Á , x k] Ϫ f [x 0, Á , x k؊1]
.
xk Ϫ x0

If n ϭ 1, then pn؊1(x n) ϭ p0(x 1) ϭ f0 because p0(x) is constant and equal to f0, the value
of f (x) at x 0. Hence (7) gives
a1 ϭ

f1 Ϫ p0(x 1)
f1 Ϫ f0
x 1 Ϫ x 0 ϭ x 1 Ϫ x 0 ϭ f [x 0, x 1],

and (6) and (6 s ) give the Newton interpolation polynomial of the first degree
p1(x) ϭ f0 ϩ (x Ϫ x 0) f [x 0, x 1].
If n ϭ 2, then this p1 and (7) give
a2 ϭ

f2 Ϫ p1(x 2)
f2 Ϫ f0 Ϫ (x 2 Ϫ x 0) f [x 0, x 1]
ϭ
ϭ f [x 0, x 1, x 2]
(x 2 Ϫ x 0)(x 2 Ϫ x 1)
(x 2 Ϫ x 0)(x 2 Ϫ x 1)

where the last equality follows by straightforward calculation and comparison with the
definition of the right side. (Verify it; be patient.) From (6) and (6 s ) we thus obtain the
second Newton polynomial

c19-a.qxd

11/2/10

814

8:20 PM

Page 814

CHAP. 19 Numerics in General

p2(x) ϭ f0 ϩ (x Ϫ x 0) f [x 0, x 1] ϩ (x Ϫ x 0)(x Ϫ x 1) f [x 0, x 1, x 2].
For n ϭ k, formula (6) gives
pk(x) ϭ pk؊1(x) ϩ (x Ϫ x 0)(x Ϫ x 1) Á (x Ϫ x k؊1) f [x 0, Á , x k].

(9)

With p0(x) ϭ f0 by repeated application with k ϭ 1, Á , n this finally gives Newton’s
divided difference interpolation formula
(10)

f (x) Ϸ f0 ϩ (x Ϫ x 0) f [x 0, x 1] ϩ (x Ϫ x 0)(x Ϫ x 1) f [x 0, x 1, x 2]
ϩ Á ϩ (x Ϫ x 0)(x Ϫ x 1) Á (x Ϫ x n؊1) f [x 0, Á , x n].

An algorithm is shown in Table 19.2. The first do-loop computes the divided differences
ˆ.
and the second the desired value pn(x)
Example 4 shows how to arrange differences near the values from which they are
obtained; the latter always stand a half-line above and a half-line below in the preceding
column. Such an arrangement is called a (divided) difference table.

Table 19.2 Newton’s Divided Difference Interpolation

ALGORITHM INTERPOL (x0, Á , xn; ƒ0, Á , ƒn; xˆ)
This algorithm computes an approximation pn(xˆ) of ƒ(xˆ) at xˆ.
INPUT: Data (x0, ƒ0), (x1, ƒ1), Á , (xn, ƒn); xˆ
OUTPUT: Approximation pn(xˆ) of ƒ(xˆ)
Set ƒ[xj] ϭ ƒj ( j ϭ 0, Á , n).
For m ϭ 1, Á , n Ϫ 1) do:
For j ϭ 0, Á , n Ϫ m do:

f [x j, Á , x jϩm] ϭ

f [x jϩ1, Á , x jϩm] Ϫ f [x j, Á , x jϩm؊1]
x jϩm Ϫ x j

End
End
Set p0(x) ϭ ƒ0.
For k ϭ 1, Á , n do:

pk(xˆ) ϭ pk؊1(xˆ) ϩ (xˆ Ϫ x0) Á (xˆ Ϫ xk؊1)ƒ[x0, Á , xk]
End
OUTPUT pn(xˆ)
End INTERPOL

c19-a.qxd

11/2/10

8:20 PM

Page 815

SEC. 19.3 Interpolation
EXAMPLE 4

815

Newton’s Divided Difference Interpolation Formula
Compute f (9.2) from the values shown in the first two columns of the following table.

xj

ƒj ϭ ƒ(xj)

8.0

2.079442

9.0

2.197225

ƒ[xj, xjϩ1]

ƒ[xj, xjϩ1, xjϩ2]

ƒ[xj, • • • , xjϩ3]

0.117783
Ϫ0.006433
0.108134
9.5

0.000411
Ϫ0.005200

2.251292
0.097735

11.0

2.397895

Solution. We compute the divided differences as shown. Sample computation:
(0.097735 Ϫ 0.108134)>(11 Ϫ 9) ϭ Ϫ0.005200.
The values we need in (10) are circled. We have
f (x) Ϸ p3(x) ϭ 2.079442 ϩ 0.117783(x Ϫ 8.0) Ϫ 0.006433(x Ϫ 8.0)(x Ϫ 9.0)
ϩ 0.000411(x Ϫ 8.0)(x Ϫ 9.0)(x Ϫ 9.5).
At x ϭ 9.2,
f (9.2) Ϸ 2.079442 ϩ 0.141340 Ϫ 0.001544 Ϫ 0.000030 ϭ 2.219208.
The value exact to 6D is f (9.2) ϭ ln 9.2 ϭ 2.219203. Note that we can nicely see how the accuracy increases
from term to term:
p1(9.2) ϭ 2.220782,

p2(9.2) ϭ 2.219238,

᭿

p3(9.2) ϭ 2.219208.

Equal Spacing: Newton’s Forward Difference Formula
Newton’s formula (10) is valid for arbitrarily spaced nodes as they may occur in practice in
experiments or observations. However, in many applications the x j’s are regularly spaced—
for instance, in measurements taken at regular intervals of time. Then, denoting the distance
by h, we can write
(11)

x 0,

x 1 ϭ x 0 ϩ h, x 2 ϭ x 0 ϩ 2h,

Á , x n ϭ x 0 ϩ nh.

We show how (8) and (10) now simplify considerably!
To get started, let us define the first forward difference of f at x j by
¢fj ϭ fjϩ1 Ϫ fj,
the second forward difference of f at x j by
¢ 2fj ϭ ¢fjϩ1 Ϫ ¢fj,
and, continuing in this way, the kth forward difference of f at x j by
(12)

¢ kfj ϭ ¢ k؊1fjϩ1 Ϫ ¢ k؊1fj

(k ϭ 1, 2, Á ).

c19-a.qxd

11/2/10

8:20 PM

816

Page 816

CHAP. 19 Numerics in General

Examples and an explanation of the name “forward” follow on the next page. What is the
point of this? We show that if we have regular spacing (11), then
f [x 0, Á , x k] ϭ

(13)
PROOF

1
k

k!h

¢ kf0.

We prove (13) by induction. It is true for k ϭ 1 because x 1 ϭ x 0 ϩ h, so that
f [x 0, x 1] ϭ

f1 Ϫ f0
x1 Ϫ x0

ϭ

1
1
( f1 Ϫ f0) ϭ
¢f0 .
h
1!h

Assuming (13) to be true for all forward differences of order k, we show that (13) holds for
k ϩ 1. We use (8) with k ϩ 1 instead of k; then we use (k ϩ 1)h ϭ x kϩ1 Ϫ x 0, resulting
from (11), and finally (12) with j ϭ 0, that is, ¢ kϩ1f0 ϭ ¢ kf1 Ϫ ¢ kf0. This gives
f [x 0, Á , x kϩ1] ϭ
ϭ
ϭ

f [x 1, Á , x kϩ1] Ϫ f [x 0, Á , x k]
(k ϩ 1)h
1
1
1
¢ kf1 Ϫ
¢ kf0 d
c
(k ϩ 1)h k!hk
k!hk
1
(k ϩ 1)!hkϩ1

¢ kϩ1 f0

which is (13) with k ϩ 1 instead of k. Formula (13) is proved.

᭿

In (10) we finally set x ϭ x 0 ϩ rh. Then x Ϫ x 0 ϭ rh, x Ϫ x 1 ϭ (r Ϫ 1)h since
x 1 Ϫ x 0 ϭ h, and so on. With this and (13), formula (10) becomes Newton’s (or
Gregory2–Newton’s) forward difference interpolation formula

(14)

n
r
f (x) Ϸ pn(x) ϭ a a b¢ s f0
s
sϭ0

ϭ f0 ϩ r¢f0 ϩ

(x ϭ x 0 ϩ rh, r ϭ (x Ϫ x 0)>h)

r (r Ϫ 1) 2
r (r Ϫ 1) Á (r Ϫ n ϩ 1) n
¢ f0 ϩ Á ϩ
¢ f0
2!
n!

where the binomial coefficients in the first line are defined by
(15)

r
r
r (r Ϫ 1)(r Ϫ 2) Á (r Ϫ s ϩ 1)
a b ϭ 1, a b ϭ
s!
0
s

(s Ͼ 0, integer)

and s! ϭ 1 ؒ 2 Á s.
Error.
(16)

From (5) we get, with x Ϫ x 0 ϭ rh, x Ϫ x 1 ϭ (r Ϫ 1)h, etc.,
Pn(x) ϭ f (x) Ϫ pn(x) ϭ

hnϩ1
r (r Ϫ 1) Á (r Ϫ n) f (nϩ1)(t)
(n ϩ 1)!
˛

with t as characterized in (5).
2
JAMES GREGORY (1638–1675), Scots mathematician, professor at St. Andrews and Edinburgh. ⌬ in (14)
and ٌ2 (on p. 818) have nothing to do with the Laplacian.

c19-a.qxd

11/2/10

8:20 PM

Page 817

SEC. 19.3 Interpolation

817

Formula (16) is an exact formula for the error, but it involves the unknown t. In
Example 5 (below) we show how to use (16) for obtaining an error estimate and an
interval in which the true value of f (x) must lie.
Comments on Accuracy. (A) The order of magnitude of the error Pn(x) is about equal
to that of the next difference not used in pn(x).
(B) One should choose x 0, Á , x n such that the x at which one interpolates is as well
centered between x 0, Á , x n as possible.
The reason for (A) is that in (16),
f nϩ1(t) Ϸ

¢ nϩ1 f (t)
nϩ1

ƒ r (r Ϫ 1) Á (r Ϫ n) ƒ
Ϲ 1
1 ؒ 2 Á (n ϩ 1)

,

h

if

ƒrƒ Ϲ 1

(and actually for any r as long as we do not extrapolate). The reason for (B) is that
ƒ r (r Ϫ 1) Á (r Ϫ n) ƒ becomes smallest for that choice.
EXAMPLE 5

Newton’s Forward Difference Formula. Error Estimation
Compute cosh 0.56 from (14) and the four values in the following table and estimate the error.

j

xj

ƒj ϭ cosh xj

0

0.5

1.127626

1

0.6

1.185465

⌬ƒj

⌬2ƒj

⌬3ƒj

0.057839
0.011865
0.069704
2

0.7

1.255169

0.000697
0.012562

0.082266
3

0.8

1.337435

Solution. We compute the forward differences as shown in the table. The values we need are circled. In (14)
we have r ϭ (0.56 Ϫ 0.50)>0.1 ϭ 0.6, so that (14) gives
cosh 0.56 Ϸ 1.127626 ϩ 0.6 ؒ 0.057839 ϩ

0.6(Ϫ0.4)
0.6(Ϫ0.4)(Ϫ1.4)
ؒ 0.011865 ϩ
ؒ 0.000697
2
6

ϭ 1.127626 ϩ 0.034703 Ϫ 0.001424 ϩ 0.000039
ϭ 1.160944.

Error estimate. From (16), since the fourth derivative is cosh(4) t ϭ cosh t,
P3(0.56) ϭ

0.14
4!

ؒ 0.6 (Ϫ0.4)(Ϫ1.4)(Ϫ2.4) cosh t

ϭ A cosh t,
where A ϭ Ϫ0.00000336 and 0.5 Ϲ t Ϲ 0.8. We do not know t, but we get an inequality by taking the largest
and smallest cosh t in that interval:
A cosh 0.8 Ϲ P3(0.62) Ϲ A cosh 0.5.
Since
f (x) ϭ p3(x) ϩ P3(x),

c19-a.qxd

11/2/10

8:20 PM

818

Page 818

CHAP. 19 Numerics in General
this gives
p3(0.56) ϩ A cosh 0.8 Ϲ cosh 0.56 Ϲ p3(0.56) ϩ A cosh 0.5.
Numeric values are
1.160939 Ϲ cosh 0.56 Ϲ 1.160941.
The exact 6D-value is cosh 0.56 ϭ 1.160941. It lies within these bounds. Such bounds are not always so tight.
Also, we did not consider roundoff errors, which will depend on the number of operations.
᭿

This example also explains the name “forward difference formula”: we see that the
differences in the formula slope forward in the difference table.

Equal Spacing: Newton’s Backward Difference Formula
Instead of forward-sloping differences we may also employ backward-sloping differences. The difference table remains the same as before (same numbers, in the same
positions), except for a very harmless change of the running subscript j (which we explain
in Example 6, below). Nevertheless, purely for reasons of convenience it is standard to
introduce a second name and notation for differences as follows. We define the first
backward difference of f at x j by
ٌfj ϭ fj Ϫ f j؊1,
the second backward difference of f at x j by
ٌ2fj ϭ ٌ fj Ϫ ٌ fj؊1,
and, continuing in this way, the kth backward difference of f at x j by
ٌkfj ϭ ٌk؊1 fj Ϫ ٌk؊1 fj؊1

(17)

(k ϭ 1, 2, Á ).

A formula similar to (14) but involving backward differences is Newton’s (or
Gregory–Newton’s) backward difference interpolation formula

n

f (x) Ϸ pn(x) ϭ a a
(18)

sϭ0

ϭ f0 ϩ rٌf0 ϩ

EXAMPLE 6

rϩsϪ1 s
b ٌ f0
s

(x ϭ x 0 ϩ rh, r ϭ (x Ϫ x 0)>h)

r(r ϩ 1) 2
r(r ϩ 1) Á (r ϩ n Ϫ 1) n
ٌ f0 ϩ Á ϩ
ٌ f0 .
2!
n!

Newton’s Forward and Backward Interpolations
Compute a 7D-value of the Bessel function J0(x) for x ϭ 1.72 from the four values in the following table, using
(a) Newton’s forward formula (14), (b) Newton’s backward formula (18).

c19-a.qxd

11/2/10

8:20 PM

Page 819

SEC. 19.3 Interpolation

819

jfor

jback

xj

J0(xj)

0

Ϫ3

1.7

0.3979849

1

Ϫ2

1.8

0.3399864

1st Diff.

2nd Diff.

3rd Diff.

Ϫ0.0579985
Ϫ0.0001693
Ϫ0.0581678
2

Ϫ1

1.9

0.2818186

3

0

2.0

0.2238908

0.0004093
0.0002400

Ϫ0.0579278

Solution. The computation of the differences is the same in both cases. Only their notation differs.
(a) Forward. In (14) we have r ϭ (1.72 Ϫ 1.70)>0.1 ϭ 0.2, and j goes from 0 to 3 (see first column). In
each column we need the first given number, and (14) thus gives
0.2(Ϫ0.8)
0.2(Ϫ0.8)(Ϫ1.8)
(Ϫ0.0001693) ϩ
ؒ 0.0004093
2
6
ϭ 0.3979849 Ϫ 0.0115997 ϩ 0.0000135 ϩ 0.0000196 ϭ 0.3864183,

J0 (1.72) Ϸ 0.3979849 ϩ 0.2 (Ϫ0.0579985) ϩ

which is exact to 6D, the exact 7D-value being 0.3864185.
(b) Backward. For (18) we use j shown in the second column, and in each column the last number. Since
r ϭ (1.72 Ϫ 2.00)>0.1 ϭ Ϫ2.8, we thus get from (18)
J0(1.72) Ϸ 0.2238908 Ϫ 2.8 (Ϫ0.0579278) ϩ

Ϫ2.8 (Ϫ1.8)
Ϫ2.8(Ϫ1.8)(Ϫ0.8)
ؒ 0.0002400 ϩ
ؒ 0.0004093
2
6

ϭ 0.2238908 ϩ 0.1621978 ϩ 0.0006048 Ϫ 0.0002750
ϭ 0.3864184.

᭿

There is a third notation for differences, called the central difference notation. It
is used in numerics for ODEs and certain interpolation formulas. See Ref. [E5] listed in
App. 1.

PROBLEM SET 19.3
1. Linear interpolation. Calculate p1(x) in Example 1
and from it ln 9.3.
2. Error estimate. Estimate the error in Prob. 1 by (5).
3. Quadratic interpolation. Gamma function. Calculate
the Lagrange polynomial p2(x) for the values
⌫(1.00) ϭ 1.0000, ⌫(1.02) ϭ 0.9888, ⌫(1.04) ϭ 0.9784
of the gamma function [(24) in App. A3.1] and from it
approximations of ⌫(1.01) and ⌫(1.03).
4. Error estimate for quadratic interpolation. Estimate
the error for p2(9.2) in Example 2 from (5).
5. Linear and quadratic interpolation. Find e؊0.25 and
e؊0.75 by linear interpolation of e؊x with x 0 ϭ 0,
x 1 ϭ 0.5 and x 0 ϭ 0.5, x 1 ϭ 1, respectively. Then find
p2(x) by quadratic interpolation of e؊x with x 0 ϭ 0,
x 1 ϭ 0.5, x 2 ϭ 1 and from it e؊0.25 and e؊0.75.
Compare the errors. Use 4S-values of e؊x.

6. Interpolation and extrapolation. Calculate p2(x) in
Example 2. Compute from it approximations of
ln 9.4, ln 10, ln 10.5, ln 11.5, and ln 12. Compute the
errors by using exact 5S-values and comment.
7. Interpolation and extrapolation. Find the quadratic
polynomial that agrees with sin x at x ϭ 0, p>4, p>2
and use it for the interpolation and extrapolation of sin x
at x ϭ Ϫp>8, p>8, 3p>8, 5p>8. Compute the errors.
8. Extrapolation. Does a sketch of the product of the
(x Ϫ x j) in (5) for the data in Example 2 indicate that
extrapolation is likely to involve larger errors than
interpolation does?
9. Error function (35) in App. A3.1. Calculate the
Lagrange polynomial p2(x) for the 5S-values f (0.25) ϭ
0.27633, f (0.5) ϭ 0.52050, f (1.0) ϭ 0.84270 and from
p2(x) an approximation of f (0.75) (ϭ 0.71116).

c19-a.qxd

11/2/10

8:20 PM

820

Page 820

CHAP. 19 Numerics in General

10. Error bound. Derive an error bound in Prob. 9 from (5).
11. Cubic Lagrange interpolation. Bessel function J0.
Calculate and graph L 0, L 1, L 2, L 3 with x 0 ϭ 0,
x 1 ϭ 1, x 2 ϭ 2, x 3 ϭ 3 on common axes. Find p3(x)
for the data (0, 1), (1, 0.765198), (2, 0.223891),
(3, Ϫ0.260052) [values of the Bessel function J0(x)].
Find p3 for x ϭ 0.5, 1.5, 2.5 and compare with the 6Sexact values 0.938470, 0.511828, Ϫ0.048384.
12. Newton’s forward formula (14). Sine integral. Using
(14), find f (1.25) by linear, quadratic, and cubic
interpolation of the data (values of (40) in App. A31); 6Svalue Si(1.25) ϭ 1.14645) f (1.0) ϭ 0.94608, f (1.5) ϭ
1.32468, f (2.0) ϭ 1.60541, f (2.5) ϭ 1.77852, and compute the errors. For the linear interpolation use f (1.0)
and f (1.5), for the quadratic f (1.0), f (1.5), f (2.0), etc.
13 Lower degree. Find the degree of the interpolation
polynomial for the data (Ϫ4, 50), (Ϫ2, 18), (0, 2), (2, 2),
(4, 18), using a difference table. Find the polynomial.
14. Newton’s forward formula (14). Gamma function.
Set up (14) for the data in Prob. 3 and compute ⌫(1.01),
⌫(1.03), ⌫(1.05).
15. Divided differences. Obtain p2 in Example 2 from (10).
16. Divided differences. Error function. Compute p2(0.75)
from the data in Prob. 9 and Newton’s divided difference
formula (10).
17. Backward difference formula (18). Use p2(x) in (18)
and the values of erf x, x ϭ 0.2, 0.4, 0.6 in Table A4 of
App. 5, compute erf 0.3 and the error. (4S-exact erf 0.3 ϭ
0.3286).

19.4

18. In Example 5 of the text, write down the difference table
as needed for (18), then write (18) with general x and
then with x ϭ 0.56 to verify the answer in Example 5.
19. CAS EXPERIMENT. Adding Terms in Newton
Formulas. Write a program for the forward formula
(14). Experiment on the increase of accuracy by
successively adding terms. As data use values of some
function of your choice for which your CAS gives the
values needed in determining errors.
20. TEAM PROJECT. Interpolation and Extrapolation.
(a) Lagrange practical error estimate (after Theorem 1). Apply this to p1(9.2) and p2(9.2) for the data
x 0 ϭ 9.0, x 1 ϭ 9.5, x 2 ϭ 11.0, f0 ϭ ln x 0, f1 ϭ ln x 1,
f2 ϭ ln x 2 (6S-values).
(b) Extrapolation. Given (x j, f (x j)) ϭ (0.2, 0.9980),
(0.4, 0.9686), (0.6, 0.8443), (0.8, 0.5358), (1.0, 0). Find
f (0.7) from the quadratic interpolation polynomials
based on (a) 0.6, 0.8, 1.0, (b) 0.4, 0.6, 0.8, (g) 0.2, 0.4,
0.6. Compare the errors and comment. [Exact f (x) ϭ
cos (12 px 2), f (0.7) ϭ 0.7181 (4S).]
(c) Graph the product of factors (x Ϫ x j) in the error
formula (5) for n ϭ 2, Á , 10 separately. What do
these graphs show regarding accuracy of interpolation
and extrapolation?
21. WRITING PROJECT. Comparison of interpolation
methods. List 4–5 ideas that you feel are most important
in this section. Arrange them in best logical order.
Discuss them in a 2–3 page report.

Spline Interpolation
Given data (function values, points in the xy-plane) (x 0, f0), (x 1, f1), Á , (x n, fn) can be
interpolated by a polynomial Pn(x) of degree n or less so that the curve of Pn(x) passes
through these n ϩ 1 points (x j, fj); here f0 ϭ f (x 0), Á , fn ϭ f (x n), See Sec. 19.3.
Now if n is large, there may be trouble: Pn(x) may tend to oscillate for x between the nodes
x 0, Á , x n. Hence we must be prepared for numeric instability (Sec. 19.1). Figure 434 shows
a famous example by C. Runge3 for which the maximum error even approaches ϱ as n : ϱ
(with the nodes kept equidistant and their number increased). Figure 435 illustrates the increase
of the oscillation with n for some other function that is piecewise linear.
Those undesirable oscillations are avoided by the method of splines initiated by I. J.
Schoenberg in 1946 (Quarterly of Applied Mathematics 4, pp. 45–99, 112–141). This
method is widely used in practice. It also laid the foundation for much of modern CAD
(computer-aided design). Its name is borrowed from a draftman’s spline, which is an
elastic rod bent to pass through given points and held in place by weights. The mathematical
idea of the method is as follows:
3

CARL RUNGE (1856–1927), German mathematician, also known for his work on ODEs (Sec. 21.1).

c19-b.qxd

11/2/10

8:33 PM

Page 821

SEC. 19.4 Spline Interpolation

821
y

P10(x)
f(x)

–5

0

5

x

Fig. 434. Runge’s example ƒ(x) ϭ 1/(1 ϩ x 2) and interpolating polynomial P10(x)

–4

f(x)

4

–4

4

–4

4

P2(x)

P4(x)

P8(x)

Fig. 435. Piecewise linear function ƒ(x) and interpolation polynomials of increasing degrees

Instead of using a single high-degree polynomial Pn over the entire interval a Ϲ x Ϲ b
in which the nodes lie, that is,
a ϭ x 0 Ͻ x 1 Ͻ Á Ͻ x n ϭ b,

(1)

we use n low-degree, e.g., cubic, polynomials
q0(x),

q1(x),

Á,

qn؊1(x),

one over each subinterval between adjacent nodes, hence q0 from x 0 to x 1, then q1 from
x 1 to x 2, and so on. From this we compose an interpolation function g(x), called a spline,
by fitting these polynomials together into a single continuous curve passing through the
data points, that is,
(2)

g(x 0) ϭ f (x 0) ϭ f0,

g(x 1) ϭ f (x 1) ϭ f1,

Á,

g(x n) ϭ f (x n) ϭ fn.

Note that g(x) ϭ q0(x) when x 0 Ϲ x Ϲ x 1, then g(x) ϭ q1(x) when x 1 Ϲ x Ϲ x 2, and so
on, according to our construction of g.
Thus spline interpolation is piecewise polynomial interpolation.
The simplest qj’s would be linear polynomials. However, the curve of a piecewise linear
continuous function has corners and would be of little interest in general—think of
designing the body of a car or a ship.
We shall consider cubic splines because these are the most important ones in applications.
By definition, a cubic spline g(x) interpolating given data (x 0, f0), Á , (x n, fn) is a continuous
function on the interval a ϭ x 0 Ϲ x Ϲ x n ϭ b that has continuous first and second
derivatives and satisfies the interpolation condition (2); furthermore, between adjacent nodes,
g(x) is given by a polynomial qj (x) of degree 3 or less.
We claim that there is such a cubic spline. And if in addition to (2) we also require that
(3)

g r(x 0) ϭ k 0,

g r(x n) ϭ k n

c19-b.qxd

11/2/10

8:33 PM

822

Page 822

CHAP. 19 Numerics in General

(given tangent directions of g(x) at the two endpoints of the interval a Ϲ x Ϲ b), then we
have a uniquely determined cubic spline. This is the content of the following existence
and uniqueness theorem, whose proof will also suggest the actual determination of splines.
(Condition (3) will be discussed after the proof.)
THEOREM 1

Existence and Uniqueness of Cubic Splines

Let (x 0, f0), (x 1, f1), Á , (x n, fn) with given (arbitrarily spaced) x j [see (1)] and
given fj ϭ f (x j), j ϭ 0, 1, Á , n. Let k 0 and k n be any given numbers. Then there
is one and only one cubic spline g(x) corresponding to (1) and satisfying (2)
and (3).

PROOF

By definition, on every subinterval Ij given by x j Ϲ x Ϲ x jϩ1, the spline g(x) must agree
with a polynomial qj(x) of degree not exceeding 3 such that
(4)

qj(x j) ϭ f (x j),

qj(x jϩ1) ϭ f (x jϩ1)

( j ϭ 0, 1, Á , n Ϫ 1).

qrj (x jϩ1) ϭ k jϩ1

( j ϭ 0, 1, Á , n Ϫ 1)

For the derivatives we write
(5)

qrj (x j) ϭ k j,

with k 0 and k n given and k 1, Á , k n؊1 to be determined later. Equations (4) and (5) are
four conditions for each qj(x). By direct calculation, using the notation
1
1
cj ϭ h ϭ x
j
jϩ1 Ϫ x j

(6*)

( j ϭ 0, 1, Á , n Ϫ 1)

we can verify that the unique cubic polynomial qj(x) ( j ϭ 0, 1, Á , n Ϫ 1) satisfying (4)
and (5) is
qj(x) ϭ f (x j)c2j (x Ϫ x jϩ1)2[1 ϩ 2cj(x Ϫ x j)]
(6)

ϩ f (x jϩ1)c2j (x Ϫ x j)2[1 Ϫ 2cj(x Ϫ x jϩ1)]
ϩ k jc2j (x Ϫ x j)(x Ϫ x jϩ1)2
ϩ k jϩ1c2j (x Ϫ x j)2(x Ϫ x jϩ1).

Differentiating twice, we obtain
(7)

qsj (x j) ϭ Ϫ6c2j f (x j) ϩ 6c2j f (x jϩ1) Ϫ 4cjk j Ϫ 2cjk jϩ1

(8)

qsj (x jϩ1) ϭ 6c2j f (x j) Ϫ 6c2j f (x jϩ1) ϩ 2cjk j ϩ 4cjk jϩ1.

By definition, g(x) has continuous second derivatives. This gives the conditions
qj؊1
s (x j) ϭ qsj (x j)

( j ϭ 1, Á , n Ϫ 1).

c19-b.qxd

11/2/10

8:33 PM

Page 823

SEC. 19.4 Spline Interpolation

823

If we use (8) with j replaced by j Ϫ 1, and (7), these n Ϫ 1 equations become
(9)

cj؊1k j؊1 ϩ 2(cj؊1 ϩ cj)k j ϩ cjk jϩ1 ϭ 3[c2j؊1ٌfj ϩ c2j ٌfjϩ1]

where ٌfj ϭ f (x j) Ϫ f (x j؊1) and ٌfjϩ1 ϭ f (x jϩ1) Ϫ f (x j) and j ϭ 1, Á , n Ϫ 1, as before.
This linear system of n Ϫ 1 equations has a unique solution k 1, Á , k n؊1 since the coefficient
matrix is strictly diagonally dominant (that is, in each row the (positive) diagonal entry is
greater than the sum of the other (positive) entries). Hence the determinant of the matrix
cannot be zero (as follows from Theorem 3 in Sec. 20.7), so that we may determine unique
values k 1, Á , k n؊1 of the first derivative of g(x) at the nodes. This proves the theorem. ᭿
Storage and Time Demands in solving (9) are modest, since the matrix of (9) is sparse
(has few nonzero entries) and tridiagonal (may have nonzero entries only on the diagonal
and on the two adjacent “parallels” above and below it). Pivoting (Sec. 7.3) is not necessary
because of that dominance. This makes splines efficient in solving large problems with
thousands of nodes or more. For some literature and some critical comments, see American
Mathematical Monthly 105 (1998), 929–941.
Condition (3) includes the clamped conditions
g r(x 0) ϭ f r(x 0),

(10)

g r(x n) ϭ f r(x n),

in which the tangent directions f r(x 0) and f r(x n) at the ends are given. Other conditions
of practical interest are the free or natural conditions
g s(x 0) ϭ 0,

(11)

g s(x n) ϭ 0

(geometrically: zero curvature at the ends, as for the draftman’s spline), giving a natural
spline. These names are motivated by Fig. 293 in Problem Set 12.3.
Determination of Splines. Let k 0 and k n be given. Obtain k 1, Á , k n؊1 by solving the
linear system (9). Recall that the spline g(x) to be found consists of n cubic polynomials
q0, Á , qn؊1. We write these polynomials in the form
(12)

qj(x) ϭ aj0 ϩ aj1(x Ϫ x j) ϩ aj2(x Ϫ x j)2 ϩ aj3(x Ϫ x j)3

where j ϭ 0, Á , n Ϫ 1. Using Taylor’s formula, we obtain

(13)

a j0 ϭ qj(x j) ϭ fj

by (2),

a j1 ϭ qrj (x j) ϭ k j

by (5),

aj2 ϭ

1
3
1
qsj (x j) ϭ 2 ( fjϩ1 Ϫ fj) Ϫ
(k jϩ1 ϩ 2k j)
2
hj
hj

aj3 ϭ

1
2
1
qt
j (x j) ϭ 3 ( fj Ϫ fjϩ1) ϩ 2 (k jϩ1 ϩ k j)
6
hj
hj

by (7),

c19-b.qxd

11/2/10

8:33 PM

824

Page 824

CHAP. 19 Numerics in General

with aj3 obtained by calculating qsj (x jϩ1) from (12) and equating the result to (8),
that is,
qsj (x jϩ1) ϭ 2aj2 ϩ 6aj3h j ϭ

6
2
( fj Ϫ fjϩ1) ϩ
(k j ϩ 2k jϩ1),
hj
h 2j

and now subtracting from this 2aj2 as given in (13) and simplifying.
Note that for equidistant nodes of distance h j ϭ h we can write cj ϭ c ϭ 1>h in (6*)
and have from (9) simply
k j؊1 ϩ 4k j ϩ k jϩ1 ϭ

(14)

EXAMPLE 1

3
( fjϩ1 Ϫ fj؊1)
h

( j ϭ 1, Á , n Ϫ 1).

Spline Interpolation. Equidistant Nodes
Interpolate f (x) ϭ x 4 on the interval Ϫ1 Ϲ x Ϲ 1 by the cubic spline g(x) corresponding to the nodes x 0 ϭ Ϫ1,
x 1 ϭ 0, x 2 ϭ 1 and satisfying the clamped conditions g r (Ϫ1) ϭ f r (Ϫ1), g r (1) ϭ f r (1).

Solution. In our standard notation the given data are f0 ϭ f (Ϫ1) ϭ 1, f1 ϭ f (0) ϭ 0, f2 ϭ f (1) ϭ 1.
We have h ϭ 1 and n ϭ 2, so that our spline consists of n ϭ 2 polynomials

q0(x) ϭ a00 ϩ a01(x ϩ 1) ϩ a02(x ϩ 1)2 ϩ a03(x ϩ 1)3
q1(x) ϭ a10 ϩ a11x ϩ a12x ϩ a13x
2

(Ϫ1 Ϲ x Ϲ 0),
(0 Ϲ x Ϲ 1).

3

We determine the k j from (14) (equidistance!) and then the coefficients of the spline from (13). Since n ϭ 2,
the system (14) is a single equation (with j ϭ 1 and h ϭ 1)
k 0 ϩ 4k 1 ϩ k 2 ϭ 3( f2 Ϫ f0).
Here f0 ϭ f2 ϭ 1 (the value of x 4 at the ends) and k 0 ϭ Ϫ4, k 2 ϭ 4, the values of the derivative 4x 3 at the
ends Ϫ1 and 1. Hence
Ϫ4 ϩ 4k 1 ϩ 4 ϭ 3(1 Ϫ 1) ϭ 0,

k 1 ϭ 0.

From (13) we can now obtain the coefficients of q0, namely, a00 ϭ f0 ϭ 1, a01 ϭ k 0 ϭ Ϫ4, and
a02 ϭ
a03 ϭ

3
2

1

2
13

( f1 Ϫ f0) Ϫ
( f0 Ϫ f1) ϩ

1
1

(k 1 ϩ 2k 0) ϭ 3(0 Ϫ 1) Ϫ (0 Ϫ 8) ϭ 5

1
12

(k 1 ϩ k 0) ϭ 2(1 Ϫ 0) ϩ (0 Ϫ 4) ϭ Ϫ2.

Similarly, for the coefficients of q1 we obtain from (13) the values a10 ϭ f1 ϭ 0, a11 ϭ k 1 ϭ 0, and
a12 ϭ 3( f2 Ϫ f1) Ϫ (k 2 ϩ 2k 1) ϭ 3(1 Ϫ 0) Ϫ (4 ϩ 0) ϭ Ϫ1
a13 ϭ 2( f1 Ϫ f2) ϩ (k 2 ϩ k 1) ϭ 2(0 Ϫ 1) ϩ (4 ϩ 0) ϭ 2.
This gives the polynomials of which the spline g(x) consists, namely,
g(x) ϭ b

q0(x) ϭ 1 Ϫ 4 (x ϩ 1) ϩ 5 (x ϩ 1)2 Ϫ 2 (x ϩ 1)3 ϭ Ϫx 2 Ϫ 2x 3

if

Ϫ1 Ϲ x Ϲ 0

q1(x) ϭ Ϫx ϩ 2x

if

0 Ϲ x Ϲ 1.

2

3

Figure 436 shows f (x) and this spline. Do you see that we could have saved over half of our work by using
symmetry?
᭿

c19-b.qxd

11/2/10

8:33 PM

Page 825

SEC. 19.4 Spline Interpolation

825

1

f(x)
–1

x

1
g(x)

Fig. 436. Function ƒ(x) ϭ x 4 and cubic spline g(x) in Example 1

EXAMPLE 2

Natural Spline. Arbitrarily Spaced Nodes
Find a spline approximation and a polynomial approximation for the curve of the cross section of the circularshaped Shrine of the Book in Jerusalem shown in Fig. 437.

3
2
1

–6

–5

–4

–3

–2

–1

0

1

Fig. 437. Shrine of the Book in Jerusalem (Architects F. Kissler and A. M. Bartus)

Solution. Thirteen points, about equally distributed along the contour (not along the x-axis!), give these data:
xj

Ϫ5.8

ƒj

0

Ϫ5.0

Ϫ4.0

Ϫ2.5

Ϫ1.5

Ϫ0.8

1.5

1.8

2.2

2.7

3.5

0

0.8

1.5

2.5

4.0

5.0

5.8

3.9

3.5

2.7

2.2

1.8

1.5

0

The figure shows the corresponding interpolation polynomial of 12th degree, which is useless because of its
oscillation. (Because of roundoff your software will also give you small error terms involving odd powers of x.)
The polynomial is
P12(x) ϭ 3.9000 Ϫ 0.65083x 2 ϩ 0.033858x 4 ϩ 0.011041x 6 Ϫ 0.0014010x 8
ϩ 0.000055595x 10 Ϫ 0.00000071867x 12.
The spline follows practically the contour of the roof, with a small error near the nodes Ϫ0.8 and 0.8. The spline
is symmetric. Its six polynomials corresponding to positive x have the following coefficients of their
representations (12). (Note well that (12) is in terms of powers of x Ϫ x j, not x!)

j

x-interval

aj0

aj1

aj2

aj3

0
1
2
3
4
5

0.0...0.8
0.8...1.5
1.5...2.5
2.5...4.0
4.0...5.0
5.0...5.8

3.9
3.5
2.7
2.2
1.8
1.5

0.00
Ϫ1.01
Ϫ0.95
Ϫ0.32
Ϫ0.027
Ϫ1.13

Ϫ0.61
Ϫ0.65
0.73
Ϫ0.091
0.29
Ϫ1.39

Ϫ0.015
0.66
Ϫ0.27
0.084
Ϫ0.56
0.58

c19-b.qxd

11/2/10

8:33 PM

826

Page 826

CHAP. 19 Numerics in General

PROBLEM SET 19.4
1. WRITING PROJECT. Splines. In your own words,
and using as few formulas as possible, write a short
report on spline interpolation, its motivation, a
comparison with polynomial interpolation, and its
applications.
2–9

VERIFICATIONS. DERIVATIONS.
COMPARISONS

2. Individual polynomial qj. Show that qj(x) in (6)
satisfies the interpolation condition (4) as well as the
derivative condition (5).
3. Verify the differentiations that give (7) and (8) from (6).
4. System for derivatives. Derive the basic linear system
(9) for k 1, Á , k n؊1 as indicated in the text.
5. Equidistant nodes. Derive (14) from (9).
6. Coefficients. Give the details of the derivation of aj2
and aj3 in (13).
7. Verify the computations in Example 1.
8. Comparison. Compare the spline g in Example 1 with
the quadratic interpolation polynomial over the whole
interval. Find the maximum deviations of g and p2
from f. Comment.
9. Natural spline condition. Using the given coefficients,
verify that the spline in Example 2 satisfies g s(x) ϭ 0
at the ends.
10–16
DETERMINATION OF SPLINES
Find the cubic spline g(x) for the given data with k 0 and
k n as given.
10. f (Ϫ2) ϭ f (Ϫ1) ϭ f (1) ϭ f (2) ϭ 0, f (0) ϭ 1,
k0 ϭ k4 ϭ 0
11. If we started from the piecewise linear function in
Fig. 438, we would obtain g(x) in Prob. 10 as the spline
satisfying g r(Ϫ2) ϭ f r(Ϫ2) ϭ 0, g r(2) ϭ f r(2) ϭ 0.
Find and sketch or graph the corresponding interpolation
polynomial of 4th degree and compare it with the spline.
Comment.

12. f0 ϭ f (0) ϭ 1, f1 ϭ f (2) ϭ 9, f2 ϭ f (4) ϭ 41,
f3 ϭ f (6) ϭ 41, k 0 ϭ 0, k 3 ϭ Ϫ12
13. f0 ϭ f (0) ϭ 1, f1 ϭ f (1) ϭ 0, f2 ϭ f (2) ϭ Ϫ1,
f3 ϭ f (3) ϭ 0, k 0 ϭ 0, k 3 ϭ Ϫ6
14. f0 ϭ f (0) ϭ 2, f1 ϭ f (1) ϭ 3, f2 ϭ f (2) ϭ 8,
f3 ϭ f (3) ϭ 12, k 0 ϭ k 3 ϭ 0
15. f0 ϭ f (0) ϭ 4, f1 ϭ f (2) ϭ 0, f2 ϭ f (4) ϭ 4,
f3 ϭ f (6) ϭ 80, k 0 ϭ k 3 ϭ 0
16. f0 ϭ f (0) ϭ 2, f1 ϭ f (2) ϭ Ϫ2, f2 ϭ f (4) ϭ 2,
f3 ϭ f (6) ϭ 78, k 0 ϭ k 3 ϭ 0. Can you obtain the
answer from that of Prob. 15?
17. If a cubic spline is three times continuously differentiable (that is, it has continuous first, second, and third
derivatives), show that it must be a single polynomial.
18. CAS EXPERIMENT. Spline versus Polynomial. If
your CAS gives natural splines, find the natural splines
when x is integer from Ϫm to m, and y (0) ϭ 1 and all
other y equal to 0. Graph each such spline along with
the interpolation polynomial p2m. Do this for m ϭ 2 to
10 (or more). What happens with increasing m?
19. Natural conditions. Explain the remark after (11).
20. TEAM PROJECT. Hermite Interpolation and Bezier
Curves. In Hermite interpolation we are looking for
a polynomial p(x) (of degree 2n ϩ 1 or less) such that
p (x) and its derivative p r(x) have given values at n ϩ 1
nodes. (More generally, p(x), p r(x), p s(x), Á may be
required to have given values at the nodes.)
(a) Curves with given endpoints and tangents. Let
C be a curve in the xy-plane parametrically represented
by r (t) ϭ [x (t), y (t)], 0 Ϲ t Ϲ 1 (see Sec. 9.5). Show
that for given initial and terminal points of a curve and
given initial and terminal tangents, say,
A:

r0 ϭ [x (0), y (0)]
ϭ [x 0, y0],

B:

r1 ϭ [x (1), y (1)]
ϭ [x 1, y1]
v0 ϭ [x r(0), y r(0)]
ϭ [x 0r , y0r ],
v1 ϭ [x r(1), y r(1)]
ϭ [x 1r , y1r ]

0.5
–2

–1

1

2

0

x

we can find a curve C, namely,
r (t) ϭ r0 ϩ v0 t

Fig. 438. Spline and interpolation polynomial in
Probs. 10 and 11

(15)

ϩ (3(r1 Ϫ r0) Ϫ (2v0 ϩ v1))t 2
ϩ (2(r0 Ϫ r1) ϩ v0 ϩ v1)t 3;

c19-b.qxd

11/2/10

8:33 PM

Page 827

SEC. 19.5 Numeric Integration and Differentiation
in components,
x (t) ϭ x 0 ϩ x 0r t ϩ (3(x 1 Ϫ x 0) Ϫ (2x 0r ϩ x 1r ))t 2
ϩ (2(x 0 Ϫ x 1) ϩ x 0r ϩ x 1r )t 3
y (t) ϭ y0 ϩ y0r t ϩ (3( y1 Ϫ y0) Ϫ (2y0r ϩ y1r ))t 2
ϩ (2( y0 Ϫ y1) ϩ y0r ϩ y1r )t 3.
Note that this is a cubic Hermite interpolation polynomial, and n ϭ 1 because we have two nodes (the
endpoints of C ). (This has nothing to do with the
Hermite polynomials in Sec. 5.8.) The two points
GA: g 0 ϭ r0 ϩ v0
ϭ [x 0 ϩ x 0r , y0 ϩ y0r ]

827
Automobile Company, who introduced them in the
early 1960s in designing car bodies. Bezier curves (and
surfaces) are used in computer-aided design (CAD) and
computer-aided manufacturing (CAM). (For more
details, see Ref. [E21] in App. 1.)
(b) Find and graph the Bezier curve and its
guidepoints if A: [0, 0], B: [1, 0], v0 ϭ [12 , 12 ],
v1 ϭ [Ϫ12 , Ϫ14 13].
(c) Changing guidepoints changes C. Moving guidepoints farther away results in C “staying near the
tangents for a longer time.” Confirm this by changing
v0 and v1 in (b) to 2v0 and 2v1 (see Fig. 439).
(d) Make experiments of your own. What happens if
you change v1 in (b) to Ϫv1. If you rotate the tangents?
If you multiply v0 and v1 by positive factors less than 1?

and
GA(c)

GB: g 1 ϭ r1 Ϫ v1
ϭ [x 1 Ϫ x 1r , y1 Ϫ y1r ]
are called guidepoints because the segments AGA and
BGB specify the tangents graphically. A, B, GA, GB
determine C, and C can be changed quickly by moving
the points. A curve consisting of such Hermite
interpolation polynomials is called a Bezier curve,
after the French engineer P. Bezier of the Renault

19.5

y

GA(b)

0.4
0.2

(b)

A

GB(c)

(c)
GB(b)
1
B

x

Fig. 439. Team Project 20(b) and (c): Bezier curves

Numeric Integration and Differentiation
In applications, the engineer often encounters integrals that are very difficult or even
impossible to solve analytically. For example, the error function, the Fresnel integrals
(see Probs. 16–25 on nonelementary integrals in this section), and others cannot
be evaluated by the usual methods of calculus (see App. 3, (24)–(44) for such
“difficult” integrals). We then need methods from numerical analysis to evaluate such
integrals. We also need numerics when the integrand of the integral to be evaluated
consists of an empirical function, where we are given some recorded values of that
function. Methods that address these kinds of problems are called methods of numeric
integration.
Numeric integration means the numeric evaluation of integrals
b

Jϭ

Ύ f (x) dx
a

where a and b are given and f is a function given analytically by a formula or empirically
by a table of values. Geometrically, J is the area under the curve of f between a and b
(Fig. 440), taken with a minus sign where f is negative.

c19-b.qxd

11/2/10

828

8:33 PM

Page 828

CHAP. 19 Numerics in General

We know that if f is such that we can find a differentiable function F whose derivative
is f, then we can evaluate J directly, i.e., without resorting to numeric integration, by
applying the familiar formula
b

Jϭ

Ύ f (x) dx ϭ F (b) Ϫ F (a)

[F r(x) ϭ f (x)].

a

Your CAS (Mathematica, Maple, etc.) or tables of integrals may be helpful for this purpose.

Rectangular Rule. Trapezoidal Rule
Numeric integration methods are obtained by approximating the integrand f by functions
that can easily be integrated.
The simplest formula, the rectangular rule, is obtained if we subdivide the interval of
integration a Ϲ x Ϲ b into n subintervals of equal length h ϭ (b Ϫ a)>n and in each
subinterval approximate f by the constant f (x*j ), the value of f at the midpoint x*j of the jth
subinterval (Fig. 441). Then f is approximated by a step function (piecewise constant function),
the n rectangles in Fig. 441 have the areas f (x *1 )h, Á , f (x n*)h, and the rectangular rule is
b

Ύ f (x) dx Ϸ h[ f (x *) ϩ f (x*) ϩ Á ϩ f (x *)]

Jϭ

(1)

1

2

n

a

ah ϭ

bϪa
n b.

The trapezoidal rule is generally more accurate. We obtain it if we take the same
subdivision as before and approximate f by a broken line of segments (chords) with
endpoints [a, f (a)], [x 1, f (x 1)], Á , [b, f (b)] on the curve of f (Fig. 442). Then the area
under the curve of f between a and b is approximated by n trapezoids of areas
1
2[

f (a) ϩ f (x 1)]h,

1
2[

f (x 1) ϩ f (x 2)]h,

1
2[

Á,

f (x n؊1) ϩ f (b)]h.

y

y
y = f(x)

y = f(x)

R

a

x

b

a x1*

Fig. 440. Geometric interpretation
of a definite integral

x2*

x*n b

Fig. 441. Rectangular rule

y
y = f(x)

a

x1

x2

xn – 1

Fig. 442. Trapezoidal rule

b

x

x

c19-b.qxd

11/2/10

8:33 PM

Page 829

SEC. 19.5 Numeric Integration and Differentiation

829

By taking their sum we obtain the trapezoidal rule
b

Jϭ

(2)

Ύ f (x) dx Ϸ h c 21 f (a) ϩ f (x ) ϩ f (x ) ϩ Á ϩ f (x
1

2

n؊1)

ϩ

a

1
f (b) d
2

where h ϭ (b Ϫ a)>n, as in (1). The x j’s and a and b are called nodes.
EXAMPLE 1

Trapezoidal Rule
1

Ύe

Evaluate J ϭ

؊x2

dx by means of (2) with n ϭ 10.

0

Note that this integral cannot be evaluated by elementary calculus, but leads to the error function (see Eq. (35),
App. 3).

᭿

Solution. J Ϸ 0.1(0.5 ؒ 1.367879 ϩ 6.778167) ϭ 0.746211 from Table 19.3.
Table 19.3 Computations in Example 1
j

xj

xj2

0
1
2
3
4
5
6
7
8
9
10

0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0

0
0.01
0.04
0.09
0.16
0.25
0.36
0.49
0.64
0.81
1.00

2

e؊xj
1.000000

0.990050
0.960789
0.913931
0.852144
0.778801
0.697676
0.612626
0.527292
0.444858
0.367879

Sums

1.367879

6.778167

Error Bounds and Estimate for the Trapezoidal Rule
An error estimate for the trapezoidal rule can be derived from (5) in Sec. 19.3 with n ϭ 1
by integration as follows. For a single subinterval we have
f (x) Ϫ p1(x) ϭ (x Ϫ x 0)(x Ϫ x 1)

f s(t)
2

with a suitable t depending on x, between x 0 and x 1. Integration over x from a ϭ x 0 to
x 1 ϭ x 0 ϩ h gives

Ύ

x0ϩh

x0

f (x) dx Ϫ

h
[ f (x 0) ϩ f (x 1)] ϭ
2

Ύ

x0ϩh

x0

(x Ϫ x 0)(x Ϫ x 0 Ϫ h)

f s(t (x))
dx.
2

c19-b.qxd

11/2/10

8:33 PM

830

Page 830

CHAP. 19 Numerics in General

Setting x Ϫ x 0 ϭ v and applying the mean value theorem of integral calculus, which we can
use because (x Ϫ x 0)(x Ϫ x 0 Ϫ h) does not change sign, we find that the right side equals

Ύ

(3*)

h

v(v Ϫ h) dv

0

f s(t~)
h3
h3 f s(t~)
h3
ϭa Ϫ b
ϭϪ
f s(t~)
2
3
2
2
12

t is a (suitable, unknown) value between x 0 and x 1. This is the error for the
where ~
trapezoidal rule with n ϭ 1, often called the local error.
Hence the error P of (2) with any n is the sum of such contributions from the n
subintervals; since h ϭ (b Ϫ a)>n, nh3 ϭ n(b Ϫ a)3>n 3, and (b Ϫ a)2 ϭ n 2h2, we obtain
PϭϪ

(3)

(b Ϫ a)3
bϪa 2
f s(tˆ) ϭ Ϫ
h f s(tˆ)
12
12n2

with (suitable, unknown) tˆ between a and b.
Because of (3) the trapezoidal rule (2) is also written
b

(2*) J ϭ

Ύ f (x) dx Ϸ h c 21 f (a) ϩ f (x ) ϩ Á ϩ f (x
1

n؊1)

a

ϩ

1
bϪa 2
f (b) d Ϫ
h f s (tˆ).
2
12

Error Bounds are now obtained by taking the largest value for f s , say, M 2, and the
smallest value, M 2*, in the interval of integration. Then (3) gives (note that K is negative)
(4)

KM2 Ϲ P Ϲ KM*2

where

KϭϪ

(b Ϫ a)3
bϪa 2
ϭϪ
h .
12
12n2

Error Estimation by Halving h is advisable if f s is very complicated or unknown, for
instance, in the case of experimental data. Then we may apply the Error Principle of
Sec. 19.1. That is, we calculate by (2), first with h, obtaining, say, J ϭ Jh ϩ Ph, and then
with 12 h, obtaining J ϭ Jh>2 ϩ Ph>2. Now if we replace h2 in (3) with (12 h)2, the error is
multiplied by 14 . Hence Ph>2 Ϸ 14 Ph (not exactly because tˆ may differ). Together,
Jh>2 ϩ Ph>2 ϭ Jh ϩ Ph Ϸ Jh ϩ 4Ph>2. Thus Jh>2 Ϫ Jh ϭ (4 Ϫ 1)Ph>2. Division by 3
gives the error formula for Jh>2
Ph>2 Ϸ 13 (Jh>2 Ϫ Jh).

(5)

EXAMPLE 2

Error Estimation for the Trapezoidal Rule by (4) and (5)
Estimate the error of the approximate value in Example 1 by (4) and (5).

Solution. (A) Error bounds by (4). By differentiation, f s(x) ϭ 2(2x 2 Ϫ 1)e؊x . Also, f t(x) Ͼ 0 if 0 Ͻ x Ͻ 1,
so that the minimum and maximum occur at the ends of the interval. We compute M 2 ϭ f s(1) ϭ 0.735759 and
M*2 ϭ f s(0) ϭ Ϫ2. Furthermore, K ϭ Ϫ1>1200, and (4) gives
2

Ϫ0.000614 Ϲ P Ϲ 0.001667.
Hence the exact value of J must lie between
0.746211 Ϫ 0.000614 ϭ 0.745597
Actually, J ϭ 0.746824, exact to 6D.

and

0.746211 ϩ 0.001667 ϭ 0.747878.

c19-b.qxd

11/2/10

8:33 PM

Page 831

SEC. 19.5 Numeric Integration and Differentiation

831

(B) Error estimate by (5). Jh ϭ 0.746211 in Example 1. Also,
19
2
1
Jh>2 ϭ 0.05 c a e؊( j>20) ϩ (1 ϩ 0.367879) d ϭ 0.746671.
2
jϭ1

᭿

Hence Ph>2 ϭ 13 (Jh>2 Ϫ Jh) ϭ 0.000153 and Jh>2 ϩ Ph>2 ϭ 0.746824, exact to 6D.

Simpson’s Rule of Integration
Piecewise constant approximation of f led to the rectangular rule (1), piecewise linear
approximation to the trapezoidal rule (2), and piecewise quadratic approximation will lead
to Simpson’s rule, which is of great practical importance because it is sufficiently accurate
for most problems, but still sufficiently simple.
To derive Simpson’s rule, we divide the interval of integration a Ϲ x Ϲ b into an even
number of equal subintervals, say, into n ϭ 2m subintervals of length h ϭ (b Ϫ a)>(2m),
with endpoints x 0 (ϭ a), x 1, Á , x 2m؊1, x 2m (ϭ b); see Fig. 443. We now take the first
two subintervals and approximate f (x) in the interval x 0 Ϲ x Ϲ x 2 ϭ x 0 ϩ 2h by the
Lagrange polynomial p2(x) through (x 0, f0), (x 1, f1), (x 2, f2), where fj ϭ f (x j). From (3)
in Sec. 19.3 we obtain
(6) p2(x) ϭ

(x Ϫ x 1)(x Ϫ x 2)
(x Ϫ x 0)(x Ϫ x 2)
(x Ϫ x 0)(x Ϫ x 1)
f0 ϩ
f1 ϩ
f2.
(x 0 Ϫ x 1)(x 0 Ϫ x 2)
(x 1 Ϫ x 0)(x 1 Ϫ x 2)
(x 2 Ϫ x 0)(x 2 Ϫ x 1)

The denominators in (6) are 2h2, Ϫh2, and 2h2, respectively. Setting s ϭ (x Ϫ x 1)>h, we
have
x Ϫ x 1 ϭ sh,

x Ϫ x 0 ϭ x Ϫ (x 1 Ϫ h) ϭ (s ϩ 1)h

x Ϫ x 2 ϭ x Ϫ (x 1 ϩ h) ϭ (s Ϫ 1)h
and we obtain
p2(x) ϭ 12 s(s Ϫ 1) f0 Ϫ (s ϩ 1)(s Ϫ 1) f1 ϩ 12 (s ϩ 1)sf2.
We now integrate with respect to x from x 0 to x 2. This corresponds to integrating with
respect to s from Ϫ1 to 1. Since dx ϭ h ds, the result is
(7*)

Ύ

x2

f (x) dx Ϸ

x0

Ύ

x2

x0

y

p2(x) dx ϭ h a

1
4
1
f0 ϩ f1 ϩ f2 b .
3
3
3

First parabola
Second parabola
Last parabola

y = f(x)

a

x1

x2

x3

x4

x2m–2

Fig. 443. Simpson’s rule

x2m–1 b

x

c19-b.qxd

11/2/10

832

8:33 PM

Page 832

CHAP. 19 Numerics in General

A similar formula holds for the next two subintervals from x 2 to x 4, and so on. By summing
all these m formulas we obtain Simpson’s rule4
b

Ύ f (x) dx Ϸ 3h ( f

(7)

0

ϩ 4f1 ϩ 2f2 ϩ 4f3 ϩ Á ϩ 2f2m؊2 ϩ 4f2m؊1 ϩ f2m),

a

where h ϭ (b Ϫ a)>(2m) and fj ϭ f (x j). Table 19.4 shows an algorithm for Simpson’s
rule.
Table 19.4 Simpson’s Rule of Integration

ALGORITHM SIMPSON (a, b, m, ƒ0, ƒ1, • • • , ƒ2m)
This algorithm computes the integral J ϭ ͐ab f (x) dx from given values ƒj ϭ ƒ(xj) at
equidistant x0 ϭ a, x1 ϭ x0 ϩ h, • • • , x2m ϭ x0 ϩ 2mh ϭ b by Simpson’s rule (7),
where h ϭ (b Ϫ a)>(2m).
a, b, m, ƒ0, • • • , ƒ2m

INPUT:

ෂ
OUTPUT: Approximate value J of J
Compute

s0 ϭ f0 ϩ f2m
s1 ϭ ƒ1 ϩ ƒ3 ϩ • • • ϩ ƒ2m؊1
s2 ϭ ƒ2 ϩ ƒ4 ϩ • • • ϩ ƒ2m؊2
h ϭ (b Ϫ a)/2m

ෂ h
J ϭ (s0 ϩ 4s1 ϩ 2s2)
3
ෂ
OUTPUT J . Stop.
End SIMPSON

Error of Simpson’s Rule (7). If the fourth derivative f (4) exists and is continuous on
a Ϲ x Ϲ b, the error of (7), call it Ps, is
PS ϭ Ϫ

(8)

(b Ϫ a)5
4

180 (2m)

f (4)(tˆ ) ϭ Ϫ

b Ϫ a 4 (4)
h f (tˆ );
180

here tˆ is a suitable unknown value between a and b. This is obtained similarly to (3).
With this we may also write Simpson’s rule (7) as
b

(7**)

Ύ f (x) dx ϭ 3h ( f

0

a

b Ϫ a 4 (4)
ϩ 4f1 ϩ Á ϩ f2m) Ϫ
h f (tˆ ).
180

4
THOMAS SIMPSON (1710–1761), self-taught English mathematician, author of several popular textbooks.
Simpson’s rule was used much earlier by Torricelli, Gregory (in 1668), and Newton (in 1676).

c19-b.qxd

11/2/10

8:33 PM

Page 833

SEC. 19.5 Numeric Integration and Differentiation

833

Error Bounds. By taking for f (4) in (8) the maximum M 4 and minimum M*4 on the interval
of integration we obtain from (8) the error bounds (note that C is negative)

(9)

CM 4 Ϲ PS Ϲ CM 4*

where

CϭϪ

(b Ϫ a)5
bϪa 4
ϭϪ
h .
180
180(2m)4

Degree of Precision (DP) of an integration formula. This is the maximum degree of
arbitrary polynomials for which the formula gives exact values of integrals over any
intervals.
Hence for the trapezoidal rule,
DP ϭ 1
because we approximate the curve of f by portions of straight lines (linear polynomials).
For Simpson’s rule we might expect DP ϭ 2 (why?). Actually,
DP ϭ 3
by (9) because f (4) is identically zero for a cubic polynomial. This makes Simpson’s rule
sufficiently accurate for most practical problems and accounts for its popularity.
Numeric Stability with respect to rounding is another important property of Simpson’s
rule. Indeed, for the sum of the roundoff errors Pj of the 2m ϩ 1 values fj in (7) we obtain,
since h ϭ (b Ϫ a)>2m,
h
bϪa
6mu ϭ (b Ϫ a)u
ƒ P0 ϩ 4P1 ϩ Á ϩ P2m ƒ Ϲ
3
3.2m
where u is the rounding unit (u ϭ 12 ؒ 10؊6 if we round off to 6D; see Sec. 19.1). Also
6 ϭ 1 ϩ 4 ϩ 1 is the sum of the coefficients for a pair of intervals in (7); take m ϭ 1 in
(7) to see this. The bound (b Ϫ a)u is independent of m, so that it cannot increase with
increasing m, that is, with decreasing h. This proves stability.
᭿
Newton–Cotes Formulas. We mention that the trapezoidal and Simpson rules are special
closed Newton–Cotes formulas, that is, integration formulas in which f (x) is interpolated
at equally spaced nodes by a polynomial of degree n (n ϭ 1 for trapezoidal, n ϭ 2 for
Simpson), and closed means that a and b are nodes (a ϭ x 0, b ϭ x n). n ϭ 3 and higher
n are used occasionally. From n ϭ 8 on, some of the coefficients become negative, so
that a positive fj could make a negative contribution to an integral, which is absurd. For
more on this topic see Ref. [E25] in App. 1.
EXAMPLE 3

Simpson’s Rule. Error Estimate
1

Evaluate J ϭ

Ύe

؊x2

dx by Simpson’s rule with 2m ϭ 10 and estimate the error.

0

Solution. Since h ϭ 0.1, Table 19.5 gives
JϷ

0.1
3

(1.367879 ϩ 4 # 3.740266 ϩ 2 # 3.037901) ϭ 0.746825.

c19-b.qxd

11/2/10

8:33 PM

834

Page 834

CHAP. 19 Numerics in General
Estimate of error. Differentiation gives f (4)(x) ϭ 4 (4x 4 Ϫ 12x 2 ϩ 3)e؊x . By considering the derivative f (5)
of f (4) we find that the largest value of f (4) in the interval of integration occurs at 0 and the smallest value at
x* ϭ (2.5 Ϫ 0.5 110)1>2. Computation gives the values M 4 ϭ f (4)(0) ϭ 12 and M*4 ϭ f (4)(x*) ϭ Ϫ7.419. Since
2m ϭ 10 and b Ϫ a ϭ 1, we obtain C ϭ Ϫ1>1800000 ϭ Ϫ0.00000056. Therefore, from (9),
2

Ϫ0.000007 Ϲ Ps Ϲ 0.000005.
Hence J must lie between 0.746825 Ϫ 0.000007 ϭ 0.746818 and 0.746825 ϩ 0.000005 ϭ 0.746830, so that at
least four digits of our approximate value are exact. Actually, the value 0.746825 is exact to 5D because
J ϭ 0.746824 (exact to 6D).
Thus our result is much better than that in Example 1 obtained by the trapezoidal rule, whereas the number
᭿
of operations is nearly the same in both cases.

Table 19.5 Computations in Example 3
2

j

xj

xj2

0

0

0

1

0.1

0.01

2

0.2

0.04

3
4
5
6
7
8
9
10

0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0

0.09
0.16
0.25
0.36
0.49
0.64
0.81
1.00

e؊xj
1.000000

0.990050
0.960789
0.913931
0.852144
0.778801
0.697676
0.612626
0.527292
0.444858
0.367879

Sums

1.367879

3.740266

3.037901

Instead of picking an n ϭ 2m and then estimating the error by (9), as in Example 3, it is
better to require an accuracy (e.g., 6D) and then determine n ϭ 2m from (9).
EXAMPLE 4

Determination of n ϭ 2m in Simpson’s Rule from the Required Accuracy
What n should we choose in Example 3 to get 6D-accuracy?

Solution. Using M 4 ϭ 12 (which is bigger in absolute value than M*4 , we get from (9), with b Ϫ a ϭ 1 and
the required accuracy,
ƒ CM 4 ƒ ϭ

12
180(2m)4

ϭ

1
2

ؒ 10؊6,

thus

mϭ c

2 ؒ 106 ؒ 12
180 ؒ 24

d

1>4

ϭ 9.55.

Hence we should choose n ϭ 2m ϭ 20. Do the computation, which parallels that in Example 3.
Note that the error bounds in (4) or (9) may sometimes be loose, so that in such a case a smaller n ϭ 2m
may already suffice.
᭿

Error Estimation for Simpson’s Rule by Halving h.
and gives
(10)

The idea is the same as in (5)

1
Ph>2 Ϸ 15
(Jh>2 Ϫ Jh).

Jh is obtained by using h and Jh>2 by using 12 h, and Ph>2 is the error of Jh>2.

c19-b.qxd

11/2/10

8:33 PM

Page 835

SEC. 19.5 Numeric Integration and Differentiation

835

Derivation. In (5) we had 13 as the reciprocal of 3 ϭ 4 Ϫ 1 and 14 ϭ (12)2 resulted from
1
h in (3) by replacing h with 12 h. In (10) we have 15
as the reciprocal of 15 ϭ 16 Ϫ 1
1 4
1
4
and 16 ϭ (2) results from h in (8) by replacing h with 12 h.
2

EXAMPLE 5

Error Estimation for Simpson’s Rule by Halving
Integrate f (x) ϭ 14 px 4 cos 14 px from 0 to 2 with h ϭ 1 and apply (10).

Solution. The exact 5D-value of the integral is J ϭ 1.25953. Simpson’s rule gives
Jh ϭ 13 3 f (0) ϩ 4 f (1) ϩ f (2)4 ϭ 13 (0 ϩ 4 ؒ 0.555360 ϩ 0) ϭ 0.740480,
Jh>2 ϭ 16 [ f (0) ϩ 4 f (12 ) ϩ 2 f (1) ϩ 4f (32 ) ϩ f (2)]
ϭ 16 [0 ϩ 4 ؒ 0.045351 ϩ 2 ؒ 0.555361 ϩ 4 ؒ 1.521579 ϩ 0] ϭ 1.22974.
1
(1.22974 Ϫ 0.74048) ϭ 0.032617 and thus J Ϸ Jh>2 ϩ Ph>2 ϭ 1.26236, with an
Hence (10) gives Ph>2 ϭ 15
1
error Ϫ0.00283 which is less in absolute value than 10
of the error 0.02979 of Jh>2. Hence the use of (10) was
᭿
well worthwhile.

Adaptive Integration
The idea is to adapt step h to the variability of f (x). That is, where f varies but little, we can
proceed in large steps without causing a substantial error in the integral, but where f varies
rapidly, we have to take small steps in order to stay everywhere close enough to the curve
of f.
Changing h is done systematically, usually by halving h, and automatically (not “by hand”)
depending on the size of the (estimated) error over a subinterval. The subinterval is halved
if the corresponding error is still too large, that is, larger than a given tolerance TOL
(maximum admissible absolute error), or is not halved if the error is less than or equal to
TOL (or doubled if the error is very small).
Adapting is one of the techniques typical of modern software. In connection with
integration it can be applied to various methods. We explain it here for Simpson’s rule. In
Table 19.6 an asterisk means that for that subinterval, TOL has been reached.
EXAMPLE 6

Adaptive Integration with Simpson’s Rule
Integrate f (x) ϭ 14 px 4 cos 14 px from x ϭ 0 to 2 by adaptive integration and with Simpson’s rule and
TOL[0, 2] ϭ 0.0002.

Solution. Table 19.6 shows the calculations. Figure 444 shows the integrand f (x) and the adapted intervals
used. The first two intervals ([0, 0.5], [0.5, 1.0]) have length 0.5, hence h ϭ 0.25 [because we use 2m ϭ 2
subintervals in Simpson’s rule (7**)]. The next two intervals ([1.00, 1.25], [1.25, 1.50]) have length 0.25
(hence h ϭ 0.125) and the last four intervals have length 0.125. Sample computations. For 0.740480 see
Example 5. Formula (10) gives (0.123716 Ϫ 0.122794)>15 ϭ 0.000061. Note that 0.123716 refers to [0, 0.5]
and [0.5, 1], so that we must subtract the value corresponding to [0, 1] in the line before. Etc.
TOL[0, 2] ϭ 0.0002 gives 0.0001 for subintervals of length 1, 0.00005 for length 0.5, etc. The value of the
integral obtained is the sum of the values marked by an asterisk (for which the error estimate has become
less than TOL). This gives
J Ϸ 0.123716 ϩ 0.528895 ϩ 0.388263 ϩ 0.218483 ϭ 1.25936.
The exact 5D-value is J ϭ 1.25953. Hence the error is 0.00017. This is about 1>200 of the absolute value of
that in Example 5. Our more extensive computation has produced a much better result.
᭿

c19-b.qxd

11/2/10

836

8:33 PM

Page 836

CHAP. 19 Numerics in General
Table 19.6 Computations in Example 6
Interval

Integral

Error (10)

TOL

Comment

[0, 2]

0.740480

[0, 1]
[1, 2]

0.122794
1.10695
Sum ϭ 1.22974

0.032617

0.0002

Divide further

0.004782
0.118934
Sum ϭ 0.123716*

0.000061

0.0001

TOL reached

0.528176
0.605821
Sum ϭ 1.13300

0.001803

0.0001

Divide further

0.200544
0.328351
Sum ϭ 0.528895*

0.000048

0.00005

TOL reached

0.388235
0.218457
Sum ϭ 0.606692

0.000058

0.00005

Divide further

0.196244
0.192019
Sum ϭ 0.388263*

0.000002

0.000025

TOL reached

0.153405
0.065078
Sum ϭ 0.218483*

0.000002

0.000025

TOL reached

[0.0, 0.5]
[0.5, 1.0]
[1.0, 1.5]
[1.5, 2.0]
[1.00, 1.25]
[1.25, 1.50]
[1.50, 1.75]
[1.75, 2.00]
[1.500, 1.625]
[1.625, 1.750]
[1.750, 1.875]
[1.875, 2.000]

0.0002

f(x)
1.5
1.0
0.5
0

0

0.5

1

1.5

2

x

Fig. 444. Adaptive integration in Example 6

Gauss Integration Formulas
Maximum Degree of Precision
Our integration formulas discussed so far use function values at predetermined
(equidistant) x-values (nodes) and give exact results for polynomials not exceeding a

c19-b.qxd

11/2/10

8:33 PM

Page 837

SEC. 19.5 Numeric Integration and Differentiation

837

certain degree [called the degree of precision; see after (9)]. But we can get much more
accurate integration formulas as follows. We set

Ύ

(11)

1

n

f (t) dt Ϸ a Aj fj

؊1

[ fj ϭ f (t j)]

jϭ1

with fixed n, and t ϭ Ϯ1 obtained from x ϭ a, b by setting x ϭ 12 [a(t Ϫ 1) ϩ b(t ϩ 1)].
Then we determine the n coefficients A1, Á , An and n nodes t 1, Á , t n so that (11) gives
exact results for polynomials of degree k as high as possible. Since n ϩ n ϭ 2n is the
number of coefficients of a polynomial of degree 2n Ϫ 1, it follows that k Ϲ 2n Ϫ1.
Gauss has shown that exactness for polynomials of degree not exceeding 2n Ϫ 1 (instead
of n Ϫ 1 for predetermined nodes) can be attained, and he has given the location of the
t j (ϭ the jth zero of the Legendre polynomial Pn in Sec. 5.3) and the coefficients Aj which
depend on n but not on f(t), and are obtained by using Lagrange’s interpolation polynomial,
as shown in Ref. [E5] listed in App. 1. With these t j and Aj, formula (11) is called a Gauss
integration formula or Gauss quadrature formula. Its degree of precision is 2n Ϫ 1, as
just explained. Table 19.7 gives the values needed for n ϭ 2, Á , 5. (For larger n, see
pp. 916–919 of Ref. [GenRef1] in App. 1.)

Table 19.7 Gauss Integration: Nodes tj and Coefficients Aj
n
2

3

4

5

EXAMPLE 7

Nodes tj

Coefficients Aj

Ϫ0.5773502692

1

0.5773502692

1

Ϫ0.7745966692
0
0.7745966692

0.5555555556
0.8888888889
0.5555555556

Ϫ0.8611363116
Ϫ0.3399810436

0.3478548451
0.6521451549

0.3399810436
0.8611363116

0.6521451549
0.3478548451

Ϫ0.9061798459
Ϫ0.5384693101
0
0.5384693101
0.9061798459

0.2369268851
0.4786286705
0.5688888889
0.4786286705
0.2369268851

Degree of Precision
3

5

7

9

Gauss Integration Formula with n ‫ ؍‬3
Evaluate the integral in Example 3 by the Gauss integration formula (11) with n ϭ 3.

Solution. We have to convert our integral from 0 to 1 into an integral from Ϫ1 to 1. We set x ϭ 12 (t ϩ 1).
Then dx ϭ 12 dt, and (11) with n ϭ 3 and the above values of the nodes and the coefficients yields

c19-b.qxd

11/2/10

8:33 PM

838

Page 838

CHAP. 19 Numerics in General
1

Ύ exp (Ϫx ) dx ϭ 2 Ύ
1

2

0

Ϸ

1

1
exp aϪ (t ϩ 1)2 b dt
4
؊1

8
5
1
3 2
1
3 2
1
1 5
c exp aϪ a1 Ϫ
b b ϩ exp aϪ b ϩ exp aϪ a1 ϩ
b b d ϭ 0.746815
2 9
4
B5
9
4
9
4
B5

(exact to 6D: 0.746825), which is almost as accurate as the Simpson result obtained in Example 3 with a much
larger number of arithmetic operations. With 3 function values (as in this example) and Simpson’s rule we would
get 16 (1 ϩ 4e؊0.25 ϩ e؊1) ϭ 0.747180, with an error over 30 times that of the Gauss integration.
᭿

EXAMPLE 8

Gauss Integration Formula with n ‫ ؍‬4 and 5
Integrate f (x) ϭ 14 px 4 cos 14 px from x ϭ 0 to 2 by Gauss. Compare with the adaptive integration in Example 6
and comment.

Solution. x ϭ t ϩ 1 gives f (t) ϭ 14 p(t ϩ 1)4 cos (14 p (t ϩ 1)), as needed in (11). For n ϭ 4 we calculate (6S)
J Ϸ A1 f1 ϩ Á ϩ A4 f4 ϭ A1( f1 ϩ f4) ϩ A2( f2 ϩ f3)
ϭ 0.347855(0.000290309 ϩ 1.02570) ϩ 0.652145(0.129464 ϩ 1.25459) ϭ 1.25950.
The error is 0.00003 because J ϭ 1.25953 (6S). Calculating with 10S and n ϭ 4 gives the same result; so the
error is due to the formula, not rounding. For n ϭ 5 and 10S we get J Ϸ 1.259526185, too large by the amount
0.000000250 because J ϭ 1.259525935 (10S). The accuracy is impressive, particularly if we compare the amount
᭿
of work with that in Example 6.

Gauss integration is of considerable practical importance. Whenever the integrand f is
given by a formula (not just by a table of numbers) or when experimental measurements
can be set at times t j (or whatever t represents) shown in Table 19.7 or in Ref. [GenRef1],
then the great accuracy of Gauss integration outweighs the disadvantage of the complicated
t j and Aj (which may have to be stored). Also, Gauss coefficients Aj are positive for all
n, in contrast with some of the Newton–Cotes coefficients for larger n.
Of course, there are frequent applications with equally spaced nodes, so that Gauss
integration does not apply (or has no great advantage if one first has to get the t j in (11)
by interpolation).
Since the endpoints Ϫ1 and 1 of the interval of integration in (11) are not zeros of Pn,
they do not occur among t 0, Á , t n, and the Gauss formula (11) is called, therefore, an
open formula, in contrast with a closed formula, in which the endpoints of the interval
of integration are t 0 and t n. [For example, (2) and (7) are closed formulas.]

Numeric Differentiation
Numeric differentiation is the computation of values of the derivative of a function f from
given values of f. Numeric differentiation should be avoided whenever possible. Whereas
integration is a smoothing process and is not very sensitive to small inaccuracies in function
values, differentiation tends to make matters rough and generally gives values of f r that are
much less accurate than those of f. The difficulty with differentiation is tied in with the
definition of the derivative, which is the limit of the difference quotient, and, in that quotient,
you usually have the difference of a large quantity divided by a small quantity. This can
cause numerical instability. While being aware of this caveat, we must still develop basic
differentiation formulas for use in numeric solutions of differential equations.
We use the notations fjr ϭ f r(xj), fjs ϭ f s(xj), etc., and may obtain rough approximation
formulas for derivatives by remembering that
f r(x) ϭ lim

h:0

f (x ϩ h) Ϫ f (x)
.
h

c19-b.qxd

11/2/10

8:33 PM

Page 839

SEC. 19.5 Numeric Integration and Differentiation

839

This suggests
f 1>2
r Ϸ

(12)

df1>2
h

ϭ

f1 Ϫ f0
h

.

Similarly, for the second derivative we obtain
f 1s Ϸ

(13)

d2f1
2

h

ϭ

f2 Ϫ 2 f1 ϩ f0
h2

,

etc.

More accurate approximations are obtained by differentiating suitable Lagrange
polynomials. Differentiating (6) and remembering that the denominators in (6) are 2h2,
Ϫh2, 2h2, we have
f r(x) Ϸ pr2 (x) ϭ

2x Ϫ x 1 Ϫ x 2

f0 Ϫ

2

2h

2x Ϫ x 0 Ϫ x 2
2

h

f1 ϩ

2x Ϫ x 0 Ϫ x 1
2h2

f2.

Evaluating this at x 0, x 1, x 2, we obtain the “three-point formulas”
(a)
(14)

f0r Ϸ

1
(Ϫ3f0 ϩ 4f1 Ϫ f2),
2h

(b) f1r Ϸ

1
(Ϫf0 ϩ f2),
2h

(c) f2r Ϸ

1
( f0 Ϫ 4f1 ϩ 3f2).
2h

Applying the same idea to the Lagrange polynomial p4(x), we obtain similar formulas,
in particular,
(15)

f2r Ϸ

1
( f0 Ϫ 8f1 ϩ 8f3 Ϫ f4).
12h

Some examples and further formulas are included in the problem set as well as in
Ref. [E5] listed in App. 1.

PROBLEM SET 19.5
1–6

RECTANGULAR AND TRAPEZOIDAL RULES

1. Rectangular rule. Evaluate the integral in Example
1 by the rectangular rule (1) with subintervals of
length 0.1. Compare with Example 1. (6S-exact:
0.746824)
2. Bounds for (1). Derive a formula for lower and upper
bounds for the rectangular rule. Apply it to Prob. 1.

3. Trapezoidal rule. To get a feel for increase in accuracy,
integrate x 2 from 0 to 1 by (2) with h ϭ 1, 0.5, 0.25, 0.1.
4. Error estimation by halfing. Integrate f (x) ϭ x 4 from
0 to 1 by (2) with h ϭ 1, h ϭ 0.5, h ϭ 0.25 and estimate the error for h ϭ 0.5 and h ϭ 0.25 by (5).
5. Error estimation. Do the tasks in Prob. 4 for
f (x) ϭ sin 12 px.

c19-b.qxd

11/2/10

8:33 PM

Page 840

840

CHAP. 19 Numerics in General

6. Stability. Prove that the trapezoidal rule is stable with
respect to rounding.

SIMPSON’S RULE

7–15

Evaluate the integrals A ϭ

Ύ

2

dx
x , Bϭ

1

Jϭ

Ύ

1

Ύ

0.4

xe؊x dx,
2

0

dx

by Simpson’s rule with 2m as indicated,
1 ϩ x2
and compare with the exact value known from calculus.
0

7. A, 2m ϭ 4

8. A, 2m ϭ 10

9. B, 2m ϭ 4

10. B, 2m ϭ 10

11. J, 2m ϭ 4

12. J, 2m ϭ 10

13. Error estimate. Compute the integral J by Simpson’s
rule with 2m ϭ 8 and use the value and that in Prob.
11 to estimate the error by (10).
14. Error bounds and estimate. Integrate e؊x from 0 to 2
by (7) with h ϭ 1 and with h ϭ 0.5. Give error bounds
for the h ϭ 0.5 value and an error estimate by (10).
15. Given TOL. Find the smallest n in computing A (see
Probs. 7 and 8) such that 5S-accuracy is guaranteed
(a) by (4) in the use of (2), (b) by (9) in the use of (7).
16–21

26. TEAM PROJECT. Romberg Integration (W. Romberg, Norske Videnskab. Trondheim, F␾rh. 28, Nr. 7,
1955). This method uses the trapezoidal rule and gains
precision stepwise by halving h and adding an error
estimate. Do this for the integral of f (x) ϭ e؊x from
x ϭ 0 to x ϭ 2 with TOL ϭ 10؊3, as follows.
Step 1. Apply the trapezoidal rule (2) with h ϭ 2
(hence n ϭ 1) to get an approximation J11. Halve h
and use (2) to get J21 and an error estimate
P21 ϭ

1
2 Ϫ1
2

(J21 Ϫ J11).

If ƒ P21 ƒ Ϲ TOL, stop. The result is J22 ϭ J21 ϩ P21.
Step 2. Show that P21 ϭ Ϫ0.066596, hence
ƒ P21 ƒ Ͼ TOL and go on. Use (2) with h>4 to get J31
and add to it the error estimate P31 ϭ 13 (J31 Ϫ J21) to
get the better J32 ϭ J31 ϩ P31. Calculate
P32 ϭ

1
1
(J32 Ϫ J22) ϭ
(J32 Ϫ J22).
15
2 Ϫ1
4

If ƒ P32 ƒ Ϲ TOL, stop. The result is J33 ϭ J32 ϩ P32.
(Why does 24 ϭ 16 come in?) Show that we obtain
P32 ϭ Ϫ0.000266, so that we can stop. Arrange your
J- and P-values in a kind of “difference table.”

NONELEMENTARY INTEGRALS

The following integrals cannot be evaluated by the usual
methods of calculus. Evaluate them as indicated. Compare
your value with that possibly given by your CAS. Si (x) is
the sine integral. S(x) and C(x) are the Fresnel integrals.
See App. A3.1. They occur in optics.
Si(x) ϭ

Ύ

x

0

S(x) ϭ

Ύ

C(x) ϭ

0

J22
⑀31

J31

x

2

⑀21

J21

sin x*
dx*,
x*

x

sin (x* ) dx*,

J11

Ύ cos (x* ) dx*
2

0

⑀32
J32

J33

If ƒ P32 ƒ were greater than TOL, you would have to
go on and calculate in the next step J41 from (2) with
h ϭ 14 ; then

16. Si (1) by (2), n ϭ 5, n ϭ 10, and apply (5).

J42 ϭ J41 ϩ P41

with

17. Si (1) by (7), 2m ϭ 2, 2m ϭ 4

J43 ϭ J42 ϩ P42

with

1
P42 ϭ 15
(J42 Ϫ J32)

18. Obtain a better value in Prob. 17. Hint. Use (10).

J44 ϭ J43 ϩ P43

with

1
(J43 Ϫ J33)
P43 ϭ 63

19. Si (1) by (7), 2m ϭ 10
20. S(1.25) by (7), 2m ϭ 10
21. C(1.25) by (7), 2m ϭ 10
22–25

GAUSS INTEGRATION

Integrate by (11) with n ϭ 5:
22. cos x from 0 to 12 p
23. xe؊x from 0 to 1
24. sin (x 2) from 0 to 1.25
25. exp (Ϫx 2) from 0 to 1

P41 ϭ 13 (J41 Ϫ J31)

where 63 ϭ 26 Ϫ 1. (How does this come in?)
Apply the Romberg method to the integral
of f (x) ϭ 14 px 4 cos 14 px from x ϭ 0 to 2 with
TOL ϭ 10؊4.
27–30

DIFFERENTIATION

27. Consider f (x) ϭ x 4 for x 0 ϭ 0, x 1 ϭ 0.2, x 2 ϭ 0.4,
x 3 ϭ 0.6, x 4 ϭ 0.8. Calculate f2r from (14a), (14b),
(14c), (15). Determine the errors. Compare and
comment.

c19-b.qxd

11/2/10

8:33 PM

Page 841

Chapter 19 Review Questions and Problems
28. A “four-point formula” for the derivative is
1
f2r Ϸ
(Ϫ2f1 Ϫ 3f2 ϩ 6f3 Ϫ f4).
6h
Apply it to f (x) ϭ x 4 with x 1, Á , x 4 as in Prob. 27,
determine the error, and compare it with that in the case
of (15).
29. The derivative f r(x) can also be approximated in
terms of first-order and higher order differences (see
Sec. 19.3):

841
f r(x 0) Ϸ

1
1
a¢f0 Ϫ ¢ 2f0
h
2
1
1
ϩ ¢ 3f0 Ϫ ¢ 4f0 ϩ Ϫ Á b .
3
4

Compute f r(0.4) in Prob. 27 from this formula, using
differences up to and including first order, second
order, third order, fourth order.
30. Derive the formula in Prob. 29 from (14) in Sec. 19.3.

CHAPTER 19 REVIEW QUESTIONS AND PROBLEMS
1. What is a numeric method? How has the computer
influenced numerics?
2. What is an error? A relative error? An error bound?
3. Why are roundoff errors important? State the rounding
rules.
4. What is an algorithm? Which of its properties are
important in software implementation?
5. What do you know about stability?
6. Why is the selection of a good method at least as
important on a large computer as it is on a small one?
7. Can the Newton (–Raphson) method diverge? Is it fast?
Same questions for the bisection method.
8. What is fixed-point iteration?
9. What is the advantage of Newton’s interpolation
formulas over Lagrange’s?
10. What is spline interpolation? Its advantage over
polynomial interpolation?
11. List and compare the integration methods we have
discussed.
12. How did we use an interpolation polynomial in deriving
Simpson’s rule?
13. What is adaptive integration? Why is it useful?
14. In what sense is Gauss integration optimal?
15. How did we obtain formulas for numeric differentiation?
16. Write Ϫ46.9028104, 0.000317399, 54>7, Ϫ890>3 in
floating-point form with 5S (5 significant digits,
properly rounded).
17. Compute (5.346 Ϫ 3.644)>(3.444 Ϫ 3.055) as given
and then rounded stepwise to 3S, 2S, 1S. Comment.
(“Stepwise” means rounding the rounded numbers, not
the given ones.)
18. Compute 0.38755>(5.6815 Ϫ 0.38419) as given and
then rounded stepwise to 4S, 3S, 2S, 1S. Comment.
19. Let 19.1 and 25.84 be correctly rounded. Find the
shortest interval in which the sum s of the true
(unrounded) numbers must lie.

20. Do the same task as in Prob. 19 for the difference
3.2 Ϫ 6.29.
ෂ in terms of that of ෂ
21. What is the relative error of na
a?
22. Show that the relative error of ෂ
a 2 is about twice that
of ෂ
a.
23. Solve x 2 Ϫ 40x ϩ 2 ϭ 0 in two ways (cf. Sec. 19.1).
Use 4S-arithmetic.
24. Solve x 2 Ϫ 100x ϩ 1 ϭ 0. Use 5S-arithmetic.
25. Compute the solution of x 4 ϭ x ϩ 0.1 near x ϭ 0 by
transforming the equation algebraically to the form
x ϭ g(x) and starting from x 0 ϭ 0.
26. Solve cos x ϭ x 2 by Newton’s method, starting from
x ϭ 0.5.
27. Solve Prob. 25 by bisection (3S-accuracy).
28. Compute sinh 0.4 from sinh 0, sinh 0.5 ϭ 0.521,
sinh 1.0 ϭ 1.175 by quadratic interpolation.
29. Find the cubic spline for the data f (0) ϭ 0, f (1) ϭ 0,
f (2) ϭ 4, k 0 ϭ Ϫ1, k 2 ϭ 5.
30. Find the cubic spline q and the interpolation polynomial
p for the data (0, 0), (1, 1), (2, 6), (3, 10), with
q r (0) ϭ 0, q r (3) ϭ 0 and graph p and q on common
axes.
31. Compute the integral of x 3 from 0 to 1 by the
trapezoidal rule with n ϭ 5. What error bounds are
obtained from (4) in Sec. 19.5? What is the actual error
of the result?
32. Compute the integral of cos (x 2) from 0 to 1 by
Simpson’s rule with 2m ϭ 4.
33. Solve Prob. 32 by Gauss integration with n ϭ 3 and
n ϭ 5.
34. Compute f r(0.2) for f (x) ϭ x 3 using (14b) in Sec. 19.5
with (a) h ϭ 0.2, (b) h ϭ 0.1. Compare the accuracy.
35. Compute f s(0.2) for f (x) ϭ x 3 using (13) in Sec. 19.5
with (a) h ϭ 0.2, (b) h ϭ 0.1.

c19-b.qxd

11/2/10

842

8:33 PM

Page 842

CHAP. 19 Numerics in General

SUMMARY OF CHAPTER

19

Numerics in General
In this chapter we discussed concepts that are relevant throughout numeric work as
a whole and methods of a general nature, as opposed to methods for linear algebra
(Chap. 20) or differential equations (Chap. 21).
In scientific computations we use the floating-point representation of numbers
(Sec. 19.1); fixed-point representation is less suitable in most cases.
Numeric methods give approximate values ෂ
a of quantities. The error P of ෂ
a is
(1)

PϭaϪෂ
a

(Sec. 19.1)

where a is the exact value. The relative error of ෂ
a is P>a. Errors arise from rounding,
inaccuracy of measured values, truncation (that is, replacement of integrals by sums,
series by partial sums), and so on.
An algorithm is called numerically stable if small changes in the initial data give
only correspondingly small changes in the final results. Unstable algorithms are
generally useless because errors may become so large that results will be very
inaccurate. The numeric instability of algorithms must not be confused with the
mathematical instability of problems (“ill-conditioned problems,” Sec. 19.2).
Fixed-point iteration is a method for solving equations f (x) ϭ 0 in which the
equation is first transformed algebraically to x ϭ g(x), an initial guess x 0 for the
solution is made, and then approximations x 1, x 2, Á , are successively computed
by iteration from (see Sec. 19.2)
(2)

x nϩ1 ϭ g(x n)

(n ϭ 0, 1, Á ).

Newton’s method for solving equations f (x) ϭ 0 is an iteration
(3)

x nϩ1 ϭ x n Ϫ

f (x n)
f r(x n)

(Sec. 19.2).

Here x nϩ1 is the x-intercept of the tangent of the curve y ϭ f (x) at the point x n.
This method is of second order (Theorem 2, Sec. 19.2). If we replace f r in (3) by
a difference quotient (geometrically: we replace the tangent by a secant), we obtain
the secant method; see (10) in Sec. 19.2. For the bisection method (which converges
slowly) and the method of false position, see Problem Set 19.2.
Polynomial interpolation means the determination of a polynomial pn(x) such
that pn(x j) ϭ fj, where j ϭ 0, Á , n and (x 0, f0), Á , (x n, fn) are measured or
observed values, values of a function, etc. pn(x) is called an interpolation polynomial.
For given data, pn(x) of degree n (or less) is unique. However, it can be written in
different forms, notably in Lagrange’s form (4), Sec. 19.3, or in Newton’s divided
difference form (10), Sec. 19.3, which requires fewer operations. For regularly
spaced x 0, x 1 ϭ x 0 ϩ h, Á , x n ϭ x 0 ϩ nh the latter becomes Newton’s forward
difference formula (formula (14) in Sec. 19.3):

c19-b.qxd

11/2/10

8:33 PM

Page 843

Summary of Chapter 19

843

r (r Ϫ 1) Á (r Ϫ n ϩ 1) n
f (x) Ϸ pn(x) ϭ f0 ϩ r ¢f0 ϩ Á ϩ
¢ f0
n!

(4)

where r ϭ (x Ϫ x 0)>h and the forward differences are ¢fj ϭ fjϩ1 Ϫ fj and
¢ kfj ϭ ¢ k؊1fjϩ1 Ϫ ¢ k؊1fj

(k ϭ 2, 3, Á ).

A similar formula is Newton’s backward difference interpolation formula (formula
(18) in Sec. 19.3).
Interpolation polynomials may become numerically unstable as n increases, and
instead of interpolating and approximating by a single high-degree polynomial it is
preferable to use a cubic spline g(x), that is, a twice continuously differentiable
interpolation function [thus, g(x j) ϭ fj], which in each subinterval x j Ϲ x Ϲ x jϩ1
consists of a cubic polynomial qj(x); see Sec. 19.4.
Simpson’s rule of numeric integration is [see (7), Sec. 19.5]
b

(5)

Ύ f (x) dx Ϸ 3h ( f

0

ϩ 4f1 ϩ 2f2 ϩ 4f3 ϩ Á ϩ 2f2m؊2 ϩ 4f2m؊1 ϩ f2m)

a

with equally spaced nodes x j ϭ x 0 ϩ jh, j ϭ 1, Á , 2m, h ϭ (b Ϫ a)>(2m), and
fj ϭ f (x j). It is simple but accurate enough for many applications. Its degree of
precision is DP ϭ 3 because the error (8), Sec. 19.5, involves h4. A more practical
error estimate is (10), Sec. 19.5,
1
Ph>2 ϭ 15
(Jh>2 Ϫ Jh),
1
obtained by first computing with step h, then with step h>2, and then taking 15
of
the difference of the results.
Simpson’s rule is the most important of the Newton–Cotes formulas, which are
obtained by integrating Lagrange interpolation polynomials, linear ones for the
trapezoidal rule (2), Sec. 19.5, quadratic for Simpson’s rule, cubic for the threeeights rule (see the Chap. 19 Review Problems), etc.

Adaptive integration (Sec. 19.5, Example 6) is integration that adjusts
(“adapts”) the step (automatically) to the variability of f (x).
Romberg integration (Team Project 26, Problem Set 19.5) starts from the
trapezoidal rule (2), Sec. 19.5, with h, h>2, h>4, etc. and improves results by
systematically adding error estimates.
Gauss integration (11), Sec. 19.5, is important because of its great accuracy
(DP ϭ 2n Ϫ 1, compared to Newton–Cotes’s DP ϭ n Ϫ 1 or n). This is achieved
by an optimal choice of the nodes, which are not equally spaced; see Table 19.7,
Sec. 19.5.
Numeric differentiation is discussed at the end of Sec. 19.5. (Its main application
(to differential equations) follows in Chap. 21.)

c20-a.qxd

11/2/10

8:57 PM

Page 844

CHAPTER

20

Numeric Linear Algebra
This chapter deals with two main topics. The first topic is how to solve linear systems of
equations numerically. We start with Gauss elimination, which may be familiar to some
readers, but this time in an algorithmic setting with partial pivoting. Variants of this method
(Doolittle, Crout, Cholesky, Gauss–Jordan) are discussed in Sec. 20.2. All these methods
are direct methods, that is, methods of numerics where we know in advance how many
steps they will take until they arrive at a solution. However, small pivots and roundoff
error magnification may produce nonsensical results, such as in the Gauss method. A shift
occurs in Sec. 20.3, where we discuss numeric iteration methods or indirect methods to
address our first topic. Here we cannot be totally sure how many steps will be needed to
arrive at a good answer. Several factors—such as how far is the starting value from our
initial solution, how is the problem structure influencing speed of convergence, how
accurate would we like our result to be—determine the outcome of these methods.
Moreover, our computation cycle may not converge. Gauss–Seidel iteration and Jacobi
iteration are discussed in Sec. 20.3. Section 20.4 is at the heart of addressing the pitfalls
of numeric linear algebra. It is concerned with problems that are ill-conditioned. We learn
to estimate how “bad” such a problem is by calculating the condition number of its matrix.
The second topic (Secs. 20.6–20.9) is how to solve eigenvalue problems numerically.
Eigenvalue problems appear throughout engineering, physics, mathematics, economics,
and many areas. For large or very large matrices, determining the eigenvalues is difficult
as it involves finding the roots of the characteristic equations, which are high-degree
polynomials. As such, there are different approaches to tackling this problem. Some
methods, such as Gerschgorin’s method and Collatz’s method only provide a range in
which eigenvalues lie and thus are known as inclusion methods. Others such as
tridiagonalization and QR-factorization actually find all the eigenvalues. The area is quite
ingeneous and should be fascinating to the reader.
COMMENT. This chapter is independent of Chap. 19 and can be studied immediately
after Chap. 7 or 8.
Prerequisite: Secs. 7.1, 7.2, 8.1.
Sections that may be omitted in a shorter course: 20.4, 20.5, 20.9.
References and Answers to Problems: App. 1 Part E, App. 2.

20.1

Linear Systems: Gauss Elimination
The basic method for solving systems of linear equations by Gauss elimination and back
substitution was explained in Sec. 7.3. If you covered Sec. 7.3, you may wonder why we
cover Gauss elimination again. The reason is that here we cover Gauss elimination in the

844

c20-a.qxd

11/2/10

8:57 PM

Page 845

SEC. 20.1 Linear Systems: Gauss Elimination

845

setting of numerics and introduce new material such as pivoting, row scaling, and operation
count. Furthermore, we give an algorithmic representation of Gauss elimination in Table 20.1
that can be readily converted into software. We also show when Gauss elimination runs
into difficulties with small pivots and what to do about it. The reader should pay close
attention to the material as variants of Gauss elimination are covered in Sec. 20.2 and,
furthermore, the general problem of solving linear systems is the focus of the first half of
this chapter.
A linear system of n equations in n unknowns x1, Á , x n is a set of equations
E 1, Á , E n of the form

(1)

E 1:

a11x1 ϩ Á ϩ a1nx n ϭ b1

E 2:

a21x1 ϩ Á ϩ a2nx n ϭ b2

# # # # # # # # # # # # #
E n:

an1x1 ϩ Á ϩ annx n ϭ bn

where the coefficients ajk and the bj are given numbers. The system is called homogeneous
if all the bj are zero; otherwise it is called nonhomogeneous. Using matrix multiplication
(Sec. 7.2), we can write (1) as a single vector equation
Ax ‫ ؍‬b

(2)

where the coefficient matrix A ϭ [ajk] is the n ϫ n matrix
a12

Á

a1n

a21

a22

Á

a2n

#

#

Á

#

an1

an2

Á

ann

AϭE

a11

x1

b1

U , and x ϭ E o U

and b ϭ E o U

xn

bn

ෂ is called the augmented matrix of the
are column vectors. The following matrix A
system (1):
a11

Á

a1n

b1

a21
ෂ
A ϭ [A b] ϭ E

Á

a2n

b2

#

Á

#

#

an1

Á

ann

bn

U.

A solution of (1) is a set of numbers x 1, Á , x n that satisfy all the n equations, and a
solution vector of (1) is a vector x whose components constitute a solution of (1).
The method of solving such a system by determinants (Cramer’s rule in Sec. 7.7) is
not practical, even with efficient methods for evaluating the determinants.
A practical method for the solution of a linear system is the so-called Gauss elimination,
which we shall now discuss ( proceeding independently of Sec. 7.3).

c20-a.qxd

11/2/10

8:57 PM

846

Page 846

CHAP. 20 Numeric Linear Algebra

Gauss Elimination
This standard method for solving linear systems (1) is a systematic process of elimination
that reduces (1) to triangular form because the system can then be easily solved by back
substitution. For instance, a triangular system is
3x1 ϩ 5x 2 ϩ 2x 3 ϭ

8

8x 2 ϩ 2x 3 ϭ Ϫ7
6x 3 ϭ

3

and back substitution gives x 3 ϭ 36 ϭ 12 from the third equation, then
x 2 ϭ 18 (Ϫ7 Ϫ 2x 3) ϭ Ϫ1
from the second equation, and finally from the first equation
x1 ϭ 13 (8 Ϫ 5x 2 Ϫ 2x 3) ϭ 4.
How do we reduce a given system (1) to triangular form? In the first step we eliminate
x1 from equations E 2 to E n in (1). We do this by adding (or subtracting) suitable multiples of E 1 to (from) equations E 2, Á , E n and taking the resulting equations, call them
E*2, Á , E *n as the new equations. The first equation, E 1, is called the pivot equation in
this step, and a11 is called the pivot. This equation is left unaltered. In the second step
we take the new second equation E*2 (which no longer contains x1) as the pivot equation
and use it to eliminate x2 from E *3 to E*n. And so on. After n Ϫ 1 steps this gives a
triangular system that can be solved by back substitution as just shown. In this way we
obtain precisely all solutions of the given system (as proved in Sec. 7.3).
The pivot akk (in step k) must be different from zero and should be large in absolute
value to avoid roundoff magnification by the multiplication in the elimination. For this
we choose as our pivot equation one that has the absolutely largest ajk in column k on or
below the main diagonal (actually, the uppermost if there are several such equations). This
popular method is called partial pivoting. It is used in CASs (e.g., in Maple).
Partial pivoting distinguishes it from total pivoting, which involves both row and
column interchanges but is hardly used in practice.
Let us illustrate this method with a simple example.
EXAMPLE 1

Gauss Elimination. Partial Pivoting
Solve the system
E 1:

8x 2 ϩ 2x 3 ϭ Ϫ7

E 2:

3x 1 ϩ 5x 2 ϩ 2x 3 ϭ

E 3:

6x 1 ϩ 2x 2 ϩ 8x 3 ϭ 26.

8

Solution. We must pivot since E 1 has no x 1-term. In Column 1, equation E 3 has the largest coefficient.
Hence we interchange E 1 and E 3,
6x 1 ϩ 2x 2 ϩ 8x 3 ϭ 26
3x 1 ϩ 5x 2 ϩ 2x 3 ϭ

8

8x 2 ϩ 2x 3 ϭ Ϫ7.

c20-a.qxd

11/2/10

8:57 PM

Page 847

SEC. 20.1 Linear Systems: Gauss Elimination

847

Step 1. Elimination of x1
It would suffice to show the augmented matrix and operate on it. We show both the equations and the augmented
matrix. In the first step, the first equation is the pivot equation. Thus
Pivot 6 wwö 6x 1 ϩ 2x 2 ϩ 8x 3 ϭ 26

6

2

8

8

D3

5

2

8x 2 ϩ 2x 3 ϭ Ϫ7

0

8

2

wö 3x 1 ϩ 5x 2 ϩ 2x 3 ϭ
Eliminate w

26

|
|
|
|
|
|

8T .
Ϫ7

To eliminate x1 from the other equations (here, from the second equation), do:
Subtract

3
6

ϭ 12 times the pivot equation from the second equation.

The result is
6x 1 ϩ 2x 2 ϩ 8x 3 ϭ 26

6

2

8

4x 2 Ϫ 2x 3 ϭ Ϫ5

D0

4

Ϫ2

8x 2 ϩ 2x 3 ϭ Ϫ7

0

8

2

|
|
|
|
|
|

26
Ϫ5T .
Ϫ7

Step 2. Elimination of x2
The largest coefficient in Column 2 is 8. Hence we take the new third equation as the pivot equation, interchanging
equations 2 and 3,
6x 1 ϩ 2x 2 ϩ 8x 3 ϭ 26

6

2

8

Pivot 8 wwö

8x 2 ϩ 2x 3 ϭ Ϫ7

D0

8

2

wö
Eliminate w

4x 2 Ϫ 2x 3 ϭ Ϫ5

0

4

Ϫ2

26

|
|
|
|
|
|

Ϫ7T .
Ϫ5

To eliminate x 2 from the third equation, do:
Subtract

1
2

times the pivot equation from the third equation.

The resulting triangular system is shown below. This is the end of the forward elimination. Now comes the back
substitution.

Back substitution. Determination of x3, x2, x1
The triangular system obtained in Step 2 is
6x 1 ϩ 2x 2 ϩ 8x 3 ϭ 26

6

2

8

8x 2 ϩ 2x 3 ϭ Ϫ7

D0

8

2

Ϫ32

0

0

Ϫ3

Ϫ 3x 3 ϭ

|
|
|
|
|
|

26
Ϫ7T .
Ϫ32

From this system, taking the last equation, then the second equation, and finally the first equation, we compute
the solution
x 3 ϭ 12
x 2 ϭ 18 (Ϫ7 Ϫ 2x 3) ϭ Ϫ1
x 1 ϭ 16 (26 Ϫ 2x 2 Ϫ 8x 3) ϭ 4.
This agrees with the values given above, before the beginning of the example.

᭿

The general algorithm for the Gauss elimination is shown in Table 20.1. To help explain
the algorithm, we have numbered some of its lines. bj is denoted by aj,nϩ1, for uniformity.
In lines 1 and 2 we look for a possible pivot. [For k ϭ 1 we can always find one; otherwise
x1 would not occur in (1).] In line 2 we do pivoting if necessary, picking an ajk of greatest
absolute value (the one with the smallest j if there are several) and interchange the

c20-a.qxd

11/2/10

8:57 PM

848

Page 848

CHAP. 20 Numeric Linear Algebra

corresponding rows. If ƒ akk ƒ is greatest, we do no pivoting. m jk in line 4 suggests
multiplier, since these are the factors by which we have to multiply the pivot equation E*k
in Step k before subtracting it from an equation E*j below E*k from which we want to
eliminate x k. Here we have written E*k and E*j to indicate that after Step 1 these are no
longer the equations given in (1), but these underwent a change in each step, as indicated
in line 5. Accordingly, ajk etc. in all lines refer to the most recent equations, and j м k
in line 1 indicates that we leave untouched all the equations that have served as pivot
equations in previous steps. For p ϭ k in line 5 we get 0 on the right, as it should be in
the elimination,
ajk
ajk Ϫ m jkakk ϭ ajk Ϫ a akk ϭ 0.
kk
In line 3, if the last equation in the triangular system is 0 ϭ b*n 0, we have no
solution. If it is 0 ϭ b*n ϭ 0, we have no unique solution because we then have fewer
equations than unknowns.
EXAMPLE 2

Gauss Elimination in Table 20.1, Sample Computation
In Example 1 we had a11 ϭ 0, so that pivoting was necessary. The greatest coefficient in Column 1 was a31.
~
Thus j ϭ 3 in line 2, and we interchanged E 1 and E 3. Then in lines 4 and 5 we computed m 21 ϭ 36 ϭ 12 and
a22 ϭ 5 Ϫ 12 ؒ 2 ϭ 4,

a23 ϭ 2 Ϫ 12 ؒ 8 ϭ Ϫ2,

a24 ϭ 8 Ϫ 12 ؒ 26 ϭ Ϫ5 ,

and then m 31 ϭ 06 ϭ 0, so that the third equation 8x 2 ϩ 2x 3 ϭ Ϫ7 did not change in Step 1. In Step 2 (k ϭ 2)
~
we had 8 as the greatest coefficient in Column 2, hence j ϭ 3. We interchanged equations 2 and 3, computed
4
1
1
m 32 ϭ Ϫ8 ϭ Ϫ2 in line 5, and the a33 ϭ Ϫ2 Ϫ 2 ؒ 2 ϭ Ϫ3, a34 ϭ Ϫ5 Ϫ 12 (Ϫ7) ϭ Ϫ32 . This produced the
triangular form used in the back substitution.
᭿

If akk ϭ 0 in Step k, we must pivot. If ƒ akk ƒ is small, we should pivot because of roundoff
error magnification that may seriously affect accuracy or even produce nonsensical
results.
EXAMPLE 3

Difficulty with Small Pivots
The solution of the system
0.0004x 1 ϩ 1.402x 2 ϭ 1.406
0.4003x 1 Ϫ 1.502x 2 ϭ 2.501
is x 1 ϭ 10, x 2 ϭ 1. We solve this system by the Gauss elimination, using four-digit floating-point arithmetic.
(4D is for simplicity. Make an 8D-arithmetic example that shows the same.)
(a) Picking the first of the given equations as the pivot equation, we have to multiply this equation by
m ϭ 0.4003>0.0004 ϭ 1001 and subtract the result from the second equation, obtaining
Ϫ1405x 2 ϭ Ϫ1404.
Hence x 2 ϭ Ϫ1404>(Ϫ1405) ϭ 0.9993, and from the first equation, instead of x 1 ϭ 10, we get
x1 ϭ

1
0.0004

(1.406 Ϫ 1.402 ؒ 0.9993) ϭ

0.005
0.0004

ϭ 12.5.

This failure occurs because ƒ a11 ƒ is small compared with ƒ a12 ƒ , so that a small roundoff error in x 2 leads to a
large error in x 1.

c20-a.qxd

11/2/10

8:57 PM

Page 849

SEC. 20.1 Linear Systems: Gauss Elimination

849

(b) Picking the second of the given equations as the pivot equation, we have to multiply this equation by
0.0004>0.4003 ϭ 0.0009993 and subtract the result from the first equation, obtaining
1.404x 2 ϭ 1.404.
Hence x 2 ϭ 1, and from the pivot equation x 1 ϭ 10. This success occurs because ƒ a21 ƒ is not very small
compared to ƒ a22 ƒ , so that a small roundoff error in x 2 would not lead to a large error in x 1. Indeed, for
instance, if we had the value x 2 ϭ 1.002, we would still have from the pivot equation the good value
x 1 ϭ (2.501 ϩ 1.505)>0.4003 ϭ 10.01.
᭿

Table 20.1 Gauss Elimination

ෂ ϭ a ϭ [A b])
ALGORITHM GAUSS (A
[ jk]
This algorithm computes a unique solution x ϭ [xj] of the system (1) or indicates that
(1) has no unique solution.
ෂ
INPUT: Augmented n ϫ (n ϩ 1) matrix A ϭ [ajk], where aj,nϩ1 ϭ bj
OUTPUT: Solution x ϭ [xj] of (1) or message that the system (1) has no
unique solution
For k ϭ 1, • • • , n Ϫ 1, do:
mϭk

1

For j ϭ k ϩ 1, • • • , n, do:
If ( ƒ amk ƒ Ͻ ƒ a jk ƒ ) then m ϭ j
End
If amk ϭ 0 then OUTPUT “No unique solution exists”
Stop
[Procedure completed unsuccessfully]
2

Else exchange row k and row m

3

If ann ϭ 0 then OUTPUT “No unique solution exists.”
Stop
Else
For j ϭ k ϩ 1, • • • , n, do:

4

ajk
m jk: ϭ a
kk
For p ϭ k ϩ 1, • • • , n ϩ 1, do:

5

ajp: ϭ ajp Ϫ mjk akp
End
End
End

an,nϩ1
xn ϭ a
nn

6

[Start back substitution]

For i ϭ n Ϫ 1, • • • , 1, do:
n

1
x i ϭ a aai,nϩ1 Ϫ a aijx j b
ii

7

jϭiϩ1

End
OUTPUT x ϭ [xj]. Stop
End GAUSS

c20-a.qxd

11/2/10

850

8:57 PM

Page 850

CHAP. 20 Numeric Linear Algebra

Error estimates for the Gauss elimination are discussed in Ref. [E5] listed in App. 1.
Row scaling means the multiplication of each Row j by a suitable scaling factor sj. It is
done in connection with partial pivoting to get more accurate solutions. Despite much
research (see Refs. [E9], [E24] in App. 1) and the proposition of several principles, scaling
is still not well understood. As a possibility, one can scale for pivot choice only (not in
the calculation, to avoid additional roundoff) and take as first pivot the entry aj1 for which
ƒ aj1 ƒ > ƒ Aj ƒ is largest; here Aj is an entry of largest absolute value in Row j. Similarly in
the further steps of the Gauss elimination.
For instance, for the system
4.0000x 1 ϩ 14020x 2 ϭ 14060
0.4003x 1 Ϫ 1.502x 2 ϭ 2.501
we might pick 4 as pivot, but dividing the first equation by 104 gives the system in
Example 3, for which the second equation is a better pivot equation.

Operation Count
Quite generally, important factors in judging the quality of a numeric method are
Amount of storage
Amount of time (ϵ number of operations)
Effect of roundoff error
For the Gauss elimination, the operation count for a full matrix (a matrix with relatively
many nonzero entries) is as follows. In Step k we eliminate x k from n Ϫ k equations.
This needs n Ϫ k divisions in computing the m jk (line 3) and (n Ϫ k)(n Ϫ k ϩ 1)
multiplications and as many subtractions (both in line 4). Since we do n Ϫ 1 steps, k
goes from 1 to n Ϫ 1 and thus the total number of operations in this forward
elimination is
n؊1

n؊1

f (n) ϭ a (n Ϫ k) ϩ 2 a (n Ϫ k)(n Ϫ k ϩ 1)
kϭ1
n؊1

(write n Ϫ k ϭ s)

kϭ1
n؊1

ϭ a s ϩ 2 a s (s ϩ 1) ϭ 12 (n Ϫ 1)n ϩ 23 (n 2 Ϫ 1)n Ϸ 23 n 3
sϭ1

sϭ1

where 2n >3 is obtained by dropping lower powers of n. We see that f (n) grows about
proportional to n 3. We say that f (n) is of order n 3 and write
3

f (n) ϭ O(n 3)
where O suggests order. The general definition of O is as follows. We write
f (n) ϭ O(h (n))
if the quotients ƒ f (n)>h(n) ƒ and ƒ h(n)>f (n) ƒ remain bounded (do not trail off to infinity)
as n : ϱ. In our present case, h(n) ϭ n 3 and, indeed, f (n)>n 3 : 23 because the omitted
terms divided by n 3 go to zero as n : ϱ.

c20-a.qxd

11/2/10

8:57 PM

Page 851

SEC. 20.1 Linear Systems: Gauss Elimination

851

In the back substitution of x i we make n Ϫ i multiplications and as many subtractions,
as well as 1 division. Hence the number of operations in the back substitution is
n

n

b(n) ϭ 2 a (n Ϫ i) ϩ n ϭ 2 a s ϩ n ϭ n(n ϩ 1) ϩ n ϭ n 2 ϩ 2n ϭ O(n 2).
iϭ1

sϭ1

We see that it grows more slowly than the number of operations in the forward elimination
of the Gauss algorithm, so that it is negligible for large systems because it is smaller by
a factor n, approximately. For instance, if an operation takes 10؊9 sec, then the times
needed are:
Algorithm

n ϭ 1000

n ϭ 10000

Elimination
Back substitution

0.7 sec
0.001 sec

11 min
0.1 sec

PROBLEM SET 20.1
APPLICATIONS of linear systems see Secs. 7.1 and 8.2.
1–3

7. Ϫ3x 1 ϩ 6x 2 Ϫ 9x 3 ϭ Ϫ46.725
x 1 Ϫ 4x 2 ϩ 3x 3 ϭ

GEOMETRIC INTERPRETATION

Solve graphically and explain geometrically.
1. x 1 Ϫ 4x 2 ϭ 20.1

2x 1 ϩ 5x 2 Ϫ 7x 3 ϭ Ϫ20.073
8. 5x 1 ϩ 3x 2 ϩ

3x 1 ϩ 5x 2 ϭ 5.9

10x 1 Ϫ 6x 2 ϩ 26x 3 ϭ

10.25x 1 Ϫ 17.22x 2 ϭ 0

2

Ϫ 8x 3 ϭ Ϫ85.88

6x 1

Ϫ14.4x 1 ϩ 7.0x 2 ϭ 31.0

13x 1 Ϫ 8x 2

GAUSS ELIMINATION

Solve the following linear systems by Gauss elimination,
with partial pivoting if necessary (but without scaling). Show
the intermediate steps. Check the result by substitution. If no
solution or more than one solution exists, give a reason.

0

6x 2 ϩ 13x 3 ϭ 137.86

9.
7.2x 1 Ϫ 3.5x 2 ϭ 16.0

4–16

x3 ϭ

Ϫ4x 2 ϩ 8x 3 ϭ Ϫ3

2. Ϫ5.00x 1 ϩ 8.40x 2 ϭ 0

3.

19.571

ϭ 178.54

10. 4x 1 ϩ 4x 2 ϩ 2x 3 ϭ 0
3x 1 Ϫ x 2 ϩ 2x 3 ϭ 0
3x 1 ϩ 7x 2 ϩ x 3 ϭ 0
11. 3.4x 1 Ϫ 6.12x 2 Ϫ 2.72x 3 ϭ 0

4. 6x 1 ϩ x 2 ϭ Ϫ3
6

Ϫx 1 ϩ 1.80x 2 ϩ 0.80x 3 ϭ 0

5. 2x 1 Ϫ 8x 2 ϭ Ϫ4

2.7x 1 Ϫ 4.86x 2 ϩ 2.16x 3 ϭ 0

4x 1 Ϫ 2x 2 ϭ

3x 1 ϩ x 2 ϭ
6.

12. 5x 1 ϩ 3x 2 ϩ

7

25.38x 1 Ϫ 15.48x 2 ϭ

30.60

Ϫ14.10x 1 ϩ 8.60x 2 ϭ Ϫ17.00

x3 ϭ

2

Ϫ4x 2 ϩ 8x 3 ϭ Ϫ3
10x 1 Ϫ 6x 2 ϩ 26x 3 ϭ

0

c20-a.qxd

11/2/10

8:57 PM

852

Page 852

CHAP. 20 Numeric Linear Algebra
3x 2 ϩ 5x 3 ϭ

13.

3x 1 Ϫ 4x 2
5x 1

1.20736

compare the calculations step by step. Explain why the
elimination fails if no solution exists.

ϭ Ϫ2.34066

x1 ϩ x2 ϩ x3 ϭ 3

ϩ 6x 3 ϭ Ϫ0.329193

4x 1 ϩ 2x 2 Ϫ x 3 ϭ 5

14. Ϫ47x 1 ϩ 4x 2 Ϫ 7x 3 ϭ Ϫ118
19x 1 Ϫ 3x 2 ϩ 2x 3 ϭ

ϭ Ϫ25

x1 ϩ x2 ϩ x3 ϭ 3

2.2x 2 ϩ 1.5x 3 Ϫ 3.3x 4 ϭ Ϫ9.30

4x 1 ϩ 2x 2 Ϫ x 3 ϭ 5

Ϫ15x 1 ϩ 5x 2
15.

9x 1 ϩ 5x 2 Ϫ x 3 ϭ 13

43

0.2x 1 ϩ 1.8x 2

ϩ 4.2x 4 ϭ

Ϫx 1 Ϫ 3.1x 2 ϩ 2.5x 3

ϭ Ϫ8.70

Ϫ 3.8x 3 ϩ 1.5x 4 ϭ 11.94

0.5x 1
16. 3.2x 1 ϩ 1.6x 2

ϭ Ϫ0.8

1.6x 1 Ϫ 0.8x 2 ϩ 2.4x 3

ϭ

16.0

2.4x 2 Ϫ 4.8x 3 ϩ 3.6x 4 ϭ Ϫ39.0
3.6x 3 ϩ 2.4x 4 ϭ

10.2

17. CAS EXPERIMENT. Gauss Elimination. Write a
program for the Gauss elimination with pivoting.
Apply it to Probs. 13–16. Experiment with systems
whose coefficient determinant is small in absolute
value. Also investigate the performance of your
program for larger systems of your choice, including
sparse systems.
18. TEAM PROJECT. Linear Systems and Gauss
Elimination. (a) Existence and uniqueness. Find a
and b such that ax 1 ϩ x 2 ϭ b, x 1 ϩ x 2 ϭ 3 has (i) a
unique solution, (ii) infinitely many solutions, (iii) no
solutions.
(b) Gauss elimination and nonexistence. Apply the
Gauss elimination to the following two systems and

20.2

9x 1 ϩ 5x 2 Ϫ x 3 ϭ 12.

9.24

(c) Zero determinant. Why may a computer program
give you the result that a homogeneous linear system
has only the trivial solution although you know its
coefficient determinant to be zero?
(d) Pivoting. Solve System (A) (below) by the Gauss
elimination first without pivoting. Show that for any
fixed machine word length and sufficiently small P Ͼ 0
the computer gives x 2 ϭ 1 and then x 1 ϭ 0. What
is the exact solution? Its limit as P : 0? Then solve
the system by the Gauss elimination with pivoting.
Compare and comment.
(e) Pivoting. Solve System (B) by the Gauss elimination
and three-digit rounding arithmetic, choosing (i) the first
equation, (ii) the second equation as pivot equation.
(Remember to round to 3S after each operation before
doing the next, just as would be done on a computer!)
Then use four-digit rounding arithmetic in those two
calculations. Compare and comment.
(A)

Px 1 ϩ x 2 ϭ 1
x1 ϩ x2 ϭ 2

(B) 4.03x 1 ϩ 2.16x 2 ϭ Ϫ4.61
6.21x 1 ϩ 3.35x 2 ϭ Ϫ7.19

Linear Systems: LU-Factorization,
Matrix Inversion
We continue our discussion of numeric methods for solving linear systems of n equations
in n unknowns x 1, Á , x n,
(1)

Ax ϭ b

where A ϭ [ajk] is the n ϫ n given coefficient matrix and x T ϭ [x 1, Á , x n] and
bT ϭ [b1, Á , bn]. We present three related methods that are modifications of the Gauss

c20-a.qxd

11/2/10

8:57 PM

Page 853

SEC. 20.2 Linear Systems: LU-Factorization, Matrix Inversion

853

elimination, which require fewer arithmetic operations. They are named after Doolittle,
Crout, and Cholesky and use the idea of the LU-factorization of A, which we explain
first.
An LU-factorization of a given square matrix A is of the form
A ϭ LU

(2)

where L is lower triangular and U is upper triangular. For example,
Aϭ

c

2

3

8

5

d

ϭ LU ϭ

c

1

0

4

1

dc

2

3

0

Ϫ7

d.

It can be proved that for any nonsingular matrix (see Sec. 7.8) the rows can be reordered
so that the resulting matrix A has an LU-factorization (2) in which L turns out to be the
matrix of the multipliers m jk of the Gauss elimination, with main diagonal 1, Á , 1, and
U is the matrix of the triangular system at the end of the Gauss elimination. (See Ref.
[E5], pp. 155–156, listed in App. 1.)
The crucial idea now is that L and U in (2) can be computed directly, without solving
simultaneous equations (thus, without using the Gauss elimination). As a count shows,
this needs about n 3>3 operations, about half as many as the Gauss elimination, which
needs about 2n 3>3 (see Sec. 20.1). And once we have (2), we can use it for solving Ax ϭ b
in two steps, involving only about n 2 operations, simply by noting that Ax ϭ LUx ϭ b
may be written
(3)

(a)

Ly ϭ b

where

Ux ϭ y

(b)

and solving first (3a) for y and then (3b) for x. Here we can require that L have main
diagonal 1, Á , 1 as stated before; then this is called Doolittle’s method.1 Both systems
(3a) and (3b) are triangular, so we can solve them as in the back substitution for the Gauss
elimination.
A similar method, Crout’s method,2 is obtained from (2) if U (instead of L) is required
to have main diagonal 1, Á , 1. In either case the factorization (2) is unique.
EXAMPLE 1

Doolittle’s Method
Solve the system in Example 1 of Sec. 20.1 by Doolittle’s method.

Solution. The decomposition (2) is obtained from
a11

a12

a13

3

5

2

A ϭ [ajk] ϭ Da21

a22

a23T ϭ D0

8

a31

a32

a33

2

6

1

0

0

2T ϭ Dm 21

1

8

m 32

m 31

u 11

u 12

u 13

0T D0

u 22

u 23T

1

0

u 33

0

1
MYRICK H. DOOLITTLE (1830–1913). American mathematician employed by the U.S. Coast and Geodetic
Survey Office. His method appeared in U.S. Coast and Geodetic Survey, 1878, 115–120.
2
PRESCOTT DURAND CROUT (1907–1984), American mathematician, professor at MIT, also worked at
General Electric.

c20-a.qxd

11/2/10

854

8:57 PM

Page 854

CHAP. 20 Numeric Linear Algebra
by determining the m jk and u jk, using matrix multiplication. By going through A row by row we get successively
a11 ϭ 3 ϭ 1 ؒ u 11 ϭ u 11

a12 ϭ 5 ϭ 1 ؒ u 12 ϭ u 12

a13 ϭ 2 ϭ 1 ؒ u 13 ϭ u 13

a21 ϭ 0 ϭ m 21u 11

a22 ϭ 8 ϭ m 21u 12 ϩ u 22

a23 ϭ 2 ϭ m 21u 13 ϩ u 23

m 21 ϭ 0

u 22 ϭ 8

a31 ϭ 6 ϭ m 31u 11

u 23 ϭ 2

a32 ϭ 2 ϭ m 31u 12 ϩ m 32u 22

ϭ m 31 ؒ 3

a33 ϭ 8 ϭ m 31u 13 ϩ m 32u 23 ϩ u 33

ϭ 2 ؒ 5 ϩ m 32 ؒ 8

m 31 ϭ 2

ϭ 2 ؒ 2 Ϫ 1 ؒ 2 ϩ u 33

m 32 ϭ Ϫ1

u 33 ϭ 6

Thus the factorization (2) is
3

5

2

1

0

0

3

5

2

D0

8

2T ϭ LU ϭ D0

1

0T D0

8

2T .

6

2

8

1

0

6

2

Ϫ1

0

We first solve Ly ϭ b, determining y1 ϭ 8, then y2 ϭ Ϫ7, then y3 from 2y1 Ϫ y2 ϩ y3 ϭ 16 ϩ 7 ϩ y3 ϭ 26;
thus (note the interchange in b because of the interchange in A!)
1

0

0

D0

1

0T Dy2T ϭ DϪ7T .

2

Ϫ1

1

y1

y3

8

8
Solution

y ϭ DϪ7T .

26

3

Then we solve Ux ϭ y, determining x 3 ϭ 36 then x 2, then x 1, that is,
3

5

2

x1

D0

8

2T Dx 2T ϭ DϪ7T .

0

0

6

x3

8

4
Solution

x ϭ DϪ1T .
1
2

3

᭿

This agrees with the solution in Example 1 of Sec. 20.1.

Our formulas in Example 1 suggest that for general n the entries of the matrices L ϭ [m jk]
(with main diagonal 1, Á , 1 and m jk suggesting “multiplier”) and U ϭ [u jk] in the
Doolittle method are computed from

(4)

u 1k ϭ a1k

k ϭ 1, Á , n

aj1
m j1 ϭ u
11

j ϭ 2, Á , n
j؊1

u jk ϭ ajk Ϫ a m jsu sk

k ϭ j, Á , n; j м 2

sϭ1
k؊1

1
m jk ϭ u
aajk Ϫ a m jsu sk b
kk
sϭ1

j ϭ k ϩ 1, Á , n; k м 2.

c20-a.qxd

11/10/10

2:33 AM

Page 855

SEC. 20.2 Linear Systems: LU-Factorization, Matrix Inversion

Row Interchanges.

855

Matrices, such as

c

0

1

1

1

d

c

or

0

1

1

0

d

have no LU-factorization (try!). This indicates that for obtaining an LU-factorization, row
interchanges of A (and corresponding interchanges in b) may be necessary.

Cholesky’s Method
For a symmetric, positive definite matrix A (thus A ϭ AT, x TAx Ͼ 0 for all x 0) we
can in (2) even choose U ϭ LT, thus u jk ϭ m kj (but cannot impose conditions on the
main diagonal entries). For example,

(5)

4

2

AϭD 2

17

14

Ϫ5

14

2

0

0

2

1

7

Ϫ5T ϭ LLT ϭ D 1

4

0T D 0

4

Ϫ3T .

5

0

5

83

7

Ϫ3

0

The popular method of solving Ax ϭ b based on this factorization A ϭ LLT is called
Cholesky’s method.3 In terms of the entries of L ϭ [l jk] the formulas for the factorization
are
l 11 ϭ 1a11
l j1 ϭ
(6)

l jj ϭ

aj1

j ϭ 2, Á , n

l 11
j؊1

a Ϫ a l 2js
B jj

j ϭ 2, Á , n

sϭ1

j؊1

l pj ϭ

1
aapj Ϫ a l jsl ps b
l jj
sϭ1

p ϭ j ϩ 1, Á , n; j м 2.

If A is symmetric but not positive definite, this method could still be applied, but then
leads to a complex matrix L, so that the method becomes impractical.
EXAMPLE 2

Cholesky’s Method
Solve by Cholesky’s method:
4x 1 ϩ 2x 2 ϩ 14x 3 ϭ

14

2x 1 ϩ 17x 2 Ϫ 5x 3 ϭ Ϫ101
14x 1 Ϫ 5x 2 ϩ 83x 3 ϭ
3

155.

ANDRÉ-LOUIS CHOLESKY (1875–1918), French military officer, geodecist, and mathematician. Surveyed
Crete and North Africa. Died in World War I. His method was published posthumously in Bulletin Géodésique
in 1924 but received little attention until JOHN TODD (1911–2007) — Irish-American mathematician, numerical
analysist, and early pioneer of computer methods in numerics, professor at Caltech, and close personal friend
and collaborator of ERWIN KREYSZIG, see [E20]—taught Cholesky’s method in his analysis course at King’s
College, London, in the 1940s.

c20-a.qxd

11/2/10

8:57 PM

856

Page 856

CHAP. 20 Numeric Linear Algebra

Solution. From (6) or from the form of the factorization
4

2

D 2

17

14

Ϫ5

0

0

Ϫ5T ϭ Dl 21

l 22

l 31

l 32

14

l 11

83

l 11

l 21

l 31

0 T D0

l 22

l 32T

l 33

0

l 33

0

we compute, in the given order,
l 11 ϭ 1a11 ϭ 2

l 21 ϭ

a21
l 11

ϭ

2
2

ϭ1

l 31 ϭ

a31
l 11

ϭ

14
2

ϭ7

l 22 ϭ 2a22 Ϫ l 221 ϭ 117 Ϫ 1 ϭ 4
l 32 ϭ

1
l 23

(a32 Ϫ l 31l 21) ϭ

1
4

(Ϫ5 Ϫ 7 # 1) ϭ Ϫ3

l 33 ϭ 2a33 Ϫ l 231 Ϫ l 232 ϭ 283 Ϫ 72 Ϫ (Ϫ3)2 ϭ 5.
This agrees with (5). We now have to solve Ly ϭ b, that is,
2

0

0

D1

4

0T Dy2T ϭ DϪ101T .

7

Ϫ3

5

y1

y3

14

7
Solution

y ϭ DϪ27T .

155

5

As the second step, we have to solve Ux ϭ LTx ϭ y, that is,

THEOREM 1

2

1

D0

4

0

0

7

x1

7

Ϫ3T Dx 2T ϭ DϪ27T .
5

x3

3
Solution

x ϭ DϪ6T .

5

᭿

1

Stability of the Cholesky Factorization

The Cholesky LLT-factorization is numerically stable (as defined in Sec. 19.1).

PROOF

We have ajj ϭ l 2j1 ϩ l 2j2 ϩ Á ϩ l 2jj by squaring the third formula in (6) and solving it
for ajj. Hence for all l jk (note that l jk ϭ 0 for k Ͼ j) we obtain (the inequality being trivial)
l 2jk Ϲ l 2j1 ϩ l 2j2 ϩ Á ϩ l 2jj ϭ ajj.
That is, l 2jk is bounded by an entry of A, which means stability against rounding.

᭿

Gauss–Jordan Elimination. Matrix Inversion
Another variant of the Gauss elimination is the Gauss–Jordan elimination, introduced
by W. Jordan in 1920, in which back substitution is avoided by additional computations
that reduce the matrix to diagonal form, instead of the triangular form in the Gauss
elimination. But this reduction from the Gauss triangular to the diagonal form requires
more operations than back substitution does, so that the method is disadvantageous for
solving systems Ax ϭ b. But it may be used for matrix inversion, where the situation is
as follows.

c20-a.qxd

11/2/10

8:57 PM

Page 857

SEC. 20.2 Linear Systems: LU-Factorization, Matrix Inversion

857

The inverse of a nonsingular square matrix A may be determined in principle by solving
the n systems
Ax ϭ bj

(7)

( j ϭ 1, Á , n)

where bj is the jth column of the n ϫ n unit matrix.
However, it is preferable to produce A؊1 by operating on the unit matrix I in the same
way as the Gauss–Jordan algorithm, reducing A to I. A typical illustrative example of this
method is given in Sec. 7.8.

PROBLEM SET 20.2
1–5

DOOLITTLE’S METHOD

(e) When can you obtain Crout’s factorization from
Doolittle’s by transposition?

Show the factorization and solve by Doolittle’s method.
1. 4x 1 ϩ 5x 2 ϭ 14
12x 1 ϩ 14x 2 ϭ 36
2. 2x 1 ϩ 9x 2 ϭ

7. 9x 1 ϩ 6x 2 ϩ 12x 3 ϭ 17.4

82

6x 1 ϩ 13x 2 ϩ 11x 3 ϭ 23.6

3x 1 Ϫ 5x 2 ϭ Ϫ62
3. 5x 1 ϩ 4x 2 ϩ

12x 1 ϩ 11x 2 ϩ 26x 3 ϭ 30.8

x 3 ϭ 6.8

8. 4x 1 ϩ 6x 2 ϩ

10x 1 ϩ 9x 2 ϩ 4x 3 ϭ 17.6

8x 3 ϭ

0

6x 1 ϩ 34x 2 ϩ 52x 3 ϭ Ϫ160

10x 1 ϩ 13x 2 ϩ 15x 3 ϭ 38.4
4.

CHOLESKY’S METHOD

7–12

Show the factorization and solve.

8x 1 ϩ 52x 2 ϩ 129x 3 ϭ Ϫ452

2x 1 ϩ x 2 ϩ 2x 3 ϭ 0

9. 0.01x 1

Ϫ2x 1 ϩ 2x 2 ϩ x 3 ϭ 0

ϩ 0.03x 3 ϭ 0.14
0.16x 2 ϩ 0.08x 3 ϭ 0.16

x 1 ϩ 2x 2 Ϫ 2x 3 ϭ 18

0.03x 1 ϩ 0.08x 2 ϩ 0.14x 3 ϭ 0.54

5. 3x 1 ϩ 9x 2 ϩ 6x 3 ϭ 4.6

10. 4x 1

18x 1 ϩ 48x 2 ϩ 39x 3 ϭ 27.2

ϩ 2x 3 ϭ 1.5
4x 2 ϩ x 3 ϭ 4.0

9x 1 Ϫ 27x 2 ϩ 42x 3 ϭ 9.0
6. TEAM PROJECT. Crout’s method factorizes
A ϭ LU, where L is lower triangular and U is upper
triangular with diagonal entries u jj ϭ 1, j ϭ 1, Á , n.
(a) Formulas. Obtain formulas for Crout’s method
similar to (4).
(b) Examples. Solve Prob. 5 by Crout’s method.
(c) Factor the following matrix by the Doolittle,
Crout, and Cholesky methods.

2x 1 ϩ x 2 ϩ 2x 3 ϭ 2.5
11.

x 1 Ϫ x 2 ϩ 3x 3 ϩ 2x 4 ϭ

Ϫx 1 ϩ 5x 2 Ϫ 5x 3 Ϫ 2x 4 ϭ Ϫ35
3x 1 Ϫ 5x 2 ϩ 19x 3 ϩ 3x 4 ϭ

94

2x 1 Ϫ 2x 2 ϩ 3x 3 ϩ 21x 4 ϭ

1

12. 4x 1 ϩ 2x 2 ϩ 4x 3

ϭ 20

1

Ϫ4

2

2x 1 ϩ 2x 2 ϩ 3x 3 ϩ 2x 4 ϭ 36

DϪ4

25

4T

4x 1 ϩ 3x 2 ϩ 6x 3 ϩ 3x 4 ϭ 60

2

4

24

(d) Give the formulas for factoring a tridiagonal
matrix by Crout’s method.

15

2x 2 ϩ 3x 3 ϩ 9x 4 ϭ 122
13. Definiteness. Let A, B be n ϫ n and positive definite.
Are ϪA, AT, A ϩ B, A Ϫ B positive definite?

c20-a.qxd

11/2/10

8:57 PM

Page 858

858

CHAP. 20 Numeric Linear Algebra

14. CAS PROJECT. Cholesky’s Method. (a) Write a
program for solving linear systems by Cholesky’s
method and apply it to Example 2 in the text, to Probs.
7–9, and to systems of your choice.

15–19

(b) Splines. Apply the factorization part of the
program to the following matrices (as they occur in
(9), Sec. 19.4 (with cj ϭ 1), in connection with
splines).

2

1

2

1

0

0

1

4

1

0

1
3

1
4

2

0

1

4

1

A ϭ DϪ19

1

1
7T

0

0

1

2

4
63

3
Ϫ28

0

D1

4

1T ,

0

1

2

20.3

INVERSE

Find the inverse by the Gauss–Jordan method, showing the
details.
15. In Prob. 1
16. In Prob. 4
17. In Team Project 6(c)
18. In Prob. 9
19. In Prob. 12
20. Rounding. For the following matrix A find det A.
What happens if you roundoff the given entries to
(a) 5S, (b) 4S, (c) 3S, (d) 2S, (e) lS? What is the
practical implication of your work?

E

U.

13
49

Linear Systems: Solution by Iteration
The Gauss elimination and its variants in the last two sections belong to the direct methods
for solving linear systems of equations; these are methods that give solutions after an
amount of computation that can be specified in advance. In contrast, in an indirect or
iterative method we start from an approximation to the true solution and, if successful,
obtain better and better approximations from a computational cycle repeated as often as
may be necessary for achieving a required accuracy, so that the amount of arithmetic
depends upon the accuracy required and varies from case to case.
We apply iterative methods if the convergence is rapid (if matrices have large main
diagonal entries, as we shall see), so that we save operations compared to a direct method.
We also use iterative methods if a large system is sparse, that is, has very many zero
coefficients, so that one would waste space in storing zeros, for instance, 9995 zeros per
equation in a potential problem of 104 equations in 104 unknowns with typically only 5
nonzero terms per equation (more on this in Sec. 21.4).

Gauss–Seidel Iteration Method4
This is an iterative method of great practical importance, which we can simply explain in
terms of an example.
EXAMPLE 1

Gauss–Seidel Iteration
We consider the linear system
x 1 Ϫ 0.25x 2 Ϫ 0.25x 3
(1)

Ϫ0.25x 1 ϩ
Ϫ0.25x 1

ϭ 50
Ϫ 0.25x 4 ϭ 50

x2
ϩ

x 3 Ϫ 0.25x 4 ϭ 25

Ϫ 0.25x 2 Ϫ 0.25x 3 ϩ

x 4 ϭ 25.

4
PHILIPP LUDWIG VON SEIDEL (1821–1896), German mathematician. For Gauss see footnote 5 in
Sec. 5.4.

c20-a.qxd

11/2/10

8:57 PM

Page 859

SEC. 20.3 Linear Systems: Solution by Iteration

859

(Equations of this form arise in the numeric solution of PDEs and in spline interpolation.) We write the system
in the form
x1 ϭ
(2)

0.25x 2 ϩ 0.25x 3

ϩ 50

x 2 ϭ 0.25x 1

ϩ 0.25x 4 ϩ 50

x 3 ϭ 0.25x 1

ϩ 0.25x 4 ϩ 25

x4 ϭ

0.25x 2 ϩ 0.25x 3

ϩ 25.

These equations are now used for iteration; that is, we start from a (possibly poor) approximation to the solution,
(0)
(0)
(0)
say x (0)
1 ϭ 100, x 2 ϭ 100, x 3 ϭ 100, x 4 ϭ 100, and compute from (2) a perhaps better approximation
Use “old” values
(“New” values here not yet available)

0.25 x2(0) + 0.25 x3(0)

x1(1) =

(3)

+ 50.00 = 100.00

x2(1) =

0.25 x1(1)

0.25 x4(0)

+ 50.00 = 100.00

x3(1) =

0.25 x1(1)

0.25 x4(0)

+ 25.00 = 75.00

x4(1)

0.25 x2(1) + 0.25 x3(1)

=

+ 25.00 = 68.75

Use “new” values

These equations (3) are obtained from (2) by substituting on the right the most recent approximation for each
unknown. In fact, corresponding values replace previous ones as soon as they have been computed, so that in
(0)
(1)
(1)
the second and third equations we use x (1)
1 (not x 1 ), and in the last equation of (3) we use x 2 and x 3 (not
(0)
x (0)
2 and x 3 ). Using the same principle, we obtain in the next step
x (2)
1 ϭ

(1)
0.25x (1)
2 ϩ 0.25x 3

ϭ

0.25x (2)
1

ϭ

0.25x (2)
1

x (2)
2
x (2)
3
x (2)
4

ϭ

0.25x (2)
2

ϩ

ϩ 50.00 ϭ 93.750
ϩ

0.25x (1)
4

ϩ 50.00 ϭ 90.625

ϩ

0.25x (1)
4

ϩ 25.00 ϭ 65.625
ϩ 25.00 ϭ 64.062

0.25x (2)
3

Further steps give the values

x1

x2

x3

x4

89.062
87.891
87.598
87.524
87.506

88.281
87.695
87.549
87.512
87.503

63.281
62.695
62.549
62.512
62.503

62.891
62.598
62.524
62.506
62.502

Hence convergence to the exact solution x 1 ϭ x 2 ϭ 87.5, x 3 ϭ x 4 ϭ 62.5 (verify!) seems rather fast.

᭿

An algorithm for the Gauss–Seidel iteration is shown in Table 20.2. To obtain the
algorithm, let us derive the general formulas for this iteration.
We assume that ajj ϭ 1 for j ϭ 1, Á , n. (Note that this can be achieved if we can
rearrange the equations so that no diagonal coefficient is zero; then we may divide each
equation by the corresponding diagonal coefficient.) We now write

c20-a.qxd

11/2/10

860

8:57 PM

Page 860

CHAP. 20 Numeric Linear Algebra

AϭIϩLϩU

(4)

(ajj ϭ 1)

where I is the n ϫ n unit matrix and L and U are, respectively, lower and upper triangular
matrices with zero main diagonals. If we substitute (4) into Ax ϭ b, we have
Ax ‫( ؍‬I ؉ L ؉ U)x ‫ ؍‬b.
Taking Lx and Ux to the right, we obtain, since Ix ‫ ؍‬x,
x ‫ ؍‬b ؊ Lx ؊ Ux.

(5)

Remembering from (3) in Example 1 that below the main diagonal we took “new”
approximations and above the main diagonal “old” ones, we obtain from (5) the desired
iteration formulas
“New”

“Old”

x (mϩ1) ϭ b Ϫ Lx (mϩ1) Ϫ Ux (m)

(6)

(ajj ϭ 1)

(mϩ1)
where x (m) ϭ [x (m)
ϭ [x (mϩ1)
] is the (m ϩ 1)st
j ] is the mth approximation and x
j
approximation. In components this gives the formula in line 1 in Table 20.2. The matrix
A must satisfy ajj 0 for all j. In Table 20.2 our assumption ajj ϭ 1 is no longer required,
but is automatically taken care of by the factor 1>ajj in line 1.

Table 20.2 Gauss–Seidel Iteration

ALGORITHM GAUSS–SEIDEL (A, b, x(0), P, N)
This algorithm computes a solution x of the system Ax ϭ b given an initial approximation
x(0), where A ϭ [ajk] is an n ϫ n matrix with ajj 0, j ϭ 1, • • • , n.
INPUT: A, b, initial approximation x(0), tolerance P Ͼ 0, maximum number
of iterations N
(N)
OUTPUT: Approximate solution x (m) ϭ [x (m)
does
j ] or failure message that x
not satisfy the tolerance condition

For m ϭ 0, • • • , N Ϫ 1, do:
For j ϭ 1, • • • , n, do:
x (mϩ1)
ϭ
j

1

2

j؊1
1
abj Ϫ a ajkx (mϩ1)
Ϫ
k
ajj
kϭ1

n

(m)
a ajkx k b

kϭjϩ1

End
(mϩ1)
If max ͉x (mϩ1)
Ϫ x (m)
͉ then OUTPUT x(mϩ1). Stop
j
j ͉ Ͻ P ͉x j
j

[Procedure completed successfully]
End
OUTPUT: “No solution satisfying the tolerance condition obtained after N
iteration steps.” Stop
[Procedure completed unsuccessfully]
End GAUSS–SEIDEL

c20-a.qxd

11/2/10

8:57 PM

Page 861

SEC. 20.3 Linear Systems: Solution by Iteration

861

Convergence and Matrix Norms
An iteration method for solving Ax ϭ b is said to converge for an initial x(0) if the
corresponding iterative sequence x (0), x (1), x (2), Á converges to a solution of the given
system. Convergence depends on the relation between x (m) and x (mϩ1). To get this relation
for the Gauss–Seidel method, we use (6). We first have
(I ϩ L) x (mϩ1) ϭ b Ϫ Ux (m)
and by multiplying by (I ϩ L)؊1 from the left,
(7)

x (mϩ1) ϭ Cx (m) ϩ (I ϩ L)؊1 b

C ϭ Ϫ(I ϩ L)؊1 U.

where

The Gauss–Seidel iteration converges for every x (0) if and only if all the eigenvalues
(Sec. 8.1) of the “iteration matrix” C ϭ [cjk] have absolute value less than 1. (Proof in
Ref. [E5], p. 191, listed in App. 1.)
CAUTION! If you want to get C, first divide the rows of A by ajj to have main diagonal
1, Á , 1. If the spectral radius of C (ϭ maximum of those absolute values) is small, then
the convergence is rapid.
Sufficient Convergence Condition. A sufficient condition for convergence is
(8)

ʈC ʈ Ͻ 1.

Here ʈC ʈ is some matrix norm, such as
(9)

ʈC ʈ ϭ

n

n
2

a a cjk
B jϭ1
kϭ1

(Frobenius norm)

or the greatest of the sums of the ƒ cjk ƒ in a column of C
n

(10)

ʈC ʈ ϭ max a ƒ cjk ƒ
k

(Column “sum” norm)

jϭ1

or the greatest of the sums of the ƒ cjk ƒ in a row of C
n

(11)

ʈC ʈ ϭ max a ƒ cjk ƒ
j

(Row “sum” norm).

kϭ1

These are the most frequently used matrix norms in numerics.
In most cases the choice of one of these norms is a matter of computational convenience.
However, the following example shows that sometimes one of these norms is preferable
to the others.

c20-a.qxd

11/2/10

8:57 PM

862
EXAMPLE 2

Page 862

CHAP. 20 Numeric Linear Algebra
Test of Convergence of the Gauss–Seidel Iteration
Test whether the Gauss–Seidel iteration converges for the system
x ϭ 2 Ϫ 12 y Ϫ 12 z

2x ϩ y ϩ z ϭ 4
x ϩ 2y ϩ z ϭ 4

y ϭ 2 Ϫ 12 x Ϫ 12 z

written

z ϭ 2 Ϫ 12 x Ϫ 12 y.

x ϩ y ϩ 2z ϭ 4

Solution. The decomposition (multiply the matrix by 12 – why?) is
1

1
2

1
2

D 12

1

1
2T

1
2

1
2

0

0

0

0

1
2

1
2

ϭ I ϩ L ϩ U ϭ I ϩ D 12

0

0T ϩ D0

0

1
2T

1
2

1
2

0

0

0

1

0

.

It shows that
1

0

0

0

1
2

1
2

C ϭ Ϫ(I ϩ L)؊1 U ϭ Ϫ D Ϫ12

1

0T D0

0

1
2T

Ϫ14

Ϫ12

1

0

0

0

0

Ϫ12

Ϫ12

ϭ D0

1
4

Ϫ14 T .

0

1
8

3
8

We compute the Frobenius norm of C
1
1
1
9 1>2
1>2
ʈC ʈ ϭ A 14 ϩ 14 ϩ 16
ϩ 16
ϩ 64
ϩ 64
ϭ 0.884 Ͻ 1
B ϭ A 50
64 B

and conclude from (8) that this Gauss–Seidel iteration converges. It is interesting that the other two norms would
permit no conclusion, as you should verify. Of course, this points to the fact that (8) is sufficient for convergence
rather than necessary.
᭿

Residual. Given a system Ax ϭ b, the residual r of x with respect to this system is
defined by
(12)

r ϭ b Ϫ Ax.

Clearly, r ϭ 0 if and only if x is a solution. Hence r 0 for an approximate solution. In
the Gauss–Seidel iteration, at each stage we modify or relax a component of an
approximate solution in order to reduce a component of r to zero. Hence the Gauss–Seidel
iteration belongs to a class of methods often called relaxation methods. More about the
residual follows in the next section.

Jacobi Iteration
The Gauss–Seidel iteration is a method of successive corrections because for each
component we successively replace an approximation of a component by a corresponding
new approximation as soon as the latter has been computed. An iteration method is called
a method of simultaneous corrections if no component of an approximation x (m) is used
until all the components of x (m) have been computed. A method of this type is the Jacobi
iteration, which is similar to the Gauss–Seidel iteration but involves not using improved
values until a step has been completed and then replacing x (m) by x (mϩ1) at once, directly
before the beginning of the next step. Hence if we write Ax ϭ b (with ajj ϭ 1 as before!)
in the form x ϭ b ϩ (I Ϫ A)x, the Jacobi iteration in matrix notation is
(13)

x (mϩ1) ϭ b ϩ (I Ϫ A)x (m)

(ajj ϭ 1).

c20-a.qxd

11/2/10

8:57 PM

Page 863

SEC. 20.3 Linear Systems: Solution by Iteration

863

This method converges for every choice of x (0) if and only if the spectral radius of I Ϫ A
is less than 1. It has recently gained greater practical interest since on parallel processors
all n equations can be solved simultaneously at each iteration step.
For Jacobi, see Sec. 10.3. For exercises, see the problem set.

PROBLEM SET 20.3
1. Verify the solution in Example 1 of the text.
2. Show that for the system in Example 2 the Jacobi
iteration diverges. Hint. Use eigenvalues.
3. Verify the claim at the end of Example 2.

GAUSS–SEIDEL ITERATION

4 –10

Do 5 steps, starting from x0 ϭ [1 1 1]T and using 6S in
the computation. Hint. Make sure that you solve each equation
for the variable that has the largest coefficient (why?). Show
the details.
4. 4x 1 Ϫ x 2

ϭ

21

5. 10x 1 ϩ

x3 ϭ 6

x 1 ϩ 10x 2 ϩ

x3 ϭ 6

x 2 ϩ 10x 3 ϭ 6
x 2 ϩ 7x 3 ϭ

6.

5x 1 ϩ x 2

ϭ

5x 1 Ϫ 2x 2

0

ϭ

18

Ϫ2x 1 ϩ 10x 2 Ϫ 2x 3 ϭ Ϫ60
Ϫ 2x 2 ϩ 15x 3 ϭ 128
8. 3x 1 ϩ 2x 2 ϩ x 3 ϭ 7
x 1 ϩ 3x 2 ϩ 2x 3 ϭ 4

t

t

A(t) ϭ D t

1

tT ,

t

t

1

2
b ϭ D2T .
2

(c) Successive overrelaxation (SOR). Show that by
adding and subtracting x (m) on the right, formula (6)
can be written
x (mϩ1) ϭ x (m) ϩ b Ϫ Lx (mϩ1) Ϫ (U ϩ I)x (m)
(ajj ϭ 1).
Anticipation of further corrections motivates the
introduction of an overrelaxation factor v Ͼ 1 to get
the SOR formula for Gauss–Seidel

2x 1 ϩ x 2 ϩ 3x 3 ϭ 7
9. 5x 1 ϩ x 2 ϩ 2x 3 ϭ 19
x 1 ϩ 4x 2 Ϫ 2x 3 ϭ Ϫ2
2x 1 ϩ 3x 2 ϩ 8x 3 ϭ 39
ϩ 5x 3 ϭ

12.5

x 1 ϩ 6x 2 ϩ 2x 3 ϭ

18.5

10. 4x 1

1

For t ϭ 0.2, 0.5, 0.8, 0.9 determine the number of
steps to obtain the exact solution to 6S and the
corresponding spectral radius of C. Graph the number
of steps and the spectral radius as functions of t and
comment.

25.5

x 1 ϩ 6x 2 ϩ x 3 ϭ Ϫ10.5
7.

13. CAS Experiment. Gauss–Seidel Iteration. (a) Write
a program for Gauss–Seidel iteration.

33

x2 ϩ

x1 ϩ

12. In Prob. 5, compute C (a) if you solve the first equation
for x 1, the second for x 2, the third for x 3, proving
convergence; (b) if you nonsensically solve the third
equation for x 1, the first for x 2, the second for x 3, proving
divergence.

(b) Apply the program A(t)x ϭ b, to starting from
[0 0 0]T, where

Ϫx 1 ϩ 4x 2 Ϫ x 3 ϭ Ϫ45
Ϫ x 2 ϩ 4x 3 ϭ

11. Apply the Gauss–Seidel iteration (3 steps) to the system
in Prob. 5, starting from (a) 0, 0, 0 (b) 10, 10, 10.
Compare and comment.

8x 1 ϩ 2x 2 ϩ x 3 ϭ Ϫ11.5

(14)

x (mϩ1) ϭ x (m) ϩ v(b Ϫ Lx (mϩ1)
(ajj ϭ 1)
Ϫ (U ϩ I)x (m))

intended to give more rapid convergence. A recommended value is v ϭ 2>(1 ϩ 11 Ϫ r), where r is
the spectral radius of C in (7). Apply SOR to the matrix
in (b) for t ϭ 0.5 and 0.8 and notice the improvement of
convergence. (Spectacular gains are made with larger
systems.)

c20-a.qxd

11/2/10

8:57 PM

864
14–17

Page 864

CHAP. 20 Numeric Linear Algebra

JACOBI ITERATION

18–20

Do 5 steps, starting from x0 ϭ [1 1 1]. Compare with
the Gauss–Seidel iteration. Which of the two seems to
converge faster? Show the details of your work.
14. The system in Prob. 4
15. The system in Prob. 9
16. The system in Prob. 10
17. Show convergence in Prob. 16 by verifying that I Ϫ A,
where A is the matrix in Prob. 16 with the rows divided
by the corresponding main diagonal entries, has the
eigenvalues Ϫ0.519589 and 0.259795 Ϯ 0.246603i.

20.4

NORMS

Compute the norms (9), (10), (11) for the following (square)
matrices. Comment on the reasons for greater or smaller
differences among the three numbers.
18. The matrix in Prob. 10
19. The matrix in Prob. 5
2k

Ϫk

20. D k

Ϫ2k

Ϫk

Ϫk

Ϫk
kT
2k

Linear Systems: Ill-Conditioning, Norms
One does not need much experience to observe that some systems Ax ϭ b are good,
giving accurate solutions even under roundoff or coefficient inaccuracies, whereas others
are bad, so that these inaccuracies affect the solution strongly. We want to see what is
going on and whether or not we can “trust” a linear system. Let us first formulate the two
relevant concepts (ill- and well-conditioned) for general numeric work and then turn to
linear systems and matrices.
A computational problem is called ill-conditioned (or ill-posed) if “small” changes in
the data (the input) cause “large” changes in the solution (the output). On the other hand,
a problem is called well-conditioned (or well-posed) if “small” changes in the data cause
only “small” changes in the solution.
These concepts are qualitative. We would certainly regard a magnification of inaccuracies
by a factor 100 as “large,” but could debate where to draw the line between “large” and
“small,” depending on the kind of problem and on our viewpoint. Double precision may
sometimes help, but if data are measured inaccurately, one should attempt changing the
mathematical setting of the problem to a well-conditioned one.
Let us now turn to linear systems. Figure 445 explains that ill-conditioning occurs if
and only if the two equations give two nearly parallel lines, so that their intersection point
(the solution of the system) moves substantially if we raise or lower a line just a little.
For larger systems the situation is similar in principle, although geometry no longer helps.
We shall see that we may regard ill-conditioning as an approach to singularity of the
matrix.

y

y

γ

x
(a)

x
(b)

Fig. 445. (a) Well-conditioned and (b) ill-conditioned
linear system of two equations in two unknowns

c20-a.qxd

11/2/10

8:57 PM

Page 865

SEC. 20.4 Linear Systems: Ill-Conditioning, Norms
EXAMPLE 1

865

An Ill-Conditioned System
You may verify that the system
0.9999x Ϫ 1.0001y ϭ 1
xϪ

yϭ1

has the solution x ϭ 0.5, y ϭ Ϫ0.5, whereas the system
0.9999x Ϫ 1.0001y ϭ 1
xϪ

yϭ1ϩP

has the solution x ϭ 0.5 ϩ 5000.5P, y ϭ Ϫ0.5 ϩ 4999.5P. This shows that the system is ill-conditioned because
a change on the right of magnitude P produces a change in the solution of magnitude 5000P, approximately.
We see that the lines given by the equations have nearly the same slope.
᭿

Well-conditioning can be asserted if the main diagonal entries of A have large absolute
values compared to those of the other entries. Similarly if A؊1 and A have maximum
entries of about the same absolute value.
Ill-conditioning is indicated if A؊1 has entries of large absolute value compared to those
of the solution (about 5000 in Example 1) and if poor approximate solutions may still
produce small residuals.
Residual. The residual r of an approximate solution ෂ
x of Ax ϭ b is defined as
ෂ.
r ϭ b Ϫ Ax

(1)
Now b ϭ Ax, so that

ෂ).
r ϭ A(x Ϫ Ax

(2)

Hence r is small if ෂ
x has high accuracy, but the converse may be false:
EXAMPLE 2

Inaccurate Approximate Solution with a Small Residual
The system
1.0001x 1 ϩ

x 2 ϭ 2.0001

x 1 ϩ 1.0001x 2 ϭ 2.0001
has the exact solution x 1 ϭ 1, x 2 ϭ 1. Can you see this by inspection? The very inaccurate approximation
ෂx 1 ϭ 2.0000, ෂx 2 ϭ 0.0001 has the very small residual (to 4D)
rϭ

c

2.0001
2.0001

d

Ϫ

c

1.0001

1.0000

1.0000

1.0001

dc

2.0000
0.0001

d

ϭ

c

2.0001
2.0001

d

Ϫ

c

2.0003
2.0001

d

ϭ

c

Ϫ0.0002
0.0000

d.

From this, a naive person might draw the false conclusion that the approximation should be accurate to 3 or 4
decimals.
Our result is probably unexpected, but we shall see that it has to do with the fact that the system is
᭿
ill-conditioned.

Our goal is to show that ill-conditioning of a linear system and of its coefficient matrix A
can be measured by a number, the condition number ␬(A). Other measures for ill-conditioning

c20-a.qxd

11/2/10

8:57 PM

866

Page 866

CHAP. 20 Numeric Linear Algebra

have also been proposed, but ␬(A) is probably the most widely used one. ␬(A) is defined in
terms of norm, a concept of great general interest throughout numerics (and in modern
mathematics in general!). We shall reach our goal in three steps, discussing
1. Vector norms
2. Matrix norms
3. Condition number ␬ of a square matrix

Vector Norms
A vector norm for column vectors x ϭ [x j] with n components (n fixed) is a generalized
length or distance. It is denoted by ʈ x ʈ and is defined by four properties of the usual
length of vectors in three-dimensional space, namely,
ʈ x ʈ is a nonnegative real number.

(a)

(b) ʈ x ʈ ϭ 0

(3)

if and only if x ϭ 0.

ʈ kx ʈ ϭ ƒ k ƒ ʈ x ʈ

(c)

for all k.

(d) ʈ x ϩ y ʈ Ϲ ʈ x ʈ ϩ ʈ y ʈ

(Triangle inequality).

If we use several norms, we label them by a subscript. Most important in connection with
computations is the p-norm defined by
ʈ x ʈp ϭ ( ƒ x 1 ƒ p ϩ ƒ x 2 ƒ p ϩ Á ϩ ƒ x n ƒ p)1>p

(4)

where p is a fixed number and p м 1. In practice, one usually takes p ϭ 1 or 2 and, as a
third norm, ʈ x ʈؕ (the latter as defined below), that is,
(5)

ʈ x ʈ1 ϭ ƒ x1 ƒ ϩ Á ϩ ƒ x n ƒ

(“l 1-norm”)

(6)

ʈ x ʈ2 ϭ 2x 12 ϩ Á ϩ x n2

(“Euclidean” or “l 2-norm”)

(7)

ʈ x ʈؕ ϭ max ƒ x j ƒ

(“l ؕ-norm”).

j

For n ϭ 3 the l 2-norm is the usual length of a vector in three-dimensional space. The
l 1-norm and l ؕ-norm are generally more convenient in computation. But all three norms
are in common use.
EXAMPLE 3

Vector Norms
If x T ϭ [2

Ϫ3

0

1

Ϫ4], then ʈ x ʈ1 ϭ 10, ʈ x ʈ2 ϭ 130,

ʈ x ʈؕ ϭ 4.

᭿

In three-dimensional space, two points with position vectors x and ෂ
x have distance ƒ x Ϫ ෂ
xƒ
from each other. For a linear system Ax ϭ b, this suggests that we take ʈx Ϫ ෂ
x ʈ as a
measure of inaccuracy and call it the distance between an exact and an approximate
solution, or the error of ෂ
x.

Matrix Norm
If A is an n ϫ n matrix and x any vector with n components, then Ax is a vector with n
components. We now take a vector norm and consider ʈ x ʈ and ʈAxʈ. One can prove (see

c20-a.qxd

11/2/10

8:57 PM

Page 867

SEC. 20.4 Linear Systems: Ill-Conditioning, Norms

867

Ref. [E17]. pp. 77, 92–93, listed in App. 1) that there is a number c (depending on A)
such that
ʈ Ax ʈ Ϲ cʈ x ʈ

(8)

for all x.

Let x 0. Then ʈ x ʈ Ͼ 0 by (3b) and division gives ʈ Ax ʈ>ʈ x ʈ Ϲ c. We obtain the smallest
possible c valid for all x ( 0) by taking the maximum on the left. This smallest c is
called the matrix norm of A corresponding to the vector norm we picked and is denoted
by ʈ A ʈ. Thus
ʈAʈ ϭ max

(9)

the maximum being taken over all x

ʈ Ax ʈ

(x

ʈxʈ

0),

0. Alternatively [see (c) in Team Project 24],

ʈ A ʈ ϭ max ʈ Ax ʈ.

(10)

ʈ x ʈϭ1

The maximum in (10) and thus also in (9) exists. And the name “matrix norm” is
justified because ʈ A ʈ satisfies (3) with x and y replaced by A and B. (Proofs in Ref. [E17]
pp. 77, 92–93.)
Note carefully that ʈ A ʈ depends on the vector norm that we selected. In particular, one
can show that
for the l 1-norm (5) one gets the column “sum” norm (10), Sec. 20.3,
for the l ϱ-norm (7) one gets the row “sum” norm (11), Sec. 20.3.
By taking our best possible (our smallest) c ϭ ʈ A ʈ we have from (8)
ʈ Ax ʈ Ϲ ʈ A ʈ ʈ x ʈ .

(11)

This is the formula we shall need. Formula (9) also implies for two n ϫ n matrices (see
Ref. [E17], p. 98)
(12)

ʈ AB ʈ Ϲ ʈ A ʈ ʈBʈ,

ʈ Anʈ Ϲ ʈ A ʈn .

thus

See Refs. [E9] and [E17] for other useful formulas on norms.
Before we go on, let us do a simple illustrative computation.

EXAMPLE 4

Matrix Norms
Compute the matrix norms of the coefficient matrix A in Example 1 and of its inverse A؊1, assuming that we
use (a) the l 1-vector norm, (b) the l ϱ`-vector norm.

Solution. We use (4*), Sec. 7.8, for the inverse and then (10) and (11) in Sec. 20.3. Thus
0.9999

Ϫ1.0001

1.0000

Ϫ1.0000

AϭB

R,

Ϫ5000.0

5000.5

Ϫ5000.0

4999.5

A؊1 ϭ B

R.

(a) The l 1-vector norm gives the column “sum” norm (10), Sec. 20.3; from Column 2 we thus obtain
ʈ A ʈ ϭ ƒ Ϫ1.0001 ƒ ϩ ƒ Ϫ1.0000 ƒ ϭ 2.0001. Similarly, ʈ A؊1ʈ ϭ 10,000.

c20-a.qxd

11/2/10

8:57 PM

868

Page 868

CHAP. 20 Numeric Linear Algebra
(b) The l ϱ-vector norm gives the row “sum” norm (11), Sec. 20.3; thus ʈ A ʈ ϭ 2, ʈ A؊1ʈ ϭ 10000.5 from
Row 1. We notice that ʈ A؊1 ʈ is surprisingly large, which makes the product ʈ A ʈ ʈ A؊1ʈ large (20,001). We shall
see below that this is typical of an ill-conditioned system.

Condition Number of a Matrix
We are now ready to introduce the key concept in our discussion of ill-conditioning, the
condition number ␬(A) of a (nonsingular) square matrix A, defined by
␬(A) ϭ ʈ A ʈ ʈ A؊1 ʈ .

(13)

The role of the condition number is seen from the following theorem.
THEOREM 1

Condition Number

A linear system of equations Ax ϭ b and its matrix A whose condition number (13)
is small are well-conditioned. A large condition number indicates ill-conditioning.

PROOF

b ϭ Ax and (11) give ʈ b ʈ Ϲ ʈ A ʈ ʈ x ʈ. Let b
gives

0. Then division by ʈ b ʈ ʈ x ʈ

0 and x

ʈAʈ
1
.
Ϲ
ʈxʈ
ʈbʈ

(14)

Multiplying (2) r ϭ A(x Ϫ ෂ
x ) by A؊1 from the left and interchanging sides, we have
؊1
ෂ
x Ϫ x ϭ A r. Now (11) with A؊1 and r instead of A and x yields
ʈx Ϫ ෂ
x ʈ ϭ ʈ A؊1r ʈ Ϲ ʈ A؊1 ʈ ʈ r ʈ .
Division by ʈ x ʈ [note that ʈ x ʈ
(15)

ʈx Ϫ ෂ
xʈ
ʈxʈ

Ϲ

0 by (3b)] and use of (14) finally gives

ʈAʈ
ʈrʈ
1
ʈ A؊1 ʈ ʈ r ʈ Ϲ
ʈ A؊1ʈ ʈ r ʈ ϭ ␬(A)
.
ʈxʈ
ʈbʈ
ʈbʈ

Hence if ␬(A) is small, a small ʈ r ʈ>ʈ b ʈ implies a small relative error ʈ x Ϫ ෂ
x ʈ>ʈ x ʈ, so
that the system is well-conditioned. However, this does not hold if ␬(A) is large; then a
small ʈ r ʈ>ʈ b ʈ does not necessarily imply a small relative error ʈ x Ϫ ෂ
x ʈ>ʈ x ʈ.
᭿
EXAMPLE 5

Condition Numbers. Gauss–Seidel Iteration
5

1

12

Ϫ2

DϪ2

19

Ϫ2

Ϫ9

1

A ϭ D1

4

2T

1

2

4

؊1

has the inverse

A

ϭ

1
56

Since A is symmetric, (10) and (11) in Sec. 20.3 give the same condition number
␬(A) ϭ ʈ A ʈ ʈ A؊1 ʈ ϭ 7 #

1
56

# 30 ϭ 3.75.

We see that a linear system Ax ϭ b with this A is well-conditioned.

Ϫ2
Ϫ9T .
19

c20-a.qxd

11/2/10

8:57 PM

Page 869

SEC. 20.4 Linear Systems: Ill-Conditioning, Norms

869

For instance, if b ϭ [14 0 28]T, the Gauss algorithm gives the solution x ϭ [2 Ϫ5 9]T, (confirm
this). Since the main diagonal entries of A are relatively large, we can expect reasonably good convergence of
the Gauss–Seidel iteration. Indeed, starting from, say, x0 ϭ [1 1 1]T, we obtain the first 8 steps (3D values)

EXAMPLE 6

x1

x2

x3

1.000
2.400
1.630
1.870
1.967
1.993
1.998
2.000
2.000

1.000
Ϫ1.100
Ϫ3.882
Ϫ4.734
Ϫ4.942
Ϫ4.988
Ϫ4.997
Ϫ5.000
Ϫ5.000

1.000
6.950
8.534
8.900
8.979
8.996
8.999
9.000
9.000

᭿

Ill-Conditioned Linear System
Example 4 gives by (10) or (11), Sec. 20.3, for the matrix in Example 1 the very large condition number
␬(A) ϭ 2.0001 ؒ 10000 ϭ 2 ؒ 10000.5 ϭ 200001. This confirms that the system is very ill-conditioned.
Similarly in Example 2, where by (4*), Sec. 7.8 and 6D-computation,
A؊1 ϭ

1
0.0002

c

1.0001

Ϫ1.0000

Ϫ1.0000

1.0001

d

ϭ

c

5000.5

Ϫ5.000.0

Ϫ5000.0

5000.5

d

so that (10), Sec. 20.3, gives a very large ␬(A), explaining the surprising result in Example 2,
␬(A) ϭ (1.0001 ϩ 1.0000)(5000.5 ϩ 5000.0) Ϸ 20,002.

᭿

In practice, A؊1 will not be known, so that in computing the condition number ␬(A), one
must estimate ʈ A؊1ʈ. A method for this (proposed in 1979) is explained in Ref. [E9] listed
in App. 1.
Inaccurate Matrix Entries. ␬(A) can be used for estimating the effect dx of an inaccuracy
dA of A (errors of measurements of the ajk, for instance). Instead of Ax ϭ b we then have
(A ϩ dA)(x ϩ dx) ϭ b.
Multiplying out and subtracting Ax ϭ b on both sides, we obtain
Adx ϩ dA(x ϩ dx) ϭ 0.
Multiplication by A؊1 from the left and taking the second term to the right gives
dx ϭ ϪA؊1dA(x ϩ dx).
Applying (11) with A؊1 and vector dA(x ϩ dx) instead of A and x, we get
ʈ dx ʈ ϭ ʈ A؊1dA(x ϩ dx)ʈ Ϲ ʈ A؊1 ʈ ʈdA(x ϩ dx) ʈ .
Applying (11) on the right, with dA and x Ϫ dx instead of A and x, we obtain
ʈ dx ʈ Ϲ ʈ A؊1 ʈ ʈ dAʈ ʈ x ϩ dx ʈ .

c20-a.qxd

11/2/10

8:57 PM

870

Page 870

CHAP. 20 Numeric Linear Algebra

Now ʈ A؊1ʈ ϭ ␬(A)>ʈ A ʈ by the definition of ␬(A), so that division by ʈ x ϩ dx ʈ shows
that the relative inaccuracy of x is related to that of A via the condition number by the
inequality
ʈ dx ʈ

(16)

ʈxʈ

Ϸ

ʈ dx ʈ
ʈ x ϩ dx ʈ

Ϲ ʈ A؊1ʈ ʈ dA ʈ ϭ ␬(A)

ʈ dA ʈ
ʈAʈ

.

Conclusion. If the system is well-conditioned, small inaccuracies ʈ dA ʈ>ʈ A ʈ can have
only a small effect on the solution. However, in the case of ill-conditioning, if ʈ dA ʈ>ʈ A ʈ
is small, ʈ dx ʈ>ʈ x ʈ may be large.
Inaccurate Right Side. You may show that, similarly, when A is accurate, an inaccuracy
db of b causes an inaccuracy dx satisfying
ʈ dx ʈ

(17)

ʈxʈ

Ϲ ␬(A)

ʈ db ʈ
ʈbʈ

.

Hence ʈ dx ʈ>ʈ x ʈ must remain relatively small whenever ␬(A) is small.
EXAMPLE 7

Inaccuracies. Bounds (16) and (17)
If each of the nine entries of A in Example 5 is measured with an inaccuracy of 0.1, then ʈ dA ʈ ϭ 9 ؒ 0.1 and
(16) gives
ʈ dx ʈ
ʈxʈ

Ϲ 7.5 ؒ

3 # 0.1
7

ϭ 0.321

thus

ʈ dx ʈ Ϲ 0.321 ʈ x ʈ ϭ 0.321 ؒ 16 ϭ 5.14.

By experimentation you will find that the actual inaccuracy ʈ dx ʈ is only about 30% of the bound 5.14. This is
typical.
Similarly, if db ϭ [0.1 0.1 0.1]T, then ʈ db ʈ ϭ 0.3 and ʈ b ʈ ϭ 42 in Example 5, so that (17) gives
ʈ dx ʈ
ʈxʈ

Ϲ 7.5 ؒ

0.3
42

ϭ 0.0536,

hence

ʈ dx ʈ Ϲ 0.0536 ؒ 16 ϭ 0.857

but this bound is again much greater than the actual inaccuracy, which is about 0.15.

Further Comments on Condition Numbers.
may be helpful.

᭿

The following additional explanations

1. There is no sharp dividing line between “well-conditioned” and “ill-conditioned,”
but generally the situation will get worse as we go from systems with small ␬(A) to systems
with larger ␬(A). Now always ␬(A) м 1, so that values of 10 or 20 or so give no reason
for concern, whereas ␬(A) ϭ 100, say, calls for caution, and systems such as those in
Examples 1 and 2 are extremely ill-conditioned.
2. If ␬(A) is large (or small) in one norm, it will be large (or small, respectively) in
any other norm. See Example 5.
3. The literature on ill-conditioning is extensive. For an introduction to it, see [E9].
This is the end of our discussion of numerics for solving linear systems. In the next section
we consider curve fitting, an important area in which solutions are obtained from linear systems.

c20-a.qxd

11/2/10

8:57 PM

Page 871

SEC. 20.4 Linear Systems: Ill-Conditioning, Norms

871

PROBLEM SET 20.4
VECTOR NORMS

1–6

Compute the norms (5), (6), (7). Compute a corresponding
unit vector (vector of norm 1) with respect to the l ϱ-norm.
1. [1 Ϫ3 8

0

Ϫ6 0]

2. [4 Ϫ1 8]

4k, k 3], k Ͼ 4

2

4. [k ,
5. [1

1

1

1 1]

6. [0

0

0

1 0]

7. For what x ϭ [a b

c] will ʈ x ʈ1 ϭ ʈ x ʈ2?

Compute the matrix norm and the condition number
corresponding to the l 1-vector norm.

9.

11.

c
c

15

1

0

4

0

d

10.

5
Ϫ 15

d

12.

Ϫ2

4

13. DϪ2

3

7

Ϫ12

2

0

0

Ϫ20
15. D 0

0.05

0

0

21

10.5

c

2.1

4.5

0.5

1.8

c

7

6

6

5

Ϫ1
0T

1
14. D0.01
0

4.50

3.55

3.55

2.80

20. A ϭ

c

3.0

1.7

1.7

1.0

d
0.01

0

1

0.01T

0.01

1

b1 ϭ

b1 ϭ

c

c

5.2
4.1

4.7
2.7

d,

d,

b2 ϭ

b2 ϭ

c

c

5.2
4.0

4.7
2.71

d

d

1

1
2

1
3

H 3 ϭ D12

1
3

1
4T

1
3

1
4

1
5

.

The n ϫ n Hilbert matrix is H n ϭ [h jk], where
h jk ϭ 1>( j ϩ k Ϫ 1). (Similar matrices occur in
curve fitting by least squares.) Compute the condition
number ␬(H n) for the matrix norm corresponding to
the l ϱ- (or l 1-) vector norm, for n ϭ 2, 3, Á , 6 (or
further if you wish). Try to find a formula that gives
reasonable approximate values of these rapidly
growing numbers.
Solve a few linear systems of your choice, involving
an H n.
24. TEAM PROJECT. Norms. (a) Vector norms in our
text are equivalent, that is, they are related by double
inequalities; for instance,

20
7

d,

d,

23. CAS EXPERIMENT. Hilbert Matrices. The 3 ϫ 3
Hilbert matrix is

d

0T

(a) ʈ x ʈϱ Ϲ ʈ x ʈ1 Ϲ nʈ x ʈϱ

5.25

10.5

7

5.25

4.2

7

5.25

4.2

3.5

5.25

4.2

3.5

3

16. E

c

22. Show that ␬(A) м 1 for the matrix norms (10), (11),
Sec. 20.3, and ␬(A) м 1n for the Frobenius norm (9),
Sec. 20.3.

MATRIX NORMS,
CONDITION NUMBERS

2

19. A ϭ

21. Residual. For Ax ϭ b1 in Prob. 19 guess what the
residual of ෂ
x ϭ [Ϫ10.0 14.1]T, very poorly approximating [Ϫ2 4]T, might be. Then calculate and
comment.

8. Show that ʈ x ʈϱ Ϲ ʈ x ʈ2 Ϲ ʈ x ʈ1.
9–16

Solve Ax ϭ b1, Ax ϭ b2. Compare the solutions and
comment. Compute the condition number of A.

Ϫ2.1 3.0]

3. [0.2 0.6

ILL-CONDITIONED SYSTEMS

19–20

(18)

U

17. Verify (11) for x ϭ [3 15 Ϫ4]T taken with the
l ϱ-norm and the matrix in Prob. 13.
18. Verify (12) for the matrices in Probs. 9 and 10.

(b)

1
n ʈ x ʈ1 Ϲ ʈ x ʈϱ Ϲ ʈ x ʈ1.

Hence if for some x, one norm is large (or small), the
other norm must also be large (or small). Thus in many
investigations the particular choice of a norm is not
essential. Prove (18).
(b) The Cauchy–Schwarz inequality is
ƒ x Ty ƒ Ϲ ʈ x ʈ2 ʈ y ʈ2.

c20-a.qxd

11/2/10

8:57 PM

872

Page 872

CHAP. 20 Numeric Linear Algebra
It is very important. (Proof in Ref. [GenRef7] listed
in App. 1.) Use it to prove
(19a)
(19b)

ʈ A ʈ м 0.

ʈ x ʈ2 Ϲ ʈ x ʈ1 Ϲ 1n ʈ x ʈ2
1
1n

ʈ A ʈ ϭ 0 if and only if A ϭ 0,
ʈ kA ʈ ϭ ƒ k ƒ ʈ A ʈ,

ʈ x ʈ1 Ϲ ʈ x ʈ2 Ϲ ʈ x ʈ1.

ʈ A ϩ B ʈ Ϲ ʈ A ʈ ϩ ʈ B ʈ.

(c) Formula (10) is often more practical than (9).
Derive (10) from (9).
(d) Matrix norms. Illustrate (11) with examples. Give
examples of (12) with equality as well as with strict

20.5

inequality. Prove that the matrix norms (10), (11) in
Sec. 20.3 satisfy the axioms of a norm

25. WRITING PROJECT. Norms and Their Use in
This Section. Make a list of the most important of the
many ideas covered in this section and write a twopage report on them.

Least Squares Method
Having discussed numerics for linear systems, we now turn to an important application,
curve fitting, in which the solutions are obtained from linear systems.
In curve fitting we are given n points (pairs of numbers) (x 1, y1), Á , (x n, yn) and we
want to determine a function f (x) such that
f (x 1) Ϸ y1, Á , f (x n) Ϸ yn,
approximately. The type of function (for example, polynomials, exponential functions,
sine and cosine functions) may be suggested by the nature of the problem (the underlying
physical law, for instance), and in many cases a polynomial of a certain degree will be
appropriate.
Let us begin with a motivation.
If we require strict equality f (x 1) ϭ y1, Á , f (x n) ϭ yn and use polynomials of
sufficiently high degree, we may apply one of the methods discussed in Sec. 19.3 in
connection with interpolation. However, in certain situations this would not be the
appropriate solution of the actual problem. For instance, to the four points
(1)

(Ϫ1.3, 0.103),

(Ϫ0.1, 1.099),

(0.2, 0.808),

(1.3, 1.897)

there corresponds the interpolation polynomial f (x) ϭ x 3 Ϫ x ϩ 1 (Fig. 446), but if we
graph the points, we see that they lie nearly on a straight line. Hence if these values
are obtained in an experiment and thus involve an experimental error, and if the nature
of the experiment suggests a linear relation, we better fit a straight line through
the points (Fig. 446). Such a line may be useful for predicting values to be expected
for other values of x. A widely used principle for fitting straight lines is the method
y
2

–1

1

x

Fig. 446. Approximate fitting of a straight line

c20-a.qxd

11/2/10

8:57 PM

Page 873

SEC. 20.5 Least Squares Method

873

of least squares by Gauss and Legendre. In the present situation it may be formulated
as follows.
Method of Least Squares. The straight line
y ϭ a ϩ bx

(2)

should be fitted through the given points (x 1, y1), Á , (x n, yn) so that the sum of the
squares of the distances of those points from the straight line is minimum, where
the distance is measured in the vertical direction (the y-direction).
The point on the line with abscissa x j has the ordinate a ϩ bx j. Hence its distance from
(x j, yj) is ƒ yj Ϫ a Ϫ bx j ƒ (Fig. 447) and that sum of squares is
n

q ϭ a ( yj Ϫ a Ϫ bx j)2.
jϭ1

q depends on a and b. A necessary condition for q to be minimum is
0q
0a
0q

(3)

0b

ϭ Ϫ2 a ( yj Ϫ a Ϫ bx j) ϭ 0
ϭ Ϫ2 a x j ( yj Ϫ a Ϫ bx j) ϭ 0

(where we sum over j from 1 to n). Dividing by 2, writing each sum as three sums, and
taking one of them to the right, we obtain the result
ϩ b a x j ϭ a yj
a a x j ϩ b a x 2j ϭ a x jyj.
an

(4)

These equations are called the normal equations of our problem.
y

( xj , yj )
yj – a – bxj
y = a + bx
a + bxj

0
0

xj

x

Fig. 447. Vetrical distance of a point (xj, yj)
from a straight line y ϭ a ϩ bx

EXAMPLE 1

Straight Line
Using the method of least squares, fit a straight line to the four points given in formula (1).

Solution. We obtain
n ϭ 4,

a x j ϭ 0.1,

2
a x j ϭ 3.43,

a yj ϭ 3.907,

a x jyj ϭ 2.3839.

c20-a.qxd

11/2/10

8:57 PM

874

Page 874

CHAP. 20 Numeric Linear Algebra
Hence the normal equations are
4a ϩ 0.10b ϭ 3.9070
0.1a ϩ 3.43b ϭ 2.3839.
The solution (rounded to 4D) is a ϭ 0.9601, b ϭ 0.6670, and we obtain the straight line (Fig. 446)

᭿

y ϭ 0.9601 ϩ 0.6670x.

Curve Fitting by Polynomials of Degree m
Our method of curve fitting can be generalized from a polynomial y ϭ a ϩ bx to a
polynomial of degree m
p(x) ϭ b0 ϩ b1x ϩ Á ϩ bmx m

(5)

where m Ϲ n Ϫ 1. Then q takes the form
n

q ϭ a ( yj Ϫ p(x j))2
jϭ1

and depends on m ϩ 1 parameters b0, Á , bm. Instead of (3) we then have m ϩ 1
conditions
0q
(6)
0b0

ϭ 0,

0q
Á,
0bm

ϭ0

which give a system of m ϩ 1 normal equations.
In the case of a quadratic polynomial
p(x) ϭ b0 ϩ b1x ϩ b2x 2

(7)

the normal equations are (summation from 1 to n)
b0n
(8)

ϩ b1 a x j ϩ b2 a x 2j ϭ a yj

b0 a x j ϩ b1 a x 2j ϩ b2 a x 3j ϭ a x jyj
b0 a x 2j ϩ b1 a x 3j ϩ b2 a x 4j ϭ a x 2j yj.

The derivation of (8) is left to the reader.
EXAMPLE 2

Quadratic Parabola by Least Squares
Fit a parabola through the data (0, 5), (2, 4), (4, 1), (6, 6), (8, 7).

Solution. For the normal equations we need n ϭ 5, g x j ϭ 20, g x 2j ϭ 120, g x 3j ϭ 800, g x 4j ϭ 5664,
g yj ϭ 23, gx jyj ϭ 104, g x 2j yj ϭ 696. Hence these equations are

5b0 ϩ 20b1 ϩ 120b2 ϭ 23
20b0 ϩ 120b1 ϩ 800b2 ϭ 104
120b0 ϩ 800b1 ϩ 5664b2 ϭ 696.

c20-a.qxd

11/2/10

8:57 PM

Page 875

SEC. 20.5 Least Squares Method

875

Solving them we obtain the quadratic least squares parabola (Fig. 448)

᭿

y ϭ 5.11429 Ϫ 1.41429x ϩ 0.21429x 2.
y
8

6

4

2

0

2

4

6

8

x

Fig. 448. Least squares parabola in Example 2

For a general polynomial (5) the normal equations form a linear system of equations in
the unknowns b0, Á , bm. When its matrix M is nonsingular, we can solve the system
by Cholesky’s method (Sec. 20.2) because then M is positive definite (and symmetric).
When the equations are nearly linearly dependent, the normal equations may become
ill-conditioned and should be replaced by other methods; see [E5], Sec. 5.7, listed in
App. 1.
The least squares method also plays a role in statistics (see Sec. 25.9).

PROBLEM SET 20.5
1–6

FITTING A STRAIGHT LINE

8–11

Fit a straight line to the given points (x, y) by least squares.
Show the details. Check your result by sketching the points
and the line. Judge the goodness of fit.
1. (0, 2),

(2, 0), (3, Ϫ2), (5, Ϫ3)

2. How does the line in Prob. 1 change if you add a point
far above it, say, (1, 3)? Guess first.
3. (0, 1.8), (1, 1.6),

(2, 1.1),

(3, 1.5),

(4, 2.3)

4. Hooke’s law F ϭ ks. Estimate the spring modulus k
from the force F [lb] and the elongation s [cm], where
(F, s) ϭ (1, 0.3), (2, 0.7), (4, 1.3), (6, 1.9), (10, 3.2),
(20, 6.3).
5. Average speed. Estimate the average speed vav of a
car traveling according to s ϭ v ⅐ t [km] (s ϭ distance
traveled, t [hr] ϭ time) from (t, s) ϭ (9, 140), (10, 220),
(11, 310), (12, 410).
6. Ohm’s law U ϭ Ri. Estimate R from (i, U) ϭ (2, 104),
(4, 206), (6, 314), (10, 530).
7. Derive the normal equations (8).

FITTING A QUADRATIC PARABOLA

Fit a parabola (7) to the points (x, y). Check by sketching.
8. (Ϫ1, 5), (1, 3),

(2, 4),

(3, 8)

9. (2, Ϫ3), (3, 0),

(5, 1),

(6, 0) (7, Ϫ2)

10. t [hr] ϭ Worker’s time on duty, y [sec] ϭ His>her
reaction time, (t, y) ϭ (1, 2.0), (2, 1.78), (3, 1.90),
(4, 2.35), (5, 2.70)
11. The data in Prob. 3. Plot the points, the line, and the
parabola jointly. Compare and comment.
12. Cubic parabola. Derive the formula for the normal
equations of a cubic least squares parabola.
13. Fit curves (2) and (7) and a cubic parabola by least squares
to (x, y) ϭ (Ϫ2, Ϫ30), (Ϫ1, Ϫ4), (0, 4), (1, 4), (2, 22),
(3, 68). Graph these curves and the points on common
axes. Comment on the goodness of fit.
14. TEAM PROJECT. The least squares approximation
of a function f (x) on an interval a Ϲ x Ϲ b by a
function
Fm(x) ϭ a0y0(x) ϩ a1 y1(x) ϩ Á ϩ am ym(x)

c20-a.qxd

11/2/10

8:57 PM

Page 876

876

CHAP. 20 Numeric Linear Algebra
where y0(x), Á , ym(x) are given functions, requires the
determination of the coefficients a0, Á , am such that
b

(9)

Ύ [ f (x) Ϫ F

2
m(x)]

dx

a

becomes minimum. This integral is denoted by
ʈ f Ϫ Fmʈ2, and ʈ f Ϫ Fmʈ is called the L 2-norm of
f Ϫ Fm (L suggesting Lebesgue5). A necessary condition
for that minimum is given by 0ʈ f Ϫ Fmʈ2>0aj ϭ 0,
j ϭ 0, Á , m [the analog of (6)]. (a) Show that this
leads to m ϩ 1 normal equations ( j ϭ 0, Á , m)
m

a h jkak ϭ bj

where

kϭ0
b

(10)

h jk ϭ

Ύ y (x)y (x) dx,
j

k

a

b

bj ϭ

Ύ f (x)y (x) dx.
j

a

20.6

(b) Polynomial. What form does (10) take if
Fm(x) ϭ a0 ϩ a1x ϩ Á ϩ amx m? What is the
coefficient matrix of (10) in this case when the interval
is 0 Ϲ x Ϲ 1?
(c) Orthogonal functions. What are the solutions of
(10) if y0(x), Á , ym(x) are orthogonal on the interval
a Ϲ x Ϲ b? (For the definition, see Sec. 11.5. See also
Sec. 11.6.)
15. CAS EXPERIMENT. Least Squares versus Interpolation. For the given data and for data of your
choice find the interpolation polynomial and the least
squares approximations (linear, quadratic, etc.).
Compare and comment.
(a) (Ϫ2, 0), (Ϫ1, 0), (0, 1), (1, 0), (2, 0)
(b) (Ϫ4, 0), (Ϫ3, 0), (Ϫ2, 0), (Ϫ1, 0), (0, 1),
(1, 0), (2, 0), (3, 0), (4, 0)
(c) Choose five points on a straight line, e.g., (0, 0),
(1, 1), Á , (4, 4). Move one point 1 unit upward and
find the quadratic least squares polynomial. Do this for
each point. Graph the five polynomials on common
axes. Which of the five motions has the greatest effect?

Matrix Eigenvalue Problems: Introduction
We now come to the second part of our chapter on numeric linear algebra. In the first
part of this chapter we discussed methods of solving systems of linear equations, which
included Gauss elimination with backward substitution. This method is known as a direct
method since it gives solutions after a prescribed amount of computation. The Gauss
method was modified by Doolittle’s method, Crout’s method, and Cholesky’s method,
each requiring fewer arithmetic operations than Gauss. Finally we presented indirect
methods of solving systems of linear equations, that is, the Gauss–Seidel method and the
Jacobi iteration. The indirect methods require an undetermined number of iterations. That
number depends on how far we start from the true solution and what degree of accuracy
we require. Moreover, depending on the problem, convergence may be fast or slow or our
computation cycle might not even converge. This led to the concepts of ill-conditioned
problems and condition numbers that help us gain some control over difficulties inherent
in numerics.
The second part of this chapter deals with some of the most important ideas and numeric
methods for matrix eigenvalue problems. This very extensive part of numeric linear algebra
is of great practical importance, with much research going on, and hundreds, if not
thousands, of papers published in various mathematical journals (see the references in
[E8], [E9], [E11], [E29]). We begin with the concepts and general results we shall need
in explaining and applying numeric methods for eigenvalue problems. (For typical models
of eigenvalue problems see Chap. 8.)

5

HENRI LEBESGUE (1875–1941), great French mathematician, creator of a modern theory of measure and
integration in his famous doctoral thesis of 1902.

c20-a.qxd

11/2/10

8:57 PM

Page 877

SEC. 20.6 Matrix Eigenvalue Problems: Introduction

877

An eigenvalue or characteristic value (or latent root) of a given n ϫ n matrix A ϭ [ajk]
is a real or complex number l such that the vector equation
Ax ϭ lx

(1)

has a nontrivial solution, that is, a solution x 0, which is then called an eigenvector or
characteristic vector of A corresponding to that eigenvalue l. The set of all eigenvalues
of A is called the spectrum of A. Equation (1) can be written
(A Ϫ lI)x ϭ 0

(2)

where I is the n ϫ n unit matrix. This homogeneous system has a nontrivial solution if
and only if the characteristic determinant det (A Ϫ lI) is 0 (see Theorem 2 in Sec. 7.5).
This gives (see Sec. 8.1)
THEOREM 1

Eigenvalues

The eigenvalues of A are the solutions l of the characteristic equation

(3)

det (A Ϫ lI) ϭ 5

a11 Ϫ l

a12

Á

a1n

a21

a22 Ϫ l

Á

a2n

#

#

Á

#

an1

an2

Á

ann Ϫ l

5 ϭ 0.

Developing the characteristic determinant, we obtain the characteristic polynomial of A,
which is of degree n in l. Hence A has at least one and at most n numerically different
eigenvalues. If A is real, so are the coefficients of the characteristic polynomial. By familiar
algebra it follows that then the roots (the eigenvalues of A) are real or complex conjugates
in pairs.
To give you some orientation of the underlying approaches of numerics for eigenvalue
problems, note the following. For large or very large matrices it may be very difficult to
determine the eigenvalues, since, in general, it is difficult to find the roots of characteristic
polynomials of higher degrees. We will discuss different numeric methods for finding
eigenvalues that achieve different results. Some methods, such as in Sec. 20.7, will give
us only regions in which complex eigenvalues lie (Geschgorin’s method) or the intervals
in which the largest and smallest real eigenvalue lie (Collatz method). Other methods
compute all eigenvalues, such as the Householder tridiagonalization method and the
QR-method in Sec. 20.9.
To continue our discussion, we shall usually denote the eigenvalues of A by
l1, l2, Á , ln
with the understanding that some (or all) of them may be equal.
The sum of these n eigenvalues equals the sum of the entries on the main diagonal of
A, called the trace of A; thus
n

(4)

n

trace A ϭ a ajj ϭ a lk.
jϭ1

kϭ1

c20-a.qxd

11/2/10

8:57 PM

878

Page 878

CHAP. 20 Numeric Linear Algebra

Also, the product of the eigenvalues equals the determinant of A,
det A ϭ l1l2 Á ln.

(5)

Both formulas follow from the product representation of the characteristic polynomial,
which we denote by f (l),
f (l) ϭ (Ϫ1)n(l Ϫ l1)(l Ϫ l2) Á (l Ϫ ln).
If we take equal factors together and denote the numerically distinct eigenvalues of A by
l1, Á , lr (r Ϲ n), then the product becomes
f (l) ϭ (Ϫ1)n(l Ϫ l1)m1(l Ϫ l2)m2 Á (l Ϫ lr)mr.

(6)

The exponent m j is called the algebraic multiplicity of lj. The maximum number of
linearly independent eigenvectors corresponding to lj is called the geometric multiplicity
of lj. It is equal to or smaller than m j.
A subspace S of R n or C n (if A is complex) is called an invariant subspace of A if
for every v in S the vector Av is also in S. Eigenspaces of A (spaces of eigenvectors;
Sec. 8.1) are important invariant subspaces of A.
An n ϫ n matrix B is called similar to A if there is a nonsingular n ϫ n matrix T such that
B ϭ T ؊1AT.

(7)

Similarity is important for the following reason.
THEOREM 2

Similar Matrices

Similar matrices have the same eigenvalues. If x is an eigenvector of A, then
y ϭ T ؊1x is an eigenvector of B in (7) corresponding to the same eigenvalue. (Proof
in Sec. 8.4.)
Another theorem that has various applications in numerics is as follows.
THEOREM 3

Spectral Shift

If A has the eigenvalues l1, Á , ln, then A Ϫ k I with arbitrary k has the eigenvalues
l1 Ϫ k, Á , ln Ϫ k.
This theorem is a special case of the following spectral mapping theorem.
THEOREM 4

Polynomial Matrices

If l is an eigenvalue of A, then
q(l) ϭ asls ϩ as؊1ls؊1 ϩ Á ϩ a1l ϩ a0
is an eigenvalue of the polynomial matrix
q(A) ϭ as As ϩ as؊1As؊1 ϩ Á ϩ a1A ϩ a0I.

c20-a.qxd

11/2/10

8:57 PM

Page 879

SEC. 20.7 Inclusion of Matrix Eigenvalues

PROOF

879

Ax ϭ lx implies A2x ϭ Alx ϭ lAx ϭ l2x, A3x ϭ l3x, etc. Thus
q(A)x ϭ (asAs ϩ as؊1As؊1 ϩ Á ) x
ϭ asAsx ϩ as؊1As؊1x ϩ Á
ϭ aslsx ϩ as؊1ls؊1x ϩ Á ϭ q(l) x.

᭿

The eigenvalues of important special matrices can be characterized as follows.
THEOREM 5

Special Matrices
T

The eigenvalues of Hermitian matrices (i.e., A ϭ A), hence of real symmetric matrices
T
(i.e., AT ϭ A), are real. The eigenvalues of skew-Hermitian matrices (i.e., A ϭ ϪA),
T
hence of real skew-symmetric matrices (i.e., A ϭ ϪA), are pure imaginary or 0. The
T
eigenvalues of unitary matrices (i.e., A ϭ A؊1), hence of orthogonal matrices (i.e.,
T
؊1
A ϭ A ), have absolute value 1. (Proofs in Secs. 8.3 and 8.5.)
The choice of a numeric method for matrix eigenvalue problems depends essentially on
two circumstances, on the kind of matrix (real symmetric, real general, complex, sparse,
or full) and on the kind of information to be obtained, that is, whether one wants to know
all eigenvalues or merely specific ones, for instance, the largest eigenvalue, whether
eigenvalues and eigenvectors are wanted, and so on. It is clear that we cannot enter into
a systematic discussion of all these and further possibilities that arise in practice, but we
shall concentrate on some basic aspects and methods that will give us a general
understanding of this fascinating field.

20.7

Inclusion of Matrix Eigenvalues
The whole of numerics for matrix eigenvalues is motivated by the fact that, except for a
few trivial cases, we cannot determine eigenvalues exactly by a finite process because these
values are the roots of a polynomial of nth degree. Hence we must mainly use iteration.
In this section we state a few general theorems that give approximations and error
bounds for eigenvalues. Our matrices will continue to be real (except in formula (5) below),
but since (nonsymmetric) matrices may have complex eigenvalues, complex numbers will
play a (very modest) role in this section.
The important theorem by Gerschgorin gives a region consisting of closed circular disks
in the complex plane and including all the eigenvalues of a given matrix. Indeed, for each
j ϭ 1, Á , n the inequality (1) in the theorem determines a closed circular disk in the
complex l-plane with center ajj and radius given by the right side of (1); and Theorem 1
states that each of the eigenvalues of A lies in one of these n disks.
Gerschgorin’s Theorem6

THEOREM 1

Let l be an eigenvalue of an arbitrary n ϫ n matrix A ϭ [ajk]. Then for some
integer j (1 Ϲ j Ϲ n) we have
(1)

6

ƒ ajj Ϫ l ƒ Ϲ ƒ aj1 ƒ ϩ ƒ aj2 ƒ ϩ Á ϩ ƒ aj, jϪ1 ƒ ϩ ƒ aj, jϩ1 ƒ ϩ Á ϩ ƒ ajn ƒ .

SEMYON ARANOVICH GERSCHGORIN (1901–1933), Russian mathematician.

c20-b.qxd

11/2/10

9:25 PM

880

Page 880

CHAP. 20 Numeric Linear Algebra

PROOF

Let x be an eigenvector corresponding to an eigenvalue l of A. Then
Ax ϭ lx

(2)

(A Ϫ lI)x ϭ 0.

or

Let x j be a component of x that is largest in absolute value. Then we have ƒ x m>x j ƒ Ϲ 1
for m ϭ 1, Á , n. The vector equation (2) is equivalent to a system of n equations for the
n components of the vectors on both sides. The jth of these n equations with j as just
indicated is
aj1x 1 ϩ Á ϩ aj, j؊1x j؊1 ϩ (ajj Ϫ l)x j ϩ aj, jϩ1x jϩ1 ϩ Á ϩ ajnx n ϭ 0.
Division by x j (which cannot be zero; why?) and reshuffling terms gives
x jϪ1
x jϩ1
x1
xn
ajj Ϫ l ϭ Ϫaj1 x Ϫ Á Ϫ aj, jϪ1 x Ϫ aj, jϩ1 x Ϫ Á Ϫ ajn x .
j

j

j

j

By taking absolute values on both sides of this equation, applying the triangle inequality
ƒ a ϩ b ƒ Ϲ ƒ a ƒ ϩ ƒ b ƒ (where a and b are any complex numbers), and observing that
because of the choice of j (which is crucial!), ƒ x 1>x j ƒ Ϲ 1, Á , ƒ x n>x j ƒ Ϲ 1, we obtain (1),
᭿
and the theorem is proved.
EXAMPLE 1

Gerschgorin’s Theorem
For the eigenvalues of the matrix
0

1
2

1
2

A ϭ D12

5

1T

1
2

1

1

we get the Gerschgorin disks (Fig. 449)
D1: Center 0, radius 1,

D2: Center 5, radius 1.5,

D3: Center 1, radius 1.5.

The centers are the main diagonal entries of A. These would be the eigenvalues of A if A were diagonal. We
can take these values as crude approximations of the unknown eigenvalues (3D-values) l1 ϭ Ϫ0.209,
l2 ϭ 5.305, l3 ϭ 0.904 (verify this); then the radii of the disks are corresponding error bounds.
Since A is symmetric, it follows from Theorem 5, Sec. 20.6, that the spectrum of A must actually lie in the
intervals [Ϫ1, 2.5] and [3.5, 6.5].
It is interesting that here the Gerschgorin disks form two disjoint sets, namely, D1 ´ D3, which contains two
᭿
eigenvalues, and D2, which contains one eigenvalue. This is typical, as the following theorem shows.

y
D3

D1
0

1

D2
5

Fig. 449. Gerschgorin disks in Example 1

x

c20-b.qxd

11/2/10

9:25 PM

Page 881

SEC. 20.7 Inclusion of Matrix Eigenvalues

THEOREM 2

881

Extension of Gerschgorin’s Theorem

If p Gerschgorin disks form a set S that is disjoint from the n Ϫ p other disks of a
given matrix A, then S contains precisely p eigenvalues of A (each counted with its
algebraic multiplicity, as defined in Sec. 20.6).
Idea of Proof. Set A ϭ B ϩ C, where B is the diagonal matrix with entries ajj, and
apply Theorem 1 to A t ϭ B ϩ tC with real t growing from 0 to 1.
᭿
EXAMPLE 2

Another Application of Gerschgorin’s Theorem. Similarity
Suppose that we have diagonalized a matrix by some numeric method that left us with some off-diagonal entries
of size 10؊5, say,
10؊5

2
A ϭ D10؊5

10؊5T .

2

10؊5

10؊5

10؊5

4

What can we conclude about deviations of the eigenvalues from the main diagonal entries?

Solution. By Theorem 2, one eigenvalue must lie in the disk of radius 2 ؒ 10؊5 centered at 4 and two

eigenvalues (or an eigenvalue of algebraic multiplicity 2) in the disk of radius 2 ؒ 10؊5 centered at 2. Actually,
since the matrix is symmetric, these eigenvalues must lie in the intersections of these disks and the real axis,
by Theorem 5 in Sec. 20.6.
We show how an isolated disk can always be reduced in size by a similarity transformation. The matrix
1
BϭT

0

؊1

AT ϭ D0

0

1

0

0
2

ϭ D 10

؊5

؊10

10

0

10؊5

2
؊5

T D10

؊5

10

10

10؊5

1

2

1T

10

؊10

2

؊5

10؊5

1

0

0

10

T D0

1

0T

4

0

0

؊5

10

؊5

105

4

is similar to A. Hence by Theorem 2, Sec. 20.6, it has the same eigenvalues as A. From Row 3 we get the
smaller disk of radius 2 ؒ 10؊10. Note that the other disks got bigger, approximately by a factor of 105. And in
choosing T we have to watch that the new disks do not overlap with the disk whose size we want to decrease.
For further interesting facts, see the book [E28].
᭿

By definition, a diagonally dominant matrix A ϭ [ajk] is an n ϫ n matrix such that
ƒ ajj ƒ м a ƒ ajk ƒ

(3)

k

j ϭ 1, Á , n

j

where we sum over all off-diagonal entries in Row j. The matrix is said to be strictly
diagonally dominant if Ͼ in (3) for all j. Use Theorem 1 to prove the following basic
property.
THEOREM 3

Strict Diagonal Dominance

Strictly diagonally dominant matrices are nonsingular.

c20-b.qxd

11/2/10

9:25 PM

882

Page 882

CHAP. 20 Numeric Linear Algebra

Further Inclusion Theorems
An inclusion theorem is a theorem that specifies a set which contains at least one
eigenvalue of a given matrix. Thus, Theorems 1 and 2 are inclusion theorems; they even
include the whole spectrum. We now discuss some famous theorems that yield further
inclusions of eigenvalues. We state the first two of them without proofs (which would
exceed the level of this book).
Schur’s Theorem7

THEOREM 4

Let A ϭ [ajk] be a n ϫ n matrix. Then for each of its eigenvalues l1, Á , ln,
n

(4)

n

n

ƒ lm ƒ 2 Ϲ a ƒ li ƒ 2 Ϲ a a ƒ ajk ƒ 2 (Schur’s inequality).
iϭ1

jϭ1 kϭ1

In (4) the second equality sign holds if and only if A is such that
(5)

T

T

A A ϭ AA .

Matrices that satisfy (5) are called normal matrices. It is not difficult to see that Hermitian,
skew-Hermitian, and unitary matrices are normal, and so are real symmetric, skew-symmetric,
and orthogonal matrices.
EXAMPLE 3

Bounds for Eigenvalues Obtained from Schur’s Inequality
For the matrix
26

Ϫ2

2

AϭD 2

21

4T

4

2

28

we obtain from Schur’s inequality ƒ l ƒ Ϲ 11949 ϭ 44.1475. You may verify that the eigenvalues are 30, 25,
and 20. Thus 302 ϩ 252 ϩ 202 ϭ 1925 Ͻ 1949; in fact, A is not normal.
᭿

The preceding theorems are valid for every real or complex square matrix. Other theorems
hold for special classes of matrices only. Famous is the following one, which has various
applications, for instance, in economics.
Perron’s Theorem8

THEOREM 5

Let A be a real n ϫ n matrix whose entries are all positive. Then A has a positive
real eigenvalue l ϭ r of multiplicity 1. The corresponding eigenvector can be
chosen with all components positive. (The other eigenvalues are less than r in
absolute value.)

7

ISSAI SCHUR (1875–1941), German mathematician, also known by his important work in group theory.
OSKAR PERRON (1880–1975) and GEORG FROBENIUS (1849–1917), German mathematicians, known
for their work in potential theory, ODEs (Sec. 5.4), and group theory.
8

c20-b.qxd

11/2/10

9:25 PM

Page 883

SEC. 20.7 Inclusion of Matrix Eigenvalues

883

For a proof see Ref. [B3], vol. II, pp. 53–62. The theorem also holds for matrices with
nonnegative real entries (“Perron–Frobenius Theorem”8) provided A is irreducible, that
is, it cannot be brought to the following form by interchanging rows and columns; here
B and F are square and 0 is a zero matrix.

c

B

C

0

F

d

Perron’s theorem has various applications, for instance, in economics. It is interesting
that one can obtain from it a theorem that gives a numeric algorithm:
Collatz Inclusion Theorem9

THEOREM 6

Let A ϭ [ajk] be a real n ϫ n matrix whose elements are all positive. Let x be any
real vector whose components x 1, Á , x n are positive, and let y1, Á , yn be the
components of the vector y ϭ Ax. Then the closed interval on the real axis bounded
by the smallest and the largest of the n quotients qj ϭ yj>x j contains at least one
eigenvalue of A.

PROOF

We have Ax ϭ y or
y Ϫ Ax ϭ 0.

(6)

The transpose AT satisfies the conditions of Theorem 5. Hence AT has a positive eigenvalue
l and, corresponding to this eigenvalue, an eigenvector u whose components u j are all
positive. Thus ATu ϭ lu and by taking the transpose we obtain uTA ϭ luT. From this
and (6) we have
uT(y Ϫ Ax) ϭ uTy Ϫ uTAx ϭ uTy Ϫ luTx ϭ uT(y Ϫ lx) ϭ 0
or written out
n

a u j(yj Ϫ lx j) ϭ 0.
jϭ1

Since all the components u j are positive, it follows that
(7)

yj Ϫ lx j м 0,

that is,

qj м l

for at least one j,

yj Ϫ lx j Ϲ 0,

that is,

qj Ϲ l

for at least one j.

and

Since A and AT have the same eigenvalues, l is an eigenvalue of A, and from (7) the
statement of the theorem follows.
᭿

9

LOTHAR COLLATZ (1910–1990), German mathematician known for his work in numerics.

c20-b.qxd

11/2/10

9:25 PM

Page 884

884

CHAP. 20 Numeric Linear Algebra

EXAMPLE 4

Bounds for Eigenvalues from Collatz’s Theorem. Iteration
For a given matrix A with positive entries we choose an x ϭ x0 and iterate, that is, we compute x1 ϭ Ax0,
x2 ϭ Ax1, Á , x20 ϭ Ax19. In each step, taking x ϭ xj and y ϭ Axj ϭ xjϩ1 we compute an inclusion interval
by Collatz’s theorem. This gives (6S)
0.49

0.02

0.22

1

0.73

A ϭ D0.02

0.28

0.20T , x0 ϭ D1T , x1 ϭ D0.50T , x2 ϭ D0.3186T ,

0.22

0.20

0.40

1

0.00216309

0.5481

0.82

0.5886

0.00155743

Á , x19 ϭ D0.00108155T , x20 ϭ D0.000778713T
0.00216309

0.00155743

and the intervals 0.5 Ϲ l Ϲ 0.82, 0.3186>0.50 ϭ 0.6372 Ϲ l Ϲ 0.5481>0.73 ϭ 0.750822, etc. These intervals
have length

j

1

2

3

10

15

20

Length

0.32

0.113622

0.0539835

0.0004217

0.0000132

0.0000004

Using the characteristic polynomial, you may verify that the eigenvalues of A are 0.72, 0.36, 0.09, so that those
intervals include the largest eigenvalue, 0.72. Their lengths decreased with j, so that the iteration was worthwhile.
The reason will appear in the next section, where we discuss an iteration method for eigenvalues.
᭿

PROBLEM SET 20.7
GERSCHGORIN DISKS

1–6

Find and sketch disks or intervals that contain the
eigenvalues. If you have a CAS, find the spectrum and
compare.
5

5

2

4

1. DϪ2

0

2T

2

4

7

0
3. DϪ0.4

0.4
0

2. D10؊2
10؊2

Ϫ0.1
0.3 T

10؊2

8

10؊2T

10؊2

9

1

0

1

4. D0

4

3T

1

3

0.1

Ϫ0.3

0

2

i

1ϩi

5. D Ϫi

3

0 T 6. D

0

8

1Ϫi

10؊2

10

12–16

12
0.1

9. If a symmetric n ϫ n matrix A ϭ [ajk] has been
diagonalized except for small off-diagonal entries of
size 10؊5, what can you say about the eigenvalues?
10. Optimality of Gerschgorin disks. Illustrate with a
2 ϫ 2 matrix that an eigenvalue may very well lie on
a Gerschgorin circle, so that Gerschgorin disks can
generally not be replaced with smaller disks without
losing the inclusion property.
11. Spectral radius ␳ (A). Using Theorem 1, show that
r(A) cannot be greater than the row sum norm of A.

Ϫ0.2

0.1

6

0 T

Ϫ0.2

0

3

؊T

7. Similarity. In Prob. 2, find T AT such that the radius
of the Gerschgorin circle with center 5 is reduced by a
factor 1>100.
8. By what integer factor can you at most reduce the
Gerschgorin circle with center 3 in Prob. 6?

SPECTRAL RADIUS

Use (4) to obtain an upper bound for the spectral radius:
12. In Prob. 4
13. In Prob. 1
14. In Prob. 6
15. In Prob. 3
16. In Prob. 5
17. Verify that the matrix in Prob. 5 is normal.
18. Normal matrices. Show that Hermitian, skewHermitian, and unitary matrices (hence real symmetric,
skew-symmetric, and orthogonal matrices) are normal.
Why is this of practical interest?
19. Prove Theorem 3 by using Theorem 1.
20. Extended Gerschgorin theorem. Prove Theorem 2.
Hint. Let A ϭ B ϩ C, B ϭ diag (ajj), A t ϭ B ϩ tC,
and let t increase continuously from 0 to 1.

c20-b.qxd

11/2/10

9:25 PM

Page 885

SEC. 20.8 Power Method for Eigenvalues

20.8

885

Power Method for Eigenvalues
A simple standard procedure for computing approximate values of the eigenvalues of an
n ϫ n matrix A ϭ [ajk] is the power method. In this method we start from any vector
x0 ( 0) with n components and compute successively
x1 ϭ Ax0,

x2 ϭ Ax1,

xs ϭ Axs؊1 .

Á,

For simplifying notation, we denote xs؊1 by x and xs by y, so that y ϭ Ax.
The method applies to any n ϫ n matrix A that has a dominant eigenvalue (a l such
that ƒ l ƒ is greater than the absolute values of the other eigenvalues). If A is symmetric, it
also gives the error bound (2), in addition to the approximation (1).
THEOREM 1

Power Method, Error Bounds

Let A be an n ϫ n real symmetric matrix. Let x (
components. Furthermore, let
y ϭ Ax,

m 0 ϭ x Tx,

0) be any real vector with n

m 1 ϭ x Ty,

m 2 ϭ y Ty.

Then the quotient
(1)

m1
qϭ m
0

(Rayleigh10 quotient)

is an approximation for an eigenvalue l of A (usually that which is greatest in
absolute value, but no general statements are possible).
Furthermore, if we set q ϭ l Ϫ P, so that P is the error of q, then
(2)

PROOF

ƒPƒ Ϲ d ϭ

B

m2
2
m0 Ϫ q .

d2 denotes the radicand in (2). Since m 1 ϭ qm 0 by (1), we have
(3)

(y Ϫ qx)T(y Ϫ qx) ϭ m 2 Ϫ 2qm 1 ϩ q 2m 0 ϭ m 2 Ϫ q 2m 0 ϭ d2m 0 .

Since A is real symmetric, it has an orthogonal set of n real unit eigenvectors z 1, Á , z n
corresponding to the eigenvalues l1, Á , ln, respectively (some of which may be equal).
(Proof in Ref. [B3], vol. 1, pp. 270–272, listed in App. 1.) Then x has a representation of
the form
x ϭ a1z 1 ϩ Á ϩ an z n .
10

LORD RAYLEIGH (JOHN WILLIAM STRUTT) (1842–1919), great English physicist and mathematician,
professor at Cambridge and London, known for his important contributions to various branches of applied
mathematics and theoretical physics, in particular, the theory of waves, elasticity, and hydrodynamics. In 1904
he received a Nobel Prize in physics.

c20-b.qxd

11/2/10

9:25 PM

886

Page 886

CHAP. 20 Numeric Linear Algebra

Now Az 1 ϭ l1z 1, etc., and we obtain
y ϭ Ax ϭ a1l1z 1 ϩ Á ϩ anlnz n
and, since the z j are orthogonal unit vectors,
m 0 ϭ x Tx ϭ a 21 ϩ Á ϩ a 2n .

(4)
It follows that in (3),

y Ϫ qx ϭ a1(l1 Ϫ q)z 1 ϩ Á ϩ an(ln Ϫ q)z n .
Since the z j are orthogonal unit vectors, we thus obtain from (3)
(5)

d2m 0 ϭ (y Ϫ qx)T(y Ϫ qx) ϭ a 21(l1 Ϫ q)2 ϩ Á ϩ a 2n(ln Ϫ q)2 .

Now let lc be an eigenvalue of A to which q is closest, where c suggests “closest.” Then
(lc Ϫ q)2 Ϲ (lj Ϫ q)2 for j ϭ 1, Á , n. From this and (5) we obtain the inequality
d2m 0 м (lc Ϫ q)2(a 21 ϩ Á ϩ a 2n) ϭ (lc Ϫ q)2m 0.
Dividing by m 0, taking square roots, and recalling the meaning of d2 gives
dϭ

B

m2
2
m 0 Ϫ q м ƒ lc Ϫ q ƒ .

This shows that d is a bound for the error P of the approximation q of an eigenvalue of
A and completes the proof.
᭿
The main advantage of the method is its simplicity. And it can handle sparse matrices
too large to store as a full square array. Its disadvantage is its possibly slow convergence.
From the proof of Theorem 1 we see that the speed of convergence depends on the ratio
of the dominant eigenvalue to the next in absolute value (2:1 in Example 1, below).
If we want a convergent sequence of eigenvectors, then at the beginning of each step
we scale the vector, say, by dividing its components by an absolutely largest one, as in
Example 1, as follows.
EXAMPLE 1

Application of Theorem 1. Scaling
For the symmetric matrix A in Example 4, Sec. 20.7, and x0 ϭ [1 1
indicated scaling
0.49

0.02

0.22

A ϭ D0.02

0.28

0.20T , x0 ϭ D1T , x1 ϭ D0.609756T , x2 ϭ D0.541284T

0.22

0.20

0.40

0.990663
x5 ϭ D0.504682T ,
1

1

1]T we obtain from (1) and (2) and the

0.890244

1

0.931193

1
0.999707

x10 ϭ D0.500146T ,
1

1
0.999991
x15 ϭ D0.500005T .
1

c20-b.qxd

11/2/10

9:25 PM

Page 887

SEC. 20.8 Power Method for Eigenvalues

887

Here Ax0 ϭ [0.73 0.5 0.82]T, scaled to x1 ϭ [0.73>0.82 0.5>0.82 1]T, etc. The dominant eigenvalue is
0.72, an eigenvector [1 0.5 1]T. The corresponding q and d are computed each time before the next scaling.
Thus in the first step,
qϭ
dϭa

2.05
m1
x T0 Ax0
ϭ 0.683333
ϭ T
ϭ
m0
3
x 0 x0

1>2
1>2
1>2
m2
(Ax0)TAx0
1.4553
Ϫ q 2b ϭ a
Ϫ q 2b ϭ a
Ϫ q 2 b ϭ 0.134743.
m0
3
T
x 0 x0

This gives the following values of q, d, and the error P ϭ 0.72 Ϫ q (calculations with 10D, rounded to 6D):

j

1

2

5

10

q
d
P

0.683333
0.134743
0.036667

0.716048
0.038887
0.003952

0.719944
0.004499
0.000056

0.720000
0.000141
5 ؒ 10؊8

The error bounds are much larger than the actual errors. This is typical, although the bounds cannot be improved;
that is, for special symmetric matrices they agree with the errors.
Our present results are somewhat better than those of Collatz’s method in Example 4 of Sec. 20.7, at the
expense of more operations.
᭿

Spectral shift, the transition from A to A Ϫ kI, shifts every eigenvalue by Ϫk. Although
finding a good k can hardly be made automatic, it may be helped by some other method
or small preliminary computational experiments. In Example 1, Gerschgorin’s theorem
gives Ϫ0.02 Ϲ l Ϲ 0.82 for the whole spectrum (verify!). Shifting by Ϫ0.4 might be too
much (then Ϫ0.42 Ϲ l Ϲ 0.42), so let us try Ϫ0.2.
EXAMPLE 2

Power Method with Spectral Shift
For A Ϫ 0.2I with A as in Example 1 we obtain the following substantial improvements (where the index 1
refers to Example 1 and the index 2 to the present example).

j

1

2

5

d1
d2
P1
P2

0.134743
0.134743
0.036667
0.036667

0.038887
0.034474
0.003952
0.002477

0.004499
0.000693
0.000056
1.3 ⅐ 10؊6

10
0.000141
1.8 ⅐ 10؊6
5 ⅐ 10؊8
9 ⅐ 10؊12

᭿

PROBLEM SET 20.8
POWER METHOD WITHOUT SCALING

1–4

Apply the power method without scaling (3 steps), using
x0 ϭ [1, 1]T or [1 1 1]T. Give Rayleigh quotients and
error bounds. Show the details of your work.

1.

c

9

4

4

3

d

2.

c

7

Ϫ3

Ϫ3

Ϫ1

d

2 Ϫ1

1

3. DϪ1

3

2T

1

2

3

5–8

3.6

Ϫ1.8

4. DϪ1.8

2.8

1.8

Ϫ2.6

1.8
Ϫ2.6 T
2.8

POWER METHOD WITH SCALING

Apply the power method (3 steps) with scaling, using
x0 ϭ [1 1 1]T or [1 1 1 1]T, as applicable. Give

c20-b.qxd

11/2/10

9:25 PM

888

Page 888

CHAP. 20 Numeric Linear Algebra

Rayleigh quotients and error bounds. Show the details of
your work.
5. The matrix in Prob. 3
4

2

3

6. D2

7

6T

3

6

4

5

1

0

0

1
7. E
0

3

1

0

1

3

1

0

0

1

5

2

4

0

1

4

1

2

8

8. E

U

U

0

2

5

2

1

8

2

0

9. Prove that if x is an eigenvector, then d ϭ 0 in (2).
Give two examples.
10. Rayleigh quotient. Why does q generally approximate
the eigenvalue of greatest absolute value? When will
q be a good approximation?
11. Spectral shift, smallest eigenvalue. In Prob. 3 set
B ϭ A Ϫ 3I (as perhaps suggested by the diagonal
entries) and see whether you may get a sequence of q’s
converging to an eigenvalue of A that is smallest (not
largest) in absolute value. Use x0 ϭ [1 1 1]T. Do
8 steps. Verify that A has the spectrum {0, 3, 5}.

20.9

12. CAS EXPERIMENT. Power Method with
Scaling. Shifting. (a) Write a program for n ϫ n
matrices that prints every step. Apply it to the
(nonsymmetric!) matrix (20 steps), starting from
[1 1 1]T.
15

12

3

A ϭ D 18

44

18T .

Ϫ19

Ϫ36

Ϫ7

(b) Experiment in (a) with shifting. Which shift do you
find optimal?
(c) Write a program as in (a) but for symmetric matrices
that prints vectors, scaled vectors, q, and d. Apply it to
the matrix in Prob. 8.
(d). Optimality of ␦. Consider A ϭ
take x 0 ϭ

c

3
Ϫ1

c

0.6

0.8

0.8

Ϫ0.6

d and

d . Show that q ϭ 0, d ϭ 1 for all steps

and the eigenvalues are Ϯ1, so that the interval
[q Ϫ d, q ϩ d] cannot be shortened (by omitting Ϯ1)
without losing the inclusion property. Experiment with
other x0’s.
(e) Find a (nonsymmetric) matrix for which d in (2) is
no longer an error bound.
(f) Experiment systematically with speed of convergence by choosing matrices with the second greatest
eigenvalue (i) almost equal to the greatest, (ii) somewhat different, (iii) much different.

Tridiagonalization and QR-Factorization
We consider the problem of computing all the eigenvalues of a real symmetric matrix
A ϭ 3ajk4, discussing a method widely used in practice. In the first stage we reduce the
given matrix stepwise to a tridiagonal matrix, that is, a matrix having all its nonzero
entries on the main diagonal and in the positions immediately adjacent to the main diagonal
(such as A 3 in Fig. 450, Third Step). This reduction was invented by A. S. Householder11
(J. Assn. Comput. Machinery 5 (1958), 335–342). See also Ref. [E29] in App. 1.
This Householder tridiagonalization will simplify the matrix without changing its
eigenvalues. The latter will then be determined (approximately) by factoring the tridiagonalized matrix, as discussed later in this section.
11

ALSTON SCOTT HOUSEHOLDER (1904–1993), American mathematician, known for his work in
numerical analysis and mathematical biology. He was head of the mathematics division at Oakridge National
Laboratory and later professor at the University of Tennessee. He was both president of ACM (Association for
Computing Machinery) 1954–1956 and SIAM (Society for Industrial and Applied Mathematics) 1963–1964.

c20-b.qxd

11/2/10

9:25 PM

Page 889

SEC. 20.9 Tridiagonalization and QR-Factorization

889

Householder’s Tridiagonalization Method11

An n ϫ n real symmetric matrix A ϭ 3ajk4 being given, we reduce it by n Ϫ 2 successive
similarity transformations (see Sec. 20.6) involving matrices P1, Á , Pn؊2 to tridiagonal
form. These matrices are orthogonal and symmetric. Thus P 1؊1 ϭ P 1T ϭ P1 and similarly
for the others. These transformations produce, from the given A 0 ϭ A ϭ 3ajk4, the matrices
(2) Á
A 1 ϭ 3a (1)
, A nϪ2 ϭ 3a (n؊2)
4 in the form
jk 4, A 2 ϭ 3a jk 4,
jk
A 1 ϭ P1A 0P1
A 2 ϭ P2A 1P2

(1)

.

# # # # # # # # # # #
B ϭ A n؊2 ϭ Pn؊2A n؊3Pn؊2.
The transformations (1) create the necessary zeros, in the first step in Row 1 and Column 1,
in the second step in Row 2 and Column 2, etc., as Fig. 450 illustrates for a 5 ϫ 5 matrix.
B is tridiagonal.
* *
* * *
* *
* *
* *

*
*
*
*

*
*
*
*

First Step
A1 = P1AP1

* *
* * *
* * * *
* * *
* * *

* *
* * *
* * *
* * *
* *

Second Step
A2 = P2 A1P2

Third Step
A3 = P3 A2P3

Fig. 450. Householder’s method for a 5 ϫ 5 matrix.
Positions left blank are zeros created by the method.

How do we determine P1, P2, Á , Pn؊2? Now, all these Pr are of the form
Pr ϭ I Ϫ 2vrv Tr

(2)

(r ϭ 1, Á , n Ϫ 2)

where I is the n ϫ n unit matrix and vr ϭ [vjr] is a unit vector with its first r components
0; thus

(3)

0

0

0

*

0

0

v1 ϭ G*W ,

v2 ϭ G*W ,

Á,

vn؊2 ϭ G o W

o

o

*

*

*

*

where the asterisks denote the other components (which will be nonzero in general).

c20-b.qxd

11/2/10

9:25 PM

890

Page 890

CHAP. 20 Numeric Linear Algebra

Step 1.

v1 has the components
v11 ϭ 0

(4)

(a)

v21 ϭ

(b)

vj1 ϭ

R

ƒ a21 ƒ
1
a1 ϩ
b
2
S1

aj1 sgn a21

j ϭ 3, 4, Á , n

2v21S1

where
(c)

S1 ϭ 2a 221 ϩ a 231 ϩ Á ϩ a 2n1

where S1 Ͼ 0, and sgn a21 ϭ ϩ1 if a21 м 0 and sgn a21 ϭ Ϫ1 if a21 Ͻ 0. With this we
compute P1 by (2) and then A 1 by (1). This was the first step.
Step 2. We compute v2 by (4) with all subscripts increased by 1 and the ajk replaced
by a (1)
jk , the entries of A 1 just computed. Thus [see also (3)]
v12 ϭ v22 ϭ 0
v32 ϭ

(4*)

vj2 ϭ

ƒ a (1)
32 ƒ
1
a1 ϩ
b
B2
S2
(1)
a (1)
j2 sgn a 32

j ϭ 4, 5, Á , n

2v32S2

where
2
2
(1)2
Á ϩ a (1)
S2 ϭ 2a (1)
32 ϩ a 42 ϩ
n2 .

With this we compute P2 by (2) and then A 2 by (1).
Step 3. We compute v3 by (4*) with all subscripts increased by 1 and the a (1)
jk replaced
by the entries a (2)
of
and
so
on.
A
,
jk
2
EXAMPLE 1

Householder Tridiagonalization
Tridiagonalize the real symmetric matrix
6

4

1

1

4

6

1

1

1

1

5

2

1

1

2

5

A ϭ A0 ϭ E

U.

Solution. Step 1. We compute S 21 ϭ 42 ϩ 12 ϩ 12 ϭ 18 from (4c). Since a21 ϭ 4 Ͼ 0, we have sgn a21 ϭ ϩ1
in (4b) and get from (4) by straightforward computation

c20-b.qxd

11/2/10

9:25 PM

Page 891

SEC. 20.9 Tridiagonalization and QR-Factorization

891
0

0

v21

0.98559856

v31

0.11957316

v41

0.11957316

v1 ϭ E

UϭE

U.

From this and (2),
1

0

0

0

0

Ϫ0.94280904

Ϫ0.23570227

Ϫ0.23570227

0

Ϫ0.23570227

0.97140452

Ϫ0.02859548

0

Ϫ0.23570227

Ϫ0.02859548

0.97140452

P1 ϭ E

U.

From the first line in (1) we now get
6

Ϫ 118

0

0

7

Ϫ1

Ϫ1

Ϫ1

9
2

3
2

Ϫ1

3
2

9
2

Ϫ 118
A 1 ϭ P1A 0P1 ϭ E
0
0

U.

Step 2. From (4*) we compute S 22 ϭ 2 and
0
0

v2 ϭ E

0
0

UϭE

v32

0.92387953

v42

0.38268343

U.

From this and (2),
1

0

0

0

0

1

0

0

0

0

Ϫ1> 12

Ϫ1> 12

0

0

Ϫ1> 12

Ϫ1> 12

P2 ϭ E

U.

The second line in (1) now gives
6

Ϫ 118

0

0

Ϫ 118
B2 ϭ A 2 ϭ P2A 1P2 ϭ E
0

7

12

0

12

6

0

0

0

0

3

U.

This matrix B is tridiagonal. Since our given matrix has order n ϭ 4, we needed n Ϫ 2 ϭ 2 steps to accomplish
this reduction, as claimed. (Do you see that we got more zeros than we can expect in general?)
B is similar to A, as we now show in general. This is essential because B thus has the same spectrum as A,
᭿
by Theorem 2 in Sec. 20.6.

B Similar to A.
indeed,

We assert that B in (1) is similar to A ϭ A 0. The matrix Pr is symmetric;

P rT ϭ (I Ϫ 2vrv rT)T ϭ I T Ϫ 2(vrv rT)T ϭ I Ϫ 2vrv rT ϭ Pr

c20-b.qxd

11/2/10

892

9:25 PM

Page 892

CHAP. 20 Numeric Linear Algebra

Also, Pr is orthogonal because vr is a unit vector, so that v rTvr ϭ 1 and thus
PrP rT ϭ P r2 ϭ (I Ϫ 2vrv rT)2 ϭ I Ϫ 4vrv rT ϩ 4vrv rTvrv rT
ϭ I Ϫ 4vrv rT ϩ 4vr(v rTvr)v rT ϭ I.
Hence P ؊1
ϭ P rT ϭ Pr and from (1) we now obtain
r
B ϭ Pn؊2A n؊3Pn؊2 ϭ Á
Á ϭ Pn؊2Pn؊3 Á P1AP1 Á Pn؊3Pn؊2
؊1 Á ؊1
ϭ P ؊1
P 1 AP1 Á Pn؊3Pn؊2
n؊2P n؊3

ϭ P ؊1AP
where P ϭ P1P2 Á Pn؊2 . This proves our assertion.

᭿

QR-Factorization Method
In 1958 H. Rutishauser12 of Switzerland proposed the idea of using the LU-factorization
(Sec. 20.2; he called it LR-factorization) in solving eigenvalue problems. An improved
version of Rutishauser’s method (avoiding breakdown if certain submatrices become
singular, etc.; see Ref. [E29]) is the QR-method, independently proposed by the American
J. G. F. Francis (Computer J. 4 (1961–62), 265–271, 332–345) and the Russian V. N.
Kublanovskaya (Zhurnal Vych. Mat. i Mat. Fiz. 1 (1961), 555–570). The QR-method uses
the factorization QR with orthogonal Q and upper triangular R. We discuss the QR-method
for a real symmetric matrix. (For extensions to general matrices see Ref. [E29] in App. 1.)
In this method we first transform a given real symmetric n ϫ n matrix A into a
tridiagonal matrix B0 ϭ B by Householder’s method. This creates many zeros and thus
reduces the amount of further work. Then we compute B1, B2, Á stepwise according to
the following iteration method.
Step 1. Factor B0 ϭ Q 0R 0 with orthogonal Q 0 and upper triangular R 0. Then compute
B1 ϭ R 0Q 0.
Step 2. Factor B1 ϭ Q 1R 1. Then compute B2 ϭ R 1Q 1.
General Step s ϩ 1.
(a)

Factor Bs ϭ Q sR s.

(b)

Compute Bsϩ1 ϭ R sQ s.

(5)

Here Q s is orthogonal and R s upper triangular. The factorization (5a) will be explained
below.
Bsϩ1 Similar to B. Convergence to a Diagonal Matrix. From (5a) we have R s ϭ Q ؊1
s Bs.
Substitution into (5b) gives
(6)

Bsϩ1 ϭ R sQ s ϭ Q ؊1
s BsQ s.

12
HEINZ RUTISHAUSER (1918–1970). Swiss mathematician, professor at ETH Zurich. Known for his
pioneering work in numerics and computer science.

c20-b.qxd

11/3/10

5:19 PM

Page 893

SEC. 20.9 Tridiagonalization and QR-Factorization

893

Thus Bsϩ1 is similar to Bs. Hence Bsϩ1 is similar to B0 ϭ B for all s. By Theorem 2, Sec.
20.6, this implies that Bsϩ1 has the same eigenvalues as B.
Also, Bsϩ1 is symmetric. This follows by induction. Indeed, B0 ϭ B is symmetric.
Assuming Bs to be symmetric, that is, Bs T ϭ Bs, and using Q ؊1
ϭ Q s T (since Q s is
s
orthogonal), we get from (6) the symmetry,
Bsϩ1 T ϭ (Q s TBsQ s)T ϭ Qs TBs TQ s ϭ Q s TBsQ s ϭ Bsϩ1.
If the eigenvalues of B are different in absolute value, say, ƒ l1 ƒ Ͼ ƒ l2 ƒ Ͼ Á Ͼ ƒ ln ƒ ,
then
lim Bs ϭ D

s: ؕ

where D is diagonal, with main diagonal entries l1, l2, Á , ln. (Proof in Ref. [E29] listed
in App. 1.)
How to Get the QR-Factorization, say, B ϭ B0 ϭ [bjk] ϭ Q 0R 0. The tridiagonal matrix
B has n Ϫ 1 generally nonzero entries below the main diagonal. These are
b21, b32, Á , bn,n؊1. We multiply B from the left by a matrix C2 such that C2B ϭ [b (2)
jk ]
(3)
(3)
has b (2)
21 ϭ 0. We multiply this by a matrix C 3 such that C3C 2B ϭ [b jk ] has b 32 ϭ 0,
etc. After n Ϫ 1 such multiplications we are left with an upper triangular matrix R 0,
namely,
CnCn؊1 Á C3C2B0 ϭ R 0.

(7)

These n ϫ n matrices Cj are very simple. Cj has the 2 ϫ 2 submatrix

c

cos uj

sin uj

Ϫsin uj

cos uj

d

(uj suitable)

in Rows j Ϫ 1 and j and Columns j Ϫ 1 and j; everywhere else on the main diagonal the
matrix Cj has entries 1; and all its other entries are 0. (This submatrix is the matrix of a
plane rotation through the angle uj; see Team Project 30, Sec. 7.2.) For instance, if n ϭ 4,
writing cj ϭ cos uj, sj ϭ sin uj, we have

C2 ϭ E

c2

s2

0

0

1

0

0

0

Ϫs2

c2

0

0

0

c3

s3

0

0

1

0

0

Ϫs3

0

0

0

1

0

0

U , C3 ϭ E

1

0

0

0

0

0

1

0

0

c3

0

0

0

c4

s4

0

1

0

0

Ϫs4

c4

U , C4 ϭ E

U.

These Cj are orthogonal. Hence their product in (7) is orthogonal, and so is the inverse
of this product. We call this inverse Q 0. Then from (7),
B0 ϭ Q 0R 0

(8)
where, with C ؊1
ϭ C j T,
j
(9)

Q 0 ϭ (CnCn؊1 Á C3C2)؊1 ϭ C 2TC 3T Á Cn؊1TCnT.

c20-b.qxd

11/2/10

9:25 PM

894

Page 894

CHAP. 20 Numeric Linear Algebra

This is our QR-factorization of B0. From it we have by (5b) with s ϭ 0
B1 ϭ R 0Q 0 ϭ R 0C 2TC3T Á Cn؊1 TC nT.

(10)

We do not need Q 0 explicitly, but to get B1 from (10), we first compute R 0C 2T, then
(R 0C T2 )C 3T, etc. Similarly in the further steps that produce B2, B3, Á .
Determination of cos ␪j and sin ␪j . We finally show how to find the angles of rotation.
cos u2 and sin u2 in C2 must be such that b (2)
21 ϭ 0 in the product

C2B ϭ E

c2

s2

0

Á

Ϫs2

c2

0

Á

#

#

#

Á

#

#

#

Á

b11

b12

b13

Á

b21

b22

b23

Á

#

#

#

Á

#

#

#

Á

U E

U.

Now b (2)
21 is obtained by multiplying the second row of C2 by the first column of B,
b (2)
21 ϭ Ϫs2b11 ϩ c2b21 ϭ Ϫ(sin u2)b11 ϩ (cos u2)b21 ϭ 0.
Hence tan u2 ϭ s2>c2 ϭ b21>b11, and
cos u2 ϭ
(11)
sin u2 ϭ

1

ϭ

21 ϩ tan u2
2

tan u2

ϭ

21 ϩ tan u2
2

1

21 ϩ (b21>b11)2
b21>b11

21 ϩ (b21>b11)2

.

Similarly for u3, u4, Á . The next example illustrates all this.
EXAMPLE 2

QR-Factorization Method
Compute all the eigenvalues of the matrix
6

4

1

1

4

6

1

1

1

1

5

2

1

1

2

5

AϭE

U.

Solution. We first reduce A to tridiagonal form. Applying Householder’s method, we obtain (see Example 1)
6
A2 ϭ E

Ϫ 118

Ϫ 118
7

0

0

12

0

U.

0

12

6

0

0

0

0

3

c20-b.qxd

11/2/10

9:25 PM

Page 895

SEC. 20.9 Tridiagonalization and QR-Factorization

895

From the characteristic determinant we see that A 2, hence A, has the eigenvalue 3. (Can you see this directly
from A 2?) Hence it suffices to apply the QR-method to the tridiagonal 3 ϫ 3 matrix
Ϫ 118

6
B0 ϭ B ϭ DϪ 118

0
12T .

7
12

0

6

Step 1. We multiply B from the left by
cos u2
C2 ϭ DϪsin u2
0

sin u2

0

cos u2

0T

0

1

1

0

C3 ϭ D0

and then C2B by

0

cos u3
Ϫsin u3

0
sin u3 T .
cos u3

Here (Ϫsin u2) # 6 ϩ (cos u2)(Ϫ118) ϭ 0 gives (11) cos u2 ϭ 0.81649658 and sin u2 ϭ Ϫ0.57735027. With
these values we compute
7.34846923

Ϫ7.50555350

Ϫ0.81649658

C2B ϭ D0

3.26598632

1.15470054T .

0

1.41421356

6.00000000

In C3 we get from (Ϫsin u3) # 3.26598632 ϩ (cos u3) # 1.41421356 ϭ 0 the values cos u3 ϭ 0.91766294 and
sin u3 ϭ 0.39735971. This gives
Ϫ7.50555350

7.34846923
R 0 ϭ C3C2B ϭ D0
0

Ϫ0.81649658

3.55902608

3.44378413T .

0

5.04714615

From this we compute
10.33333333

Ϫ2.05480467

B1 ϭ R 0C 2 C 3 ϭ DϪ2.05480467

4.03508772

2.00553251T

2.00553251

4.63157895

T

T

0

0

which is symmetric and tridiagonal. The off-diagonal entries in B1 are still large in absolute value. Hence we
have to go on.
Step 2. We do the same computations as in the first step, with B0 ϭ B replaced by B1 and C2 and C3 changed
accordingly, the new angles being u2 ϭ Ϫ0.196291533 and u3 ϭ 0.513415589. We obtain
10.53565375
R1 ϭ D 0
0

Ϫ2.80232241

Ϫ0.39114588

4.08329584

3.98824028T

0

3.06832668

and from this
10.87987988

Ϫ0.79637918

B2 ϭ DϪ0.79637918

5.44738664

1.50702500T .

0

1.50702500

2.67273348

0

We see that the off-diagonal entries are somewhat smaller in absolute value than those of B1, but still much too
large for the diagonal entries to be good approximations of the eigenvalues of B.

c20-b.qxd

11/2/10

9:25 PM

896

Page 896

CHAP. 20 Numeric Linear Algebra

Further Steps. We list the main diagonal entries and the absolutely largest off-diagonal entry, which is
( j)

( j)

ƒ b 12 ƒ ϭ ƒ b 21 ƒ in all steps. You may show that the given matrix A has the spectrum 11, 6, 3, 2.

Step j

b (11j)

b(22j)

b(33j)

3
5
7
9

10.9668929
10.9970872
10.9997421
10.9999772

5.94589856
6.00181541
6.00024439
6.00002267

2.08720851
2.00109738
2.00001355
2.00000017

max j

(J)

k

͉b jk ͉

0.58523582
0.12065334
0.03591107
0.01068477

᭿

Looking back at our discussion, we recognize that the purpose of applying Householder’s
tridiagonalization before the QR-factorization method is a substantial reduction of cost in
each QR-factorization, in particular if A is large.
Convergence acceleration and thus further reduction of cost can be achieved by a
spectral shift, that is, by taking Bs Ϫ k sI instead of Bs with a suitable k s. Possible choices
of k s are discussed in Ref. [E29], p. 510.

PROBLEM SET 20.9
HOUSEHOLDER TRIDIAGONALIZATION

1–5

52

10

42

52

59

44

80

10

44

39

42

42

80

42

35

Tridiagonalize. Show the details.

3

0.98

0.04

0.44

1. D0.04

0.56

0.40T

0.44

0.40

0.80

0

1

1

2. D1

0

1T

1

1

0

7

2

3

3. D2

10

6T

3

6

7

U

QR-FACTORIZATION

6–9

Do three QR-steps to find approximations of the eigenvalues of:
6. The matrix in the answer to Prob. 1
7. The matrix in the answer to Prob. 3
14.2

Ϫ0.1

0

8. DϪ0.1

Ϫ6.3

0.2T

0

5

4

1

1

4

5

1

1

1

1

4

2

1

1

2

4

4. E

5. E

U

0.2

2.1

140

10

0

9. D 10

70

2T

0

2

Ϫ30

10. CAS EXPERIMENT. QR-Method. Try to find out
experimentally on what properties of a matrix the speed
of decrease of off-diagonal entries in the QR-method
depends. For this purpose write a program that first
tridiagonalizes and then does QR-steps. Try the
program out on the matrices in Probs. 1, 3, and 4.
Summarize your findings in a short report.

CHAPTER 20 REVIEW QUESTIONS AND PROBLEMS
1. What are the main problem areas in numeric linear
algebra?
2. When would you apply Gauss elimination and when
Gauss–Seidel iteration?

3. What is pivoting? Why and how is it done?
4. What happens if you apply Gauss elimination to a
system that has no solutions?
5. What is Cholesky’s method? When would you apply it?

c20-b.qxd

11/2/10

9:25 PM

Page 897

Chapter 20 Review Questions and Problems
6. What do you know about the convergence of the
Gauss–Seidel iteration?
7. What is ill-conditioning? What is the condition number
and its significance?
8. Explain the idea of least squares approximation.
9. What are eigenvalues of a matrix? Why are they
important? Give typical examples.
10. How did we use similarity transformations of matrices
in designing numeric methods?
11. What is the power method for eigenvalues? What are
its advantages and disadvantages?
12. State Gerschgorin’s theorem from memory. Give typical
applications.
13. What is tridiagonalization and QR? When would you
apply it?

GAUSS ELIMINATION

14–17

5

1

1

20. D1

6

0T

1

0

8

21–23

GAUSS–SEIDEL ITERATION

Do 3 steps without scaling, starting from [1 1
21. 4x 1 Ϫ x 2

3x 2 Ϫ 6x 3 ϭ

0

4x 1 Ϫ x 2 ϩ 2x 3 ϭ

16

14.

Ϫ5x 1 ϩ 2x 2 Ϫ 4x 3 ϭ Ϫ20
8x 2 Ϫ 6x 3 ϭ 23.6

15.

10x 1 ϩ 6x 2 ϩ 2x 3 ϭ 68.4
12x 1 Ϫ 14x 2 ϩ 4x 3 ϭ Ϫ6.2
16. 5x 1 ϩ x 2 Ϫ 3x 3 ϭ

17

Ϫ 5x 2 ϩ 15x 3 ϭ Ϫ10
2x 1 Ϫ 3x 2 ϩ 9x 3 ϭ

0

42x 1 ϩ 74x 2 ϩ 36x 3 ϭ 96
Ϫ46x 1 Ϫ 12x 2 Ϫ 2x 3 ϭ 82
3x 1 ϩ 25x 2 ϩ 5x 3 ϭ 19

Ϫx 1

INVERSE MATRIX

Compute the inverse of:
2.0

0.1

3.3

18. D1.6

4.4

0.5T

0.3

Ϫ4.3

15

20

10

19. D20

35

15T

10

15

90

2.8

ϭ 22.0

ϩ 4x 3 ϭ Ϫ2.4

22. 0.2x 1 ϩ 4.0x 2 Ϫ 0.4x 3 ϭ

32.0

0.5x 1 Ϫ 0.2x 2 ϩ 2.5x 3 ϭ Ϫ5.1
7.5x 1 ϩ 0.1x 2 Ϫ 1.5x 3 ϭ Ϫ12.7
x2 Ϫ

x 3 ϭ 17

2x 1 ϩ 20x 2 ϩ

x 3 ϭ 28

3x 1 Ϫ
24–26

x 2 ϩ 25x 3 ϭ 105

VECTOR NORMS

Compute the /1-, /2-, and /ؕ-norms of the vectors.
24. [0.2 Ϫ8.1 0.4 0 0 Ϫ1.3 2]T
25. [8 Ϫ21 13 0]T
26. [0 0 0 Ϫ1 0]T
27–30

MATRIX NORM

Compute the matrix norm corresponding to the /ؕ-vector
norm for the coefficient matrix:
27. In Prob. 15
28. In Prob. 17
29. In Prob. 21
30. In Prob. 22
31–33

18–20

1]T.

4x 2 Ϫ x 3 ϭ 13.4

23. 10x 1 ϩ

Solve

17.

897

CONDITION NUMBER

Compute the condition number (corresponding to the
/ؕ-vector norm) of the coefficient matrix:
31. In Prob. 19
32. In Prob. 18
33. In Prob. 21
34–35

FITTING BY LEAST SQUARES

Fit and graph:
34. A straight line to (Ϫ1, 0), (0, 2), (1, 2), (2, 3),
(3, 3)
35. A quadratic parabola to the data in Prob. 34.

c20-b.qxd

11/3/10

898
36–39

5:19 PM

Page 898

CHAP. 20 Numeric Linear Algebra

EIGENVALUES

Find and graph three circular disks that must contain all the
eigenvalues of the matrix:
36. In Prob. 18
37. In Prob. 19

SUMMARY OF CHAPTER

38. In Prob. 20
39. Of the coefficients in Prob. 14
40. Power method. Do 4 steps with scaling for the matrix
in Prob. 19, starting for [1 1 1] and computing the
Rayliegh quotients and error bounds.

20

Numeric Linear Algebra
Main tasks are the numeric solution of linear systems (Secs. 20.1–20.4), curve fitting
(Sec. 20.5), and eigenvalue problems (Secs. 20.6–20.9).
Linear systems Ax ϭ b with A ϭ [ajk], written out

(1)

E 1:

a11x 1 ϩ Á ϩ a1nx n ϭ b1

E 2:

a21x 1 ϩ Á ϩ a2nx n ϭ b2

Á Á Á Á Á Á Á Á Á Á
E n:

an1x 1 ϩ Á ϩ annx n ϭ bn

can be solved by a direct method (one in which the number of numeric operations
can be specified in advance, e.g., Gauss’s elimination) or by an indirect or iterative
method (in which an initial approximation is improved stepwise).
The Gauss elimination (Sec. 20.1) is direct, namely, a systematic elimination
process that reduces (1) stepwise to triangular form. In Step 1 we eliminate x 1 from
equations E 2 to E n by subtracting (a21>a11) E 1 from E 2, then (a31>a11) E 1 from
E 3, etc. Equation E 1 is called the pivot equation in this step and a11 the pivot. In
Step 2 we take the new second equation as pivot equation and eliminate x 2, etc. If
the triangular form is reached, we get x n from the last equation, then x n؊1 from
the second last, etc. Partial pivoting (ϭ interchange of equations) is necessary if
candidates for pivots are zero, and advisable if they are small in absolute value.
Doolittle’s, Crout’s, and Cholesky’s methods in Sec. 20.2 are variants of the
Gauss elimination. They factor A ϭ LU (L lower triangular, U upper triangular)
and solve Ax ϭ LUx ϭ b by solving Ly ϭ b for y and then Ux ϭ y for x.
In the Gauss–Seidel iteration (Sec. 20.3) we make a11 ϭ a22 ϭ Á ϭ ann ϭ 1
(by division) and write Ax ϭ (I ϩ L ϩ U)x ϭ b; thus x ϭ b Ϫ (L ϩ U)x, which
suggests the iteration formula
(2)

x (mϩ1) ϭ b Ϫ Lx (mϩ1) Ϫ Ux (m)

in which we always take the most recent approximate x j’s on the right. If ʈCʈ Ͻ 1,
where C ϭ Ϫ(I ϩ L)؊1U, then this process converges. Here, ʈCʈ denotes any
matrix norm (Sec. 20.3).

c20-b.qxd

11/2/10

9:25 PM

Page 899

Summary of Chapter 20

899

If the condition number k(A) ϭ ʈ A ʈ ʈ A؊1ʈ of A is large, then the system Ax ϭ b
is ill-conditioned (Sec. 20.4), and a small residual r ϭ b Ϫ A ෂ
x does not imply
that ෂ
x is close to the exact solution.
The fitting of a polynomial p(x) ϭ b0 ϩ b1x ϩ Á ϩ bmx m through given data
(points in the xy-plane) (x 1, y1), Á , (x n, yn) by the method of least squares is
discussed in Sec. 20.5 (and in statistics in Sec. 25.9). If m ϭ n, the least squares
polynomial will be the same as an interpolating polynomial (uniqueness).
Eigenvalues l (values l for which Ax ϭ lx has a solution x 0, called an
eigenvector) can be characterized by inequalities (Sec. 20.7), e.g. in Gerschgorin’s
theorem, which gives n circular disks which contain the whole spectrum (all
eigenvalues) of A, of centers ajj and radii S ƒ ajk ƒ (sum over k from 1 to n, k j).
Approximations of eigenvalues can be obtained by iteration, starting from an
x0 0 and computing x1 ϭ Ax0, x2 ϭ Ax1, Á , xn ϭ Axn؊1. In this power
method (Sec. 20.8) the Rayleigh quotient
˛

(3)

qϭ

˛

(Ax)T)x
x Tx

˛

(x ϭ xn)

gives an approximation of an eigenvalue (usually that of the greatest absolute value)
and, if A is symmetric, an error bound is
(4)

ƒPƒ Ϲ

(Ax)TAx Ϫ q 2.
B x Tx

Convergence may be slow but can be improved by a spectral shift.
For determining all the eigenvalues of a symmetric matrix A it is best to first
tridiagonalize A and then to apply the QR-method (Sec. 20.9), which is based on a
factorization A ϭ QR with orthogonal Q and upper triangular R and uses similarity
transformations.

c21-a.qxd

11/3/10

2:44 PM

Page 900

CHAPTER

21

Numerics for ODEs and PDEs
Ordinary differential equations (ODEs) and partial differential equations (PDEs) play a
central role in modeling problems of engineering, mathematics, physics, aeronautics,
astronomy, dynamics, elasticity, biology, medicine, chemistry, environmental science,
economics, and many other areas. Chapters 1–6 and 12 explained the major approaches
to solving ODEs and PDEs analytically. However, in your career as an engineer, applied
mathematicians, or physicist you will encounter ODEs and PDEs that cannot be solved
by those analytic methods or whose solutions are so difficult that other approaches are
needed. It is precisely in these real-world projects that numeric methods for ODEs and
PDEs are used, often as part of a software package. Indeed, numeric software has become
an indispensable tool for the engineer.
This chapter is evenly divided between numerics for ODEs and numerics for PDEs.
We start with ODEs and discuss, in Sec. 21.1, methods for first-order ODEs. The main
initial idea is that we can obtain approximations to the solution of such an ODE at points
that are a distance h apart by using the first two terms of Taylor’s formula from calculus.
We use these approximations to construct the iteration formula for a method known as
Euler’s method. While this method is rather unstable and of little practical use, it serves
as a pedagogical tool and a starting point toward understanding more sophisticated methods
such as the Runge–Kutta method and its variant the Runga–Kutta–Fehlberg (RKF) method,
which are popular and useful in practice. As is usual in mathematics, one tends to
generalize mathematical ideas. The methods of Sec. 21.1 are one-step methods, that is,
the current approximation uses only the approximation from the previous step. Multistep
methods, such as the Adams–Bashforth methods and Adams–Moulton methods, use values
computed from several previous steps. We conclude numerics for ODEs with applying
Runge–Kutta–Nyström methods and other methods to higher order ODEs and systems of
ODEs.
Numerics for PDEs are perhaps even more exciting and ingenious than those for ODEs.
We first consider PDEs of the elliptic type (Laplace, Poisson). Again, Taylor’s formula
serves as a starting point and lets us replace partial derivatives by difference quotients.
The end result leads to a mesh and an evaluation scheme that uses the Gauss–Seidel
method (here also know as Liebmann’s method). We continue with methods that use grids
to solve Neuman and mixed problems (Sec. 21.5) and conclude with the important
Crank–Nicholson method for parabolic PDEs in Sec. 21.6.
Sections 21.1 and 21.2 may be studied immediately after Chap. 1 and Sec. 21.3
immediately after Chaps. 2–4, because these sections are independent of Chaps. 19 and 20.
Sections 21.4–21.7 on PDEs may be studied immediately after Chap. 12 if students
have some knowledge of linear systems of algebraic equations.
Prerequisite: Secs. 1.1–1.5 for ODEs, Secs. 12.1–12.3, 12.5, 12.10 for PDEs.
References and Answers to Problems: App. 1 Part E (see also Parts A and C), App. 2.
900

c21-a.qxd

11/3/10

2:44 PM

Page 901

SEC. 21.1 Methods for First-Order ODEs

21.1

901

Methods for First-Order ODEs
Take a look at Sec. 1.2, where we briefly introduced Euler’s method with an example.
We shall develop Euler’s method more rigorously. Pay close attention to the derivation
that uses Taylor’s formula from calculus to approximate the solution to a first-order ODE
at points that are a distance h apart. If you understand this approach, which is typical for
numerics for ODEs, then you will understand other methods more easily.
From Chap. 1 we know that an ODE of the first order is of the form F(x, y, y r ) ϭ 0
and can often be written in the explicit form y r ϭ f (x, y). An initial value problem for
this equation is of the form
y r ϭ f (x, y),

(1)

y(x 0) ϭ y0

where x 0 and y0 are given and we assume that the problem has a unique solution on some
open interval a Ͻ x Ͻ b containing x 0.
In this section we shall discuss methods of computing approximate numeric values of
the solution y(x) of (1) at the equidistant points on the x-axis
x 1 ϭ x 0 ϩ h,

x 2 ϭ x 0 ϩ 2h,

x 3 ϭ x 0 ϩ 3h,

Á

where the step size h is a fixed number, for instance, 0.2 or 0.1 or 0.01, whose choice we
discuss later in this section. Those methods are step-by-step methods, using the same
formula in each step. Such formulas are suggested by the Taylor series
(2)

y(x ϩ h) ϭ y(x) ϩ hy r(x) ϩ

h2
y s(x) ϩ Á .
2

Formula (2) is the key idea that lets us develop Euler’s method and its variant called—
you guessed it—improved Euler method, also known as Heun’s method. Let us start by
deriving Euler’s method.
For small h the higher powers h2, h3, Á in (2) are very small. Dropping all of them
gives the crude approximation
y(x ϩ h) Ϸ y(x) ϩ hy r(x)
ϭ y(x) ϩ hf (x, y)
and the corresponding Euler method (or Euler–Cauchy method)
(3)

ynϩ1 ϭ yn ϩ hf (x n, yn)

(n ϭ 0, 1, Á )

discussed in Sec. 1.2. Geometrically, this is an approximation of the curve of y(x) by a
polygon whose first side is tangent to this curve at x 0 (see Fig. 8 in Sec. 1.2).

Error of the Euler Method. Recall from calculus that Taylor’s formula with
remainder has the form
y(x ϩ h) ϭ y(x) ϩ hy r(x) ϩ 12 h2y s(␰)

c21-a.qxd

11/3/10

902

2:44 PM

Page 902

CHAP. 21 Numerics for ODEs and PDEs

(where x Ϲ ␰ Ϲ x ϩ h). It shows that, in the Euler method, the truncation error in each
step or local truncation error is proportional to h2, written O(h2), where O suggests order
(see also Sec. 20.1). Now, over a fixed x-interval in which we want to solve an ODE, the
number of steps is proportional to 1>h. Hence the total error or global error is proportional
to h2(1>h) ϭ h1. For this reason, the Euler method is called a first-order method. In
addition, there are roundoff errors in this and other methods, which may affect the
accuracy of the values y1, y2, Á more and more as n increases.

Automatic Variable Step Size Selection in Modern Software. The idea of
adaptive integration, as motivated and explained in Sec. 19.5, applies equally well to the
numeric solution of ODEs. It now concerns automatically changing the step size h depending
on the variability of y r ϭ f determined by
y s ϭ f r ϭ fx ϩ fyy r ϭ fx ϩ fy f.

(4*)

Accordingly, modern software automatically selects variable step sizes h n so that the error
of the solution will not exceed a given maximum size TOL (suggesting tolerance). Now for
the Euler method, when the step size is h ϭ h n, the local error at x n is about 12 h 2n ƒ y s(␰n) ƒ .
We require that this be equal to a given tolerance TOL,
(4)

(a)

1 2
2 hn ƒ y

s(␰n) ƒ ϭ TOL,

thus

(b) h n ϭ

2 TOL
B ƒ y s(␰n) ƒ

.

y s(x) must not be zero on the interval J: x 0 Ϲ x ϭ x N on which the solution is wanted.
Let K be the minimum of ƒ y s(x) ƒ on J and assume that K Ͼ 0. Minimum ƒ y s(x) ƒ
corresponds to maximum h ϭ H ϭ 12 TOL>K by (4). Thus, 12 TOL ϭ H1K. We can
insert this into (4b), obtaining by straightforward algebra
(5)

h n ϭ ␸(x n)H

where

␸(x n) ϭ

K
B ƒ y s(␰n) ƒ

.

For other methods, automatic step size selection is based on the same principle.

Improved Euler Method. Predictor, Corrector. Euler’s method is generally much
too inaccurate. For a large h (0.2) this is illustrated in Sec. 1.2 by the computation for
(6)

y r ϭ y ϩ x,

y(0) ϭ 0.

And for small h the computation becomes prohibitive; also, roundoff in so many steps
may result in meaningless results. Clearly, methods of higher order and precision are
obtained by taking more terms in (2) into account. But this involves an important practical
problem. Namely, if we substitute y r ϭ f (x, y(x)) into (2), we have
(2*)

y(x ϩ h) ϭ y(x) ϩ hf ϩ 12 h2f r ϩ 16 h3 f s ϩ Á .

Now y in f depends on x, so that we have f r as shown in (4*) and f s , f t even much more
cumbersome. The general strategy now is to avoid the computation of these derivatives
and to replace it by computing f for one or several suitably chosen auxiliary values of
(x, y). “Suitably” means that these values are chosen to make the order of the method as

c21-a.qxd

11/3/10

2:44 PM

Page 903

SEC. 21.1 Methods for First-Order ODEs

903

high as possible (to have high accuracy). Let us discuss two such methods that are of
practical importance, namely, the improved Euler method and the (classical) Runge–Kutta
method.
In each step of the improved Euler method we compute two values, first the predictor
y*nϩ1 ϭ yn ϩ hf (x n, yn),

(7a)

which is an auxiliary value, and then the new y-value, the corrector
ynϩ1 ϭ yn ϩ 12 h 3 f (x n, yn) ϩ f (x nϩ1, y*nϩ1)4.

(7b)

Hence the improved Euler method is a predictor–corrector method: In each step we predict
a value (7a) and then we correct it by (7b).
In algorithmic form, using the notations k 1 ϭ hf (x n, yn) in (7a) and k 2 ϭ hf (x nϩ1,
y*nϩ1) in (7b), we can write this method as shown in Table 21.1.
Table 21.1

Improved Euler Method (Heun’s Method)

ALGORITHM EULER (ƒ, x0, y0, h, N)
This algorithm computes the solution of the initial value problem y r ϭ f (x, y), y(x 0) ϭ y0
at equidistant points x 1 ϭ x 0 ϩ h, x 2 ϭ x 0 ϩ 2h, Á , x N ϭ x 0 ϩ Nh; here ƒ is such
that this problem has a unique solution on the interval [x0, xN] (see Sec. 1.6).
INPUT:

Initial values x0, y0, step size h, number of steps N

OUTPUT: Approximation ynϩ1 to the solution y(x nϩ1) at x nϩ1 ϭ x 0 ϩ (n ϩ 1)h,
where n ϭ 0, • • • , N Ϫ 1
For n ϭ 0, 1, Á , N Ϫ 1 do:
x nϩ1 ϭ x n ϩ h

j
j

k 1 ϭ hf (x n, yn)

j

k 2 ϭ hf (x nϩ1, yn ϩ k 1)

j

ynϩ1 ϭ yn ϩ 12 (k 1 ϩ k 2)

j

OUTPUT x nϩ1, ynϩ1

End
Stop
End EULER

EXAMPLE 1

Improved Euler Method. Comparison with Euler Method.
Apply the improved Euler method to the initial value problem (6), choosing h ϭ 0.2 as in Sec. 1.2.

Solution. For the present problem we have in Table 21.1
k 1 ϭ 0.2(x n ϩ yn)
k 2 ϭ 0.2(x n ϩ 0.2 ϩ yn ϩ 0.2(x n ϩ yn))
ynϩ1 ϭ yn ϩ

0.2
2

(2.2x n ϩ 2.2yn ϩ 0.2) ϭ yn ϩ 0.22(x n ϩ yn) ϩ 0.02.

c21-a.qxd

11/3/10

2:44 PM

904

Page 904

CHAP. 21 Numerics for ODEs and PDEs
Table 21.2 shows that our present results are much more accurate than those for Euler’s method in Table 21.1 but
at the cost of more computations.
᭿

Table 21.2 Improved Euler Method for (6). Errors
n

xn

yn

Exact Values
(4D)

Error of
Improved Euler

Error of
Euler

0
1
2
3
4
5

0.0
0.2
0.4
0.6
0.8
1.0

0.0000
0.0200
0.0884
0.2158
0.4153
0.7027

0.0000
0.0214
0.0918
0.2221
0.4255
0.7183

0.0000
0.0014
0.0034
0.0063
0.0102
0.0156

0.000
0.021
0.052
0.094
0.152
0.230

Error of the Improved Euler Method. The local error is of order h3 and the global
error of order h2, so that the method is a second-order method.
PROOF

Setting fෂn ϭ f (x n, y(x n)) and using (2*) (after (6)), we have
(8a)

ෂ
ෂ
ෂ
y(x n ϩ h) Ϫ y(x n) ϭ h f n ϩ 12 h2 f nr ϩ 16 h3 f sn ϩ Á .

ෂ
Approximating the expression in the brackets in (7b) by ෂ
f n ϩ f nϩ1 and again using the
Taylor expansion, we obtain from (7b)

(8b)

ෂ
ෂ
ynϩ1 Ϫ yn Ϸ 12 h 3 f n ϩ f nϩ14
ෂ
ෂ
ෂ
ෂ
ϭ 12 h 3 f n ϩ ( f n ϩ h f nr ϩ 12 h2 f ns ϩ Á )4
ෂ
ෂ
ෂ
ϭ h f n ϩ 12 h2 f nr ϩ 14 h3 f ns ϩ Á

(where r ϭ d>dx n, etc.). Subtraction of (8b) from (8a) gives the local error
h3 ෂ
h3 ෂ
h3 ෂ
f ns Ϫ
f ns ϩ Á ϭ Ϫ
f ns ϩ Á .
6
4
12
Since the number of steps over a fixed x-interval is proportional to 1>h, the global error
᭿
is of order h3>h ϭ h2, so that the method is of second order.
Since the Euler method was an attractive pedagogical tool to teach the beginning of
solving first-order ODEs numerically but had its drawbacks in terms of accuracy and could
even produce wrong answers, we studied the improved Euler method and thereby
introduced the idea of a predictor–corrector method. Although improved Euler is better
than Euler, there are better methods that are used in industrial settings. Thus the practicing
engineer has to know about the Runga–Kutta methods and its variants.

Runge–Kutta Methods (RK Methods)
A method of great practical importance and much greater accuracy than that of the
improved Euler method is the classical Runge–Kutta method of fourth order, which we

c21-a.qxd

11/9/10

7:40 PM

Page 905

SEC. 21.1 Methods for First-Order ODEs

905

call briefly the Runge–Kutta method.1 It is shown in Table 21.3. We see that in each
step we first compute four auxiliary quantities k 1, k 2, k 3, k 4 and then the new value ynϩ1.
The method is well suited to the computer because it needs no special starting procedure,
makes light demand on storage, and repeatedly uses the same straightforward computational procedure. It is numerically stable.
Note that, if f depends only on x, this method reduces to Simpson’s rule of integration
(Sec. 19.5). Note further that k 1, Á , k 4 depend on n and generally change from step
to step.

Table 21.3 Classical Runge–Kutta Method of Fourth Order

ALGORITHM RUNGE–KUTTA (ƒ, x0, y0, h, N).
This algorithm computes the solution of the initial value problem yЈ ϭ ƒ(x, y), y(x0) ϭ y0
at equidistant points
x 1 ϭ x 0 ϩ h, x 2 ϭ x 0 ϩ 2h, Á , x N ϭ x 0 ϩ Nh;

(9)

here ƒ is such that this problem has a unique solution on the interval [x0, xN] (see Sec. 1.7).
INPUT:

Function ƒ, initial values x0, y0, step size h, number of steps N

OUTPUT: Approximation ynϩ1 to the solution y(xnϩ1) at x nϩ1 ϭ x 0 ϩ (n ϩ 1) h,
where n ϭ 0, 1, Á , N Ϫ 1
For n ϭ 0, 1, Á , N Ϫ 1 do:
j

k 1 ϭ hf (x n, yn)

j

k 2 ϭ hf (x n ϩ 12 h, yn ϩ 12 k 1)

j

k 3 ϭ hf (x n ϩ 12 h, yn ϩ 12 k 2)

j

k 4 ϭ hf (x n ϩ h, yn ϩ k 3)

j

x nϩ1 ϭ x n ϩ h

j

ynϩ1 ϭ yn ϩ 16 (k 1 ϩ 2k 2 ϩ 2k 3 ϩ k 4)

j

OUTPUT x nϩ1, ynϩ1

End
Stop
End RUNGE–KUTTA

1

Named after the German mathematicians KARL RUNGE (Sec. 19.4) and WILHELM KUTTA (1867–1944).
Runge [Math. Annalen 46 (1895), 167–178], the German mathematician KARL HEUN (1859–1929) [Zeitschr.
Math. Phys. 45 (1900), 23–38], and Kutta [Zeitschr. Math. Phys. 46 (1901), 435–453] developed various similar
methods. Theoretically, there are infinitely many fourth-order methods using four function values per step. The
method in Table 21.3 is most popular from a practical viewpoint because of its “symmetrical” form and its
simple coefficients. It was given by Kutta.

c21-a.qxd

11/3/10

2:44 PM

906
EXAMPLE 2

Page 906

CHAP. 21 Numerics for ODEs and PDEs
Classical Runge–Kutta Method
Apply the Runge–Kutta method to the initial value problem in Example 1, choosing h ϭ 0.2, as before, and
computing five steps.

Solution. For the present problem we have f (x, y) ϭ x ϩ y. Hence
k 1 ϭ 0.2(x n ϩ yn),

k 2 ϭ 0.2(x n ϩ 0.1 ϩ yn ϩ 0.5k 1),

k 3 ϭ 0.2 (x n ϩ 0.1 ϩ yn ϩ 0.5k 2),

k 4 ϭ 0.2(x n ϩ 0.2 ϩ yn ϩ k 3).

Table 21.4 shows the results and their errors, which are smaller by factors 103 and 104 than those for the two
Euler methods. See also Table 21.5. We mention in passing that since the present k 1, Á , k 4 are simple,
operations were saved by substituting k 1 into k 2, then k 2 into k 3, etc.; the resulting formula is shown in
Column 4 of Table 21.4. Keep in mind that we have four function evaluations at each step.
᭿

Table 21.4

Runge–Kutta Method Applied to (4)

n

xn

yn

0
1
2
3
4
5

0.0
0.2
0.4
0.6
0.8
1.0

0
0.021400
0.091818
0.222107
0.425521
0.718251

0.2214(xn ϩ yn)
ϩ 0.0214

Exact Values (6D)
y ϭ ex Ϫ x Ϫ 1

106 ϫ Error
of yn

0.021400
0.070418
0.130289
0.203414
0.292730

0.000000
0.021403
0.091825
0.222119
0.425541
0.718282

0
3
7
12
20
31

Table 21.5 Comparison of the Accuracy of the Three Methods under Consideration
in the Case of the Initial Value Problem (4), with h ϭ 0.2
Error
x

y ϭ ex Ϫ x Ϫ 1

0.2
0.4
0.6
0.8
1.0

0.021403
0.091825
0.222119
0.425541
0.718282

Euler
(Table 21.1)

Improved Euler
(Table 21.3)

Runge–Kutta
(Table 21.5)

0.021
0.052
0.094
0.152
0.230

0.0014
0.0034
0.0063
0.0102
0.0156

0.000003
0.000007
0.000011
0.000020
0.000031

Error and Step Size Control.
RKF (Runge–Kutta–Fehlberg)
The idea of adaptive integration (Sec. 19.5) has analogs for Runge–Kutta (and other)
methods. In Table 21.3 for RK (Runge–Kutta), if we compute in each step approximations
ෂ
y and ෂ
y with step sizes h and 2h, respectively, the latter has error per step equal to 25 ϭ 32
times that of the former; however, since we have only half as many steps for 2h, the actual
factor is 25>2 ϭ 16, so that, say,
P(2h) Ϸ 16P(h)

and thus

y (h) Ϫ y (2h) ϭ P(2h) Ϫ P(h) Ϸ (16 Ϫ 1)P(h).

c21-a.qxd

11/3/10

2:44 PM

Page 907

SEC. 21.1 Methods for First-Order ODEs

907

Hence the error P ϭ P(h) for step size h is about
1 ෂ
P ϭ 15
(y Ϫ ෂ
y)

(10)

where ෂ
yϪෂ
y ϭ y (h) Ϫ y (2h), as said before. Table 21.6 illustrates (10) for the initial value
problem
y r ϭ (y Ϫ x Ϫ 1)2 ϩ 2,

(11)

y(0) ϭ 1,

the step size h ϭ 0.1 and 0 Ϲ x Ϲ 0.4. We see that the estimate is close to the actual
error. This method of error estimation is simple but may be unstable.
Table 21.6 Runge–Kutta Method Applied to the Initial Value Problem (11)
and Error Estimate (10). Exact Solution y ϭ tan x ϩ x ϩ 1
x

ෂ
y
(Step size h)

ෂy
(Step size 2h)

Error
Estimate (10)

Actual
Error

Exact
Solution (9D)

0.0
0.1
0.2
0.3
0.4

1.000000000
1.200334589
1.402709878
1.609336039
1.822792993

1.000000000

0.000000000

1.402707408

0.000000165

1.822788993

0.000000267

0.000000000
0.000000083
0.000000157
0.000000210
0.000000226

1.000000000
1.200334672
1.402710036
1.609336250
1.822793219

RKF. E. Fehlberg [Computing 6 (1970), 61–71] proposed and developed error control
by using two RK methods of different orders to go from (x n, yn) to (x nϩ1, ynϩ1). The
difference of the computed y-values at x nϩ1 gives an error estimate to be used for step
size control. Fehlberg discovered two RK formulas that together need only six function
evaluations per step. We present these formulas here because RKF has become quite
popular. For instance, Maple uses it (also for systems of ODEs).
Fehlberg’s fifth-order RK method is
ynϩ1 ϭ yn ϩ g1k 1 ϩ Á ϩ g6k 6

(12a)

with coefficient vector g ϭ 3g1 Á g64,
(12b)

16
g ϭ 3135

0

6656
12,825

28,561
56,430

9
Ϫ50

2
55 4 .

His fourth-order RK method is
(13a)

y*nϩ1 ϭ yn ϩ g*1k 1 ϩ Á ϩ g*5k 5

with coefficient vector
(13b)

25
g* ϭ 3216

0

1408
2565

2197
4104

Ϫ154 .

c21-a.qxd

11/3/10

2:44 PM

908

Page 908

CHAP. 21 Numerics for ODEs and PDEs

In both formulas we use only six different function evaluations altogether, namely,
k 1 ϭ hf (x n, yn)

(14)

k 2 ϭ hf (x n ϩ 14 h,

yn ϩ

1
4 k 1)

k 3 ϭ hf (x n ϩ 38 h,

yn ϩ

k 4 ϭ hf (x n ϩ 12
13 h,

yn ϩ 1932
2197 k 1 Ϫ

k 5 ϭ hf (x n ϩ h,

yn ϩ

439
216 k 1

Ϫ

k 6 ϭ hf (x n ϩ 12 h,

yn Ϫ

8
27 k 1

ϩ

3
32 k 1

9
32 k 2)

ϩ

7200
2197 k 2

ϩ 7296
2197 k 3)

8k 2 ϩ

˛

3680
513 k 3

845
Ϫ 4104
k 4)

1859
11
2k 2 Ϫ 3544
2565 k 3 ϩ 4104 k 4 Ϫ 40 k 5).

˛

The difference of (12) and (13) gives the error estimate
(15)
EXAMPLE 3

1
128
2197
1
2
Pnϩ1 Ϸ ynϩ1 Ϫ y*nϩ1 ϭ 360
k 1 Ϫ 4275
k 3 Ϫ 75,240
k 4 ϩ 50
k 5 ϩ 55
k 6.

Runge–Kutta–Fehlberg
For the initial value problem (11) we obtain from (12)–(14) with h ϭ 0.1 in the first step the 12S-values
k 1 ϭ 0.200000000000

k 2 ϭ 0.200062500000

k 3 ϭ 0.200140756867

k 4 ϭ 0.200856926154

k 5 ϭ 0.201006676700

k 6 ϭ 0.200250418651

y*1 ϭ 1.20033466949
y1 ϭ 1.20033467253
and the error estimate
P1 Ϸ y1 Ϫ y*1 ϭ 0.00000000304.
The exact 12S-value is y(0.1) ϭ 1.20033467209. Hence the actual error of y1 is Ϫ4.4 ؒ 10؊10, smaller than that
in Table 21.6 by a factor of 200.
᭿

Table 21.7 summarizes essential features of the methods in this section. It can be shown
that these methods are numerically stable (definition in Sec. 19.1). They are one-step
methods because in each step we use the data of just one preceding step, in contrast to
multistep methods where in each step we use data from several preceding steps, as we
shall see in the next section.

Table 21.7

Methods Considered and Their Order (‫ ؍‬Their Global Error)

Method

Function Evaluation
per Step

Global Error

Local Error

Euler
Improved Euler
RK (fourth order)
RKF

1
2
4
6

O(h)
O(h2)
O(h4)
O(h5)

O(h2)
O(h3)
O(h5)
O(h6)

c21-a.qxd

11/3/10

2:44 PM

Page 909

SEC. 21.1 Methods for First-Order ODEs

909

Backward Euler Method. Stiff ODEs
The backward Euler formula for numerically solving (1) is
ynϩ1 ϭ yn ϩ hf (x nϩ1, ynϩ1)

(16)

(n ϭ 0, 1, Á ).

This formula is obtained by evaluating the right side at the new location (x nϩ1, ynϩ1);
this is called the backward Euler scheme. For known yn it gives ynϩ1 implicitly, so it
defines an implicit method, in contrast to the Euler method (3), which gives ynϩ1
explicitly. Hence (16) must be solved for ynϩ1. How difficult this is depends on f in (1).
For a linear ODE this provides no problem, as Example 4 (below) illustrates. The method
is particularly useful for “stiff” ODEs, as they occur quite frequently in the study of
vibrations, electric circuits, chemical reactions, etc. The situation of stiffness is roughly
as follows; for details, see, for example, [E5], [E25], [E26] in App. 1.
Error terms of the methods considered so far involve a higher derivative. And we ask
what happens if we let h increase. Now if the error (the derivative) grows fast but the desired
solution also grows fast, nothing will happen. However, if that solution does not grow fast,
then with growing h the error term can take over to an extent that the numeric result becomes
completely nonsensical, as in Fig. 451. Such an ODE for which h must thus be restricted
to small values, and the physical system the ODE models, are called stiff. This term is
suggested by a mass–spring system with a stiff spring (spring with a large k; see Sec. 2.4).
Example 4 illustrates that implicit methods remove the difficulty of increasing h in the case
of stiffness: It can be shown that in the application of an implicit method the solution remains
stable under any increase of h, although the accuracy decreases with increasing h.
EXAMPLE 4

Backward Euler Method. Stiff ODE
The initial value problem
y r ϭ f (x, y) ϭ Ϫ20hy ϩ 20x 2 ϩ 2x, y(0) ϭ 1
has the solution (verify!)
y ϭ e؊20x ϩ x 2.
The backward Euler formula (16) is
ynϩ1 ϭ yn ϩ hf (x nϩ1, ynϩ1) ϭ yn ϩ h (Ϫ20ynϩ1 ϩ 20x 2nϩ1 ϩ 2x nϩ1).
Noting that x nϩ1 ϭ x n ϩ h, taking the term Ϫ20ynϩ1 to the left, and dividing, we obtain
(16*)

ynϩ1 ϭ

yn ϩ h320 (x n ϩ h)2 ϩ 2 (x n ϩ h)4
1 ϩ 20h

.

The numeric results in Table 21.8 show the following.
Stability of the backward Euler method for h ϭ 0.05 and also for h ϭ 0.2 with an error increase by about a
factor 4 for h ϭ 0.2,
Stability of the Euler method for h ϭ 0.05 but instability for h ϭ 0.1 (Fig. 451),
Stability of RK for h ϭ 0.1 but instability for h ϭ 0.2.
This illustrates that the ODE is stiff. Note that even in the case of stability the approximation of the solution
᭿
near x ϭ 0 is poor.

Stiffness will be considered further in Sec. 21.3 in connection with systems of ODEs.

c21-a.qxd

11/3/10

910

2:44 PM

Page 910

CHAP. 21 Numerics for ODEs and PDEs
y
2.0

1.0

0

0.2

0.4

0.6

0.8

1.0 x

–1.0

Fig. 451. Euler method with h ϭ 0.1 for the stiff
ODE in Example 4 and exact solution

Table 21.8 Backward Euler Method (BEM) for Example 6. Comparison with Euler and RK
x

BEM
h ϭ 0.05

BEM
h ϭ 0.2

Euler
h ϭ 0.05

Euler
h ϭ 0.1

RK
h ϭ 0.1

RK
h ϭ 0.2

0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0

1.00000
0.26188
0.10484
0.10809
0.16640
0.25347
0.36274
0.49256
0.64252
0.81250
1.00250

1.00000

1.00000
0.00750
0.03750
0.08750
0.15750
0.24750
0.35750
0.48750
0.63750
0.80750
0.99750

1.00000
Ϫ1.00000
1.04000
Ϫ0.92000
1.16000
Ϫ0.76000
1.36000
Ϫ0.52000
1.64000
Ϫ0.20000
2.00000

1.00000
0.34500
0.15333
0.12944
0.17482
0.25660
0.36387
0.49296
0.64265
0.81255
1.00252

1.000

0.24800
0.20960
0.37792
0.65158
1.01032

5.093
25.48
127.0
634.0
3168

Exact
1.00000
0.14534
0.05832
0.09248
0.16034
0.25004
0.36001
0.49001
0.64000
0.81000
1.00000

PROBLEM SET 21.1
1–4

EULER METHOD

Do 10 steps. Solve exactly. Compute the error. Show
details.
1. y r ϩ 0.2y ϭ 0, y(0) ϭ 5, h ϭ 0.2
2. y r ϭ 12 p 21 Ϫ y 2, y(0) ϭ 0, h ϭ 0.1
3. y r ϭ (y Ϫ x)2, y(0) ϭ 0, h ϭ 0.1
4. y r ϭ (y ϩ x)2, y(0) ϭ 0, h ϭ 0.1
5–10

IMPROVED EULER METHOD

Do 10 steps. Solve exactly. Compute the error. Show
details.
5. y r ϭ y, y(0) ϭ 1, h ϭ 0.1
6. y r ϭ 2 (1 ϩ y 2), y(0) ϭ 0, h ϭ 0.05
7. y r Ϫ xy 2 ϭ 0, y(0) ϭ 1, h ϭ 0.1
8. Logistic population model. y r ϭ y Ϫ y 2, y(0) ϭ 0.2,
h ϭ 0.1

9. Do Prob. 7 using Euler’s method with h ϭ 0.1 and compare the accuracy.
10. Do Prob. 7 using the improved Euler method, 20 steps
with h ϭ 0.05. Compare.
11–17

CLASSICAL RUNGE–KUTTA METHOD
OF FOURTH ORDER

Do 10 steps. Compare as indicated. Show details.
11. y r Ϫ xy 2 ϭ 0, y(0) ϭ 1, h ϭ 0.1. Compare with
Prob. 7. Apply the error estimate (10) to y10.
12. y r ϭ y Ϫ y 2, y(0) ϭ 0.2, h ϭ 0.1. Compare with
Prob. 8.
13. y r ϭ 1 ϩ y 2, y(0) ϭ 0, h ϭ 0.1
14. y r ϭ (1 Ϫ x ؊1)y, y(1) ϭ 1, h ϭ 0.1
15. y r ϩ y tan x ϭ sin 2x, y(0) ϭ 1, h ϭ 0.1
16. Do Prob. 15 with h ϭ 0.2, 5 steps, and compare the
errors with those in Prob. 15.

c21-a.qxd

11/3/10

2:44 PM

Page 911

SEC. 21.2 Multistep Methods

911

17. y r ϭ 4x 3y 2, y(0) ϭ 0.5, h ϭ 0.1
18. Kutta’s third-order method is defined by ynϩ1 ϭ
yn ϩ 16 (k 1 ϩ 4k 2 ϩ k *3 ) with k 1 and k 2 as in RK
(Table 21.3) and k *3 ϭ hf (x nϩ1, yn Ϫ k 1 ϩ 2k 2).
Apply this method to (4) in (6). Choose h ϭ 0.2 and
do 5 steps. Compare with Table 21.5.
19. CAS EXPERIMENT. Euler–Cauchy vs. RK. Consider the initial value problem
(17)

y r ϭ (y Ϫ 0.01x 2)2 sin (x 2) ϩ 0.02x,
y(0) ϭ 0.4

(solution: y ϭ 1>32.5 Ϫ S(x)4 ϩ 0.01x 2 where S(x) is
the Fresnel integral (38) in App. 3.1).
(a) Solve (17) by Euler, improved Euler, and RK
methods for 0 Ϲ x Ϲ 5 with step h ϭ 0.2. Compare the
errors for x ϭ 1, 3, 5 and comment.

21.2

(b) Graph solution curves of the ODE in (17) for
various positive and negative initial values.
(c) Do a similar experiment as in (a) for an initial
value problem that has a monotone increasing or
monotone decreasing solution. Compare the behavior
of the error with that in (a). Comment.
20. CAS EXPERIMENT. RKF. (a) Write a program for
RKF that gives x n, yn, the estimate (10), and, if the
solution is known, the actual error Pn.
(b) Apply the program to Example 3 in the text
(10 steps, h ϭ 0.1).
(c) Pn in (b) gives a relatively good idea of the size
of the actual error. Is this typical or accidental? Find
out, by experimentation with other problems, on
what properties of the ODE or solution this might
depend.

Multistep Methods
In a one-step method we compute ynϩ1 using only a single step, namely, the previous
value yn. One-step methods are “self-starting,” they need no help to get going because
they obtain y1 from the initial value y0, etc. All methods in Sec. 21.1 are one-step.
In contrast, a multistep method uses, in each step, values from two or more previous
steps. These methods are motivated by the expectation that the additional information will
increase accuracy and stability. But to get started, one needs values, say, y0, y1, y2, y3 in
a 4-step method, obtained by Runge–Kutta or another accurate method. Thus, multistep
methods are not self-starting. Such methods are obtained as follows.

Adams–Bashforth Methods
We consider an initial value problem
y r ϭ f (x, y),

(1)

y(x 0) ϭ y0

as before, with f such that the problem has a unique solution on some open interval
containing x 0. We integrate y r ϭ f (x, y) from x n to x nϩ1 ϭ x n ϩ h. This gives

Ύ

xnϩ1

y r(x) dx ϭ y(x nϩ1) Ϫ y(x n) ϭ

xn

Ύ

xnϩ1

f (x, y(x)) dx.

xn

Now comes the main idea. We replace f (x, y(x)) by an interpolation polynomial p(x) (see
Sec. 19.3), so that we can later integrate. This gives approximations ynϩ1 of y(x nϩ1) and
yn of y(x n),
(2)

ynϩ1 ϭ yn ϩ

Ύ

xnϩ1

xn

p(x) dx.

c21-a.qxd

11/3/10

912

2:44 PM

Page 912

CHAP. 21 Numerics for ODEs and PDEs

Different choices of p(x) will now produce different methods. We explain the principle
by taking a cubic polynomial, namely, the polynomial p3(x) that at (equidistant)
x n,

x n؊1,

x n؊2,

x n؊3

has the respective values
fn ϭ f (x n, yn)
fn؊1 ϭ f (x n؊1, yn؊1)

(3)

fn؊2 ϭ f (x n؊2, yn؊2)
fn؊3 ϭ f (x n؊3, yn؊3).

This will lead to a practically useful formula. We can obtain p3 (x) from Newton’s
backward difference formula (18), Sec. 19.3:
p3(x) ϭ fn ϩ rٌfn ϩ 12 r(r ϩ 1)ٌ2fn ϩ 16 r(r ϩ 1)(r ϩ 2)ٌ3fn
where
rϭ

x Ϫ xn
h

.

We integrate p3(x) over x from x n to x nϩ1 ϭ x n ϩ h, thus over r from 0 to 1. Since
x ϭ x n ϩ hr,
The integral of 12 r(r ϩ 1) is
(4)

Ύ

xnϩ1

xn

5
12

Ύp

3

0

dx ϭ h dr.

and that of 16 r(r ϩ 1)(r ϩ 2) is 38 . We thus obtain

1

p3 dx ϭ h

we have

dr ϭ h afn ϩ

1
5 2
3
ٌfn ϩ
ٌ fn ϩ ٌ3fn b .
2
12
8

It is practical to replace these differences by their expressions in terms of f :
ٌfn ϭ fn Ϫ fn؊1
ٌ2fn ϭ fn Ϫ 2fn؊1 ϩ fn؊2
ٌ3fn ϭ fn Ϫ 3fn؊1 ϩ 3fn؊2 Ϫ fn؊3.
We substitute this into (4) and collect terms. This gives the multistep formula of the
Adams–Bashforth method2 of fourth order

(5)

ynϩ1 ϭ yn ϩ

h
(55fn Ϫ 59fn؊1 ϩ 37fn؊2 Ϫ 9fn؊3).
24

2
Named after JOHN COUCH ADAMS (1819–1892), English astronomer and mathematician, one of the
predictors of the existence of the planet Neptune (using mathematical calculations), director of the Cambridge
Observatory; and FRANCIS BASHFORTH (1819–1912), English mathematician.

c21-a.qxd

11/3/10

2:44 PM

Page 913

SEC. 21.2 Multistep Methods

913

It expresses the new value ynϩ1 [approximation of the solution y of (1) at x nϩ1] in terms
of 4 values of f computed from the y-values obtained in the preceding 4 steps. The local
truncation error is of order h5, as can be shown, so that the global error is of order h4;
hence (5) does define a fourth-order method.

Adams–Moulton Methods
Adams–Moulton methods are obtained if for p (x) in (2) we choose a polynomial that
interpolates f (x, y(x)) at x nϩ1, x n, x n؊1, Á (as opposed to x n, x n؊1, Á used before; this
is the main point). We explain the principle for the cubic polynomial ෂ
p3 (x) that interpolates
at x nϩ1, x n, x n؊1, x n؊2. (Before we had x n, x n؊1, x n؊2, x n؊3.) Again using (18) in
Sec. 19.3 but now setting r ϭ (x Ϫ x nϩ1)>h, we have
ෂ
p3(x) ϭ fnϩ1 ϩ rٌfnϩ1 ϩ 12 r(r ϩ 1)ٌ2fnϩ1 ϩ 16 r(r ϩ 1)(r ϩ 2)ٌ3fnϩ1.
We now integrate over x from x n to x nϩ1 as before. This corresponds to integrating over
r from Ϫ1 to 0. We obtain

Ύ

xnϩ1

xn

1
1 2
1 3
ෂ
p3(x) dx ϭ h afnϩ1 Ϫ ٌfnϩ1 Ϫ
ٌ fnϩ1 Ϫ
ٌ fnϩ1 b .
2
12
24

Replacing the differences as before gives
(6)

ynϩ1 ϭ yn ϩ

Ύ

xnϩ1

xn

h
ෂ
p3(x) dx ϭ yn ϩ
(9fnϩ1 ϩ 19fn Ϫ 5fn؊1 ϩ fn؊2).
24

This is usually called an Adams–Moulton formula.3 It is an implicit formula because
fnϩ1 ϭ f (x nϩ1, ynϩ1) appears on the right, so that it defines ynϩ1 only implicitly, in
contrast to (5), which is an explicit formula, not involving ynϩ1 on the right. To use (6)
we must predict a value y*
nϩ1, for instance, by using (5), that is,
(7a)

y *nϩ1 ϭ yn ϩ

h
(55fn Ϫ 59fn؊1 ϩ 37fn؊2 Ϫ 9fn؊3).
24

The corrected new value ynϩ1 is then obtained from (6) with fnϩ1 replaced by
f *nϩ1 ϭ f (x nϩ1, y*nϩ1) and the other f ’s as in (6); thus,
(7b)

ynϩ1 ϭ yn ϩ

h
* ϩ 19fn Ϫ 5fn؊1 ϩ fn؊2).
(9f nϩ1
24

This predictor–corrector method (7a), (7b) is usually called the Adams–Moulton
method of fourth order. It has the advantage over RK that (7) gives the error estimate
1
Pnϩ1 Ϸ 15
(ynϩ1 Ϫ y*
nϩ1),

as can be shown. This is the analog of (10) in Sec. 21.1.
3
FOREST RAY MOULTON (1872–1952), American astronomer at the University of Chicago. For ADAMS
see footnote 2.

c21-a.qxd

11/3/10

2:44 PM

914

Page 914

CHAP. 21 Numerics for ODEs and PDEs

Sometimes the name Adams–Moulton method is reserved for the method with several
corrections per step by (7b) until a specific accuracy is reached. Popular codes exist for
both versions of the method.
Getting Started. In (5) we need f0, f1, f2, f3. Hence from (3) we see that we must first
compute y1, y2, y3 by some other method of comparable accuracy, for instance, by RK or
by RKF. For other choices see Ref. [E26] listed in App. 1.
EXAMPLE 1

Adams–Bashforth Prediction (7a), Adams–Moulton Correction (7b)
Solve the initial value problem
y r ϭ x ϩ y,

(8)

y(0) ϭ 0

by (7a), (7b) on the interval 0 Ϲ x Ϲ 2, choosing h ϭ 0.2.

Solution. The problem is the same as in Examples 1 and 2, Sec. 21.1, so that we can compare the results.
We compute starting values y1, y2, y3 by the classical Runge–Kutta method. Then in each step we predict
by (7a) and make one correction by (7b) before we execute the next step. The results are shown and compared
with the exact values in Table 21.9. We see that the corrections improve the accuracy considerably. This is
typical.
᭿
Table 21.9 Adams–Moulton Method Applied to the Initial Value Problem (8);
Predicted Values Computed by (7a) and Corrected Values by (7b)
n

xn

0
1
2
3
4
5
6
7
8
9
10

0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0

Starting
yn

Predicted
y*n

Corrected
yn

Exact
Values

106 ⅐ Error
of yn

0.425529
0.718270
1.120106
1.655191
2.353026
3.249646
4.389062

0.000000
0.021403
0.091825
0.222119
0.425541
0.718282
1.120117
1.655200
2.353032
3.249647
4.389056

0
3
7
12
12
12
11
9
6
1
Ϫ6

0.000000
0.021400
0.091818
0.222107
0.425361
0.718066
1.119855
1.654885
2.352653
3.249190
4.388505

Comments on Comparison of Methods. An Adams–Moulton formula is generally
much more accurate than an Adams–Bashforth formula of the same order. This justifies
the greater complication and expense in using the former. The method (7a), (7b) is
numerically stable, whereas the exclusive use of (7a) might cause instability. Step size
control is relatively simple. If ƒ Corrector Ϫ Predictor ƒ Ͼ TOL, use interpolation to
generate “old” results at half the current step size and then try h>2 as the new step.
Whereas the Adams–Moulton formula (7a), (7b) needs only 2 evaluations per step,
Runge–Kutta needs 4; however, with Runge–Kutta one may be able to take a step size
more than twice as large, so that a comparison of this kind (widespread in the literature)
is meaningless.
For more details, see Refs. [E25], [E26] listed in App. 1.

c21-a.qxd

11/3/10

2:44 PM

Page 915

SEC. 21.3 Methods for Systems and Higher Order ODEs

915

PROBLEM SET 21.2
1–10

ADAMS–MOULTON METHOD

Solve the initial value problem by Adams–Moulton (7a), (7b),
10 steps with 1 correction per step. Solve exactly and compute
the error. Use RK where no starting values are given.
1. y r ϭ y, y(0) ϭ 1, h ϭ 0.1, (1.105171, 1.221403,
1.349858)
2. y r ϭ 2xy, y(0) ϭ 1, h ϭ 0.1
3. y r ϭ 1 ϩ y 2, y(0) ϭ 0, h ϭ 0.1, (0.100335,
0.202710, 0.309336)
4. Do Prob. 2 by RK, 5 steps, h ϭ 0.2. Compare the errors.
5. Do Prob. 3 by RK, 5 steps, h ϭ 0.2. Compare the errors.
6. y r ϭ (y Ϫ x Ϫ 1)2 ϩ 2, y(0) ϭ 1, h ϭ 0.1,
10 steps
7. y r ϭ 3y Ϫ 12y 2, y(0) ϭ 0.2, h ϭ 0.1
8. y r ϭ 1 Ϫ 4y 2, y(0) ϭ 0, h ϭ 0.1
9. y r ϭ 3x 2 (1 ϩ y), y(0) ϭ 0, h ϭ 0.05
10. y r ϭ x>y, y(1) ϭ 3, h ϭ 0.2
11. Do and show the calculations leading to (4)–(7) in the
text.
12. Quadratic polynomial. Apply the method in the text
to a polynomial of second degree. Show that this leads
to the predictor and corrector formulas
h
(23fn Ϫ 16fn؊1 ϩ 5fn؊2),
12
h
ynϩ1 ϭ yn ϩ
(5fnϩ1 ϩ 8fn Ϫ fn؊1).
12

y*nϩ1 ϭ yn ϩ

21.3

13. Using Prob. 12, solve y r ϭ 2xy, y(0) ϭ 1 (10 steps,
h ϭ 0.1, RK starting values). Compare with the exact
solution and comment.
14. How much can you reduce the error in Prob. 13 by
halfing h (20 steps, h ϭ 0.05)? First guess, then
compute.
15. CAS PROJECT. Adams–Moulton. (a) Accurate
starting is important in (7a), (7b). Illustrate this in
Example 1 of the text by using starting values from
the improved Euler–Cauchy method and compare the
results with those in Table 21.8.
(b) How much does the error in Prob. 11 decrease
if you use exact starting values (instead of RK
values)?
(c) Experiment to find out for what ODEs poor
starting is very damaging and for what ODEs it
is not.
(d) The classical RK method often gives the same
accuracy with step 2h as Adams–Moulton with step
h, so that the total number of function evaluations is
the same in both cases. Illustrate this with Prob. 8.
(Hence corresponding comparisons in the literature
in favor of Adams–Moulton are not valid. See also
Probs. 6 and 7.)

Methods for Systems
and Higher Order ODEs
Initial value problems for first-order systems of ODEs are of the form
(1)

y r ϭ f (x, y),

y(x 0) ϭ y0,

in components
y1r ϭ f1(x, y1, Á , ym),

y1(x 0) ϭ y10

y2r ϭ f2(x, y1, Á , ym),

y2(x 0) ϭ y20

Á Á Á Á Á Á Á

Á Á Á Á

ym
r ϭ fm(x, y1, Á , ym).

ym(x 0) ϭ ym0.

c21-a.qxd

11/3/10

2:44 PM

916

Page 916

CHAP. 21 Numerics for ODEs and PDEs

Here, f is assumed to be such that the problem has a unique solution y(x) on some open
x-interval containing x 0. Our discussion will be independent of Chap. 4 on systems.
Before explaining solution methods it is important to note that (1) includes initial value
problems for single mth-order ODEs,
y (m) ϭ f (x, y, y r, y s, Á , y (m؊1))

(2)

and initial conditions y(x 0) ϭ K 1, y r(x 0) ϭ K 2, Á , y (m؊1)(x 0) ϭ K m as special cases.
Indeed, the connection is achieved by setting
(3)

y1 ϭ y,

y2 ϭ y r ,

y3 ϭ y s,

Á , ym ϭ y (m؊1).

Then we obtain the system
y1r ϭ y2
y2r ϭ y3
(4)

o
yrm؊1 ϭ ym
ym
r ϭ f (x, y1, Á , ym)

and the initial conditions y1(x 0) ϭ K 1,

y2(x 0) ϭ K 2,

Á , ym(x 0) ϭ K m.

Euler Method for Systems
Methods for single first-order ODEs can be extended to systems (1) simply by writing vector
functions y and f instead of scalar functions y and f, whereas x remains a scalar variable.
We begin with the Euler method. Just as for a single ODE, this method will not be
accurate enough for practical purposes, but it nicely illustrates the extension principle.
EXAMPLE 1

Euler Method for a Second-Order ODE. Mass–Spring System
Solve the initial value problem for a damped mass–spring system
y s ϩ 2y r ϩ 0.75y ϭ 0,

y(0) ϭ 3,

y r(0) ϭ Ϫ2.5

by the Euler method for systems with step h ϭ 0.2 for x from 0 to 1 (where x is time).

Solution. The Euler method (3), Sec. 21.1, generalizes to systems in the form
(5)

ynϩ1 ϭ yn ϩ hf(x n, yn),

in components
y1,nϩ1 ϭ y1,n ϩ h f1(x n, y1,n, y2,n)
y2,nϩ1 ϭ y2,n ϩ h f2(x n, y1,n, y2,n)
and similarly for systems of more than two equations. By (4) the given ODE converts to the system
y1r ϭ f1(x, y1, y2) ϭ y2
y2r ϭ f2(x, y1, y2) ϭ Ϫ2y2 Ϫ 0.75y1.

c21-a.qxd

11/3/10

2:44 PM

Page 917

SEC. 21.3 Methods for Systems and Higher Order ODEs

917

Hence (5) becomes
y1,nϩ1 ϭ y1,n ϩ 0.2y2,n
y2,nϩ1 ϭ y2,n ϩ 0.2(Ϫ2y2,n Ϫ 0.75y1,n).
The initial conditions are y(0) ϭ y1 (0) ϭ 3, y r(0) ϭ y2 (0) ϭ Ϫ2.5. The calculations are shown in Table 21.10.
As for single ODEs, the results would not be accurate enough for practical purposes. The example merely serves
to illustrate the method because the problem can be readily solved exactly,
y ϭ y1 ϭ 2e؊0.5x ϩ e؊1.5x,

Table 21.10

thus

᭿

y r ϭ y2 ϭ Ϫe؊0.5x Ϫ 1.5e؊1.5x.

Euler Method for Systems in Example 1 (Mass–Spring System)

n

xn

y1,n

0
1
2
3
4
5

0.0
0.2
0.4
0.6
0.8
1.0

3.00000
2.50000
2.11000
1.80100
1.55230
1.34905

y1 Exact
Error
(5D) ⑀1 ϭ y1 Ϫ y1,n
3.00000
2.55049
2.18627
1.88821
1.64183
1.43619

0.00000
0.05049
0.76270
0.08721
0.08953
0.08714

y2,n

y2 Exact
(5D)

Error
⑀2 ϭ y2 Ϫ y2,n

Ϫ2.50000
Ϫ1.95000
Ϫ1.54500
Ϫ1.24350
Ϫ1.01625
Ϫ0.84260

Ϫ2.50000
Ϫ2.01606
Ϫ1.64195
Ϫ1.35067
Ϫ1.12211
Ϫ0.94123

0.00000
Ϫ0.06606
Ϫ0.09695
Ϫ0.10717
Ϫ0.10586
Ϫ0.09863

Runge–Kutta Methods for Systems
As for Euler methods, we obtain RK methods for an initial value problem (1) simply by
writing vector formulas for vectors with m components, which, for m ϭ 1, reduce to the
previous scalar formulas.
Thus, for the classical RK method of fourth order in Table 21.3, we obtain
y(x 0) ϭ y0

(6a)

(Initial values)

and for each step n ϭ 0, 1, Á , N Ϫ 1 we obtain the 4 auxiliary quantities
k 1 ϭ hf (x n,
(6b)

yn)

k 2 ϭ hf (x n ϩ 12 h,

yn ϩ 12 k 1)

k 3 ϭ hf (x n ϩ 12 h,

yn ϩ 12 k 2)

k 4 ϭ hf (x n ϩ h,

yn ϩ k 3)

and the new value [approximation of the solution y(x) at x nϩ1 ϭ x 0 ϩ (n ϩ 1)h]
ynϩ1 ϭ yn ϩ 16 (k 1 ϩ 2k 2 ϩ 2k 3 ϩ k 4).

(6c)
EXAMPLE 2

RK Method for Systems. Airy’s Equation. Airy Function Ai(x)
Solve the initial value problem
y s ϭ xy,

y(0) ϭ 1>(32>3 ؒ ⌫ (23 )) ϭ 0.35502805,

y r(0) ϭ Ϫ1>(31>3 ؒ ⌫ (13 )) ϭ Ϫ0.25881940

c21-a.qxd

11/3/10

918

2:44 PM

Page 918

CHAP. 21 Numerics for ODEs and PDEs
by the Runge–Kutta method for systems with h ϭ 0.2; do 5 steps. This is Airy’s equation,4 which arose in
optics (see Ref. [A13], p. 188, listed in App. 1). ⌫ is the gamma function (see App. A3.1). The initial conditions
are such that we obtain a standard solution, the Airy function Ai(x), a special function that has been thoroughly
investigated; for numeric values, see Ref. [GenRef1], pp. 446, 475.

Solution. For y s ϭ xy, setting y1 ϭ y, y2 ϭ y1r ϭ y r we obtain the system (4)
y1r ϭ y2
y2r ϭ xy1.
Hence f ϭ [ f1 f2]T in (1) has the components f1 (x, y) ϭ y2, f2 (x, y) ϭ xy1. We now write (6) in components.
The initial conditions (6a) are y1,0 ϭ 0.35502805, y2,0 ϭ Ϫ0.25881940. In (6b) we have fewer subscripts by
simply writing k 1 ϭ a, k 2 ϭ b, k 3 ϭ c, k 4 ϭ d, so that a ϭ [a1 a2]T, etc. Then (6b) takes the form

aϭh

c

y2,n

bϭh

c

y2,n ϩ 12 a2

cϭh

c

y2,n ϩ 12 b2

dϭh

c

y2,n ϩ c2

(6b*)

x ny1,n

d
d

(x n ϩ 12 h)(y1,n ϩ 12 a1)

(x n ϩ 12 h)(y1,n ϩ 12 b1)

(x n ϩ h)(y1,n ϩ c1)

d

d.

For example, the second component of b is obtained as follows. f (x, y) has the second component f2(x, y) ϭ xy1.
Now in b (ϭ k 2) the first argument is
x ϭ x n ϩ 12 h.
The second argument in b is
y ϭ yn ϩ 12 a,
and the first component of this is
y1 ϭ y1,n ϩ 12 a1.
Together,
xy1 ϭ (x n ϩ 12 h)(y1,n ϩ 12 a1).
Similarly for the other components in (6b*). Finally,
(6c*)

ynϩ1 ϭ yn ϩ 16 (a ϩ 2b ϩ 2c ϩ d).

Table 21.11 shows the values y(x) ϭ y1 (x) of the Airy function Ai(x) and of its derivative y r(x) ϭ y2 (x) as well
᭿
as of the (rather small!) error of y(x).

4
Named after Sir GEORGE BIDELL AIRY (1801–1892), English mathematician, who is known for his work
in elasticity and in PDEs.

c21-a.qxd

11/3/10

2:44 PM

Page 919

SEC. 21.3 Methods for Systems and Higher Order ODEs

919

Table 21.11 RK Method for Systems: Values y1,n (xn) of the Airy Function Ai(x)
in Example 2
n

xn

y1,n(xn)

y1(xn) Exact (8D)

108 ⅐ Error of y1

y2,n(xn)

0
1
2
3
4
5

0.0
0.2
0.4
0.6
0.8
1.0

0.35502805
0.30370303
0.25474211
0.20979973
0.16984596
0.13529207

0.35502805
0.30370315
0.25474235
0.20980006
0.16984632
0.13529242

0
12
24
33
36
35

Ϫ0.25881940
Ϫ0.25240464
Ϫ0.23583073
Ϫ0.21279185
Ϫ0.18641171
Ϫ0.15914687

Runge–Kutta–Nyström Methods (RKN Methods)
RKN methods are direct extensions of RK methods (Runge–Kutta methods) to second-order
ODEs y s ϭ f (x, y, y r ), as given by the Finnish mathematician E. J. Nyström [Acta Soc. Sci.
fenn., 1925, L, No. 13]. The best known of these uses the following formulas, where
n ϭ 0, 1, Á , N Ϫ 1 (N the number of steps):
k 1 ϭ 12 hf (x n, yn, ynr )
(7a)

k 2 ϭ 12 hf (x n ϩ 12 h, yn ϩ K, ynr ϩ k 1)

where K ϭ 12 h( ynr ϩ 12 k 1)

k 3 ϭ 12 hf (x n ϩ 12 h, yn ϩ K, ynr ϩ k 2)
k 4 ϭ 12 hf (x n ϩ h, yn ϩ L, ynr ϩ 2k 3)

where L ϭ h( ynr ϩ k 3).

From this we compute the approximation ynϩ1 of y(x nϩ1) at x nϩ1 ϭ x 0 ϩ (n ϩ 1)h,
(7b)

ynϩ1 ϭ yn ϩ h (ynr ϩ 13 (k 1 ϩ k 2 ϩ k 3)),

and the approximation ynϩ1
r of the derivative y r(x nϩ1) needed in the next step,
(7c)

ynϩ1
r ϭ ynr ϩ 13 (k 1 ϩ 2k 2 ϩ 2k 3 ϩ k 4).

RKN for ODEs y s ‫ ؍‬f (x, y) Not Containing y r . Then k 2 ϭ k 3 in (7), which makes
the method particularly advantageous and reduces (7a)–(7c) to
k 1 ϭ 12 hf (x n, yn)
k 2 ϭ 12 hf (x n ϩ 12 h, yn ϩ 12 h (ynr ϩ 12 k 1)) ϭ k 3
(7*)

k 4 ϭ 12 hf (x n ϩ h, yn ϩ h (ynr ϩ k 2))
ynϩ1 ϭ yn ϩ h( ynr ϩ 13 (k 1 ϩ 2k 2))
ynϩ1
r ϭ ynr ϩ 13 (k 1 ϩ 4k 2 ϩ k 4).

EXAMPLE 3

RKN Method. Airy’s Equation. Airy Function Ai(x)
For the problem in Example 2 and h ϭ 0.2 as before we obtain from (7*) simply k 1 ϭ 0.1x nyn and
k 2 ϭ k 3 ϭ 0.1 (x n ϩ 0.1)(yn ϩ 0.1yrn ϩ 0.05k 1),

k 4 ϭ 0.1 (x n ϩ 0.2)(yn ϩ 0.2ynr ϩ 0.2k 2).

Table 21.12 shows the results. The accuracy is the same as in Example 2, but the work was much less.

᭿

c21-a.qxd

11/3/10

2:44 PM

920

Page 920

CHAP. 21 Numerics for ODEs and PDEs
Table 21.12 Runge–Kutta–Nyström Method Applied to Airy’s Equation,
Computation of the Airy Function y ϭ Ai(x)
xn

yn

ynЈ

y(x) Exact (8D)

108 ⅐ Error
of yn

0.0
0.2
0.4
0.6
0.8
1.0

0.35502805
0.30370304
0.25474211
0.20979974
0.16984599
0.13529218

Ϫ0.25881940
Ϫ0.25240464
Ϫ0.23583070
Ϫ0.21279172
Ϫ0.18641134
Ϫ0.15914609

0.35502805
0.30370315
0.25474235
0.20980006
0.16984632
0.13529242

0
11
24
32
33
24

Our work in Examples 2 and 3 also illustrates that usefulness of methods for ODEs in the
computation of values of “higher transcendental functions.”

Backward Euler Method for Systems. Stiff Systems
The backward Euler formula (16) in Sec. 21.1 generalizes to systems in the form
ynϩ1 ϭ yn ϩ h f (x nϩ1, ynϩ1)

(8)

(n ϭ 0, 1, Á ).

This is again an implicit method, giving ynϩ1 implicitly for given yn. Hence (8) must be
solved for ynϩ1. For a linear system this is shown in the next example. This example also
illustrates that, similar to the case of a single ODE in Sec. 21.1, the method is very useful
for stiff systems. These are systems of ODEs whose matrix has eigenvalues l of very
different magnitudes, having the effect that, just as in Sec. 21.1, the step in direct methods,
RK for example, cannot be increased beyond a certain threshold without losing stability.
(l ϭ Ϫ1 and Ϫ10 in Example 4, but larger differences do occur in applications.)
EXAMPLE 4

Backward Euler Method for Systems of ODEs. Stiff Systems
Compare the backward Euler method (8) with the Euler and the RK methods for numerically solving the initial
value problem
y s ϩ 11y r ϩ 10y ϭ 10x ϩ 11,

y(0) ϭ 2,

y r(0) ϭ Ϫ10

converted to a system of first-order ODEs.

Solution. The given problem can easily be solved, obtaining
y ϭ e؊x ϩ e؊10x ϩ x
so that we can compute errors. Conversion to a system by setting y ϭ y1, y r ϭ y2 [see (4)] gives
y1r ϭ y2

y1(0) ϭ

y2r ϭ Ϫ10y1 Ϫ 11y2 ϩ 10x ϩ 11

y2(0) ϭ Ϫ10.

2

The coefficient matrix
Aϭ

c

0
Ϫ10

1
Ϫ11

d

has the characteristic determinant

2

Ϫl

1

Ϫ10

Ϫl Ϫ 11

2

whose value is l2 ϩ 11l ϩ 10 ϭ (l ϩ 1)(l ϩ 10). Hence the eigenvalues are Ϫ1 and Ϫ10 as claimed above.
The backward Euler formula is

c21-a.qxd

11/3/10

2:44 PM

Page 921

SEC. 21.3 Methods for Systems and Higher Order ODEs

ynϩ1 ϭ

c

y1,nϩ1
y2,nϩ1

d

ϭ

c

y1,n
y2,n

921

d

ϩh

c

y2,nϩ1
Ϫ10y1,nϩ1 Ϫ 11y2,nϩ1 ϩ 10x nϩ1 ϩ 11

d.

Reordering terms gives the linear system in the unknowns y1,nϩ1 and y2,nϩ1
y1,nϩ1 Ϫ

hy2,nϩ1 ϭ y1,n

10hy1,nϩ1 ϩ (1 ϩ 11h)y2,nϩ1 ϭ y2,n ϩ 10h (x n ϩ h) ϩ 11h.
The coefficient determinant is D ϭ 1 ϩ 11h ϩ 10h2, and Cramer’s rule (in Sec. 7.6) gives the solution
ynϩ1 ϭ

c

2
2
3
1 (1 ϩ 11h)y1,n ϩ hy2,n ϩ 10h x n ϩ 11h ϩ 10h

D

Ϫ10hy1,n ϩ y2,n ϩ 10hx n ϩ 11h ϩ 10h2

d.

Table 21.13 Backward Euler Method (BEM) for Example 4. Comparison with Euler and RK
x

BEM
h ϭ 0.2

BEM
h ϭ 0.4

Euler
h ϭ 0.1

Euler
h ϭ 0.2

RK
h ϭ 0.2

RK
h ϭ 0.3

0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0

2.00000
1.36667
1.20556
1.21574
1.29460
1.40599
1.53627
1.67954
1.83272
1.99386
2.16152

2.00000

2.00000
1.01000
1.56100
1.13144
1.23047
1.34868
1.48243
1.62877
1.78530
1.95009
2.12158

2.00000
0.00000
2.04000
0.11200
2.20960
0.32768
2.46214
0.60972
2.76777
0.93422
3.10737

2.00000
1.35207
1.18144
1.18585
1.26168
1.37200
1.50257
1.64706
1.80205
1.96535
2.13536

2.00000

1.31429
1.35020
1.57243
1.86191
2.18625

3.03947

5.07569

8.72329

Exact
2.00000
1.15407
1.08864
1.15129
1.24966
1.36792
1.50120
1.64660
1.80190
1.96530
2.13534

Table 21.13 shows the following.
Stability of the backward Euler method for h ϭ 0.2 and 0.4 (and in fact for any h; try h ϭ 5.0) with decreasing
accuracy for increasing h
Stability of the Euler method for h ϭ 0.1 but instability for h ϭ 0.2
Stability of RK for h ϭ 0.2 but instability for h ϭ 0.3
Figure 452 shows the Euler method for h ϭ 0.18, an interesting case with initial jumping (for about x Ͼ 3) but
᭿
later monotone following the solution curve of y ϭ y1. See also CAS Experiment 15.
y
4.0

3.0

2.0

1.0

0

1

2

3

4

x

Fig. 452. Euler method with h ϭ 0.18 in Example 4

c21-a.qxd

11/3/10

2:44 PM

922

Page 922

CHAP. 21 Numerics for ODEs and PDEs

PROBLEM SET 21.3
1–6

EULER FOR SYSTEMS AND
SECOND-ORDER ODEs

Solve by the Euler’s method. Graph the solution in the
y1y2-plane. Calculate the errors.
1. y1r ϭ 2y1 Ϫ 4y2, y2r ϭ y1 Ϫ 3y2, y1(0) ϭ 3,
y2(0) ϭ 0, h ϭ 0.1, 10 steps
2. Spiral. y1r ϭ Ϫy1 ϩ y2, y2r ϭ Ϫy1 Ϫ y2, y1(0) ϭ 0,
y2(0) ϭ 4, h ϭ 0.2, 5 steps
3. y s ϩ 14 y ϭ 0, y(0) ϭ 1, y r(0) ϭ 0, h ϭ 0.2,
5 steps
4. y1r ϭ Ϫ3y1 ϩ y2, y2r ϭ y1 Ϫ 3y2, y1(0) ϭ 2,
y2 (0) ϭ 0, h ϭ 0.1, 5 steps
5. y s Ϫ y ϭ x, y(0) ϭ 1, y r(0) ϭ Ϫ2, h ϭ 0.1,
5 steps
6. y1r ϭ y1, y2r ϭ Ϫy2, y1(0) ϭ 2, y2(0) ϭ 2,
h ϭ 0.1, 10 steps
7–10

RK FOR SYSTEMS

Solve by the classical RK.
7. The ODE in Prob. 5. By what factor did the error
decrease?
8. The system in Prob. 2
9. The system in Prob. 1
10. The system in Prob. 4
11. Pendulum equation y s ϩ sin y ϭ 0, y(p) ϭ 0,
y r(p) ϭ 1, as a system, h ϭ 0.2, 20 steps. How
does your result fit into Fig. 93 in Sec. 4.5?
12. Bessel Function J0 . xy s ϩ y r ϩ xy ϭ 0, y(1) ϭ
0.765198, y r(1) ϭ Ϫ0.440051, h ϭ 0.5, 5 steps.
(This gives the standard solution J0 (x) in Fig. 110 in
Sec. 5.4.)

21.4

13. Verify the formulas and calculations for the Airy
equation in Example 2 of the text.
14. RKN. The classical RK for a first-order ODE extends
to second-order ODEs (E. J. Nyström, Acta fenn.
No 13, 1925). If the ODE is y s ϭ f (x, y), not
containing y r , then
k 1 ϭ 12 hf (x n, yn)
k 2 ϭ 12 hf (x n ϩ 12 h, yn ϩ 12 h( yrn ϩ 12 k 1)) ϭ k 3
k 4 ϭ 12 hf (x n ϩ h, yn ϩ h( ynr ϩ k 2))
ynϩ1 ϭ yn ϩ h( ynr ϩ 13 (k 1 ϩ 2k 2))
ynϩ1
r ϭ ynr ϩ 18 (k 1 ϩ 4k 2 ϩ k 4).
Apply this RKN (Runge–Kutta–Nyström) method to
the Airy ODE in Example 2 with h ϭ 0.2 as before, to
obtain approximate values of Ai(x).
15. CAS EXPERIMENT. Backward Euler and
Stiffness. Extend Example 3 as follows.
(a) Verify the values in Table 21.13 and show them
graphically as in Fig. 452.
(b) Compute and graph Euler values for h near the
“critical” h ϭ 0.18 to determine more exactly when
instability starts.
(c) Compute and graph RK values for values of h
between 0.2 and 0.3 to find h for which the RK
approximation begins to increase away from the exact
solution.
(d) Compute and graph backward Euler values for
large h; confirm stability and investigate the error
increase for growing h.

Methods for Elliptic PDEs
We have arrived at the second half of this chapter, which is devoted to numerics for
partial differential equations (PDEs). As we have seen in Chap.12, there are many
applications to PDEs, such as in dynamics, elasticity, heat transfer, electromagnetic
theory, quantum mechanics, and others. Selected because of their importance in
applications, the PDEs covered here include the Laplace equation, the Poisson equation,
the heat equation, and the wave equation. By covering these equations based on their
importance in applications we also selected equations that are important for theoretical
considerations. Indeed, these equations serve as models for elliptic, parabolic, and
hyperbolic PDEs. For example, the Laplace equation is a representative example of an
elliptic type of PDE, and so forth.

c21-b.qxd

11/3/10

2:53 PM

Page 923

SEC. 21.4 Methods for Elliptic PDEs

923

Recall, from Sec. 12.4, that a PDE is called quasilinear if it is linear in the highest
derivatives. Hence a second-order quasilinear PDE in two independent variables x, y is of the
form
au xx ϩ 2bu xy ϩ cu yy ϭ F(x, y, u, u x, u y).

(1)

u is an unknown function of x and y (a solution sought). F is a given function of the
indicated variables.
Depending on the discriminant ac Ϫ b 2, the PDE (1) is said to be of
elliptic type

if

ac Ϫ b 2 Ͼ 0 (example: Laplace equation)

parabolic type

if

ac Ϫ b 2 ϭ 0 (example: heat equation)

hyperbolic type if

ac Ϫ b 2 Ͻ 0 (example: wave equation).

Here, in the heat and wave equations, y is time t. The coefficients a, b, c may be functions
of x, y, so that the type of (1) may be different in different regions of the xy-plane. This
classification is not merely a formal matter but is of great practical importance because
the general behavior of solutions differs from type to type and so do the additional
conditions (boundary and initial conditions) that must be taken into account.
Applications involving elliptic equations usually lead to boundary value problems in a
region R, called a first boundary value problem or Dirichlet problem if u is prescribed
on the boundary curve C of R, a second boundary value problem or Neumann problem
if u n ϭ 0u>0n (normal derivative of u) is prescribed on C, and a third or mixed problem
if u is prescribed on a part of C and u n on the remaining part. C usually is a closed curve
(or sometimes consists of two or more such curves).

Difference Equations
for the Laplace and Poisson Equations
In this section we develop numeric methods for the two most important elliptic PDEs that
appear in applications. The two PDEs are the Laplace equation
ٌ2u ϭ u xx ϩ u yy ϭ 0

(2)
and the Poisson equation

ٌ2u ϭ u xx ϩ u yy ϭ f (x, y).

(3)

The starting point for developing our numeric methods is the idea that we can replace
the partial derivatives of these PDEs by corresponding difference quotients. Details are
as follows:
To develop this idea, we start with the Taylor formula and obtain
(4)

(a)

u(x ϩ h, y) ϭ u(x, y) ϩ hu x(x, y) ϩ 12 h2u xx(x, y) ϩ 16 h3u xxx(x, y) ϩ Á

(b)

u(x Ϫ h, y) ϭ u(x, y) Ϫ hu x(x, y) ϩ 12 h2u xx(x, y) Ϫ 16 h3u xxx(x, y) ϩ Á .

c21-b.qxd

11/3/10

924

2:53 PM

Page 924

CHAP. 21 Numerics for ODEs and PDEs

We subtract (4b) from (4a), neglect terms in h3, h4, Á , and solve for u x. Then
u x(x, y) Ϸ

(5a)

1
3u(x ϩ h, y) Ϫ u(x Ϫ h, y)4.
2h

Similarly,
u(x, y ϩ k) ϭ u(x, y) ϩ ku y(x, y) ϩ 12 k 2u yy(x, y) ϩ Á
and
u(x, y Ϫ k) ϭ u(x, y) Ϫ ku y(x, y) ϩ 12 k 2u yy(x, y) ϩ Á .
By subtracting, neglecting terms in k 3, k 4, Á , and solving for u y we obtain
u y(x, y) Ϸ

(5b)

1
3u(x, y ϩ k) Ϫ u(x, y Ϫ k)4.
2k

We now turn to second derivatives. Adding (4a) and (4b) and neglecting terms in
h4, h5, Á , we obtain u(x ϩ h, y) ϩ u(x Ϫ h, y) Ϸ 2u(x, y) ϩ h2u xx(x, y). Solving for u xx
we have
u xx(x, y) Ϸ

(6a)

1
h2

3u(x ϩ h, y) Ϫ 2u(x, y) ϩ u(x Ϫ h, y)4.

Similarly,
(6b)

u yy(x, y) Ϸ

1
k2

3u(x, y ϩ k) Ϫ 2u(x, y) ϩ u(x, y Ϫ k)4.

We shall not need (see Prob. 1)
(6c)

u xy(x, y) Ϸ

1
3u(x ϩ h, y ϩ k) Ϫ u(x Ϫ h, y ϩ k)
4hk
Ϫ u(x ϩ h, y Ϫ k) ϩ u(x Ϫ h, y Ϫ k)4.

Figure 453a shows the points (x ϩ h, y), (x Ϫ h, y), Á in (5) and (6).
We now substitute (6a) and (6b) into the Poisson equation (3), choosing k ϭ h to obtain
a simple formula:
(7)

u(x ϩ h, y) ϩ u(x, y ϩ h) ϩ u(x Ϫ h, y) ϩ u(x, y Ϫ h) Ϫ 4u(x, y) ϭ h2f (x, y).

This is a difference equation corresponding to (3). Hence for the Laplace equation (2)
the corresponding difference equation is
(8)

u(x ϩ h, y) ϩ u(x, y ϩ h) ϩ u(x Ϫ h, y) ϩ u(x, y Ϫ h) Ϫ 4u(x, y) ϭ 0.

h is called the mesh size. Equation (8) relates u at (x, y) to u at the four neighboring points
shown in Fig. 453b. It has a remarkable interpretation: u at (x, y) equals the mean of the

c21-b.qxd

11/3/10

2:53 PM

Page 925

SEC. 21.4 Methods for Elliptic PDEs

925

values of u at the four neighboring points. This is an analog of the mean value property
of harmonic functions (Sec. 18.6).
Those neighbors are often called E (East), N (North), W (West), S (South). Then Fig. 453b
becomes Fig. 453c and (7) is
u(E) ϩ u(N) ϩ u(W) ϩ u(S) Ϫ 4u(x, y) ϭ h2f (x, y).

(7*)

N

(x, y + h)
( x, y + k)

(x – h, y)

h

h
k

h

h

k
( x + h, y)

(x – h, y)

h

h

(x, y)

W

(x + h, y)

h

h

E

(x, y)

(x, y)
h

h
(x, y – k)

(a) Points in (5) and (6)

(x, y – h)

S

(b) Points in (7) and (8)

(c) Notation in (7*)

Fig. 453. Points and notation in (5)–(8) and (7*)

Our approximation of h2ٌ2u in (7) and (8) is a 5-point approximation with the
coefficient scheme or stencil (also called pattern, molecule, or star)
1
(9)

d1

Ϫ4

1
1t . We may now write (7) as

1

d1

Ϫ4

1t u ϭ h2f (x, y).

1

Dirichlet Problem
In numerics for the Dirichlet problem in a region R we choose an h and introduce a square
grid of horizontal and vertical straight lines of distance h. Their intersections are called
mesh points (or lattice points or nodes). See Fig. 454.
Then we approximate the given PDE by a difference equation [(8) for the Laplace
equation], which relates the unknown values of u at the mesh points in R to each other
and to the given boundary values (details in Example 1). This gives a linear system of
algebraic equations. By solving it we get approximations of the unknown values of u at
the mesh points in R.
We shall see that the number of equations equals the number of unknowns. Now comes
an important point. If the number of internal mesh points, call it p, is small, say, p Ͻ 100,
then a direct solution method may be applied to that linear system of p Ͻ 100 equations
in p unknowns. However, if p is large, a storage problem will arise. Now since each
unknown u is related to only 4 of its neighbors, the coefficient matrix of the system is a
sparse matrix, that is, a matrix with relatively few nonzero entries (for instance, 500 of
10,000 when p ϭ 100). Hence for large p we may avoid storage difficulties by using an
iteration method, notably the Gauss–Seidel method (Sec. 20.3), which in PDEs is also

c21-b.qxd

11/3/10

2:53 PM

926

Page 926

CHAP. 21 Numerics for ODEs and PDEs

called Liebmann’s method (note the strict diagonal dominance). Remember that in this
method we have the storage convenience that we can overwrite any solution component
(value of u) as soon as a “new” value is available.
Both cases, large p and small p, are of interest to the engineer, large p if a fine grid is
used to achieve high accuracy, and small p if the boundary values are known only rather
inaccurately, so that a coarse grid will do it because in this case it would be meaningless
to try for great accuracy in the interior of the region R.
We illustrate this approach with an example, keeping the number of equations small,
for simplicity. As convenient notations for mesh points and corresponding values of the
solution (and of approximate solutions) we use (see also Fig. 454)
Pij ϭ (ih, jh),

(10)

u ij ϭ u(ih, jh).

y

Pij
P12

P22

P11

P21

0

P31

5h

x

Fig. 454. Region in the xy-plane covered by a grid of mesh h,
also showing mesh points P11 ϭ (h, h), Á , Pij ϭ (ih, jh), Á

With this notation we can write (8) for any mesh point Pij in the form
(11)

u iϩ1, j ϩ u i, jϩ1 ϩ u i؊1, j ϩ u i, j؊1 Ϫ 4u ij ϭ 0.

Remark. Our current discussion and the example that follows illustrate what we may
call the reuseability of mathematical ideas and methods. Recall that we applied the
Gauss–Seidel method to a system of ODEs in Sec. 20.3 and that we can now apply it
again to elliptic PDEs. This shows that engineering mathematics has a structure and
important mathematical ideas and methods will appear again and again in different
situations. The student should find this attractive in that previous knowledge can be
reapplied.
EXAMPLE 1

Laplace Equation. Liebmann’s Method
The four sides of a square plate of side 12 cm, made of homogeneous material, are kept at constant temperature
0°C and 100°C as shown in Fig. 455a. Using a (very wide) grid of mesh 4 cm and applying Liebmann’s method
(that is, Gauss–Seidel iteration), find the (steady-state) temperature at the mesh points.

Solution. In the case of independence of time, the heat equation (see Sec. 10.8)
u t ϭ c2(u xx ϩ u yy)
reduces to the Laplace equation. Hence our problem is a Dirichlet problem for the latter. We choose the grid
shown in Fig. 455b and consider the mesh points in the order P11, P21, P12, P22. We use (11) and, in each equation,
take to the right all the terms resulting from the given boundary values. Then we obtain the system

c21-b.qxd

11/3/10

2:53 PM

Page 927

SEC. 21.4 Methods for Elliptic PDEs

927
Ϫ4u 11 ϩ u 21 ϩ u 12
Ϫu 11 Ϫ 4u 21

(12)

ϭ Ϫ200
ϩ u 22 ϭ Ϫ200

Ϫ 4u 12 ϩ u 22 ϭ Ϫ100

u 11

u 21 ϩ u 12 Ϫ 4u 22 ϭ Ϫ100.
In practice, one would solve such a small system by the Gauss elimination, finding u 11 ϭ u 21 ϭ 87.5,
u 12 ϭ u 22 ϭ 62.5.
More exact values (exact to 3S) of the solution of the actual problem [as opposed to its model (12)] are 88.1
and 61.9, respectively. (These were obtained by using Fourier series.) Hence the error is about 1%, which is
surprisingly accurate for a grid of such a large mesh size h. If the system of equations were large, one would
solve it by an indirect method, such as Liebmann’s method. For (12) this is as follows. We write (12) in the
form (divide by Ϫ4 and take terms to the right)
u 11 ϭ

0.25u 21 ϩ 0.25u 12

ϩ 50

u 21 ϭ 0.25u 11

ϩ 0.25u 22 ϩ 50

u 12 ϭ 0.25u 11

ϩ 0.25u 22 ϩ 25

u 22 ϭ

0.25u 21 ϩ 0.25u 12

ϩ 25.

These equations are now used for the Gauss–Seidel iteration. They are identical with (2) in Sec. 20.3, where
u 11 ϭ x 1, u 21 ϭ x 2, u 12 ϭ x 3, u 22 ϭ x 4, and the iteration is explained there, with 100, 100, 100, 100 chosen as
starting values. Some work can be saved by better starting values, usually by taking the average of the boundary
values that enter into the linear system. The exact solution of the system is u 11 ϭ u 21 ϭ 87.5, u 12 ϭ u 22 ϭ 62.5,
as you may verify.
y

u=0

u=0

12
u = 100
R

u = 100

0
0

P02

P12

P22

P01

P11

P21

P10

P20

u = 100

x

12
u = 100

u = 100

(a) Given problem

(b) Grid and mesh points

Fig. 455. Example 1
Remark. It is interesting to note that, if we choose mesh h ϭ L>n (L ϭ side of R) and consider the (n Ϫ 1)2
internal mesh points (i.e., mesh points not on the boundary) row by row in the order
P11, P21, Á , Pn؊1,1, P12, P22, Á , Pn؊2,2, Á ,
then the system of equations has the (n Ϫ 1)2 ϫ (n Ϫ 1)2 coefficient matrix
B

I

I

B

I
•

(13)

Aϭ

S

Ϫ4

1

1

Ϫ4

1
•

T.

•
•
I

Here

Bϭ

S

T

•
•

B

I

I

B

1

Ϫ4

1

1

Ϫ4

c21-b.qxd

11/3/10

928

2:53 PM

Page 928

CHAP. 21 Numerics for ODEs and PDEs
is an (n Ϫ 1) ϫ (n Ϫ 1) matrix. (In (12) we have n ϭ 3, (n Ϫ 1)2 ϭ 4 internal mesh points, two submatrices
B, and two submatrices I.) The matrix A is nonsingular. This follows by noting that the off-diagonal entries in
each row of A have the sum 3 (or 2), whereas each diagonal entry of A equals Ϫ4, so that nonsingularity is
implied by Gerschgorin’s theorem in Sec. 20.7 because no Gerschgorin disk can include 0.
᭿

A matrix is called a band matrix if it has all its nonzero entries on the main diagonal
and on sloping lines parallel to it (separated by sloping lines of zeros or not). For example,
A in (13) is a band matrix. Although the Gauss elimination does not preserve zeros between
bands, it does not introduce nonzero entries outside the limits defined by the original
bands. Hence a band structure is advantageous. In (13) it has been achieved by carefully
ordering the mesh points.

ADI Method
A matrix is called a tridiagonal matrix if it has all its nonzero entries on the main
diagonal and on the two sloping parallels immediately above or below the diagonal. (See
also Sec. 20.9.) In this case the Gauss elimination is particularly simple.
This raises the question of whether, in the solution of the Dirichlet problem for the
Laplace or Poisson equations, one could obtain a system of equations whose coefficient
matrix is tridiagonal. The answer is yes, and a popular method of that kind, called the
ADI method (alternating direction implicit method ) was developed by Peaceman and
Rachford. The idea is as follows. The stencil in (9) shows that we could obtain a tridiagonal
matrix if there were only the three points in a row (or only the three points in a column).
This suggests that we write (11) in the form
(14a)

u i؊1, j Ϫ 4u ij ϩ u iϩ1, j ϭ Ϫu i, j؊1 Ϫ u i, jϩ1

so that the left side belongs to y-Row j only and the right side to x-Column i. Of course,
we can also write (11) in the form
(14b)

u i, j؊1 Ϫ 4u ij ϩ u i, jϩ1 ϭ Ϫu i؊1, j Ϫ u iϩ1, j

so that the left side belongs to Column i and the right side to Row j. In the ADI method
we proceed by iteration. At every mesh point we choose an arbitrary starting value u (0)
ij .
In each step we compute new values at all mesh points. In one step we use an iteration
formula resulting from (14a) and in the next step an iteration formula resulting from (14b),
and so on in alternating order.
In detail: suppose approximations u (m)
have been computed. Then, to obtain the next
ij
(m)
approximations u (mϩ1)
,
we
substitute
the
u
ij
ij on the right side of (14a) and solve for the
(mϩ1)
u ij
on the left side; that is, we use
(15a)

(mϩ1)
(m)
(m)
u (mϩ1)
ϩ u (mϩ1)
i؊1, j Ϫ 4u ij
iϩ1, j ϭ Ϫu i, j؊1 Ϫ u i, jϩ1.

We use (15a) for a fixed j, that is, for a fixed row j, and for all internal mesh points in
this row. This gives a linear system of N algebraic equations (N ϭ number of internal
mesh points per row) in N unknowns, the new approximations of u at these mesh points.
Note that (15a) involves not only approximations computed in the previous step but also
given boundary values. We solve the system (15a) ( j fixed!) by Gauss elimination. Then
we go to the next row, obtain another system of N equations and solve it by Gauss, and
so on, until all rows are done. In the next step we alternate direction, that is, we compute

c21-b.qxd

11/3/10

2:53 PM

Page 929

SEC. 21.4 Methods for Elliptic PDEs

929

the next approximations u (mϩ2)
column by column from the u (mϩ1)
and the given boundary
ij
ij
values, using a formula obtained from (14b) by substituting the u (mϩ1)
on the right:
ij
(15b)

(mϩ2)
(mϩ1)
(mϩ1)
u i,(mϩ2)
ϩ u (mϩ2)
j؊1 Ϫ 4u ij
i, jϩ1 ϭ Ϫu i؊1, j Ϫ u iϩ1, j .

For each fixed i, that is, for each column, this is a system of M equations (M ϭ number
of internal mesh points per column) in M unknowns, which we solve by Gauss elimination.
Then we go to the next column, and so on, until all columns are done.
Let us consider an example that merely serves to explain the entire method.
EXAMPLE 2

Dirichlet Problem. ADI Method
Explain the procedure and formulas of the ADI method in terms of the problem in Example 1, using the same
grid and starting values 100, 100, 100, 100.

Solution. While working, we keep an eye on Fig. 455b and the given boundary values. We obtain first
(1)
(1)
(1)
approximations u (1)
11 , u 21 , u 12 , u 22 from (15a) with m ϭ 0. We write boundary values contained in (15a) without
an upper index, for better identification and to indicate that these given values remain the same during the
iteration. From (15a) with m ϭ 0 we have for j ϭ 1 (first row) the system
(1)
(i ϭ 1) u 01 Ϫ 4u (1)
11 ϩ u 21

(i ϭ 2)

ϭ Ϫu 10 Ϫ u (0)
12

(1)
(0)
u (1)
11 Ϫ 4u 21 ϩ u 31 ϭ Ϫu 20 Ϫ u 22 .

(1)
The solution is u (1)
11 ϭ u 21 ϭ 100. For j ϭ 2 (second row) we obtain from (15a) the system
(1)
(i ϭ 1) u 02 Ϫ 4u (1)
12 ϩ u 22

(i ϭ 2)

ϭ Ϫu (0)
11 Ϫ u 13

(1)
(0)
u (1)
12 Ϫ 4u 22 ϩ u 32 ϭ Ϫu 21 Ϫ u 23.

(1)
The solution is u (1)
12 ϭ u 22 ϭ 66.667.
(2)
(2)
(2)
Second approximations u (2)
11 , u 21 , u 12 , u 22 are now obtained from (15b) with m ϭ 1 by using the first
approximations just computed and the boundary values. For i ϭ 1 (first column) we obtain from (15b) the system
(2)
( j ϭ 1) u 10 Ϫ 4u (2)
11 ϩ u 12

( j ϭ 2)

ϭ Ϫu 01 Ϫ u (1)
21

(2)
(1)
u (2)
11 Ϫ 4u 12 ϩ u 13 ϭ Ϫu 02 Ϫ u 22 .

(2)
The solution is u (2)
11 ϭ 91.11, u 12 ϭ 64.44, For i ϭ 2 (second column) we obtain from (15b) the system

( j ϭ 1)
( j ϭ 2)

(2)
u 20 Ϫ 4u (2)
21 ϩ u 22

ϭ Ϫu (1)
11 Ϫ u 31

(2)
(1)
u (2)
21 Ϫ 4u 22 ϩ u 23 ϭ Ϫu 12 Ϫ u 32.

(2)
The solution is u (2)
21 ϭ 91.11, u 22 ϭ 64.44.
In this example, which merely serves to explain the practical procedure in the ADI method, the accuracy of
the second approximations is about the same as that of two Gauss–Seidel steps in Sec. 20.3 (where
u 11 ϭ x 1, u 21 ϭ x 2, u 12 ϭ x 3, u 22 ϭ x 4), as the following table shows.

Method

u11

u21

u12

u22

ADI, 2nd approximations
Gauss–Seidel, 2nd approximations
Exact solution of (12)

91.11
93.75
87.50

91.11
90.62
87.50

64.44
65.62
62.50

64.44
64.06
62.50

᭿

c21-b.qxd

11/3/10

2:53 PM

Page 930

930

CHAP. 21 Numerics for ODEs and PDEs

Improving Convergence. Additional improvement of the convergence of the ADI
method results from the following interesting idea. Introducing a parameter p, we can also
write (11) in the form
(a) u i؊1, j Ϫ (2 ϩ p)u ij ϩ u iϩ1, j ϭ Ϫu i, j؊1 ϩ (2 Ϫ p)u ij Ϫ u i, jϩ1

(16)

(b) u i, j؊1 Ϫ (2 ϩ p)u ij ϩ u i, jϩ1 ϭ Ϫu i؊1, j ϩ (2 Ϫ p)u ij Ϫ u iϩ1, j .

This gives the more general ADI iteration formulas
(mϩ1)
(m)
(m)
(a) u (mϩ1)
ϩ u (mϩ1)
Ϫ u (m)
i؊1, j Ϫ (2 ϩ p)u ij
iϩ1, j ϭ Ϫu i, j؊1 ϩ (2 Ϫ p)u ij
i, jϩ1

(17)

(mϩ2)
(mϩ1)
(mϩ1)
(b) u (mϩ2)
ϩ u i,(mϩ2)
Ϫ u (mϩ1)
i, j؊1 Ϫ (2 ϩ p)u ij
jϩ1 ϭ Ϫu i؊1, j ϩ (2 Ϫ p)u ij
iϩ1, j .

For p ϭ 2, this is (15). The parameter p may be used for improving convergence. Indeed,
one can show that the ADI method converges for positive p, and that the optimum value
for maximum rate of convergence is
p0 ϭ 2 sin

(18)

p
K

where K is the larger of M ϩ 1 and N ϩ 1 (see above). Even better results can be achieved
by letting p vary from step to step. More details of the ADI method and variants are
discussed in Ref. [E25] listed in App. 1.

PROBLEM SET 21.4
1. Derive (5b), (6b), and (6c).
2. Verify the calculations in Example 1 of the text. Find
out experimentally how many steps you need to obtain
the solution of the linear system with an accuracy of 3S.
3. Use of symmetry. Conclude from the boundary values
in Example 1 that u 21 ϭ u 11 and u 22 ϭ u 12. Show
that this leads to a system of two equations and solve it.
4. Finer grid of 3 ϫ 3 inner points. Solve Example 1,
12
choosing h ϭ 12
4 ϭ 3 (instead of h ϭ 3 ϭ 4) and the
same starting values.
5–10

GAUSS ELIMINATION, GAUSS–SEIDEL
ITERATION

5. u (1, 0) ϭ 60, u (2, 0) ϭ 300, u ϭ 100 on the other
three edges.
6. u ϭ 0 on the left, x 3 on the lower edge, 27 Ϫ 9y 2 on
the right, x 3 Ϫ 27x on the upper edge.
7. U0 on the upper and lower edges, ϪU0 on the left and
right. Sketch the equipotential lines.
8. u ϭ 220 on the upper and lower edges, 110 on the left
and right.

3
2
y
1
0

For the grid in Fig. 456 compute the potential at the
four internal points by Gauss and by 5 Gauss–Seidel
steps with starting values 100, 100, 100, 100 (showing
the details of your work) if the boundary values on the
edges are:

0

P12

P22

P11

P21

1

x

2

9. u ϭ sin 13 px on the upper edge, 0 on the other edges,
10 steps.

3

Fig. 456. Problems 5–10

10. u ϭ x 4 on the lower edge, 81 Ϫ 54y 2 ϩ y 4 on the right,
x 4 Ϫ 54x 2 ϩ 81 on the upper edge, y 4 on the left.
Verify the exact solution x 4 Ϫ 6x 2y 2 ϩ y 4 and
determine the error.

c21-b.qxd

11/3/10

2:53 PM

Page 931

SEC. 21.5 Neumann and Mixed Problems. Irregular Boundary
11. Find the potential in Fig. 457 using (a) the coarse
grid, (b) the fine grid 5 ϫ 3, and Gauss elimination.
Hint. In (b), use symmetry; take u ϭ 0 as boundary
value at the two points at which the potential has a
jump.
u = 110 V

u = 110 V

P12

u = –110 V

u = –110 V

Fig. 457. Region and grids in Problem 11
12. Influence of starting values. Do Prob. 9 by Gauss–
Seidel, starting from 0. Compare and comment.
13. For the square 0 Ϲ x Ϲ 4, 0 Ϲ y Ϲ 4 let the boundary
temperatures be 0°C on the horizontal and 50°C on the
vertical edges. Find the temperatures at the interior
points of a square grid with h ϭ 1.
14. Using the answer to Prob. 13, try to sketch some
isotherms.

21.5

15. Find the isotherms for the square and grid in Prob. 13
if u ϭ sin 14 px on the horizontal and Ϫsin 14 py on the
vertical edges. Try to sketch some isotherms.
16. ADI. Apply the ADI method to the Dirichlet problem
in Prob. 9, using the grid in Fig. 456, as before and
starting values zero.
17. What p0 in (18) should we choose for Prob. 16? Apply
the ADI formulas (17) with that value of p0 to Prob. 16,
performing 1 step. Illustrate the improved convergence
by comparing with the corresponding values 0.077,
0.308 after the first step in Prob. 16. (Use the starting
values zero.)

u = 110 V

P11
u = –110 V

931

18. CAS PROJECT. Laplace Equation. (a) Write a
program for Gauss–Seidel with 16 equations in 16
unknowns, composing the matrix (13) from the indicated
4 ϫ 4 submatrices and including a transformation of
the vector of the boundary values into the vector b of
Ax ϭ b.
(b) Apply the program to the square grid in 0 Ϲ x Ϲ 5,
0 Ϲ y Ϲ 5 with h ϭ 1 and u ϭ 220 on the upper and
lower edges, u ϭ 110 on the left edge and u ϭ Ϫ10
on the right edge. Solve the linear system also by Gauss
elimination. What accuracy is reached in the 20th
Gauss–Seidel step?

Neumann and Mixed Problems.
Irregular Boundary
We continue our discussion of boundary value problems for elliptic PDEs in a region R
in the xy-plane. The Dirichlet problem was studied in the last section. In solving Neumann
and mixed problems (defined in the last section) we are confronted with a new situation,
because there are boundary points at which the (outer) normal derivative u n ϭ 0u>0n of
the solution is given, but u itself is unknown since it is not given. To handle such points
we need a new idea. This idea is the same for Neumann and mixed problems. Hence we
may explain it in connection with one of these two types of problems. We shall do so and
consider a typical example as follows.

EXAMPLE 1

Mixed Boundary Value Problem for a Poisson Equation
Solve the mixed boundary value problem for the Poisson equation
ٌ2u ϭ u xx ϩ u yy ϭ f (x, y) ϭ 12xy

c21-b.qxd

11/3/10

932

2:53 PM

Page 932

CHAP. 21 Numerics for ODEs and PDEs
shown in Fig. 458a.
P13
y

P23

un = 6x
1.0

1.0
u = 3y

P02

P12

P22
P32
un = 3 un = 6 u = 3

u=0

3

R

0.5

P01
u=0

P11

P21

P31
u = 0.375

u=0
0
0

1.5

0

x

0

P10
P20
u=0 u=0
0.5
1.0
1.5

u=0
(a) Region R and boundary values

(b) Grid (h = 0.5)

Fig. 458. Mixed boundary value problem in Example 1

Solution. We use the grid shown in Fig. 458b, where h ϭ 0.5. We recall that (7) in Sec. 21.4 has the right

side h2f (x, y) ϭ 0.52 ؒ 12xy ϭ 3xy. From the formulas u ϭ 3y 3 and u n ϭ 6x given on the boundary we compute
the boundary data
(1)

u 31 ϭ 0.375,

0u 12
0u
ϭ 12 ϭ 6 ؒ 0.5 ϭ 3.
0n
0y

u 32 ϭ 3,

0u 22
0u
ϭ 22 ϭ 6 ؒ 1 ϭ 6.
0n
0y

P11 and P21 are internal mesh points and can be handled as in the last section. Indeed, from (7), Sec. 21.4, with
h2 ϭ 0.25 and h2f (x, y) ϭ 3xy and from the given boundary values we obtain two equations corresponding to
P11 and P21, as follows (with Ϫ0 resulting from the left boundary).
ϭ 12 (0.5 ؒ 0.5) ؒ 14 Ϫ 0 ϭ 0.75

Ϫ4u 11 ϩ u 21 ϩ u 12

(2a)

ϩ u 22 ϭ 12 (1 ؒ 0.5) ؒ 14 Ϫ 0.375 ϭ 1.125.

u 11 Ϫ 4u 21

The only difficulty with these equations seems to be that they involve the unknown values u 12 and u 22 of u at
P12 and P22 on the boundary, where the normal derivative u n ϭ 0u>0n ϭ 0u> 0y is given, instead of u; but we
shall overcome this difficulty as follows.
We consider P12 and P22. The idea that will help us here is this. We imagine the region R to be extended
above to the first row of external mesh points (corresponding to y ϭ 1.5), and we assume that the Poisson
equation also holds in the extended region. Then we can write down two more equations as before (Fig. 458b)
Ϫ 4u 12 ϩ u 22 ϩ u 13

u 11

(2b)

u 21 ϩ u 12 Ϫ 4u 22

ϭ 1.5 Ϫ 0 ϭ 1.5
ϩ u 23 ϭ 3 Ϫ 3 ϭ 0.

On the right, 1.5 is 12xyh2 at (0.5, 1) and 3 is 12xyh2 at (1, 1) and 0 (at P02) and 3 (at P32) are given boundary
values. We remember that we have not yet used the boundary condition on the upper part of the boundary of
R, and we also notice that in (2b) we have introduced two more unknowns u 13, u 23. But we can now use that
condition and get rid of u 13, u 23 by applying the central difference formula for du>dy. From (1) we then obtain
(see Fig. 458b)
3ϭ
6ϭ

0u 12
0y
0u 22
0y

Ϸ
Ϸ

u 13 Ϫ u 11
2h
u 23 Ϫ u 21
2h

ϭ u 13 Ϫ u 11,

hence

u 13 ϭ u 11 ϩ 3

ϭ u 23 Ϫ u 21,

hence

u 23 ϭ u 21 ϩ 6.

Substituting these results into (2b) and simplifying, we have
2u 11

Ϫ 4u 12 ϩ u 22 ϭ 1.5 Ϫ 3 ϭ Ϫ1.5
2u 21 ϩ u 12 Ϫ 4u 22 ϭ 3 Ϫ 3 Ϫ 6 ϭ Ϫ6.

c21-b.qxd

11/3/10

2:53 PM

Page 933

SEC. 21.5 Neumann and Mixed Problems. Irregular Boundary

933

Together with (2a) this yields, written in matrix form,

(3)

Ϫ4

1

1

0

u 11

0.75

1

Ϫ4

0

1

u 21

1.125

2

0

Ϫ4

1

u 12

1.5 Ϫ 3

Ϫ1.5

0

2

1

Ϫ4

u 22

0Ϫ6

Ϫ6

E

U E

0.75

UϭE

UϭE

1.125

U.

(The entries 2 come from u 13 and u 23, and so do Ϫ3 and Ϫ6 on the right). The solution of (3) (obtained by
Gauss elimination) is as follows; the exact values of the problem are given in parentheses.
u 12 ϭ 0.866 (exact 1)

u 22 ϭ 1.812 (exact 2)

u 11 ϭ 0.077 (exact 0.125)

u 21 ϭ 0.191 (exact 0.25).

᭿

Irregular Boundary
We continue our discussion of boundary value problems for elliptic PDEs in a region R
in the xy-plane. If R has a simple geometric shape, we can usually arrange for certain
mesh points to lie on the boundary C of R, and then we can approximate partial derivatives
as explained in the last section. However, if C intersects the grid at points that are not
mesh points, then at points close to the boundary we must proceed differently, as follows.
The mesh point O in Fig. 459 is of that kind. For O and its neighbors A and P we obtain
from Taylor’s theorem
(a) u A ϭ u O ϩ ah

0u O

uP ϭ uO Ϫ h

0u O

0x

1

ϩ

2

(4)
(b)

0x

ϩ

1
2

(ah)2
h2

0 2u O
0x 2

0 2u O
0x 2

ϩ Á

ϩ Á.

We disregard the terms marked by dots and eliminate 0u O>0x. Equation (4b) times a plus
equation (4a) gives
u A ϩ au P Ϸ (1 ϩ a) u O ϩ

1
0 2u O
a (a ϩ 1) h2
.
0x 2
2

B
bh
P

O

A
ah

h
Q

C

Fig. 459. Curved boundary C of a region R, a mesh point O near C,
and neighbors A, B, P, Q

We solve this last equation algebraically for the derivative, obtaining
2
1
1
1
0 2u O
Ϸ 2 c
uA ϩ
uP Ϫ a uO d .
1ϩa
0x 2
h a (1 ϩ a)

c21-b.qxd

11/3/10

2:53 PM

934

Page 934

CHAP. 21 Numerics for ODEs and PDEs

Similarly, by considering the points O, B, and Q,
2
1
1
1
0 2u O
Ϸ 2 c
uB ϩ
uQ Ϫ uO d .
1ϩb
b
0y 2
h b(1 ϩ b)
By addition,
(5)

ٌ2u O Ϸ

uA
uB
uP
2
uQ
(a ϩ b)u O
ϩ
ϩ
ϩ
Ϫ
d .
2 c
b(1 ϩ b)
1ϩa
1ϩb
ab
h a(1 ϩ a)

For example, if a ϭ 12 , b ϭ 12 , instead of the stencil (see Sec. 21.4)
4
3

1
d1

Ϫ4

1t

d32

we now have

4
3t

Ϫ4

.

2
3

1

because 1>[a (1 ϩ a)] ϭ 43 , etc. The sum of all five terms still being zero (which is useful
for checking).
Using the same ideas, you may show that in the case of Fig. 460.

(6) ٌ2u O Ϸ

2
h

2

c

uA
a(a ϩ p)

ϩ

uB
b(b ϩ q)

uP

ϩ

p(p ϩ a)

ϩ

uQ
q(q ϩ b)

Ϫ

ap ϩ bq
abpq

uO d ,

a formula that takes care of all conceivable cases.
B
bh
P

ph

ah

O

A

qh
Q

Fig. 460. Neighboring points A, B, P, Q of a
mesh point O and notations in formula (6)

EXAMPLE 2

Dirichlet Problem for the Laplace Equation. Curved Boundary
Find the potential u in the region in Fig. 461 that has the boundary values given in that figure; here the curved
portion of the boundary is an arc of the circle of radius 10 about (0,0). Use the grid in the figure.

Solution. u is a solution of the Laplace equation. From the given formulas for the boundary values u ϭ x 3,
u ϭ 512 Ϫ 24y 2, Á we compute the values at the points where we need them; the result is shown in the figure.
For P11 and P12 we have the usual regular stencil, and for P21 and P22 we use (6), obtaining
1
(7)

P11, P12: c1

Ϫ4
1

0.5
1s ,

P21: c0.6

Ϫ2.5
0.5

0.9
0.9s,

P22: c0.6

Ϫ3
0.6

0.9s .

c21-b.qxd

11/3/10

2:53 PM

Page 935

SEC. 21.5 Neumann and Mixed Problems. Irregular Boundary

935
3

y

u = x – 243x
u = –702

3

u = 4x – 300x
u = –936

9
P12

u=0

6

P22

u = –352
u = 512 – 24y

u=0
u=0

3

0

0

P11

P21

u = 27

u=
216

3
u=x

6

2

u = 296

x

8

3

Fig. 461. Region, boundary values of the potential, and grid in Example 2
We use this and the boundary values and take the mesh points in the usual order P11, P21, P12, P22. Then we
obtain the system
Ϫ4u 11 ϩ

u 21 ϩ

ϭ

u 12

0.6u 11 Ϫ 2.5u 21

ϩ 0.5u 22
Ϫ

ϭ Ϫ27

ϭ Ϫ0.9 # 296 Ϫ 0.5 # 216 ϭ Ϫ374.4

4u 12 ϩ

u 22 ϭ

0.6u 21 ϩ 0.6u 12 Ϫ

3u 22 ϭ

u 11

0 Ϫ 27
702 ϩ 0

ϭ

702

0.9 # 352 ϩ 0.9 # 936 ϭ 1159.2

In matrix form,
1

1

0

0.6

Ϫ2.5

0

0.5

1

0

Ϫ4

0

0.6

E

Ϫ4
(8)

0.6

1

Ϫ27

u 11
u 21

U E

Ϫ3

u 12

Ϫ374.4

UϭE

u 22

U.

702
1159.2

Gauss elimination yields the (rounded) values
u 11 ϭ Ϫ55.6,

u 21 ϭ 49.2,

u 12 ϭ Ϫ298.5,

u 22 ϭ Ϫ436.3.

Clearly, from a grid with so few mesh points we cannot expect great accuracy. The exact solution of the PDE
(not of the difference equation) having the given boundary values is u ϭ x 3 Ϫ 3xy 2 and yields the values
u 11 ϭ Ϫ54,

u 21 ϭ 54,

u 12 ϭ Ϫ297,

u 22 ϭ Ϫ432.

In practice one would use a much finer grid and solve the resulting large system by an indirect method.

PROBLEM SET 21.5
1–7

y

MIXED BOUNDARY VALUE PROBLEMS

1. Check the values for the Poisson equation at the end
of Example 1 by solving (3) by Gauss elimination.

3
2

2. Solve the mixed boundary value problem for the
Poisson equation ٌ2u ϭ 2 (x 2 ϩ y 2) in the region and
for the boundary conditions shown in Fig. 462, using
the indicated grid.

2

u = 9x

u=0
1
0
0

P12

P22

P11

P21

1
u=0

2

2

ux = 6y

3

x

Fig. 462. Problems 2 and 6

᭿

c21-b.qxd

11/3/10

2:53 PM

936

Page 936

CHAP. 21 Numerics for ODEs and PDEs

3. CAS EXPERIMENT. Mixed Problem. Do Example
1 in the text with finer and finer grids of your choice
and study the accuracy of the approximate values by
comparing with the exact solution u ϭ 2xy 3. Verify the
latter.
4. Solve the mixed boundary value problem for the
Laplace equation ٌ2u ϭ 0 in the rectangle in Fig. 458a
(using the grid in Fig. 458b) and the boundary
conditions u x ϭ 0 on the left edge, u x ϭ 3 on the right
edge, u ϭ x 2 on the lower edge, and u ϭ x 2 Ϫ 1 on
the upper edge.
5. Do Example 1 in the text for the Laplace equation
(instead of the Poisson equation) with grid and
boundary data as before.
6. Solve ٌ2u ϭ Ϫp2y sin 13 px for the grid in Fig. 462
and u y(1, 3) ϭ u y(2, 3) ϭ 12 1243, u ϭ 0 on the other
three sides of the square.
7. Solve Prob. 4 when u n ϭ 110 on the upper edge and
u ϭ 110 on the other edges.
8–16
8.
9.
10.
11.
12.
13.

y

21.6

2

u = x – 1.5 x

2
u=0

P12

P22

P11

P21

u = 9 – 3y

1
0
0

1
u = 3x

2

x

3

Fig. 463. Problem 13
14. If, in Prob. 13, the axes are grounded (u ϭ 0), what
constant potential must the other portion of the
boundary have in order to produce 220 V at P11?
15. What potential do we have in Prob. 13 if u ϭ 100 V
on the axes and u ϭ 0 on the other portion of the
boundary?
16. Solve the Poisson equation ٌ2u ϭ 2 in the region and
for the boundary values shown in Fig. 464, using the
grid also shown in the figure.

IRREGULAR BOUNDARY

Verify the stencil shown after (5).
Derive (5) in the general case.
Derive the general formula (6) in detail.
Derive the linear system in Example 2 of the text.
Verify the solution in Example 2.
Solve the Laplace equation in the region and for the
boundary values shown in Fig. 463, using the
indicated grid. (The sloping portion of the boundary
is y ϭ 4.5 Ϫ x.)

u=0

3

y
3
2

u=0
P12

u = y – 3y

1.5
2

P11

u = y – 1.5y

P21

0
0

3

x

u=0

Fig. 464. Problem 16

Methods for Parabolic PDEs
The last two sections concerned elliptic PDEs, and we now turn to parabolic PDEs. Recall
that the definitions of elliptic, parabolic, and hyperbolic PDEs were given in Sec. 21.4.
There it was also mentioned that the general behavior of solutions differs from type to
type, and so do the problems of practical interest. This reflects on numerics as follows.
For all three types, one replaces the PDE by a corresponding difference equation, but
for parabolic and hyperbolic PDEs this does not automatically guarantee the convergence
of the approximate solution to the exact solution as the mesh h : 0; in fact, it does not
even guarantee convergence at all. For these two types of PDEs one needs additional
conditions (inequalities) to assure convergence and stability, the latter meaning that small
perturbations in the initial data (or small errors at any time) cause only small changes at
later times.
In this section we explain the numeric solution of the prototype of parabolic PDEs, the
one-dimensional heat equation
u t ϭ c2u xx

(c constant).

c21-b.qxd

11/3/10

2:53 PM

Page 937

SEC. 21.6 Methods for Parabolic PDEs

937

This PDE is usually considered for x in some fixed interval, say, 0 Ϲ x Ϲ L, and time
t м 0, and one prescribes the initial temperature u(x, 0) ϭ f (x) ( f given) and boundary
conditions at x ϭ 0 and x ϭ L for all t м 0, for instance, u(0, t) ϭ 0, u(L, t) ϭ 0. We may
assume c ϭ 1 and L ϭ 1; this can always be accomplished by a linear transformation of
x and t (Prob. 1). Then the heat equation and those conditions are
(1)

u t ϭ u xx

0 Ϲ x Ϲ 1, t м 0

(2)

u(x, 0) ϭ f (x)

(Initial condition)

(3)

u(0, t) ϭ u(1, t) ϭ 0

(Boundary conditions).

A simple finite difference approximation of (1) is [see (6a) in Sec. 21.4; j is the number
of the time step]
(4)

1
1
(u i, jϩ1 Ϫ u ij) ϭ 2 (u iϩ1, j Ϫ 2u ij ϩ u i؊1, j).
k
h

Figure 465 shows a corresponding grid and mesh points. The mesh size is h in the x-direction
and k in the t-direction. Formula (4) involves the four points shown in Fig. 466. On the left
in (4) we have used a forward difference quotient since we have no information for negative
t at the start. From (4) we calculate u i, jϩ1, which corresponds to time row j ϩ 1, in terms
of the three other u that correspond to time row j. Solving (4) for u i, jϩ1, we have
(5)

u i, jϩ1 ϭ (1 Ϫ 2r)u ij ϩ r(u iϩ1, j ϩ u i؊1, j),

rϭ

k
h2

.

Computations by this explicit method based on (5) are simple. However, it can be shown
that crucial to the convergence of this method is the condition
rϭ

(6)

k
1
Ϲ .
2
h2

t

( j = 3)
( j = 2)

u=0

u=0
( j = 1)
0

k h
0

1

x

u = f(x)

Fig. 465. Grid and mesh points corresponding to (4), (5)
(i, j + 1)
k
(i – 1, j)

h

(i, j)

h

(i + 1, j)

Fig. 466. The four points in (4) and (5)

c21-b.qxd

11/3/10

938

2:53 PM

Page 938

CHAP. 21 Numerics for ODEs and PDEs

That is, u ij should have a positive coefficient in (5) or (for r ϭ 12 ) be absent from (5).
Intuitively, (6) means that we should not move too fast in the t-direction. An example is
given below.

Crank–Nicolson Method
Condition (6) is a handicap in practice. Indeed, to attain sufficient accuracy, we have
to choose h small, which makes k very small by (6). For example, if h ϭ 0.1, then
k Ϲ 0.005. Accordingly, we should look for a more satisfactory discretization of the
heat equation.
A method that imposes no restriction on r ϭ k>h2 is the Crank–Nicolson (CN)
method,5 which uses values of u at the six points in Fig. 467. The idea of the method
is the replacement of the difference quotient on the right side of (4) by 12 times the
sum of two such difference quotients at two time rows (see Fig. 467). Instead of (4)
we then have
1
1
(u i, jϩ1 Ϫ u ij) ϭ 2 (u iϩ1, j
k
2h

Ϫ 2u ij

ϩ u i؊1, j)

(7)
ϩ

1
2h2

(u iϩ1, jϩ1 Ϫ 2u i, jϩ1 ϩ u iϪ1, jϩ1).

Multiplying by 2k and writing r ϭ k>h2 as before, we collect the terms corresponding to
time row j ϩ 1 on the left and the terms corresponding to time row j on the right:
(8)

(2 ϩ 2r)u i, jϩ1 Ϫ r(u iϩ1, jϩ1 ϩ u i؊1, jϩ1 ϭ (2 Ϫ 2r)u ij ϩ r(u iϩ1, j ϩ u i؊1, j).

How do we use (8)? In general, the three values on the left are unknown, whereas the
three values on the right are known. If we divide the x-interval 0 Ϲ x Ϲ 1 in (1) into n
equal intervals, we have n Ϫ 1 internal mesh points per time row (see Fig. 465, where
n ϭ 4). Then for j ϭ 0 and i ϭ 1, Á , n Ϫ 1, formula (8) gives a linear system of n Ϫ 1
equations for the n Ϫ 1 unknown values u 11, u 21, Á , u n؊1,1 in the first time row in terms
of the initial values u 00, u 10, Á , u n0 and the boundary values u 01(ϭ 0), u n1 (ϭ 0).
Similarly for j ϭ 1, j ϭ 2, and so on; that is, for each time row we have to solve such a
linear system of n Ϫ 1 equations resulting from (8).
Although r ϭ k>h2 is no longer restricted, smaller r will still give better results. In
practice, one chooses a k by which one can save a considerable amount of work, without

5
JOHN CRANK (1916–2006), English mathematician and physicist at Courtaulds Fundamental Research
Laboratory, professor at Brunel University, England. Student of Sir WILLIAM LAWRENCE BRAGG
(1890–1971), Australian British physicist, who with his father, Sir WILLIAM HENRY BRAGG (1862–1942)
won the Nobel Prize in physics in 1915 for their fundamental work in X-ray crystallography. (This is the only
case where a father and a son shared the Nobel Prize for the same research. Furthermore, W. L. Bragg is the
youngest Nobel laureate ever.) PHYLLIS NICOLSON (1917–1968), English mathematician, professor at the
University of Leeds, England.

c21-b.qxd

11/3/10

2:54 PM

Page 939

SEC. 21.6 Methods for Parabolic PDEs

939

making r too large. For instance, often a good choice is r ϭ 1 (which would be impossible
in the previous method). Then (8) becomes simply
(9)

4u i, jϩ1 Ϫ u iϩ1, jϩ1 Ϫ u i؊1, jϩ1 ϭ u iϩ1, j ϩ u i؊1, j.

Time row j + 1
k
Time row j

h

h

Fig. 467. The six points in the Crank–Nicolson formulas (7) and (8)

0.20

j=5

0.16

j=4
j=3

0.12
0.08
0.04
t=0
x=0
i=0

P12

P22

P11

P21

P10

P20

0.2
i=1

0.4
i=2

j=2
j=1
P30
0.6
i=3

P40
0.8
i=4

j=0
1.0
i=5

Fig. 468. Grid in Example 1

EXAMPLE 1

Temperature in a Metal Bar. Crank–Nicolson Method, Explicit Method
Consider a laterally insulated metal bar of length 1 and such that c2 ϭ 1 in the heat equation. Suppose that the
ends of the bar are kept at temperature u ϭ 0°C and the temperature in the bar at some instant—call it t ϭ 0—
is f (x) ϭ sin px. Applying the Crank–Nicolson method with h ϭ 0.2 and r ϭ 1, find the temperature u(x, t) in
the bar for 0 Ϲ t Ϲ 0.2. Compare the results with the exact solution. Also apply (5) with an r satisfying (6),
say, r ϭ 0.25, and with values not satisfying (6), say, r ϭ 1 and r ϭ 2.5.

Solution by Crank–Nicolson. Since r ϭ 1, formula (8) takes the form (9). Since h ϭ 0.2 and
r ϭ k>h2 ϭ 1, we have k ϭ h2 ϭ 0.04. Hence we have to do 5 steps. Figure 468 shows the grid. We shall need
the initial values
u 10 ϭ sin 0.2p ϭ 0.587785,

u 20 ϭ sin 0.4p ϭ 0.951057.

Also, u 30 ϭ u 20 and u 40 ϭ u 10. (Recall that u 10 means u at P10 in Fig. 468, etc.) In each time row in Fig.
468 there are 4 internal mesh points. Hence in each time step we would have to solve 4 equations in 4
unknowns. But since the initial temperature distribution is symmetric with respect to x ϭ 0.5, and u ϭ 0 at
both ends for all t, we have u 31 ϭ u 21, u 41 ϭ u 11 in the first time row and similarly for the other rows. This
reduces each system to 2 equations in 2 unknowns. By (9), since u 31 ϭ u 21 and u 01 ϭ 0, for j ϭ 0 these
equations are
(i ϭ 1)
(i ϭ 2)

4u 11 Ϫ u 21

ϭ u 00 ϩ u 20 ϭ 0.951057

Ϫu 11 ϩ 4u 21 Ϫ u 21 ϭ u 10 ϩ u 20 ϭ 1.538842.

The solution is u 11 ϭ 0.399274, u 21 ϭ 0.646039. Similarly, for time row j ϭ 1 we have the system
(i ϭ 1)

4u 12 Ϫ u 22 ϭ u 01 ϩ u 21 ϭ 0.646039

(i ϭ 2)

Ϫu 12 ϩ 3u 22 ϭ u 11 ϩ u 21 ϭ 1.045313.

c21-b.qxd

11/3/10

940

2:54 PM

Page 940

CHAP. 21 Numerics for ODEs and PDEs
The solution is u 12 ϭ 0.271221, u 22 ϭ 0.438844, and so on. This gives the temperature distribution
(Fig. 469):

t

xϭ0

x ϭ 0.2

x ϭ 0.4

x ϭ 0.6

x ϭ 0.8

xϭ1

0.00
0.04
0.08
0.12
0.16
0.20

0
0
0
0
0
0

0.588
0.399
0.271
0.184
0.125
0.085

0.951
0.646
0.439
0.298
0.202
0.138

0.951
0.646
0.439
0.298
0.202
0.138

0.588
0.399
0.271
0.184
0.125
0.085

0
0
0
0
0
0

u(x, t)
1

t=0

t = 0.04
0.5

0
0

t = 0.08

0.5

1 x

Fig. 469. Temperature distribution in the bar in Example 1

Comparison with the exact solution. The present problem can be solved exactly by separating
variables (Sec. 12.5); the result is
(10)

u(x, t) ϭ sin px e؊p t.
2

Solution by the explicit method (5) with r ‫ ؍‬0.25. For h ϭ 0.2 and r ϭ k>h2 ϭ 0.25 we have
k ϭ rh2 ϭ 0.25 ؒ 0.04 ϭ 0.01. Hence we have to perform 4 times as many steps as with the Crank–Nicolson
method! Formula (5) with r ϭ 0.25 is
(11)

u i, jϩ1 ϭ 0.25(u i؊1, j ϩ 2u ij ϩ u iϩ1, j).

We can again make use of the symmetry. For j ϭ 0 we need u 00 ϭ 0, u 10 ϭ 0.587785 (see p. 939),
u 20 ϭ u 30 ϭ 0.951057 and compute
u 11 ϭ 0.25(u 00 ϩ 2u 10 ϩ u 20) ϭ 0.531657
u 21 ϭ 0.25(u 10 ϩ 2u 20 ϩ u 30) ϭ 0.25(u 10 ϩ 3u 20) ϭ 0.860239.
Of course we can omit the boundary terms u 01 ϭ 0, u 02 ϭ 0, Á from the formulas. For j ϭ 1 we compute
u 12 ϭ 0.25(2u 11 ϩ u 21) ϭ 0.480888
u 22 ϭ 0.25(u 11 ϩ 3u 21) ϭ 0.778094
and so on. We have to perform 20 steps instead of the 5 CN steps, but the numeric values show that the accuracy
is only about the same as that of the Crank–Nicolson values CN. The exact 3D-values follow from (10).

c21-b.qxd

11/3/10

2:54 PM

Page 941

SEC. 21.6 Methods for Parabolic PDEs

941
x ϭ 0.2

t
0.04
0.08
0.12
0.16
0.20

x ϭ 0.4

CN

By (11)

Exact

CN

By (11)

Exact

0.399
0.271
0.184
0.125
0.085

0.393
0.263
0.176
0.118
0.079

0.396
0.267
0.180
0.121
0.082

0.646
0.439
0.298
0.202
0.138

0.637
0.426
0.285
0.191
0.128

0.641
0.432
0.291
0.196
0.132

Failure of (5) with r violating (6). Formula (5) with h ϭ 0.2 and r ϭ 1—which violates (6)—is
u i, jϩ1 ϭ u iϪ1, j Ϫ u ij ϩ u iϩ1, j
and gives very poor values; some of these are
t

x ϭ 0.2

Exact

x ϭ 0.4

Exact

0.04
0.12
0.20

0.363
0.139
0.053

0.396
0.180
0.082

0.588
0.225
0.086

0.641
0.291
0.132

Formula (5) with an even larger r ϭ 2.5 (and h ϭ 0.2 as before) gives completely nonsensical results; some of
these are
t

x ϭ 0.2

Exact

x ϭ 0.4

Exact

0.1
0.3

0.0265
0.0001

0.2191
0.0304

0.0429
0.0001

0.3545
0.0492.

᭿

PROBLEM SET 21.6
1. Nondimensional form. Show that the heat equation
ෂ
uෂt ϭ c 2ෂ
u xෂxෂ, 0 Ϲ ෂ
x Ϲ L, can be transformed to the
“nondimensional” standard form ut ϭ uxx, 0 Ϲ x Ϲ 1,
by setting x ϭ ෂ
x/L, t ϭ c 2ෂt /L2, u ϭ ෂ
u/u0, where u 0 is
any constant temperature.
2. Difference equation. Derive the difference approximation (4) of the heat equation.
3. Explicit method. Derive (5) by solving (4) for u i, jϩ1.
4. CAS EXPERIMENT. Comparison of Methods.
(a) Write programs for the explicit and the Crank—
Nicolson methods.
(b) Apply the programs to the heat problem of a
laterally insulated bar of length 1 with u(x, 0) ϭ sin px
and u(0, t) ϭ u(1, t) ϭ 0 for all t, using h ϭ 0.2,
k ϭ 0.01 for the explicit method (20 steps), h ϭ 0.2
and (9) for the Crank–Nicolson method (5 steps).
Obtain exact 6D-values from a suitable series and
compare.
(c) Graph temperature curves in (b) in two figures
similar to Fig. 299 in Sec. 12.7.

(d) Experiment with smaller h (0.1, 0.05, etc.) for both
methods to find out to what extent accuracy increases
under systematic changes of h and k.

EXPLICIT METHOD
5. Using (5) with h ϭ 1 and k ϭ 0.5, solve the heat
problem (1)–(3) to find the temperature at t ϭ 2 in a
laterally insulated bar of length 10 ft and initial
temperature f (x) ϭ x(1 Ϫ 0.1x).
6. Solve the heat problem (1)–(3) by the explicit method
with h ϭ 0.2 and k ϭ 0.01, 8 time steps, when f (x) ϭ x
if 0 Ϲ x Ͻ 12 , f (x) ϭ 1 Ϫ x if 12 Ϲ x Ϲ 1. Compare
with the 3S-values 0.108, 0.175 for t ϭ 0.08,
x ϭ 0.2, 0.4 obtained from the series (2 terms) in
Sec. 12.5.
7. The accuracy of the explicit method depends on
r (Ϲ 12). Illustrate this for Prob. 6, choosing r ϭ 12 (and
h ϭ 0.2 as before). Do 4 steps. Compare the values for
t ϭ 0.04 and 0.08 with the 3S-values in Prob. 6, which
are 0.156, 0.254 (t ϭ 0.04), 0.105, 0.170 (t ϭ 0.08).

c21-b.qxd

11/3/10

2:54 PM

942

Page 942

CHAP. 21 Numerics for ODEs and PDEs

8. In a laterally insulated bar of length 1 let the initial
temperature be f (x) ϭ x if 0 Ϲ x Ͻ 0.5, f (x) ϭ 1 Ϫ x
if 0.5 Ϲ x Ϲ 1. Let (1) and (3) hold. Apply the explicit
method with h ϭ 0.2, k ϭ 0.01, 5 steps. Can you expect
the solution to satisfy u(x, t) ϭ u(1 Ϫ x, t) for all t?
9. Solve Prob. 8 with f (x) ϭ x if 0 Ϲ x Ϲ 0.2,
f (x) ϭ 0.25(1 Ϫ x) if 0.2 Ͻ x Ϲ 1, the other data
being as before.
10. Insulated end. If the left end of a laterally insulated
bar extending from x ϭ 0 to x ϭ 1 is insulated, the
boundary condition at x ϭ 0 is u n(0, t) ϭ u x(0, t) ϭ 0.
Show that, in the application of the explicit method
given by (5), we can compute u 0jϩ1 by the formula
u 0jϩ1 ϭ (1 Ϫ 2r)u 0j ϩ 2ru 1j.
Apply this with h ϭ 0.2 and r ϭ 0.25 to determine the
temperature u(x, t) in a laterally insulated bar extending
from x ϭ 0 to 1 if u(x, 0) ϭ 0, the left end is insulated
and the right end is kept at temperature g(t) ϭ sin 50
3 pt.
Hint. Use 0 ϭ 0u 0j> 0x ϭ (u 1j Ϫ u ؊1j)>2h.

21.7

CRANK–NICOLSON METHOD
11. Solve Prob. 9 by (9) with h ϭ 0.2, 2 steps. Compare
with exact values obtained from the series in Sec. 12.5
(2 terms) with suitable coefficients.
12. Solve the heat problem (1)–(3) by Crank–Nicolson
for 0 Ϲ t Ϲ 0.20 with h ϭ 0.2 and k ϭ 0.04 when
f (x) ϭ x if 0 Ϲ x Ͻ 12, f (x) ϭ 1 Ϫ x if 12 Ϲ x Ϲ 1.
Compare with the exact values for t ϭ 0.20 obtained
from the series (2 terms) in Sec. 12.5.

13–15
Solve (1)–(3) by Crank–Nicolson with r ϭ 1 (5 steps),
where:
13. f (x) ϭ 5x if 0 Ϲ x Ͻ 0.25, f (x) ϭ 1.25(1 Ϫ x) if
0.25 Ϲ x Ϲ 1, h ϭ 0.2
14. f (x) ϭ x(1 Ϫ x), h ϭ 0.1. (Compare with Prob. 15.)
15. f (x) ϭ x(1 Ϫ x), h ϭ 0.2

Method for Hyperbolic PDEs
In this section we consider the numeric solution of problems involving hyperbolic PDEs.
We explain a standard method in terms of a typical setting for the prototype of a hyperbolic
PDE, the wave equation:
(1)

u tt ϭ u xx

0 Ϲ x Ϲ 1, t м 0

(2)

u(x, 0) ϭ f (x)

(3)

u t(x, 0) ϭ g(x)

(Given initial velocity)

(4)

u(0, t) ϭ u(1, t) ϭ 0

(Boundary conditions).

(Given initial displacement)

Note that an equation u tt ϭ c2u xx and another x-interval can be reduced to the form (1)
by a linear transformation of x and t. This is similar to Sec. 21.6, Prob. 1.
For instance, (1)–(4) is the model of a vibrating elastic string with fixed ends at x ϭ 0
and x ϭ 1 (see Sec. 12.2). Although an analytic solution of the problem is given in (13),
Sec. 12.4, we use the problem for explaining basic ideas of the numeric approach that are
also relevant for more complicated hyperbolic PDEs.
Replacing the derivatives by difference quotients as before, we obtain from (1) [see (6)
in Sec. 21.4 with y ϭ t]
(5)

1
k2

(u i, jϩ1 Ϫ 2u ij ϩ u i, jϪ1) ϭ

1
h2

(u iϩ1, j Ϫ 2u ij ϩ u iϪ1, j)

where h is the mesh size in x, and k is the mesh size in t. This difference equation relates
5 points as shown in Fig. 470a. It suggests a rectangular grid similar to the grids for

c21-b.qxd

11/3/10

2:54 PM

Page 943

SEC. 21.7 Method for Hyperbolic PDEs

943

parabolic equations in the preceding section. We choose r* ϭ k 2>h2 ϭ 1. Then u ij drops
out and we have
u i, jϩ1 ϭ u i؊1, j ϩ u iϩ1, j Ϫ u 1, j؊1

(6)

(Fig. 470b).

It can be shown that for 0 Ͻ r* Ϲ 1 the present explicit method is stable, so that from
(6) we may expect reasonable results for initial data that have no discontinuities. (For a
hyperbolic PDE the latter would propagate into the solution domain—a phenomenon that
would be difficult to deal with on our present grid. For unconditionally stable implicit
methods see [E1] in App. 1.)
Time row j + 1
k
h

Time row j

h
k

Time row j – 1
(a) Formula (5)

(b) Formula (6)

Fig. 470. Mesh points used in (5) and (6)

Equation (6) still involves 3 time steps j Ϫ 1, j, j ϩ 1, whereas the formulas in the
parabolic case involved only 2 time steps. Furthermore, we now have 2 initial conditions.
So we ask how we get started and how we can use the initial condition (3). This can be
done as follows.
From u t(x, 0) ϭ g(x) we derive the difference formula
(7)

1
(u i1 Ϫ u i,؊1) ϭ gi,
2k

hence

u i,؊1 ϭ u i1 Ϫ 2kgi

where gi ϭ g(ih). For t ϭ 0, that is, j ϭ 0, equation (6) is
u i1 ϭ u i؊1,0 ϩ u iϩ1,0 Ϫ u i,؊1.
Into this we substitute u i,Ϫ1 as given in (7). We obtain u i1 ϭ u i؊1,0 ϩ u iϩ1,0 Ϫ u i1 ϩ 2kgi
and by simplification
(8)

u i1 ϭ 12 (u i؊1,0 ϩ u iϩ1,0) ϩ kgi,

This expresses u i1 in terms of the initial data. It is for the beginning only. Then use (6).
EXAMPLE 1

Vibrating String, Wave Equation
Apply the present method with h ϭ k ϭ 0.2 to the problem (1)–(4), where
f (x) ϭ sin px,

g(x) ϭ 0.

Solution. The grid is the same as in Fig. 468, Sec. 21.6, except for the values of t, which now are 0.2, 0.4, Á
(instead of 0.04, 0.08, Á ). The initial values u 00, u 10, Á are the same as in Example 1, Sec. 21.6. From (8)
and g(x) ϭ 0 we have
u i1 ϭ 12 (u i؊1,0 ϩ u iϩ1,0).

c21-b.qxd

11/3/10

2:54 PM

Page 944

944

CHAP. 21 Numerics for ODEs and PDEs
From this we compute, using u 10 ϭ u 40 ϭ sin 0.2p ϭ 0.587785, u 20 ϭ u 30 ϭ 0.951057,
(i ϭ 1) u 11 ϭ 12 (u 00 ϩ u 20) ϭ 12 ؒ 0.951057 ϭ 0.475528
(i ϭ 2) u 21 ϭ 12 (u 10 ϩ u 30) ϭ 12 ؒ 1.538842 ϭ 0.769421
and u 31 ϭ u 21, u 41 ϭ u 11 by symmetry as in Sec. 21.6, Example 1. From (6) with j ϭ 1 we now compute,
using u 01 ϭ u 02 ϭ Á ϭ 0,
(i ϭ 1)

u 12 ϭ u 01 ϩ u 21 Ϫ u 10 ϭ 0.769421 Ϫ 0.587785

ϭ 0.181636

(i ϭ 2)

u 22 ϭ u 11 ϩ u 31 Ϫ u 20 ϭ 0.475528 ϩ 0.769421 Ϫ 0.951057 ϭ 0.293892,

and u 32 ϭ u 22, u 42 ϭ u 12 by symmetry; and so on. We thus obtain the following values of the displacement
u(x, t) of the string over the first half-cycle:

t

xϭ0

x ϭ 0.2

x ϭ 0.4

x ϭ 0.6

x ϭ 0.8

xϭ1

0.0
0.2
0.4
0.6
0.8
1.0

0
0
0
0
0
0

0.588
0.476
0.182
Ϫ0.182
Ϫ0.476
Ϫ0.588

0.951
0.769
0.294
Ϫ0.294
Ϫ0.769
Ϫ0.951

0.951
0.769
0.294
Ϫ0.294
Ϫ0.769
Ϫ0.951

0.588
0.476
0.182
Ϫ0.182
Ϫ0.476
Ϫ0.588

0
0
0
0
0
0

These values are exact to 3D (3 decimals), the exact solution of the problem being (see Sec. 12.3)
u(x, t) ϭ sin px cos pt.
The reason for the exactness follows from d’Alembert’s solution (4), Sec. 12.4. (See Prob. 4, below.)

᭿

This is the end of Chap. 21 on numerics for ODEs and PDEs, a field that continues to
develop rapidly in both applications and theoretical research. Much of the activity in the
field is due to the computer serving as an invaluable tool for solving large-scale and
complicated practical problems as well as for testing and experimenting with innovative
ideas. These ideas could be small or major improvements on existing numeric algorithms
or testing new algorithms as well as other ideas.

PROBLEM SET 21.7
VIBRATING STRING

4. Another starting formula. Show that (12) in Sec. 12.4
gives the starting formula

1–3
Using the present method, solve (1)–(4) with
h ϭ k ϭ 0.2 for the given initial deflection f (x) and initial
velocity 0 on the given t-interval.
1. f (x) ϭ x if 0 ϭ x Ͻ 15 ,
0ϹtϹ1
2. f (x) ϭ x 2 Ϫ x 3,

f (x) ϭ 14 (1 Ϫ x) if 15 Ϲ x Ϲ 1,

0ϹtϹ2

3. f (x) ϭ 0.2(x Ϫ x ), 0 Ϲ t Ϲ 2
2

u i,1 ϭ

1
1
(u iϩ1,0 ϩ u i؊1,0) ϩ
2
2

Ύ

xiϩk

g(s) ds

xi؊k

(where one can evaluate the integral numerically if
necessary). In what case is this identical with (8)?
5. Nonzero initial displacement and speed. Illustrate the
starting procedure when both f and g are not identically

c21-b.qxd

11/3/10

2:54 PM

Page 945

Chapter 21 Review Questions and Problems
zero, say, f (x) ϭ 1 Ϫ cos 2px, g(x) ϭ x(1 Ϫ x),
h ϭ k ϭ 0.1, 2 time steps.
6. Solve (1)–(3) (h ϭ k ϭ 0.2, 5 time steps) subject to
f (x) ϭ x 2, g(x) ϭ 2x, u x(0, t) ϭ 2t, u(1, t) ϭ (1 ϩ t)2.
7. Zero initial displacement. If the string governed by the
wave equation (1) starts from its equilibrium position with
initial velocity g(x) ϭ sin px, what is its displacement
at time t ϭ 0.4 and x ϭ 0.2, 0.4, 0.6, 0.8? (Use the
present method with h ϭ 0.2, k ϭ 0.2. Use (8). Compare
with the exact values obtained from (12) in Sec. 12.4.)

945
8. Compute approximate values in Prob. 7, using a finer
grid (h ϭ 0.1, k ϭ 0.1), and notice the increase in
accuracy.
9. Compute u in Prob. 5 for t ϭ 0.1 and x ϭ 0.1,
0.2, Á , 0.9, using the formula in Prob. 8, and compare
the values.
10. Show that from d’Alembert’s solution (13) in Sec.12.4
with c ϭ 1 it follows that (6) in the present section
gives the exact value u i, jϩ1 ϭ u(ih, ( j ϩ 1)h).

CHAPTER 21 REVIEW QUESTIONS AND PROBLEMS
1. Explain the Euler and improved Euler methods
in geometrical terms. Why did we consider these
methods?
2. How did we obtain numeric methods from the Taylor
series?
3. What are the local and the global orders of a method?
Give examples.

17. Solve y r ϭ y, y(0) ϭ 1 by Euler’s method, 10 steps,
h ϭ 0.1.
18. Do Prob. 17 with h ϭ 0.01, 10 steps. Compute the errors.
Compare the error for x ϭ 0.1 with that in Prob. 17.
19. Solve y r ϭ 1 ϩ y 2, y(0) ϭ 0 by the improved Euler
method, h ϭ 0.1, 10 steps.

4. Why did we compute auxiliary values in each Runge–
Kutta step? How many?

20. Solve y r ϩ y ϭ (x ϩ 1)2, y(0) ϭ 3 by the improved
Euler method, 10 steps with h ϭ 0.1. Determine the
errors.

5. What is adaptive integration? How does its idea extend
to Runge–Kutta?

21. Solve Prob. 19 by RK with h ϭ 0.1, 5 steps. Compute
the error. Compare with Prob. 19.

6. What are one-step methods? Multistep methods? The
underlying ideas? Give examples.

22. Fair comparison. Solve y r ϭ 2x ؊1 1y Ϫ ln x ϩ x ؊1,
y(1) ϭ 0 for 1 Ϲ x Ϲ 1.8 (a) by the Euler method with
h ϭ 0.1, (b) by the improved Euler method with
h ϭ 0.2, and (c) by RK with h ϭ 0.4. Verify that the
exact solution is y ϭ (ln x)2 ϩ ln x. Compute and
compare the errors. Why is the comparison fair?

7. What does it mean that a method is not self-starting?
How do we overcome this problem?
8. What is a predictor–corrector method? Give an
important example.

10. How do we extend Runge–Kutta to systems of ODEs?

23. Apply the Adams–Moulton method to y r ϭ 21 Ϫ y 2,
y(0) ϭ 0, h ϭ 0.2, x ϭ 0, Á , 1, starting with
0.198668, 0.389416, 0.564637.

11. Why did we have to treat the main types of PDEs in
separate sections? Make a list of types of problems and
numeric methods.

24. Apply the A–M method to y r ϭ (x ϩ y Ϫ 4)2, y(0) ϭ 4,
h ϭ 0.2, x ϭ 0, Á , 1, starting with 4.00271, 4.02279,
4.08413.

12. When and how did we use finite differences? Give as
many details as you can remember without looking
into the text.

25. Apply Euler’s method for systems to y s ϭ x 2y,
y(0) ϭ 1, y r (0) ϭ 0, h ϭ 0.1, 5 steps.

9. What is automatic step size control? When is it needed?
How is it done in practice?

13. How did we approximate the Laplace and Poisson
equations?
14. How many initial conditions did we prescribe for the
wave equation? For the heat equation?
15. Can we expect a difference equation to give the exact
solution of the corresponding PDE?
16. In what method for PDEs did we have convergence
problems?

26. Apply Euler’s method for systems to y1r ϭ y2,
y2r ϭ Ϫ4y1, y1(0) ϭ 2, y2(0) ϭ 0, h ϭ 0.2, 10 steps.
Sketch the solution.
27. Apply Runge–Kutta for systems to y s ϩ y ϭ 2ex,
y(0) ϭ 0, y r (0) ϭ 1, h ϭ 0.2, 5 steps. Determine the
errors.
28. Apply Runge–Kutta for systems to y1r ϭ 6y1 ϩ 9y2,
y2r ϭ y1 ϩ 6y2, y1(0) ϭ Ϫ3, y2(0) ϭ Ϫ3, h ϭ 0.05,
3 steps.

c21-b.qxd

11/3/10

2:54 PM

Page 946

946

CHAP. 21 Numerics for ODEs and PDEs

29. Find rough approximate values of the electrostatic
potential at P11, P12, P13 in Fig. 471 that lie in a field
between conducting plates (in Fig. 471 appearing as
sides of a rectangle) kept at potentials 0 and 220 V as
shown. (Use the indicated grid.)
y
u = 220 V

P13

u=0
P11

0

0

1

POTENTIAL

Find the potential in Fig. 472, using the given grid and the
boundary values:

33. u(P10) ϭ u(P30) ϭ 960, u(P20) ϭ Ϫ480, u ϭ 0
elsewhere on the boundary

P12

u=0

32–34

32. u(P01) ϭ u(P03) ϭ u(P41) ϭ u(P43) ϭ 200,
u(P10) ϭ u(P30) ϭ Ϫ400, u(P20) ϭ 1600,
u(P02) ϭ u(P42) ϭ u(P14) ϭ u(P24) ϭ u(P34) ϭ 0

4

2

u(1, t) ϭ 0 by the method in Sec. 21.7 with h ϭ 0.1
and k ϭ 0.1 for t ϭ 0.3.

34. u ϭ 70 on the upper and left sides, u ϭ 0 on the lower
and right sides
2

x

u=0

P13

P23

P33

P12

P22

P32

P11

P21

P31

Fig. 471. Problem 29
30. A laterally insulated homogeneous bar with ends at
x ϭ 0 and x ϭ 1 has initial temperature 0. Its left end
is kept at 0, whereas the temperature at the right end
varies sinusoidally according to

P10

u(t, 1) ϭ g(t) ϭ sin 25
3 pt.
Find the temperature u(x, t) in the bar [solution of (1)
in Sec. 21.6] by the explicit method with h ϭ 0.2 and
r ϭ 0.5 (one period, that is, 0 Ϲ t Ϲ 0.24).
31. Find the solution of the vibrating string problem
u tt ϭ u xx, u(x, 0) ϭ x(1 Ϫ x), u t ϭ 0, u(0, t) ϭ

SUMMARY OF CHAPTER

P20

P30

Fig. 472. Problems 32–34
35. Solve u t ϭ u xx (0 Ϲ x Ϲ 1, t м 0),
u(x, 0) ϭ x 2(1 Ϫ x), u(0, t) ϭ u(1, t) ϭ 0 by Crank–
Nicolson with h ϭ 0.2, k ϭ 0.04, 5 time steps.

21

Numerics for ODEs and PDEs
In this chapter we discussed numerics for ODEs (Secs. 21.1–21.3) and PDEs (Secs.
21.4–21.7). Methods for initial value problems
(1)

y r ϭ f (x, y),

y(x 0) ϭ y0

involving a first-order ODE are obtained by truncating the Taylor series
y(x ϩ h) ϭ y(x) ϩ hy r(x) ϩ

h2
y s(x) ϩ Á
2

c21-b.qxd

11/3/10

2:54 PM

Page 947

Summary of Chapter 21

947

where, by (1), y r ϭ f, y s ϭ f r ϭ 0f>0x ϩ (0f> 0y)y r , etc. Truncating after the term
hy r , we get the Euler method, in which we compute step by step
ynϩ1 ϭ yn ϩ hf (x n, yn)

(2)

(n ϭ 0, 1, Á ).

Taking one more term into account, we obtain the improved Euler method. Both
methods show the basic idea but are too inaccurate in most cases.
Truncating after the term in h4, we get the important classical Runge–Kutta
(RK) method of fourth order. The crucial idea in this method is the replacement
of the cumbersome evaluation of derivatives by the evaluation of f (x, y) at
suitable points (x, y); thus in each step we first compute four auxiliary quantities
(Sec. 21.1)
k 1 ϭ hf (x n, yn)
k 2 ϭ hf (x n ϩ 12 h, yn ϩ 12 k 1)

(3a)

k 3 ϭ hf (x n ϩ 12 h, yn ϩ 12 k 2)
k 4 ϭ hf (x n ϩ h, yn ϩ k 3)

and then the new value
ynϩ1 ϭ yn ϩ 16 (k 1 ϩ 2k 2 ϩ 2k 3 ϩ k 4).

(3b)

Error and step size control are possible by step halving or by RKF
(Runge–Kutta–Fehlberg).
The methods in Sec. 21.1 are one-step methods since they get ynϩ1 from the
result yn of a single step. A multistep method (Sec. 21.2) uses the values of
yn, yn؊1, Á of several steps for computing ynϩ1. Integrating cubic interpolation
polynomials gives the Adams–Bashforth predictor (Sec. 21.2)
(4a)

1
y*nϩ1 ϭ yn ϩ 24
h(55fn Ϫ 59fn؊1 ϩ 37fn؊2 Ϫ 9fn؊3)

where fj ϭ f (x j, yj), and an Adams–Moulton corrector (the actual new value)
(4b)

1
ynϩ1 ϭ yn ϩ 24
h(9f *nϩ1 ϩ 19fn Ϫ 5fn؊1 ϩ fn؊2),

where f *nϩ1 ϭ f (x nϩ1, y*nϩ1). Here, to get started, y1, y2, y3 must be computed by
the Runge–Kutta method or by some other accurate method.
Section 19.3 concerned the extension of Euler and RK methods to systems
y r ϭ f (x, y),

thus

yrj ϭ fj(x, y1, Á , ym),

j ϭ 1, Á , m.

This includes single mth-order ODEs, which are reduced to systems. Second-order
equations can also be solved by RKN (Runge–Kutta–Nyström) methods. These are
particularly advantageous for y s ϭ f (x, y) with f not containing y r .

c21-b.qxd

11/3/10

948

2:54 PM

Page 948

CHAP. 21 Numerics for ODEs and PDEs

Numeric methods for PDEs are obtained by replacing partial derivatives by
difference quotients. This leads to approximating difference equations, for the
Laplace equation to
u iϩ1, j ϩ u i, jϩ1 ϩ u i؊1, j ϩ u i, j؊1 Ϫ 4u ij ϭ 0

(5)

(Sec. 21.4)

for the heat equation to
1
1
(u i, jϩ1 Ϫ u ij) ϭ 2 (u iϩ1, j Ϫ 2u ij ϩ u i؊1, j)
k
h

(6)

(Sec. 21.6)

and for the wave equation to
(7)

1
k

2

(u i, jϩ1 Ϫ 2u i, j ϩ u i, j؊1) ϭ

1
h2

(u iϩ1, j Ϫ 2u ij ϩ u i؊1, j)

(Sec. 21.7);

here h and k are the mesh sizes of a grid in the x- and y-directions, respectively,
where in (6) and (7) the variable y is time t.
These PDEs are elliptic, parabolic, and hyperbolic, respectively. Corresponding
numeric methods differ, for the following reason. For elliptic PDEs we have
boundary value problems, and we discussed for them the Gauss–Seidel method
(also known as Liebmann’s method ) and the ADI method (Secs. 21.4, 21.5). For
parabolic PDEs we are given one initial condition and boundary conditions, and
we discussed an explicit method and the Crank–Nicolson method (Sec. 21.6). For
hyperbolic PDEs, the problems are similar but we are given a second initial
condition (Sec. 21.7).

c22.qxd

11/3/10

4:00 PM

Page 949

PART

F

Optimization,
Graphs
CHAPTER 22
CHAPTER 23

Unconstrained Optimization. Linear Programming
Graphs. Combinatorial Optimization
The material of Part F is particularly useful in modeling large-scale real-world problems.
Just as it is in numerics in Part E, where the greater availability of quality software and
computing power is a deciding factor in the continued growth of the field, so it is also in
the fields of optimization and combinatorial optimization. Problems, such as optimizing
production plans for different industries (microchips, pharmaceuticals, cars, aluminum,
steel, chemicals), optimizing usage of transportation systems (usage of runways in airports,
tracks of subways), efficiency in running of power plants, optimal shipping (delivery
services, shipping of containers, shipping goods from factories to warehouses and from
warehouses to stores), designing optimal financial portfolios, and others are all examples
where the size of the problem usually requires the use of optimization software. More
recently, environmental concerns have put new aspects into the picture, where an important
concern, added to these problems, is the minimization of environmental impact. The main
task becomes to model these problems correctly. The purpose of Part F is to introduce
the main ideas and methods of unconstrained and constrained optimization (Chap. 22),
and graphs and combinatorial optimization (Chap. 23).
Chapter 22 introduces unconstrained optimization by the method of steepest descent and
constrained optimization by the versatile simplex method. The simplex method (Secs.
22.3, 22.4) is very useful for solving many linear optimization problems (also called linear
programming problems).
Graphs let us model problems in transportation logistics, efficient use of communication
networks, best assignment of workers to jobs, and others. We consider shortest path problems
(Secs. 22.2, 22.3), shortest spanning trees (Secs. 23.4, 23.5), flow problems in networks (Secs.
23.6, 23.7), and assignment problems (Sec. 23.8). We discuss algorithms of Moore, Dijkstra
(both for shortest path), Kruskal, Prim (shortest spanning trees), and Ford–Fulkerson (for flow).
949

c22.qxd

11/3/10

4:00 PM

Page 950

CHAPTER

22

Unconstrained Optimization.
Linear Programming
Optimization is a general term used to describe types of problems and solution techniques
that are concerned with the best (“optimal”) allocation of limited resources in projects. The
problems are called optimization problems and the methods optimization methods. Typical
problems are concerned with planning and making decisions, such as selecting an optimal
production plan. A company has to decide how many units of each product from a choice
of (distinct) products it should make. The objective of the company may be to maximize
overall profit when the different products have different individual profits. In addition, the
company faces certain limitations (constraints). It may have a certain number of machines,
it takes a certain amount of time and usage of these machines to make a product, it requires
a certain number of workers to handle the machines, and other possible criteria. To solve
such a problem, you assign the first variable to number of units to be produced of the first
product, the second variable to the second product, up to the number of different (distinct)
products the company makes. When you multiply these, for example, by the price, you
obtain a linear function called the objective function. You also express the constraints in
terms of these variables, thereby obtaining several inequalities, called the constraints.
Because the variables in the objective function also occur in the constraints, the objective
function and the constraints are tied mathematically to each other and you have set up a
linear optimization problem, also called a linear programming problem.
The main focus of this chapter is to set up (Sec. 22.2) and solve (Secs. 22.3, 22.4) such
linear programming problems. A famous and versatile method for doing so is the simplex
method. In the simplex method, the objective function and the constraints are set up in
the form of an augmented matrix as in Sec. 7.3, however, the method of solving such
linear constrained optimization problems is a new approach.
The beauty of the simplex method is that it allows us to scale problems up to thousands
or more constraints, thereby modeling real-world situations. We can start with a small
model and gradually add more and more constraints. The most difficult part is modeling
the problem correctly. The actual task of solving large optimization problems is done by
software implementations for the simplex method or perhaps by other optimization methods.
Besides optimal production plans, problems in optimal shipping, optimal location of
warehouses and stores, easing traffic congestion, efficiency in running power plants are
all examples of applications of optimization. More recent applications are in minimizing
environmental damages due to pollutants, carbon dioxide emissions, and other factors.
Indeed, new fields of green logistics and green manufacturing are evolving and naturally
make use of optimization methods.
Prerequisite: a modest working knowledge of linear systems of equations.
References and Answers to Problems: App. 1 Part F, App. 2.
950

c22.qxd

11/3/10

4:00 PM

SEC. 22.1

22.1

Page 951

Basic Concepts. Unconstrained Optimization

951

Basic Concepts.
Unconstrained Optimization:
Method of Steepest Descent
In an optimization problem the objective is to optimize (maximize or minimize) some
function f. This function f is called the objective function. It is the focal point or goal of
our optimization problem.
For example, an objective function f to be maximized may be the revenue in a production
of TV sets, the rate of return of a financial portfolio, the yield per minute in a chemical
process, the mileage per gallon of a certain type of car, the hourly number of customers
served in a bank, the hardness of steel, or the tensile strength of a rope.
Similarly, we may want to minimize f if f is the cost per unit of producing certain
cameras, the operating cost of some power plant, the daily loss of heat in a heating system,
CO2 emissions from a fleet of trucks for freight transport, the idling time of some lathe,
or the time needed to produce a fender.
In most optimization problems the objective function f depends on several variables
x1, Á , x n.
These are called control variables because we can “control” them, that is, choose their values.
For example, the yield of a chemical process may depend on pressure x1 and temperature
x2. The efficiency of a certain air-conditioning system may depend on temperature x1, air
pressure x2, moisture content x 3, cross-sectional area of outlet x4, and so on.
Optimization theory develops methods for optimal choices of x1, Á , x n, which maximize
(or minimize) the objective function f, that is, methods for finding optimal values of x1, Á , xn.
In many problems the choice of values of x1, Á , x n is not entirely free but is subject
to some constraints, that is, additional restrictions arising from the nature of the problem
and the variables.
For example, if x1 is production cost, then x1 м 0, and there are many other variables
(time, weight, distance traveled by a salesman, etc.) that can take nonnegative values only.
Constraints can also have the form of equations (instead of inequalities).
We first consider unconstrained optimization in the case of a function f (x1, Á , x n).
We also write x ϭ (x1, Á , x n) and f (x), for convenience.
By definition, f has a minimum at a point x ϭ X0 in a region R (where f is defined) if
f (x) м f (X0)
for all x in R. Similarly, f has a maximum at X0 in R if
f (x) Ϲ f (X0)
for all x in R. Minima and maxima together are called extrema.
Furthermore, f is said to have a local minimum at X0 if
f (x) м f (X0)
for all x in a neighborhood of X0, say, for all x satisfying
ƒ x Ϫ X0 ƒ ϭ [(x1 Ϫ X1)2 ϩ Á ϩ (x n Ϫ Xn)2]1>2 Ͻ r,
where X0 ϭ (X1, Á , Xn) and r Ͼ 0 is sufficiently small.

c22.qxd

11/3/10

4:00 PM

952

Page 952

CHAP. 22 Unconstrained Optimization. Linear Programming

Similarly, f has a local maximum at X0 if f (x) Ϲ f (X0) for all x satisfying ƒ x Ϫ X0 ƒ Ͻ r.
If f is differentiable and has an extremum at a point X0 in the interior of a region R
(that is, not on the boundary), then the partial derivatives 0f>0x1, Á , 0f> 0x n must be zero
at X0. These are the components of a vector that is called the gradient of f and denoted
by grad f or ٌf. (For n ϭ 3 this agrees with Sec. 9.7.) Thus
ٌf (X0) ϭ 0.

(1)

A point X0 at which (1) holds is called a stationary point of f.
Condition (1) is necessary for an extremum of f at X0 in the interior of R, but is not
sufficient. Indeed, if n ϭ 1, then for y ϭ f (x), condition (1) is y r ϭ f r(X0) ϭ 0; and, for
instance, y ϭ x 3 satisfies y r ϭ 3x 2 ϭ 0 at x ϭ X0 ϭ 0 where f has no extremum but a
point of inflection. Similarly, for f (x) ϭ x 1x 2 we have ٌf (0) ϭ 0, and f does not have an
extremum but has a saddle point at 0. Hence, after solving (1), one must still find out
whether one has obtained an extremum. In the case n ϭ 1 the conditions y r(X0) ϭ 0,
y s(X0) Ͼ 0 guarantee a local minimum at X0 and the conditions y r(X0) ϭ 0, y s(X0) Ͻ 0 a
local maximum, as is known from calculus. For n Ͼ 1 there exist similar criteria. However,
in practice, even solving (1) will often be difficult. For this reason, one generally prefers
solution by iteration, that is, by a search process that starts at some point and moves
stepwise to points at which f is smaller (if a minimum of f is wanted) or larger (in the
case of a maximum).
The method of steepest descent or gradient method is of this type. We present it here
in its standard form. (For refinements see Ref. [E25] listed in App. 1.)
The idea of this method is to find a minimum of f (x) by repeatedly computing minima
of a function g(t) of a single variable t, as follows. Suppose that f has a minimum at X0
and we start at a point x. Then we look for a minimum of f closest to x along the straight
line in the direction of Ϫٌf (x), which is the direction of steepest descent (ϭ direction
of maximum decrease) of f at x. That is, we determine the value of t and the corresponding point
z(t) ϭ x Ϫ tٌf (x)

(2)
at which the function

g(t) ϭ f (z(t))

(3)

has a minimum. We take this z(t) as our next approximation to X0.
EXAMPLE 1

Method of Steepest Descent
Determine a minimum of
(4)

f (x) ϭ x 21 ϩ 3x 22 ,

starting from x0 ϭ (6, 3) ϭ 6i ϩ 3j and applying the method of steepest descent.

Solution. Clearly, inspection shows that f (x) has a minimum at 0. Knowing the solution gives us a better
feel of how the method works. We obtain ٌf (x) ϭ 2x 1i ϩ 6x 2 j and from this
z(t) ϭ x Ϫ tٌf (x) ϭ (1 Ϫ 2t)x1i ϩ (1 Ϫ 6t)x 2 j
g(t) ϭ f (z (t)) ϭ (1 Ϫ 2t)2x 21 ϩ 3 (1 Ϫ 6t)2x 22 .

c22.qxd

11/3/10

4:00 PM

SEC. 22.1

Page 953

Basic Concepts. Unconstrained Optimization

953

We now calculate the derivative
g r(t) ϭ 2 (1 Ϫ 2t)x 21(Ϫ2) ϩ 6 (1 Ϫ 6t)x 22(Ϫ6),
set g r(t) ϭ 0, and solve for t, finding
tϭ

x 21 ϩ 9x 22
2x 21 ϩ 54x 22

.

Starting from x 0 ϭ 6i ϩ 3j, we compute the values in Table 22.1, which are shown in Fig. 473.
Figure 473 suggests that in the case of slimmer ellipses (“a long narrow valley”), convergence would be
poor. You may confirm this by replacing the coefficient 3 in (4) with a large coefficient. For more sophisticated
descent and other methods, some of them also applicable to vector functions of vector variables, we refer to the
references listed in Part F of App. 1; see also [E25].
᭿
x2

x0
x2
x1

x1

Fig. 473. Method of steepest descent in Example 1

Table 22.1 Method of Steepest Descent, Computations in Example 1
n
0
1
2
3
4
5
6

x
6.000
3.484
1.327
0.771
0.294
0.170
0.065

3.000
Ϫ0.774
0.664
Ϫ0.171
0.147
Ϫ0.038
0.032

t

1 Ϫ 2t

1 Ϫ 6t

0.210
0.310
0.210
0.310
0.210
0.310

0.581
0.381
0.581
0.381
0.581
0.381

Ϫ0.258
Ϫ0.857
Ϫ0.258
Ϫ0.857
Ϫ0.258
Ϫ0.857

PROBLEM SET 22.1
1. Orthogonality. Show that in Example 1, successive
gradients are orthogonal (perpendicular). Why?
2. What happens if you apply the method of steepest
descent to f (x) ϭ x 21 ϩ x 22? First guess, then calculate.

STEEPEST DESCENT
Do steepest descent steps when:
3–9

3. f (x) ϭ 2x 21 ϩ x 22 Ϫ 4x 1 ϩ 4x 2, x0 ϭ 0, 3 steps
4. f (x) ϭ x 21 ϩ 0.5x 22 Ϫ 5.0x 1 Ϫ 3.0x 2 ϩ 24.95,
x 0 ϭ (3, 4), 5 steps

5. f (x) ϭ ax1 ϩ bx 2, a 0, b 0. First guess, then
compute.
6. f (x) ϭ x 21 Ϫ x 22, x0 ϭ (1, 2), 5 steps. First guess,
then compute. Sketch the path. What if x0 ϭ (2, 1)?
7. f (x) ϭ x 21 ϩ cx 22, x0 ϭ (c, 1). Show that 2 steps give
(c, 1) times a factor, Ϫ4c2>(c2 Ϫ 1)2. What can you
conclude from this about the speed of convergence?
8. f (x) ϭ x 21 Ϫ x 2, x0 ϭ (1, 1); 3 steps. Sketch your path.
Predict the outcome of further steps.
9. f (x) ϭ 0.1x 21 ϩ x 22 Ϫ 0.02x 1, x0 ϭ (3, 3), 5 steps

c22.qxd

11/3/10

4:00 PM

954

Page 954

CHAP. 22 Unconstrained Optimization. Linear Programming
(c) Apply your program to f (x) ϭ x 21 ϩ x 42 and to
f (x) ϭ x 41 ϩ x 42, x0 ϭ (2, 1). Graph level curves and
your path of descent. (Try to include graphing directly
in your program.)

10. CAS EXPERIMENT. Steepest Descent. (a) Write a
program for the method.
(b) Apply your program to f (x) ϭ x 21 ϩ 4x 22, experimenting with respect to speed of convergence depending
on the choice of x0.

22.2

Linear Programming
Linear programming or linear optimization consists of methods for solving optimization
problems with constraints, that is, methods for finding a maximum (or a minimum)
x ϭ (x1, Á , x n) of a linear objective function
z ϭ f (x) ϭ a1x1 ϩ a2x 2 ϩ Á ϩ anx n
satisfying the constraints. The latter are linear inequalities, such as 3x1 ϩ 4x 2 Ϲ 36, or
x1 м 0, etc. (examples below). Problems of this kind arise frequently, almost daily, for
instance, in production, inventory management, bond trading, operation of power plants,
routing delivery vehicles, airplane scheduling, and so on. Progress in computer technology
has made it possible to solve programming problems involving hundreds or thousands or
more variables. Let us explain the setting of a linear programming problem and the idea
of a “geometric” solution, so that we shall see what is going on.

EXAMPLE 1

Production Plan
Energy Savers, Inc., produces heaters of types S and L. The wholesale price is $40 per heater for S and $88 for
L. Two time constraints result from the use of two machines M 1 and M 2. On M 1 one needs 2 min for an S heater
and 8 min for an L heater. On M 2 one needs 5 min for an S heater and 2 min for an L heater. Determine production
figures x1 and x 2 for S and L, respectively (number of heaters produced per hour), so that the hourly revenue
z ϭ f (x) ϭ 40x1 ϩ 88x 2
is maximum.

Solution. Production figures x1 and x 2 must be nonnegative. Hence the objective function (to be maximized)
and the four constraints are
(0)

z ϭ 40x 1 ϩ 88x 2

(1)

2x 1 ϩ 8x 2 Ϲ 60 min time on machine M 1

(2)

5x1 ϩ 2x 2 Ϲ 60 min time on machine M 2

(3)
(4)

x1

м 0
x 2 м 0.

Figure 474 shows (0)–(4) as follows. Constancy lines
z ϭ const
are marked (0). These are lines of constant revenue. Their slope is Ϫ40>88 ϭ Ϫ5>11. To increase z we must
move the line upward (parallel to itself), as the arrow shows. Equation (1) with the equality sign is marked (1).
It intersects the coordinate axes at x 1 ϭ 60>2 ϭ 30 (set x 2 ϭ 0) and x 2 ϭ 60>8 ϭ 7.5 (set x 1 ϭ 0). The arrow
marks the side on which the points (x 1, x 2) lie that satisfy the inequality in (1). Similarly for Eqs. (2)–(4). The
blue quadrangle thus obtained is called the feasibility region. It is the set of all feasible solutions, meaning

c22.qxd

11/3/10

4:00 PM

Page 955

SEC. 22.2 Linear Programming

955

solutions that satisfy all four constraints. The figure also lists the revenue at O, A, B, C. The optimal solution
is obtained by moving the line of constant revenue up as much as possible without leaving the feasibility region
completely. Obviously, this optimum is reached when that line passes through B, the intersection (10, 5) of (1)
and (2). We see that the optimal revenue
z max ϭ 40 ؒ 10 ϩ 88 ؒ 5 ϭ $840

᭿

is obtained by producing twice as many S heaters as L heaters.
x2

O:
A:
B:
C:

20

z=0
z = 40 . 12 = 480
z = 40 . 10 + 88 . 5 = 840
z = 88 . 7.5 = 660

(3)
(2)
10
C

(0)
(1)

B

z=

con

st

(4)
O

10

A

(0)

20

z=

x1

30

0

(0)

z

ma

x

=8

40

Fig. 474. Linear programming in Example 1

Note well that the problem in Example 1 or similar optimization problems cannot be
solved by setting certain partial derivatives equal to zero, because crucial to such problems
is the region in which the control variables are allowed to vary.
Furthermore, our “geometric” or graphic method illustrated in Example 1 is confined
to two variables x1, x 2. However, most practical problems involve much more than two
variables, so that we need other methods of solution.

Normal Form of a Linear Programming Problem
To prepare for general solution methods, we show that constraints can be written more
uniformly. Let us explain the idea in terms of (1),
2x 1 ϩ 8x 2 Ϲ 60.
This inequality implies 60 Ϫ 2x1 Ϫ 8x2 м 0 (and conversely), that is, the quantity
x3 ϭ 60 Ϫ 2x1 Ϫ 8x 2
is nonnegative. Hence, our original inequality can now be written as an equation
2x1 ϩ 8x2 ϩ x3 ϭ 60,
where
x3 м 0.

c22.qxd

11/3/10

4:00 PM

956

Page 956

CHAP. 22 Unconstrained Optimization. Linear Programming

x3 is a nonnegative auxiliary variable introduced for converting inequalities to equations.
Such a variable is called a slack variable, because it “takes up the slack” or difference
between the two sides of the inequality.
EXAMPLE 2

Conversion of Inequalities by the Use of Slack Variables
With the help of two slack variables x 3, x 4 we can write the linear programming problem in Example 1 in the
following form. Maximize
f ϭ 40x 1 ϩ 88x 2
subject to the constraints
2x 1 ϩ 8x 2 ϩ x 3
5x 1 ϩ 2x 2
xi м 0

ϭ 60
ϩ x 4 ϭ 60

(i ϭ 1, Á , 4).

We now have n ϭ 4 variables and m ϭ 2 (linearly independent) equations, so that two of the four variables,
for example, x 1, x 2, determine the others. Also note that each of the four sides of the quadrangle in Fig. 474
now has an equation of the form x i ϭ 0:
OA: x 2 ϭ 0,
AB: x 4 ϭ 0,
BC: x 3 ϭ 0,
CO: x 1 ϭ 0,
A vertex of the quadrangle is the intersection of two sides. Hence at a vertex, n Ϫ m ϭ 4 Ϫ 2 ϭ 2 of the
᭿
variables are zero and the others are nonnegative. Thus at A we have x 2 ϭ 0, x 4 ϭ 0, and so on.

Our example suggests that a general linear optimization problem can be brought to the
following normal form. Maximize
(5)

f ϭ c1x 1 ϩ c2x 2 ϩ Á ϩ cnx n

subject to the constraints
a11x 1 ϩ Á ϩ a1nx n ϭ b1
a21x 1 ϩ Á ϩ a2nx n ϭ b2
(6)

Á Á Á Á Á Á Á Á
am1x 1 ϩ Á ϩ amnx n ϭ bm
xi м 0

(i ϭ 1, Á , n)

with all bj nonnegative. (If a bj Ͻ 0, multiply the equation by Ϫ1.) Here x1, Á , x n include
the slack variables (for which the cj’s in f are zero). We assume that the equations in (6)
are linearly independent. Then, if we choose values for n Ϫ m of the variables, the system
uniquely determines the others. Of course, since we must have
x1 м 0, Á , x n м 0,
this choice is not entirely free.

c22.qxd

11/3/10

4:00 PM

Page 957

SEC. 22.2 Linear Programming

957

Our problem also includes the minimization of an objective function f since this
corresponds to maximizing Ϫf and thus needs no separate consideration.
An n-tuple (x1, Á , x n) that satisfies all the constraints in (6) is called a feasible point
or feasible solution. A feasible solution is called an optimal solution if, for it, the objective
function f becomes maximum, compared with the values of f at all feasible solutions.
Finally, by a basic feasible solution we mean a feasible solution for which at least
n Ϫ m of the variables x1, Á , x n are zero. For instance, in Example 2 we have n ϭ 4,
m ϭ 2, and the basic feasible solutions are the four vertices O, A, B, C in Fig. 474. Here
B is an optimal solution (the only one in this example).
The following theorem is fundamental.
THEOREM 1

Optimal Solution

Some optimal solution of a linear programming problem (5), (6) is also a basic
feasible solution of (5), (6).

For a proof, see Ref. [F5], Chap. 3 (listed in App. 1). A problem can have many optimal
solutions and not all of them may be basic feasible solutions; but the theorem guarantees
that we can find an optimal solution by searching through the basic feasible solutions
n
n
only. This is a great simplification; but since there are a
b ϭ a b different ways
nϪm
m
of equating n Ϫ m of the n variables to zero, considering all these possibilities, dropping
those which are not feasible and then searching through the rest would still involve very
much work, even when n and m are relatively small. Hence a systematic search is needed.
We shall explain an important method of this type in the next section.

PROBLEM SET 22.2
REGIONS, CONSTRAINTS
Describe and graph the regions in the first quadrant of
the x 1x 2-plane determined by the given inequalities.
1–6

1. x 1 Ϫ 3x 2 м Ϫ6
x1 ϩ x2 Ϲ
2. 2x 1 Ϫ

x1 ϩ x2 Ϲ 5
Ϫ2x 1 ϩ x 2 Ϲ 16
6.

6

x2 м

5. Ϫx 1 ϩ x 2 м 0

x1 ϩ x2 м

2

3x 1 ϩ 5x 2 м 15
6

2x 1 Ϫ x 2 м Ϫ2

8x 1 ϩ 10x 2 Ϲ 80

Ϫx 1 ϩ 2x 2 Ϲ 10

x 1 Ϫ 2x 2 м Ϫ3
3. Ϫ0.5x 1 ϩ x 2 Ϲ 2
x1 ϩ x2 м 2
Ϫx 1 ϩ 5x 2 м 5
4. Ϫx 1 ϩ
2x 1 ϩ

x2 Ϲ

5

x 2 м 10
x2 м

4

10x 1 ϩ 15x 2 Ϲ 150

7. Location of maximum. Could we find a profit
f (x 1, x 2) ϭ a1x 1 ϩ a2x 2 whose maximum is at an
interior point of the quadrangle in Fig. 474? Give
reason for your answer.
8. Slack variables. Why are slack variables always
nonnegative? How many of them do we need?
9. What is the meaning of the slack variables x 3, x 4 in
Example 2 in terms of the problem in Example 1?
10. Uniqueness. Can we always expect a unique solution
(as in Example 1)?

c22.qxd

11/3/10

4:00 PM

958

Page 958

CHAP. 22 Unconstrained Optimization. Linear Programming

MAXIMIZATION, MINIMIZATION
Maximize or minimize the given objective function f
subject to the given constraints.
11–16

Maximize f ϭ 30x 1 ϩ 10x 2 in the region in Prob. 5.
Minimize f ϭ 45.0x 1 ϩ 22.5x 2 in the region in Prob. 4.
Maximize f ϭ 5x 1 ϩ 25x 2 in the region in Prob. 5.
Minimize f ϭ 5x 1 ϩ 25x 2 in the region in Prob. 3.
Maximize f ϭ 20x 1 ϩ 30x 2 subject to 4x 1 ϩ 3x 2 м
12, x 1 Ϫ x 2 м Ϫ3, x 2 Ϲ 6, 2x 1 Ϫ 3x 2 Ϲ 0.
16. Maximize f ϭ Ϫ10x 1 ϩ 2x 2 subject to x 1 м 0,
x2 м 0, Ϫx1 ϩ x 2 м Ϫ1, x1 ϩ x 2 Ϲ 6, x 2 Ϲ 5.
17. Maximum profit. United Metal, Inc., produces alloys
B1 (special brass) and B2 (yellow tombac). B1 contains
50% copper and 50% zinc. (Ordinary brass contains
about 65% copper and 35% zinc.) B2 contains 75%
copper and 25% zinc. Net profits are $120 per ton of
B1 and $100 per ton of B2. The daily copper supply is
45 tons. The daily zinc supply is 30 tons. Maximize
the net profit of the daily production.
18. Maximum profit. The DC Drug Company produces
two types of liquid pain killer, N (normal) and S
(Super). Each bottle of N requires 2 units of drug A, 1
unit of drug B, and 1 unit of drug C. Each bottle of S
requires 1 unit of A, 1 unit of B, and 3 units of C. The
company is able to produce, each week, only 1400 units
of A, 800 units of B, and 1800 units of C. The profit
per bottle of N and S is $11 and $15, respectively.
Maximize the total profit.
11.
12.
13.
14.
15.

22.3

19. Maximum output. Giant Ladders, Inc., wants to
maximize its daily total output of large step ladders by
producing x 1 of them by a process P1 and x 2 by a
process P2, where P1 requires 2 hours of labor and
4 machine hours per ladder, and P2 requires 3 hours of
labor and 2 machine hours. For this kind of work, 1200
hours of labor and 1600 hours on the machines are, at
most, available per day. Find the optimal x 1 and x 2.
20. Minimum cost. Hardbrick, Inc., has two kilns. Kiln
I can produce 3000 gray bricks, 2000 red bricks, and
300 glazed bricks daily. For Kiln II the corresponding
figures are 2000, 5000, and 1500. Daily operating costs
of Kilns I and II are $400 and $600, respectively. Find
the number of days of operation of each kiln so that
the operation cost in filling an order of 18,000 gray,
34,000 red, and 9000 glazed bricks is minimized.
21. Maximum profit. Universal Electric, Inc., manufactures
and sells two models of lamps, L 1 and L 2, the profit being
$150 and $100, respectively. The process involves two
workers W1 and W2 who are available for this kind
of work 100 and 80 hours per month, respectively.
W1 assembles L 1 in 20 min and L 2 in 30 min. W2 paints
L 1 in 20 min and L 2 in 10 min. Assuming that all lamps
made can be sold without difficulty, determine production
figures that maximize the profit.
22. Nutrition. Foods A and B have 600 and 500 calories,
contain 15 g and 30 g of protein, and cost $1.80 and $2.10
per unit, respectively. Find the minimum cost diet of at
least 3900 calories containing at least 150 g of protein.

Simplex Method
From the last section we recall the following. A linear optimization problem (linear
programming problem) can be written in normal form; that is:
Maximize
(1)

z ϭ f (x) ϭ c1x 1 ϩ Á ϩ cnx n
subject to the constraints
a11x 1 ϩ Á ϩ a1nx n ϭ b1
a21x 1 ϩ Á ϩ a2nx n ϭ b2

(2)

.....................
am1x 1 ϩ Á ϩ amnx n ϭ bm
xi м 0

(i ϭ 1, Á , n).

c22.qxd

11/3/10

4:00 PM

Page 959

SEC. 22.3 Simplex Method

959

For finding an optimal solution of this problem, we need to consider only the basic feasible
solutions (defined in Sec. 22.2), but there are still so many that we have to follow a
systematic search procedure. In 1948 G. B. Dantzig1 published an iterative method, called
the simplex method, for that purpose. In this method, one proceeds stepwise from one
basic feasible solution to another in such a way that the objective function f always
increases its value. Let us explain this method in terms of the example in the last section.
In its original form the problem concerned the maximization of the objective function
z ϭ 40x 1 ϩ 88x 2
2x 1 ϩ 8x 2 Ϲ 60
5x 1 ϩ 2x 2 Ϲ 60

subject to

м 0

x1

x 2 м 0.
Converting the first two inequalities to equations by introducing two slack variables x 3, x 4,
we obtained the normal form of the problem in Example 2. Together with the objective
function (written as an equation z Ϫ 40x 1 Ϫ 88x 2 ϭ 0) this normal form is
z Ϫ 40x 1 Ϫ 88x 2

ϭ 0

2x 1 ϩ 8x 2 ϩ x 3

(3)

5x 1 ϩ 2x 2

ϭ 60
ϩ x 4 ϭ 60

where x 1 м 0, Á , x 4 м 0. This is a linear system of equations. To find an optimal solution
of it, we may consider its augmented matrix (see Sec. 7.3)
z
1
(4)

T0 ϭ

x1
|
|

x2

Ϫ40 Ϫ88

x3

x4

|
|

0

0

|
|
|
|
|

1

0

0

1

b
|
|

0

|
|
|
|
|

60

– – –| – – – – – – – – – – |– – – – – – – – |– – – – –

Y 0
0

|
|
|
|
|

2

8

5

2

Z

60

1
GEORGE BERNARD DANTZIG (1914–2005), American mathematician, who is one of the pioneers of
linear programming and inventor of the simplex method. According to Dantzig himself (see G. B. Dantzig,
Linear programming: The story of how it began, in J. K. Lenestra et al., History of Mathematical Programming:
A Collection of Personal Reminiscences. Amsterdam: Elsevier, 1991, pp. 19–31), he was particularly fascinated
by Wassilly Leontief’s input–output model (Sec. 8.2) and invented his famous method to solve large-scale
planning (logistics) problems. Besides Leontief, Dantzig credits others for their pioneering work in linear
programming, that is, JOHN VON NEUMANN (1903–1957), Hungarian American mathematician, Institute for
Advanced Studies, Princeton University, who made major contributions to game theory, computer science,
functional analysis, set theory, quantum mechanics, ergodic theory, and other areas, the Nobel laureates LEONID
VITALIYEVICH KANTOROVICH (1912–1986), Russian economist, and TJALLING CHARLES
KOOPMANS (1910–1985), Dutch–American economist, who shared the 1975 Nobel Prize in Economics for
their contributions to the theory of optimal allocation of resources. Dantzig was a driving force in establishing
the field of linear programming and became professor of transportation sciences, operations research, and
computer science at Stanford University. For his work see R. W. Cottle (ed.), The Basic George B. Dantzig.
Palo Alto, CA: Stanford University Press, 2003.

c22.qxd

11/4/10

960

1:35 PM

Page 960

CHAP. 22 Unconstrained Optimization. Linear Programming

This matrix is called a simplex tableau or simplex table (the initial simplex table). These
are standard names. The dashed lines and the letters
z,

x 1,

Á, b

are for ease in further manipulation.
Every simplex table contains two kinds of variables x j. By basic variables we mean
those whose columns have only one nonzero entry. Thus x 3, x 4 in (4) are basic variables
and x 1, x 2 are nonbasic variables.
Every simplex table gives a basic feasible solution. It is obtained by setting the nonbasic
variables to zero. Thus (4) gives the basic feasible solution
x 1 ϭ 0,

x 2 ϭ 0,

x 3 ϭ 60>1 ϭ 60,

x 4 ϭ 60>1 ϭ 60,

zϭ0

with x 3 obtained from the second row and x 4 from the third.
The optimal solution (its location and value) is now obtained stepwise by pivoting,
designed to take us to basic feasible solutions with higher and higher values of z until the
maximum of z is reached. Here, the choice of the pivot equation and pivot are quite
different from that in the Gauss elimination. The reason is that x 1, x 2, x 3, x 4 are restricted
to nonnegative values.
Step 1. Operation O1: Selection of the Column of the Pivot
Select as the column of the pivot the first column with a negative entry in Row 1. In (4)
this is Column 2 (because of the Ϫ40).
Operation O2: Selection of the Row of the Pivot. Divide the right sides [60 and 60 in
(4)] by the corresponding entries of the column just selected (60>2 ϭ 30, 60>5 ϭ 12).
Take as the pivot equation the equation that gives the smallest quotient. Thus the pivot
is 5 because 60>5 is smallest.
Operation O3: Elimination by Row Operations.
pivot (as in Gauss–Jordan, Sec. 7.8).

This gives zeros above and below the

With the notation for row operations as introduced in Sec. 7.3, the calculations in Step 1
give from the simplex table T0 in (4) the following simplex table (augmented matrix),
with the blue letters referring to the previous table.
z
1
(5)

T1 ϭ

x1

x2

|
|

0

Ϫ72

|
|
|
|
|

0

7.2

5

2

x3

x4

|
|

0

8

|
|
|
|
|

1

Ϫ0.4

0

1

b
|
|

480

|
|
|
|
|

36

– – – – –| – – – – – – – – – – –| – – – – – – – – – –| – – – – –

Y 0
0

Row 1 ϩ 8 Row 3

Z Row 2 Ϫ 0.4 Row 3

60

We see that basic variables are now x 1, x 3 and nonbasic variables are x 2, x 4. Setting the
latter to zero, we obtain the basic feasible solution given by T1,
x 1 ϭ 60>5 ϭ 12,

x 2 ϭ 0,

x 3 ϭ 36>1 ϭ 36,

x 4 ϭ 0,

z ϭ 480.

This is A in Fig. 474 (Sec. 22.2). We thus have moved from O: (0, 0) with z ϭ 0 to
A: (12, 0) with the greater z ϭ 480. The reason for this increase is our elimination of a

c22.qxd

11/4/10

1:35 PM

Page 961

SEC. 22.3 Simplex Method

961

term (Ϫ40x 1) with a negative coefficient. Hence elimination is applied only to negative
entries in Row 1 but to no others. This motivates the selection of the column of the pivot.
We now motivate the selection of the row of the pivot. Had we taken the second row
of T0 instead (thus 2 as the pivot), we would have obtained z ϭ 1200 (verify!), but this
line of constant revenue z ϭ 1200 lies entirely outside the feasibility region in Fig. 474.
This motivates our cautious choice of the entry 5 as our pivot because it gave the smallest
quotient (60>5 ϭ 12).
Step 2. The basic feasible solution given by (5) is not yet optimal because of the negative
entry Ϫ72 in Row 1. Accordingly, we perform the operations O1 to O3 again, choosing a
pivot in the column of Ϫ72.
Operation O1. Select Column 3 of T1 in (5) as the column of the pivot (because Ϫ72 Ͻ 0).
Operation O2. We have 36>7.2 ϭ 5 and 60>2 ϭ 30. Select 7.2 as the pivot (because
5 Ͻ 30).
Operation O3. Elimination by row operations gives
z
1

x1

x2

|
|

0

0

|
|
|
|
|
|

0

7.2

5

0

|
|

x3

x4

10

4

b
|
|

840

Row 1 ϩ 10 Row 2

– – – –| – – – – – – – – –| – – – – – – – – – – – – –| – – – – –

T2 ϭ W 0

(6)

0

|
|
|
|
|
|

1
1
Ϫ ᎏᎏ
3.6

Ϫ0.4
1
ᎏᎏ
0.9

|
|
|
|
|
|

36 X
50

2
Row 3 Ϫ ᎏᎏ Row 2
7.2

We see that now x 1, x 2 are basic and x 3, x 4 nonbasic. Setting the latter to zero, we obtain
from T2 the basic feasible solution
x 1 ϭ 50>5 ϭ 10,

x 2 ϭ 36>7.2 ϭ 5,

x 3 ϭ 0,

x 4 ϭ 0,

z ϭ 840.

This is B in Fig. 474 (Sec. 22.2). In this step, z has increased from 480 to 840, due to the
elimination of Ϫ72 in T1. Since T2 contains no more negative entries in Row 1, we
conclude that z ϭ f (10, 5) ϭ 40 ؒ 10 ϩ 88 ؒ 5 ϭ 840 is the maximum possible revenue.
It is obtained if we produce twice as many S heaters as L heaters. This is the solution of
our problem by the simplex method of linear programming.
᭿
Minimization. If we want to minimize z ϭ f (x) (instead of maximize), we take as the
columns of the pivots those whose entry in Row 1 is positive (instead of negative). In
such a Column k we consider only positive entries t jk and take as pivot a t jk for which
bj>t jk is smallest (as before). For examples, see the problem set.

PROBLEM SET 22.3
1. Verify the calculations in Example 1 of the text.
2–14

SIMPLEX METHOD

Write in normal form and solve by the simplex method,
assuming all x j to be nonnegative.

2. The problem in the example in the text with the
constraints interchanged.
3. Maximize f ϭ 3x 1 ϩ 2x 2 subject to 3x1 ϩ 4x 2 Ϲ 60,
4x 1 ϩ 3x 2 Ϲ 60, 10x 1 ϩ 2x 2 Ϲ 120.

c22.qxd

11/3/10

4:00 PM

962

Page 962

CHAP. 22 Unconstrained Optimization. Linear Programming

4. Maximize the daily output in producing x 1 chairs by
Process P1 and x 2 chairs by Process P2 subject to
3x 1 ϩ 4x 2 Ϲ 550 (machine hours), 5x 1 ϩ 4x 2 Ϲ 650
(labor).
5. Minimize f ϭ 5x 1 Ϫ 20x 2 subject to Ϫ2x1 ϩ 10x 2
Ϲ 5, 2x1 ϩ 5x 2 Ϲ 10.
6. Prob. 19 in Sec. 22.2.
7. Suppose we produce x 1 AA batteries by Process
P1 and x 2 by Process P2, furthermore x 3 A batteries by
Process P3 and x 4 by Process P4. Let the profit for 100
batteries be $10 for AA and $20 for A. Maximize the
total profit subject to the constraints
12x 1 ϩ 8x 2 ϩ 6x 3 ϩ 4x 4 Ϲ 120 (Material)
3x 1 ϩ 6x 2 ϩ 12x 3 ϩ 24x 4 Ϲ 180 (Labor).
8. Maximize the daily profit in producing x 1 metal frames
F1 (profit $90 per frame) and x 2 frames F2 (profit $50
per frame) subject to x 1 ϩ 3x 2 Ϲ 18 (material),
x 1 ϩ x 2 Ϲ 10 (machine hours), 3x 1 ϩ x 2 Ϲ 24 (labor).
9. Maximize f ϭ 2x1 ϩ x 2 ϩ 3x 3 subject to 4x1 ϩ 3x 2 ϩ
6x 3 ϭ 12.

22.4

10. Minimize f ϭ 4x 1 Ϫ 10x 2 Ϫ 20x 3 subject to 3x1 ϩ
4x2 ϩ 5x3 Ϲ 60, 2x1 ϩ x2 Ϲ 20, 2x1 ϩ 3x3 Ϲ 30.
11. Prob. 22 in Problem Set 22.2.
12. Maximize f ϭ 2x 1 ϩ 3x 2 ϩ x 3 subject to x1 ϩ x 2 ϩ
x 3 Ϲ 4.8, 10x1 ϩ x3 Ϲ 9.9, x2 Ϫ x3 Ϲ 0.2.
13. Maximize f ϭ 34x 1 ϩ 29x 2 ϩ 32x 3 subject to 8x 1 ϩ
2x 2 ϩ x 3 Ϲ 54, 3x 1 ϩ 8x 2 ϩ 2x 3 Ϲ 59, x 1 ϩ x 2 ϩ
5x 3 Ϲ 39.
14. Maximize f ϭ 2x 1 ϩ 3x 2 subject to 5x1 ϩ 3x 2 Ϲ 105,
3x1 ϩ 6x 2 Ϲ 126.
15. CAS PROJECT. Simple Method. (a) Write a program
for graphing a region R in the first quadrant of the
x 1x 2-plane determined by linear constraints.
(b) Write a program for maximizing z ϭ a1x 1 ϩ a2x 2
in R.
(c) Write a program for maximizing z ϭ a1x 1 ϩ
Á ϩ anx n subject to linear constraints.
(d) Apply your programs to problems in this problem
set and the previous one.

Simplex Method: Difficulties
In solving a linear optimization problem by the simplex method, we proceed stepwise
from one basic feasible solution to another. By so doing, we increase the value of the
objective function f. We continue this stepwise procedure, until we reach an optimal
solution. This was all explained in Sec. 22.3. However, the method does not always proceed
so smoothly. Occasionally, but rather infrequently in practice, we encounter two kinds of
difficulties. The first one is the degeneracy and the second one concerns difficulties in
starting.

Degeneracy
A degenerate feasible solution is a feasible solution at which more than the usual number
n Ϫ m of variables are zero. Here n is the number of variables (slack and others) and m
the number of constraints (not counting the x j м 0 conditions). In the last section, n ϭ 4
and m ϭ 2, and the occurring basic feasible solutions were nondegenerate; n Ϫ m ϭ 2
variables were zero in each such solution.
In the case of a degenerate feasible solution we do an extra elimination step in which
a basic variable that is zero for that solution becomes nonbasic (and a nonbasic variable
becomes basic instead). We explain this in a typical case. For more complicated cases
and techniques (rarely needed in practice) see Ref. [F5] in App. 1.
EXAMPLE 1

Simplex Method, Degenerate Feasible Solution
AB Steel, Inc., produces two kinds of iron I1, I2 by using three kinds of raw material R1, R2, R3 (scrap iron and
two kinds of ore) as shown. Maximize the daily profit.

c22.qxd

11/3/10

4:00 PM

Page 963

SEC. 22.4 Simplex Method: Difficulties

963

Raw Material Needed
per Ton

Raw
Material

Iron I1

Iron I2

R1
R2
R3

2
1
0

1
1
1

Net profit
per ton

$150

$300

Raw Material Available
per Day (tons)
16
8
3.5

Solution. Let x 1 and x 2 denote the amount (in tons) of iron I1 and I2, respectively, produced per day. Then
our problem is as follows. Maximize
z ϭ f (x) ϭ 150x 1 ϩ 300x 2

(1)

subject to the constraints x1 м 0, x 2 м 0 and
2x 1 ϩ x 2 Ϲ 16

(raw material R1)

x1 ϩ x2 Ϲ 8

(raw material R2)

x 2 Ϲ 3.5

(raw material R3).

By introducing slack variables x 3, x 4, x 5 we obtain the normal form of the constraints
2x 1 ϩ x 2 ϩ x 3
x1 ϩ x2

(2)

ϭ 16
ϩ x4

ϭ 8
ϩ x 5 ϭ 3.5

x2
xi м 0

(i ϭ 1, Á , 5).

As in the last section we obtain from (1) and (2) the initial simplex table
z
1

x1
|

x2

Ϫ150 Ϫ300

|

x3

x4

x5

0

0

0

b
|

0

| –––
– – –| – – – – – – – – – –| – – – – – – – – – – – –

T0 ϭ

(3)

W

0
0
0

|
|
|
|
|
|
|

2

1

1

1

0

1

|
|
|
|
|
|
|

1

0

0

0

1

0

0

0

1

|
|
|
|
|
|
|

16
8

X.

3.5

We see that x 1, x 2 are nonbasic variables and x 3, x 4, x 5 are basic. With x 1 ϭ x 2 ϭ 0 we have from (3) the basic
feasible solution
x1 ϭ 0,

x 2 ϭ 0,

x 3 ϭ 16>1 ϭ 16,

x 4 ϭ 8>1 ϭ 8,

x 5 ϭ 3.5>1 ϭ 3.5,

z ϭ 0.

This is O: (0, 0) in Fig. 475. We have n ϭ 5 variables x j, m ϭ 3 constraints, and n Ϫ m ϭ 2 variables equal to
zero in our solution, which thus is nondegenerate.

Step 1 of Pivoting
Operation O1: Column Selection of Pivot. Column 2 (since Ϫ150 Ͻ 0).
Operation O2: Row Selection of Pivot. 16>2 ϭ 8, 8>1 ϭ 8; 3.5>0 is not possible. Hence we could choose
Row 2 or Row 3. We choose Row 2. The pivot is 2.

c22.qxd

11/3/10

964

4:00 PM

Page 964

CHAP. 22 Unconstrained Optimization. Linear Programming
Operation O3: Elimination by Row Operations. This gives the simplex table
z
|

1

x1

x2

0

Ϫ225

|

x3

x4

x5

75

0

0

b
|

Row 1 ϩ 75 Row 2

1200

– – – –| – – – – – – – – –| – – – – – – – – – – – – – |– – – – –

(4)

T1 ϭ

W

|
|
|
|
|
|
|

0
0
0

2

1

0

_1

0

1

2

|
|
|
|
|
|
|

1

0

0

Ϫ_12

1

0

0

0

1

|
|
|
|
|
|
|

16

X

0

Row 3 Ϫ _12 Row 2

3.5

Row 4

We see that the basic variables are x 1, x 4, x 5 and the nonbasic are x 2, x 3. Setting the nonbasic variables to zero,
we obtain from T1 the basic feasible solution

x2

f = 1725

x4 = 0

x3 = 0
x5 = 0

3.5

C

B

f=0
A
O

x1

8

Fig. 475. Example 1, where A is degenerate

x 1 ϭ 16>2 ϭ 8,

x 2 ϭ 0,

x 3 ϭ 0,

x 4 ϭ 0>1 ϭ 0,

x 5 ϭ 3.5>1 ϭ 3.5,

z ϭ 1200.

This is A: (8, 0) in Fig. 475. This solution in degenerate because x 4 ϭ 0 (in addition to x 2 ϭ 0, x 3 ϭ 0);
geometrically: the straight line x 4 ϭ 0 also passes through A. This requires the next step, in which x 4 will
become nonbasic.

Step 2 of Pivoting
Operation O1: Column Selection of Pivot. Column 3 (since Ϫ225 Ͻ 0).
Operation O2: Row Selection of Pivot. 16>1 ϭ 16, 0> 12 ϭ 0. Hence

1
2

must serve as the pivot.

Operation O3: Elimination by Row Operations. This gives the following simplex table.
z
1

|

x1

x2

0

0

x3
|

Ϫ150

x4

x5

450

0

b
|

1200

– – –| – – – – – – –| – – – – – – – – – – – – – –| – – – – –

(5)

T2 ϭ

W

0
0
0

|
|
|
|
|
|
|

2

0

0

_1

0

0

2

|
|
|
|
|
|
|

2

Ϫ2

0

Ϫ_12

1

0

1

Ϫ2

1

|
|
|
|
|
|
|

16

X

0
3.5

Row 1 ϩ 450 Row 3
Row 2 Ϫ 2 Row 3

Row 4 Ϫ 2 Row 3

We see that the basic variables are x 1, x 2, x 5 and the nonbasic are x 3, x 4. Hence x 4 has become nonbasic, as
intended. By equating the nonbasic variables to zero we obtain from T2 the basic feasible solution
x 1 ϭ 16>2 ϭ 8,

x 2 ϭ 0> 12 ϭ 0,

x 3 ϭ 0,

x 4 ϭ 0,

x 5 ϭ 3.5>1 ϭ 3.5,

z ϭ 1200.

This is still A: (8, 0) in Fig. 475 and z has not increased. But this opens the way to the maximum, which we
reach in the next step.

c22.qxd

11/3/10

4:00 PM

Page 965

SEC. 22.4 Simplex Method: Difficulties

965

Step 3 of Pivoting
Operation O1: Column Selection of Pivot. Column 4 (since Ϫ150 Ͻ 0).
Operation O2: Row Selection of Pivot. 16>2 ϭ 8, 0>(Ϫ12 ) ϭ 0, 3.5>1 ϭ 3.5. We can take 1 as the pivot.
(With Ϫ12 as the pivot we would not leave A. Try it.)
Operation O3: Elimination by Row Operations. This gives the simplex table
z
1

|

x1

x2

0

0

|

x3

x4

x5

b

0

150

150

|
|
|
|
|
|
|
|

Row 1 ϩ 150 Row 4

1725

– – –| – – – – – – – |– – – – – – – – – – – – |– – – – – –

(6)

T3 ϭ

W

0
0
0

|
|
|
|
|
|
|

2

0

0

_1

0

0

2

|
|
|
|
|
|
|

0

2

Ϫ2

0

0

_1

1

Ϫ2

1

2

9
1.75

X

Row 2 Ϫ 2 Row 4
Row 3 ϩ _12 Row 4

3.5

We see that basic variables are x 1, x 2, x 3 and nonbasic x 4, x 5. Equating the latter to zero we obtain from T3 the
basic feasible solution
x 1 ϭ 9>2 ϭ 4.5,

x 2 ϭ 1.75> 12 ϭ 3.5,

x 3 ϭ 3.5>1 ϭ 3.5,

x 4 ϭ 0,

x 5 ϭ 0,

z ϭ 1725.

This is B: (4.5, 3.5) in Fig. 475. Since Row 1 of T3 has no negative entries, we have reached the maximum daily
profit z max ϭ f (4.5, 3.5) ϭ 150 ؒ 4.5 ϩ 300 ؒ 3.5 ϭ $1725. This is obtained by using 4.5 tons of iron I1 and
3.5 tons of iron I2.
᭿

Difficulties in Starting
As a second kind of difficulty, it may sometimes be hard to find a basic feasible solution
to start from. In such a case the idea of an artificial variable (or several such variables)
is helpful. We explain this method in terms of a typical example.
EXAMPLE 2

Simplex Method: Difficult Start, Artificial Variable
Maximize
(7)

z ϭ f (x) ϭ 2x1 ϩ x 2

subject to the constraints x1 м 0, x 2 м 0 and (Fig. 476)
x1 Ϫ 12 x 2 м 1
x1 Ϫ x2 Ϲ 2
x 1 ϩ x 2 Ϲ 4.

Solution. By means of slack variables we achieve the normal form of the constraints
z Ϫ 2x 1 Ϫ x 2
x1 Ϫ
(8)

1
2 x2

x1 Ϫ x2
x1 ϩ x2

ϭ0
Ϫ x3

ϭ1
ϩ x4

ϭ2
ϩ x5 ϭ 4

x i м 0 (i ϭ 1, Á , 5).

c22.qxd

11/3/10

966

4:00 PM

Page 966

CHAP. 22 Unconstrained Optimization. Linear Programming
Note that the first slack variable is negative (or zero), which makes x 3 nonnegative within the feasibility region
(and negative outside). From (7) and (8) we obtain the simplex table
z
1

|

x1

x2

Ϫ2

Ϫ1

|

1

Ϫ_

1

Ϫ1

1

1

|
|
|
|
|
|
|

x3

x4

x5

0

0

0

b
0

|

– – – –| – – – – – – – – |– – – – – – – – – – – –| – – –

W

0
0
0

|
|
|
|
|
|
|

1
2

Ϫ1

0

0

0

1

0

0

0

1

|
|
|
|
|
|
|

1
2

X.

4

x 1, x 2 are nonbasic, and we would like to take x 3, x 4, x 5 as basic variables. By our usual process of equating
the nonbasic variables to zero we obtain from this table
x1 ϭ 0,

x 2 ϭ 0,

x 4 ϭ 21 ϭ 2,

x 3 ϭ 1>(Ϫ1) ϭ Ϫ1,

x 5 ϭ 41 ϭ 4,

z ϭ 0.

x 3 Ͻ 0 indicates that (0, 0) lies outside the feasibility region. Since x 3 Ͻ 0, we cannot proceed immediately.
Now, instead of searching for other basic variables, we use the following idea. Solving the second equation in
(8) for x 3, we have
x 3 ϭ Ϫ1 ϩ x 1 Ϫ 12 x 2.
To this we now add a variable x 6 on the right,

x2
f=7

B

2

C

1

0
0

A
1

2

3

x1

Fig. 476. Feasibility region in Example 2

x 3 ϭ Ϫ1 ϩ x 1 Ϫ 12 x 2 ϩ x 6.

(9)

x 6 is called an artificial variable and is subject to the constraint x 6 м 0.
We must take care that x 6 (which is not part of the given problem!) will disappear eventually. We shall see
that we can accomplish this by adding a term ϪMx 6 with very large M to the objective function. Because of
(7) and (9) (solved for x 6) this gives the modified objective function for this “extended problem”
(10)

zˆ ϭ z Ϫ Mx 6 ϭ 2x 1 ϩ x 2 Ϫ Mx 6 ϭ (2 ϩ M)x 1 ϩ (1 Ϫ 12 M)x 2 Ϫ Mx 3 Ϫ M.

We see that the simplex table corresponding to (10) and (8) is
zˆ
1

|

x1

x2

Ϫ2 Ϫ M
1

Ϫ1 ϩ _1 M
Ϫ_1

1

Ϫ1

1

1

1

Ϫ_12

|

x3

x4

x5

x6

M

0

0

0

Ϫ1

0

0

0

0

1

0

0

0

0

1

0

Ϫ1

0

0

1

b
|

ϪM

– – ||– – – – – – – – – – – – –2– – –|| – – – – – – – – – – – – – – – – – –|| – – – –

0

T0 ϭ

U

0
0
0

|
|
|
|
|
|
|
|

2

|
|
|
|
|
|
|
|

|
|
|
|
|
|
|
|

1
2
4
1

V.

c22.qxd

11/3/10

4:00 PM

Page 967

SEC. 22.4 Simplex Method: Difficulties

967

The last row of this table results from (9) written as x 1 Ϫ 12 x 2 Ϫ x 3 ϩ x 6 ϭ 1. We see that we can now start,
taking x 4, x 5, x 6 as the basic variables and x 1, x 2, x 3 as the nonbasic variables. Column 2 has a negative first
entry. We can take the second entry (1 in Row 2) as the pivot. This gives
zˆ
1

|

0

|
|
|
|
|
|
|
|
|

x1

x2

0

Ϫ2

|

0

Ϫ_12
Ϫ_1

0

_3

0

0

|
|
|
|
|
|
|
|
|

x3

x4

x5

x6

b

Ϫ2

0

0

0

|
|
|
|
|
|
|
|
|
|

2

– – – –| – – – – – – – –| – – – – – – – – – – – – – – – – – – –| – – –

T1 ϭ

U

0
0
0

1

2
2

Ϫ1

0

0

0

1

1

0

0

1

0

1

0

0

0

0

1

1
1

V.

3
0

This corresponds to x 1 ϭ 1, x 2 ϭ 0 (point A in Fig. 476), x 3 ϭ 0, x 4 ϭ 1, x 5 ϭ 3, x 6 ϭ 0. We can now drop
Row 5 and Column 7. In this way we get rid of x 6, as wanted, and obtain
z
1

|

x1

x2

0

Ϫ2

|

0

Ϫ_12
Ϫ_1

0

_3

|
|
|
|
|
|
|

x3

x4

x5

Ϫ2

0

0

b
|

2

– – – –| – – – – – – – –| – – – – – – – – – – – – –| – – –

T2 ϭ

0

W

0
0

In Column 3 we choose

3
2

|
|
|
|
|
|
|

1

2
2

Ϫ1

0

0

1

1

0

1

0

1

|
|
|
|
|
|
|

1
1

X.

3

as the next pivot. We obtain
z
1

|

x1

x2

0

0

|

x3

x4

x5

Ϫ_2

0

_4

Ϫ_32
_4

0

_1

3

1

_1

1

0

1

3

b
|

6

– – – –| – – – – – – – –| – – 3– – – – – – – – – – |– – –

T3 ϭ

W

0
0
0

|
|
|
|
|
|
|

1

0

0

0

0

_3
2

|
|
|
|
|
|
|

3
3

|
|
|
|
|
|
|

2
2

X.

3

This corresponds to x 1 ϭ 2, x 2 ϭ 2 (this is B in Fig. 476), x 3 ϭ 0, x 4 ϭ 2, x 5 ϭ 0. In Column 4 we choose
as the pivot, by the usual principle. This gives
z
1

|

x1

x2

0

0

|

x3

x4

x5

0

_1

_3

0

_1

_1

_4

3

1

_1

0

Ϫ_34

_3

2

2

4
3

b
|

7

| – – – – – – – – – – – |– – –
– – – –| – – – – – – – – –

T4 ϭ

W

0
0
0

|
|
|
|
|
|
|

1

0

0

0

0

_3
2

|
|
|
|
|
|
|

2

This corresponds to x 1 ϭ 3, x 2 ϭ 1 (point C in Fig. 476), x 3 ϭ
fmax ϭ f (3, 1) ϭ 7.

2
3
4

3
2,

|
|
|
|
|
|
|

3
2

X.

_3
2

x 4 ϭ 0, x 5 ϭ 0. This is the maximum

᭿

We have reached the end of our discussion on linear programming. We have presented
the simplex method in great detail as this method has many beautiful applications and
works well on most practical problems. Indeed, problems of optimization appear in civil
engineering, chemical engineering, environmental engineering, management science,
logistics, strategic planning, operations management, industrial engineering, finance, and
other areas. Furthermore, the simplex method allows your problem to be scaled up from
a small modeling attempt to a larger modeling attempt, by adding more constraints and

c22.qxd

11/3/10

968

4:00 PM

Page 968

CHAP. 22 Unconstrained Optimization. Linear Programming

variables, thereby making your model more realistic. The area of optimization is an active
field of development and research and optimization methods, besides the simplex method,
are being explored and experimented with.

PROBLEM SET 22.4
1. Maximize z ϭ f1(x) ϭ 7x 1 ϩ 14x 2 subject to 0 Ϲ x1
Ϲ 6, 0 Ϲ x 2 Ϲ 3, 7x 1 ϩ 14x 2 Ϲ 84.
2. Do Prob. 1 with the last two constraints interchanged.
3. Maximize the daily output in producing x 1 steel sheets
by process PA and x 2 steel sheets by process PB subject
to the constraints of labor hours, machine hours, and
raw material supply:
3x1 ϩ 2x 2 Ϲ 180,

4x1 ϩ 6x 2 Ϲ 200,

5x 1 ϩ 3x 2 Ϲ 160.
4. Maximize z ϭ 300x 1 ϩ 500x 2 subject to 2x1 ϩ 8x 2
Ϲ 60, 2x 1 ϩ x 2 Ϲ 30, 4x 1 ϩ 4x 2 Ϲ 60.
5. Do Prob. 4 with the last two constraints interchanged.
Comment on the resulting simplification.

6. Maximize the total output f ϭ x 1 ϩ x 2 ϩ x 3 (production from three distinct processes) subject to input
constraints (limitation of time available for production)
5x 1 ϩ 6x 2 ϩ 7x 3 Ϲ 12,
7x 1 ϩ 4x 2 ϩ x 3 Ϲ 12.
7. Maximize f ϭ 5x 1 ϩ 8x 2 ϩ 4x 3 subject to x j м 0
and
( j ϭ 1, Á , 5)
x 1 ϩ x 3 ϩ x 5 ϭ 1, x 2 ϩ x 3
ϩ x 4 ϭ 1.
8. Using an artificial variable, minimize f ϭ 4x1 Ϫ x 2 subject
to x1 ϩ x 2 м 2, Ϫ2x1 ϩ 3x2 Ϲ 1, 5x1 ϩ 4x 2 Ϲ 50.
9. Maximize f ϭ 2x 1 ϩ 3x 2 ϩ 2x 3, x1 м 0, x 2 м 0,
x 3 м 0, x1 ϩ 2x 2 Ϫ 4x 3 Ϲ 2, x1 ϩ 2x 2 ϩ 2x 3 Ϲ 5.

CHAPTER 22 REVIEW QUESTIONS AND PROBLEMS
1. What is unconstrained optimization? Constraint optimization? To which one do methods of calculus apply?
2. State the idea and the formulas of the method of steepest
descent.
3. Write down an algorithm for the method of steepest descent.
4. Design a “method of steepest ascent” for determining
maxima.
5. What is the method of steepest descent for a function
of a single variable?
6. What is the basic idea of linear programming?
7. What is an objective function? A feasible solution?
8. What are slack variables? Why did we introduce them?
9. What happens in Example 1 of Sec. 22.1 if you replace
f (x) ϭ x 21 ϩ 3x 22 with f (x) ϭ x 21 ϩ 5x 22? Start from
x0 ϭ [6 3]T. Do 5 steps. Is the convergence faster or
slower?
10. Apply the method of steepest descent to f (x) ϭ 9x 21 ϩ
x 22 ϩ 18x 1 Ϫ 4x 2, 5 steps. Start from x0 ϭ [2 4]T.
11. In Prob. 10, could you start from [0 0]T and do 5 steps?
12. Show that the gradients in Prob. 11 are orthogonal. Give
a reason.
13–16
Graph or sketch the region in the first quadrant
of the x 1x 2-plane determined by the following inequalities.
13.
x 1 Ϫ 2x 2 Ϲ Ϫ2
0.8x1 ϩ x 2 Ϲ 6

14.

x1 Ϫ 2x 2 м Ϫ4
2x 1 ϩ x 2 Ϲ 12
x1 ϩ x2 Ϲ 8

15.

x1 ϩ x2 Ϲ 5
x2 Ϲ 3
Ϫx 1 ϩ x 2 Ϲ 2

16.

2
x1 ϩ x 2 м
2x 1 Ϫ 3x 2 м Ϫ12
x1
Ϲ 15

17–20

Maximize or minimize as indicated.

17. Maximize f ϭ 10x1 ϩ 20x 2 subject to x1 Ϲ 5, x1 ϩ
x 2 Ϲ 6, x 2 Ϲ 4.
18. Maximize f ϭ x 1 ϩ x 2 subject to x 1 ϩ 2x 2 Ϲ 10,
2x 2 ϩ x 2 Ϲ 10, x 2 Ϲ 4.
19. Minimize f ϭ 2x 1 Ϫ 10x 2 subject to x 1 Ϫ x 2 Ϲ 4,
2x 1 ϩ x 2 Ϲ 14, x 1 ϩ x 2 Ϲ 9, Ϫx 1 ϩ 3x 2 Ϲ 15.
20. A factory produces two kinds of gaskets, G1, G2, with
net profit of $60 and $30, respectively, Maximize the
total daily profit subject to the constraints (x j ϭ number
of gaskets Gj produced per day):
40x 1 ϩ 40x 2 Ϲ 1800 (Machine hours),
200x 1 ϩ 20x 2 Ϲ 6300 (Labor).

c22.qxd

11/3/10

4:00 PM

Page 969

Summary of Chapter 22

969

SUMMARY OF CHAPTER

22

Unconstrained Optimization. Linear Programming
In optimization problems we maximize or minimize an objective function z ϭ f (x)
depending on control variables x1, Á , x m whose domain is either unrestricted
(“unconstrained optimization,” Sec. 22.1) or restricted by constraints in the form
of inequalities or equations or both (“constrained optimization,” Sec. 22.2).
If the objective function is linear and the constraints are linear inequalities in
x1, Á , x m, then by introducing slack variables x mϩ1, Á , x n we can write the
optimization problem in normal form with the objective function given by
(1)

f1 ϭ c1x 1 ϩ Á ϩ cnx n

(where cmϩ1 ϭ Á ϭ cn ϭ 0) and the constraints given by
a11x 1 ϩ a12x 2 ϩ Á ϩ a1nx n ϭ b1
Á Á Á Á Á Á Á Á Á Á
(2)

Á Á Á Á Á Á Á Á Á Á
am1x 1 ϩ am2x 2 ϩ Á ϩ amnx n ϭ bm
x1 м 0, Á , x n м 0.

In this case we can then apply the widely used simplex method (Sec. 22.3), a
systematic stepwise search through a very much reduced subset of all feasible
solutions. Section 22.4 shows how to overcome difficulties with this method.

c23-a.qxd

11/3/10

3:34 PM

Page 970

CHAPTER

23

Graphs.
Combinatorial Optimization
Many problems in electrical engineering, civil engineering, operations research, industrial
engineering, management, logistics, marketing, and economics can be modeled by graphs
and directed graphs, called digraphs. This is not surprising as they allow us to model
networks, such as roads and cables, where the nodes may be cities or computers. The
task then is to find the shortest path through the network or the best way to connect
computers. Indeed, many researchers who made contributions to combinatorial
optimization and graphs, and whose names lend themselves to fundamental algorithms
in this chapter, such as Fulkerson, Kruskal, Moore, and Prim, all worked at Bell
Laboratories in New Jersey, the major R&D facilities of the huge telephone and
telecommunication company AT&T. As such, they were interested in methods of
optimally building computer networks and telephone networks. The field has progressed
into looking for more and more efficient algorithms for very large problems.
Combinatorial optimization deals with optimization problems that are of a pronounced
discrete or combinatorial nature. Often the problems are very large and so a direct search
may not be possible. Just like in linear programming (Chap. 22), the computer is an
indispensible tool and makes solving large-scale modeling problems possible. Because
the area has a distinct flavor, different from ODEs, linear algebra, and other areas, we
start with the basics and gradually introduce algorithms for shortest path problems (Secs.
22.2, 22.3), shortest spanning trees (Secs. 23.4, 23.5), flow problems in networks (Secs.
23.6, 23.7), and assignment problems (Sec. 23.8).
Prerequisite: none.
References and Answers to Problems: App. 1 Part F, App. 2.

23.1

Graphs and Digraphs
Roughly, a graph consists of points, called vertices, and lines connecting them, called
edges. For example, these may be four cities and five highways connecting them, as in
Fig. 477. Or the points may represent some people, and we connect by an edge those who
do business with each other. Or the vertices may represent computers in a network and
the edge connections between them. Let us now give a formal definition.

970

c23-a.qxd

11/3/10

3:34 PM

Page 971

SEC. 23.1 Graphs and Digraphs

971
Loop

Isolated
vertex
1
e5

e1
e2

4
2

3

e4

e3
Double edge

Fig. 477. Graph consisting of
4 vertices and 5 edges

DEFINITION

Fig. 478. Isolated vertex, loop, double
edge. (Excluded by definition.)

Graph

A graph G consists of two finite sets (sets having finitely many elements), a set V
of points, called vertices, and a set E of connecting lines, called edges, such that
each edge connects two vertices, called the endpoints of the edge. We write
G ϭ (V, E).
Excluded are isolated vertices (vertices that are not endpoints of any edge), loops
(edges whose endpoints coincide), and multiple edges (edges that have both
endpoints in common). See Fig. 478.

CAUTION! Our three exclusions are practical and widely accepted, but not uniformly.
For instance, some authors permit multiple edges and call graphs without them simple
graphs.
᭿
We denote vertices by letters, u, v, Á or v1, v2, Á or simply by numbers 1, 2, Á (as
in Fig. 477). We denote edges by e1, e2, Á or by their two endpoints; for instance,
e1 ϭ (1, 4), e2 ϭ (1, 2) in Fig. 477.
An edge (vi, vj) is called incident with the vertex vi (and conversely); similarly, (vi, vj)
is incident with vj. The number of edges incident with a vertex v is called the degree of v.
Two vertices are called adjacent in G if they are connected by an edge in G (that is, if they
are the two endpoints of some edge in G).
We meet graphs in different fields under different names: as “networks” in electrical
engineering, “structures” in civil engineering, “molecular structures” in chemistry,
“organizational structures” in economics, “sociograms,” “road maps,” “telecommunication
networks,” and so on.

Digraphs (Directed Graphs)
Nets of one-way streets, pipeline networks, sequences of jobs in construction work, flows
of computation in a computer, producer–consumer relations, and many other applications
suggest the idea of a “digraph” (ϭ directed graph), in which each edge has a direction
(indicated by an arrow, as in Fig. 479).

c23-a.qxd

11/3/10

3:34 PM

972

Page 972

CHAP. 23 Graphs. Combinatorial Optimization
e1

1
e5
e4
3

2

e7

e6

e2
4

e3

Fig. 479. Digraph

DEFINITION

Digraph (Directed Graph)

A digraph G ϭ (V, E) is a graph in which each edge e ϭ (i, j) has a direction from
its “initial point” i to its “terminal point” j.

Two edges connecting the same two points i, j are now permitted, provided they have
opposite directions, that is, they are (i, j) and ( j, i). Example. (1, 4) and (4, 1) in Fig. 479.
A subgraph or subdigraph of a given graph or digraph G ϭ (V, E), respectively, is a
graph or digraph obtained by deleting some of the edges and vertices of G, retaining the
other edges of G (together with their pairs of endpoints). For instance, e1, e3 (together
with the vertices 1, 2, 4) form a subgraph in Fig. 477, and e3, e4, e5 (together with the
vertices 1, 3, 4) form a subdigraph in Fig. 479.

Computer Representation of Graphs and Digraphs
Drawings of graphs are useful to people in explaining or illustrating specific situations.
Here one should be aware that a graph may be sketched in various ways; see Fig. 480.
For handling graphs and digraphs in computers, one uses matrices or lists as appropriate
data structures, as follows.
5

8
1

4

2

3

6

1

4

2

3
5

7

8

6

(a)

5

7

8
4

1

2

3

6

(b)

7
(c)

Fig. 480. Different sketches of the same graph

Adjacency Matrix of a Graph G:
aij ϭ b

Matrix A ϭ [aij] with entries

1

if G has an edge (i, j),

0

else.

Thus aij ϭ 1 if and only if two vertices i and j are adjacent in G. Here, by definition, no
vertex is considered to be adjacent to itself; thus, aii ϭ 0. A is symmetric, aij ϭ aji. (Why?)
The adjacency matrix of a graph is generally much smaller than the so-called incidence
matrix (see Prob. 18) and is preferred over the latter if one decides to store a graph in a
computer in matrix form.

c23-a.qxd

11/3/10

3:34 PM

Page 973

SEC. 23.1 Graphs and Digraphs
EXAMPLE 1

973

Adjacency Matrix of a Graph
1

2

Vertex

1

2

3

4

Vertex 1

0

1

0

1

1

0

1

1

0

1

0

1

1

1

1

0

2
3
3

4

4

Adjacency Matrix of a Digraph G:
aij ϭ b

W

X
᭿

Matrix A ϭ [aij] with entries

1

if G has a directed edge (i, j),

0

else.

This matrix A need not be symmetric. (Why?)
EXAMPLE 2

Adjacency Matrix of a Digraph
1

To vertex

1

2

3

4

From vertex 1

0

1

0

0

1

0

0

1

0

1

0

0

0

0

0

0

2

2

W

3
3

4

4

X
᭿

Lists. The vertex incidence list of a graph shows, for each vertex, the incident edges.
The edge incidence list shows for each edge its two endpoints. Similarly for a digraph;
in the vertex list, outgoing edges then get a minus sign, and in the edge list we now have
ordered pairs of vertices.
EXAMPLE 3

Vertex Incidence List and Edge Incidence List of a Graph
This graph is the same as in Example 1, except for notation.

e1

v1
e5

v3

v2
e2

e4

e3

v4

Vertex

Incident Edges

Edge

Endpoints

v1
v2
v3
v4

e1, e5
e1, e2, e3
e2, e4
e3, e4, e5

e1
e2
e3
e4
e5

v1, v2
v2, v3
v2, v4
v3, v4
v1, v4

᭿

c23-a.qxd

11/3/10

3:34 PM

974

Page 974

CHAP. 23 Graphs. Combinatorial Optimization

Sparse graphs are graphs with few edges (far fewer than the maximum possible number
n(n Ϫ 1)>2, where n is the number of vertices). For these graphs, matrices are not efficient.
Lists then have the advantage of requiring much less storage and being easier to handle;
they can be ordered, sorted, or manipulated in various other ways directly within the
computer. For instance, in tracing a “walk” (a connected sequence of edges with pairwise
common endpoints), one can easily go back and forth between the two lists just discussed,
instead of scanning a large column of a matrix for a single 1.
Computer science has developed more refined lists, which, in addition to the actual
content, contain “pointers” indicating the preceding item or the next item to be scanned
or both items (in the case of a “walk”: the preceding edge or the subsequent one). For
details, see Refs. [E16] and [F7].
This section was devoted to basic concepts and notations needed throughout this chapter,
in which we shall discuss some of the most important classes of combinatorial optimization
problems. This will at the same time help us to become more and more familiar with
graphs and digraphs.

PROBLEM SET 23.1
1. Explain how the following can be regarded as a graph
or a digraph: a family tree, air connections between
given cities, trade relations between countries, a tennis
tournament, and memberships of some persons in some
committees.
2. Sketch the graph consisting of the vertices and edges
of a triangle. Of a pentagon. Of a tetrahedron.
3. How would you represent a net of two-way and oneway streets by a digraph?

10.

ADJACENCY MATRIX
6. Show that the adjacency matrix of a graph is symmetric.
7. When will the adjacency matrix of a digraph be
symmetric?
8–13

Find the adjacency matrix of the given graph or
digraph.
e1

8.
1

9.

e2

e7
e3

3

1

2

e4

e3
e1

e5 e
6
4

e2

5
2

3

11.

2

e4

1

e5
e3

e3

e6
3

12.

1

2

4

e2
2

13.

3

1

3

e1
e4

4

4
e2

e1

e1

4. Worker W1 can do jobs J1, J3, J4, worker W2 job J3,
and worker W3 jobs J2, J3, J4. Represent this by a
graph.
5. Find further situations that can be modeled by a graph
or diagraph.

e4

1

e5

5

e2

2

e3
3

14–15

Sketch the graph for the given adjacency matrix.

0

1

0

1

1
14. E
0

0

1

0

1

0

0

1

0

0

0

U

0

1

0

0

1

0

0

0

0

0

0

1

0

0

1

0

15. E

U

16. Complete graph. Show that a graph G with n vertices
can have at most n(n Ϫ 1)>2 edges, and G has exactly
n(n Ϫ 1)>2 edges if G is complete, that is, if every pair
of vertices of G is joined by an edge. (Recall that loops
and multiple edges are excluded.)

c23-a.qxd

11/3/10

3:34 PM

Page 975

SEC. 23.2 Shortest Path Problems. Complexity

975

17. In what case are all the off-diagonal entries of the
adjacency matrix of a graph G equal to one?
18. Incidence matrix B of a graph. The definition is
B ϭ [bjk], where
bjk ϭ b

1

if vertex j is an endpoint of edge ek,

0

otherwise.

Find the incidence matrix of the graph in Prob. 8.

ෂ
19. Incidence matrix B of a digraph. The definition is
ෂ
B ϭ [bjk], where
Ϫ1 if edge ek leaves vertex j,
ෂ
b jk ϭ d 1 if edge ek enters vertex j,
0 otherwise.
Find the incidence matrix of the digraph in Prob. 11.
20. Make the vertex incidence list of the digraph in Prob. 11.

23.2

Shortest Path Problems. Complexity
The rest of this chapter is devoted to the most important classes of problems of
combinatorial optimization that can be represented by graphs and digraphs. We selected
these problems because of their importance in applications, and present their solutions
in algorithmic form. Although basic ideas and algorithms will be explained and
illustrated by small graphs, you should keep in mind that real-life problems may often
involve many thousands or even millions of vertices and edges. Think of computer
networks, telephone networks, electric power grids, worldwide air travel, and companies
that have offices and stores in all larger cities. You can also think of other ideas for
networks related to the Internet, such as electronic commerce (networks of buyers and
sellers of goods over the Internet) and social networks and related websites, such as
Facebook. Hence reliable and efficient systematic methods are an absolute necessity—
solutions by trial and error would no longer work, even if “nearly optimal” solutions
were acceptable.
We begin with shortest path problems, as they arise, for instance, in designing shortest
(or least expensive, or fastest) routes for a traveling salesman, for a cargo ship, etc. Let
us first explain what we mean by a path.
In a graph G ϭ (V, E) we can walk from a vertex v1 along some edges to some other
vertex vk. Here we can
(A) make no restrictions, or
(B) require that each edge of G be traversed at most once, or
(C) require that each vertex be visited at most once.
In case (A) we call this a walk. Thus a walk from v1 to vk is of the form
(1)

(v1, v2), (v2, v3), Á , (vk؊1, vk),

where some of these edges or vertices may be the same. In case (B), where each edge
may occur at most once, we call the walk a trail. Finally, in case (C), where each vertex
may occur at most once (and thus each edge automatically occurs at most once), we call
the trail a path.
We admit that a walk, trail, or path may end at the vertex it started from, in which case
we call it closed; then vk ϭ v1 in (1).

c23-a.qxd

11/3/10

976

3:34 PM

Page 976

CHAP. 23 Graphs. Combinatorial Optimization

A closed path is called a cycle. A cycle has at least three edges (because we do not
have double edges; see Sec. 23.1). Figure 481 illustrates all these concepts.

5

1

2

4

3

Fig. 481. Walk, trail, path, cycle
1 Ϫ 2 Ϫ 3 Ϫ 2 is a walk (not a trail).
4 Ϫ 1 Ϫ 2 Ϫ 3 Ϫ 4 Ϫ 5 is a trail (not a path).
1 Ϫ 2 Ϫ 3 Ϫ 4 Ϫ 5 is a path (not a cycle).
1 Ϫ 2 Ϫ 3 Ϫ 4 Ϫ 1 is a cycle.

Shortest Path
To define the concept of a shortest path, we assume that G ϭ (V, E) is a weighted graph,
that is, each edge (vi, vj) in G has a given weight or length l ij Ͼ 0. Then a shortest path
v1 : vk (with fixed v1 and vk) is a path (1) such that the sum of the lengths of its edges
l 12 ϩ l 23 ϩ l 34 ϩ Á ϩ l k؊1,k
(l 12 ϭ length of (v1, v2), etc.) is minimum (as small as possible among all paths from
v1 to vk). Similarly, a longest path v1 : vk is one for which that sum is maximum.
Shortest (and longest) path problems are among the most important optimization problems.
Here, “length” l ij (often also called “cost” or “weight”) can be an actual length measured
in miles or travel time or fuel expenses, but it may also be something entirely different.
For instance, the traveling salesman problem requires the determination of a shortest
Hamiltonian1 cycle in a graph, that is, a cycle that contains all the vertices of the graph.
In more detail, the traveling salesman problem in its most basic and intuitive form can
be stated as follows. You have a salesman who has to drive by car to his customers. He
has to drive to n cities. He can start at any city and after completion of the trip he has to
return to that city. Furthermore, he can only visit each city once. All the cities are linked by
roads to each other, so any city can be visited from any other city directly, that is, if he
wants to go from one city to another city, there is only one direct road connecting those two
cities. He has to find the optimal route, that is, the route with the shortest total mileage for
the overall trip. This is a classic problem in combinatorial optimization and comes up in
many different versions and applications. The maximum number of possible paths to be
examined in the process of selecting the optimal path for n cities is (n Ϫ 1)!>2, because,
after you pick the first city, you have n Ϫ 1 choices for the second city, n Ϫ 2 choices for
the third city, etc. You get a total of (n Ϫ 1)! (see Sec. 24.4). However, since the mileage
does not depend on the direction of the tour (e.g., for n ϭ 4 (four cities 1, 2, 3, 4), the tour
1–2–3–4–1 has the same mileage as 1–4–3–2–1, etc., so that we counted all the tours twice!),
the final answer is (n Ϫ 1)!>2. Even for a small number of cities, say n ϭ 15, the maximum
number of possible paths is very large. Use your calculator or CAS to see for yourself! This
means that this is a very difficult problem for larger n and typical of problems in
combinatorial optimization, in that you want a discrete solution but where it might become
nearly impossible to explicitly search through all the possibilities and therefore some
heuristics (rules of thumbs, shortcuts) might be used, and a less than optimal answer suffices.
1

WILLIAM ROWAN HAMILTON (1805–1865), Irish mathematician, known for his work in dynamics.

c23-a.qxd

11/3/10

3:34 PM

Page 977

SEC. 23.2 Shortest Path Problems. Complexity

977

A variation of the traveling salesman problem is the following. By choosing the “most
profitable” route v1 : vk, a salesman may want to maximize Sl ij, where l ij is his expected
commission minus his travel expenses for going from town i to town j.
In an investment problem, i may be the day an investment is made, j the day it matures,
and l ij the resulting profit, and one gets a graph by considering the various possibilities
of investing and reinvesting over a given period of time.

Shortest Path If All Edges Have Length l ϭ 1
Obviously, if all edges have length l, then a shortest path v1 : vk is one that has the
smallest number of edges among all paths v1 : vk in a given graph G. For this problem
we discuss a BFS algorithm. BFS stands for Breadth First Search. This means that in
each step the algorithm visits all neighboring (all adjacent) vertices of a vertex reached,
as opposed to a DFS algorithm (Depth First Search algorithm), which makes a long trail
(as in a maze). This widely used BFS algorithm is shown in Table 23.1.
We want to find a shortest path in G from a vertex s (start) to a vertex t (terminal). To
guarantee that there is a path from s to t, we make sure that G does not consist of separate
portions. Thus we assume that G is connected, that is, for any two vertices v and w there
is a path v : w in G. (Recall that a vertex v is called adjacent to a vertex u if there is
an edge (u, v) in G.)
Table 23.1 Moore’s2 BFS for Shortest Path (All Lengths One)
Proceedings of the International Symposium for Switching Theory, Part II. pp. 285–292. Cambridge: Harvard
University Press, 1959.

ALGORITHM MOORE [G ϭ (V, E ), s, t]
This algorithm determines a shortest path in a connected graph G ϭ (V, E) from a vertex
s to a vertex t.
INPUT: Connected graph G ϭ (V, E), in which one vertex is denoted by s and
one by t, and each edge (i, j) has length l i j ϭ 1. Initially all vertices are
unlabeled.
OUTPUT: A shortest path s * t in G ϭ (V, E)
1.
2.
3.
4.
5.

Label s with 0.
Set i ϭ 0.
Find all unlabeled vertices adjacent to a vertex labeled i.
Label the vertices just found with i ϩ 1.
If vertex t is labeled, then “backtracking” gives the shortest path

k (ϭ label of t), k Ϫ 1, k Ϫ 2, • • • , 0
OUTPUT k, k Ϫ 1, k Ϫ 2, • • • , 0. Stop
Else increase i by 1. Go to Step 3.
End MOORE

2
EDWARD FORREST MOORE (1925–2003), American mathematician and computer scientist, who did
pioneering work in theoretical computer science (automata theory, Turing machines).

c23-a.qxd

11/3/10

3:34 PM

978
EXAMPLE 1

Page 978

CHAP. 23 Graphs. Combinatorial Optimization
Application of Moore’s BFS Algorithm
Find a shortest path s : t in the graph G shown in Fig. 482.

Solution. Figure 482 shows the labels. The blue edges form a shortest path (length 4). There is another
shortest path s : t. (Can you find it?) Hence in the program we must introduce a rule that makes backtracking
unique because otherwise the computer would not know what to do next if at some step there is a choice (for
instance, in Fig. 482 when it got back to the vertex labeled 2). The following rule seems to be natural.

Backtracking rule. Using the numbering of the vertices from 1 to n (not the labeling!), at each step, if a
vertex labeled i is reached, take as the next vertex that with the smallest number (not label!) among all the
vertices labeled i Ϫ 1.
᭿
2
1
3

3

1
3

s
0
2

1

2

4

t

3
4
2

Fig. 482. Example 1, given graph and result of labeling

Complexity of an Algorithm
Complexity of Moore’s algorithm. To find the vertices to be labeled 1, we have to scan
all edges incident with s. Next, when i ϭ 1, we have to scan all edges incident with vertices
labeled 1, etc. Hence each edge is scanned twice. These are 2m operations (m ϭ number of
edges of G). This is a function c(m). Whether it is 2m or 5m ϩ 3 or 12m is not so essential;
it is essential that c(m) is proportional to m (not m 2, for example); it is of the “order” m.
We write for any function am ϩ b simply O(m), for any function am 2 ϩ bm ϩ d simply
O(m 2), and so on; here, O suggests order. The underlying idea and practical aspect are
as follows.
In judging an algorithm, we are mostly interested in its behavior for very large problems
(large m in the present case), since these are going to determine the limits of the
applicability of the algorithm. Thus, the essential item is the fastest growing term
(am 2 in am 2 ϩ bm ϩ d, etc.) since it will overwhelm the others when m is large enough.
Also, a constant factor in this term is not very essential; for instance, the difference between
two algorithms of orders, say, 5m 2 and 8m 2 is generally not very essential and can be
made irrelevant by a modest increase in the speed of computers. However, it does make
a great practical difference whether an algorithm is of order m or m 2 or of a still higher
power m p. And the biggest difference occurs between these “polynomial orders” and
“exponential orders,” such as 2m.
For instance, on a computer that does 109 operations per second, a problem of size
m ϭ 50 will take 0.3 sec with an algorithm that requires m 5 operations, but 13 days with
an algorithm that requires 2m operations. But this is not our only reason for regarding
polynomial orders as good and exponential orders as bad. Another reason is the gain in
using a faster computer. For example, let two algorithms be O(m) and O(m 2). Then, since
1000 ϭ 31.62, an increase in speed by a factor 1000 has the effect that per hour we can
do problems 1000 and 31.6 times as big, respectively. But since 1000 ϭ 29.97, with an
algorithm that is O(2m), all we gain is a relatively modest increase of 10 in problem size
because 29.97 ؒ 2m ϭ 2mϩ9.97.

c23-a.qxd

11/3/10

3:34 PM

Page 979

SEC. 23.2 Shortest Path Problems. Complexity

979

The symbol O is quite practical and commonly used whenever the order of growth is
essential, but not the specific form of a function. Thus if a function g(m) is of the form
g(m) ϭ kh(m) ϩ more slowly growing terms

(k

0, constant),

we say that g(m) is of the order h(m) and write
g(m) ϭ O(h(m)).
For instance,
am ϩ b ϭ O(m),

am 2 ϩ bm ϩ d ϭ O(m 2),

5 ؒ 2m ϩ 3m 2 ϭ O(2m).

We want an algorithm Ꮽ to be “efficient,” that is, “good” with respect to
(i) Time (number cᏭ(m) of computer operations), or
(ii) Space (storage needed in the internal memory)
or both. Here cᏭ suggests “complexity” of Ꮽ. Two popular choices for cᏭ are
(Worst case) cᏭ(m) ϭ longest time Ꮽ takes for a problem of size m,
(Average case) cᏭ(m) ϭ average time Ꮽ takes for a problem of size m.
In problems on graphs, the “size” will often be m (number of edges) or n (number of
vertices). For Moore’s algorithm, cᏭ(m) ϭ 2m in both cases. Hence the complexity of
Moore’s algorithm is of order O(m).
For a “good” algorithm Ꮽ, we want that cᏭ(m) does not grow too fast. Accordingly,
we call Ꮽ efficient if cᏭ(m) ϭ O(m k) for some integer k м 0; that is, cᏭ may contain
only powers of m (or functions that grow even more slowly, such as ln m), but no
exponential functions. Furthermore, we call Ꮽ polynomially bounded if Ꮽ is efficient
when we choose the “worst case” cᏭ(m). These conventional concepts have intuitive
appeal, as our discussion shows.
Complexity should be investigated for every algorithm, so that one can also compare
different algorithms for the same task. This may often exceed the level in this chapter;
accordingly, we shall confine ourselves to a few occasional comments in this direction.

PROBLEM SET 23.2
SHORTEST PATHS, MOORE’S BFS

3.

(All edges length one)

s

1–4
Find a shortest path P: s : t and its length by
Moore’s algorithm. Sketch the graph with the labels and
indicate P by heavier lines as in Fig. 482.
1.

t

t

2.

s

s
t

4.

s
t

5. Moore’s algorithm. Show that if vertex v has label
l(v) ϭ k, then there is a path s : v of length k.
6. Maximum length. What is the maximum number of
edges that a shortest path between any two vertices in
a graph with n vertices can have? Give a reason. In a
complete graph with all edges of length 1?

c23-a.qxd

11/3/10

3:34 PM

980

Page 980

CHAP. 23 Graphs. Combinatorial Optimization

7. Nonuniqueness. Find another shortest path from s to
t in Example 1 of the text.
8. Moore’s algorithm. Call the length of a shortest path
s : v the distance of v from s. Show that if v has
distance l, it has label l(v) ϭ l.
9. CAS PROBLEM. Moore’s Algorithm. Write a
computer program for the algorithm in Table 23.1. Test
the program with the graph in Example 1. Apply it to
Probs. 1–3 and to some graphs of your own choice.
10–12

HAMILTONIAN CYCLE

10. Find and sketch a Hamiltonian cycle in the graph of a
dodecahedron, which has 12 pentagonal faces and 20
vertices (Fig. 483). This is a problem Hamilton himself
considered.

2

1

2
s 1

5

3
4

3

4

4

2

5

6

Fig. 484. Problem 13
14. Show that the length of a shortest postman trail is the
same for every starting vertex.
15–17

EULER GRAPHS

15. An Euler graph G is a graph that has a closed Euler
trail. An Euler trail is a trail that contains every edge
of G exactly once. Which subgraph with four edges of
the graph in Example 1, Sec. 23.1, is an Euler graph?
16. Find four different closed Euler trails in Fig. 485.
2

1

4

3

5

Fig. 483. Problem 10

Fig. 485. Problem 16

11. Find and sketch a Hamiltonian cycle in Prob. 1.
12. Does the graph in Prob. 4 have a Hamiltonian cycle?

17. Is the graph in Fig. 484 an Euler graph. Give reason.

13–14

POSTMAN PROBLEM

13. The postman problem is the problem of finding a
closed walk W: s : s (s the post office) in a graph G
with edges (i, j) of length l ij Ͼ 0 such that every edge
of G is traversed at least once and the length of W is
minimum. Find a solution for the graph in Fig. 484 by
inspection. (The problem is also called the Chinese
postman problem since it was published in the journal
Chinese Mathematics 1 (1962), 273–277.)

23.3

18–20

ORDER

18. Show that O(m 3) ϩ O(m 3) ϭ O(m 3) and kO(m p) ϭ
O(m p).
19. Show that 21 ϩ m 2 ϭ O(m), 0.02em ϩ 100m 2 ϭ
O(em).
20. If we switch from one computer to another that is 100
times as fast, what is our gain in problem size per hour
in the use of an algorithm that is O(m), O(m 2), O(m 5),
O(em)?

Bellman’s Principle. Dijkstra’s Algorithm
We continue our discussion of the shortest path problem in a graph G. The last section
concerned the special case that all edges had length 1. But in most applications the edges
(i, j) will have any lengths l ij Ͼ 0, and we now turn to this general case, which is of
greater practical importance. We write l ij ϭ ϱ for any edge (i, j) that does not exist in G
(setting ϱ ϩ a ϭ ϱ for any number a, as usual).
We consider the problem of finding shortest paths from a given vertex, denoted by 1
and called the origin, to all other vertices 2, 3, Á , n of G. We let L j denote the length
of a shortest path Pj: 1 : j in G.

c23-a.qxd

11/3/10

3:34 PM

Page 981

SEC. 23.3 Bellman’s Principle. Dijkstra’s Algorithm

981

Bellman’s Minimality Principle or Optimality Principle3

THEOREM 1

If Pj: 1 : j is a shortest path from 1 to j in G and (i, j) is the last edge of Pj (Fig. 486),
then Pi: 1 : i [obtained by dropping (i, j) from Pj] is a shortest path 1 : i.
Pi
i

1

j

Fig. 486. Paths P and Pi in Bellman’s minimality principle

PROOF

Suppose that the conclusion is false. Then there is a path P*i : 1 : i that is shorter than
Pi. Hence, if we now add (i, j) to P*i , we get a path 1 : j that is shorter than Pj. This
contradicts our assumption that Pj is shortest.
᭿
From Bellman’s principle we can derive basic equations as follows. For fixed j we may
obtain various paths 1 : j by taking shortest paths Pi for various i for which there is in
G an edge (i, j), and add (i, j) to the corresponding Pi. These paths obviously have lengths
L i ϩ l ij (L i ϭ length of Pi). We can now take the minimum over i, that is, pick an i for
which L i ϩ l ij is smallest. By the Bellman principle, this gives a shortest path 1 : j. It
has the length
L1 ϭ 0
(1)

L j ϭ min (L i ϩ l ij),
i

j ϭ 2, Á , n.

j

These are the Bellman equations. Since l ii ϭ 0 by definition, instead of mini j we can
simply write mini. These equations suggest the idea of one of the best-known algorithms
for the shortest path problem, as follows.

Dijkstra’s Algorithm for Shortest Paths
Dijkstra’s4 algorithm is shown in Table 23.2, where a connected graph G is a graph in
which, for any two vertices v and w in G, there is a path v : w. The algorithm is a
labeling procedure. At each stage of the computation, each vertex v gets a label, either
(PL) a permanent label ϭ length L v of a shortest path 1 : v
or
(TL) a temporary label ϭ upper bound Lෂv for the length of a shortest path 1 : v.

3

RICHARD BELLMAN (1920–1984), American mathematician, known for his work in dynamic programming.
EDSGER WYBE DIJKSTRA (1930–2002), Dutch computer scientist, 1972 recipient of the ACM Turing
Award. His algorithm appeared in Numerische Mathematik 1 (1959), 269–271.
4

c23-a.qxd

11/3/10

3:34 PM

982

Page 982

CHAP. 23 Graphs. Combinatorial Optimization

We denote by ᏼᏸ and ᐀ᏸ the sets of vertices with a permanent label and with a temporary
label, respectively. The algorithm has an initial step in which vertex 1 gets the permanent
label L 1 ϭ 0 and the other vertices get temporary labels, and then the algorithm alternates
between Steps 2 and 3. In Step 2 the idea is to pick k “minimally.” In Step 3 the idea is
that the upper bounds will in general improve (decrease) and must be updated accordingly.
Namely, the new temporary label Lෂj of vertex j will be the old one if there is no
improvement or it will be L k ϩ l kj if there is.
Table 23.2 Dijkstra’s Algorithm for Shortest Paths

ALGORITHM DIJKSTRA [G ϭ (V, E), V ϭ {1, • • • , n}, l ij for all (i, j) in E ]
Given a connected graph G ϭ (V, E) with vertices 1, • • • , n and edges (i, j) having
lengths l ij Ͼ 0, this algorithm determines the lengths of shortest paths from vertex 1 to
the vertices 2, • • • , n.
INPUT: Number of vertices n, edges (i, j), and lengths l i j
OUTPUT: Lengths Lj of shortest paths 1 * j, j ϭ 2, • • • , n
1. Initial step
Vertex 1 gets PL: L1 ϭ 0.
ෂ
Vertex j (ϭ 2, • • • , n) gets TL: Lj ϭ l 1j (ϭ ϱ if there is no edge (1, j) in G).
Set ᏼᏸ ϭ {1}, ᐀ᏸ ϭ {2, 3, • • • , n}.
2. Fixing a permanent label
ෂ
ෂ
Find a k in ᐀ᏸ for which Lk is miminum, set Lk ϭ Lk. Take the smallest k if
there are several. Delete k from ᐀ᏸ and include it in ᏼᏸ.
If ᐀ᏸ ϭ л (that is, ᐀ᏸ is empty) then
OUTPUT L 2, • • • , Ln. Stop
Else continue (that is, go to Step 3).
3. Updating temporary labels
ෂ
ෂ
ෂ
For all j in ᐀ᏸ, set L j ϭ mink {L j, Lk ϩ lk j} (that is, take the smaller of L j and
ෂ
Lk ϩ lk j as your new L j).
Go to Step 2.
End DIJKSTRA

EXAMPLE 1

Application of Dijkstra’s Algorithm
Applying Dijkstra’s algorithm to the graph in Fig. 487a, find shortest paths from vertex 1 to vertices 2, 3, 4.

Solution. We list the steps and computations.
1. L 1 ϭ 0, ෂ
L2 ϭ 8, ෂ
L3 ϭ 5, ෂ
L4 ϭ 7,
ෂ
ෂ
ෂ
2. L ϭ min {L , L , L } ϭ 5, k ϭ 3,

ᏼᏸ ϭ {1},

᐀ᏸ ϭ {2, 3, 4}

3. ෂ
L2 ϭ min {8, L 3 ϩ l 32} ϭ min {8, 5 ϩ 1} ϭ 6
ෂ
L4 ϭ min {7, L 3 ϩ l 34} ϭ min {7, ϱ} ϭ 7
ෂ ,ෂ
2. L ϭ min {L
L } ϭ min {6, 7} ϭ 6, k ϭ 2,

ᏼᏸ ϭ {1, 3},

᐀ᏸ ϭ {2, 4}

ᏼᏸ ϭ {1, 2, 3},

᐀ᏸ ϭ {4}

2. L 4 ϭ 7, k ϭ 4

ᏼᏸ ϭ {1, 2, 3, 4},

᐀ᏸ ϭ л.

3

2

3

2

2

4

4

3. ෂ
L4 ϭ min {7, L 2 ϩ l 24} ϭ min {7, 6 ϩ 2} ϭ 7

c23-a.qxd

11/3/10

3:34 PM

Page 983

SEC. 23.3 Bellman’s Principle. Dijkstra’s Algorithm

983

᭿

Figure 487b shows the resulting shortest paths, of lengths L 2 ϭ 6, L 3 ϭ 5, L 4 ϭ 7.

8

1

2

5

1
2

1

3

7

2

5
4

1

3

(a) Given graph G

7

4

(b) Shortest paths in G

Fig. 487. Example 1

Complexity.

Dijkstra’s algorithm is O(n 2).

Step 2 requires comparison of elements, first n Ϫ 2, the next time n Ϫ 3, etc., a total
of (n Ϫ 2)(n Ϫ 1)>2. Step 3 requires the same number of comparisons, a total of
(n Ϫ 2)(n Ϫ 1)>2, as well as additions, first n Ϫ 2, the next time n Ϫ 3, etc., again a total of
(n Ϫ 2)(n Ϫ )>2. Hence the total number of operations is 3(n Ϫ 2)(n Ϫ 1)>2 ϭ O(n 2). ᭿

PROOF

PROBLEM SET 23.3
1. The net of roads in Fig. 488 connecting four villages
is to be reduced to minimum length, but so that one
can still reach every village from every other village.
Which of the roads should be retained? Find the
solution (a) by inspection, (b) by Dijkstra’s algorithm.

5.

3

3

5

1

1

28
2

16

8

6.

20

5

1

5

7

3

6
3

2

Fig. 488. Problem 1
2. Show that in Dijkstra’s algorithm, for L k there is a path
P: 1 : k of length L k.
3. Show that in Dijkstra’s algorithm, at each instant the
demand on storage is light (data for fewer than n edges).

4

8

4

36

2

7.

1

10

17

2

6

3

DIJKSTRA’S ALGORITHM

4–9

For each graph find the shortest paths.
4.
4
5
2

10

4

9
3

13
2

15
1

3
6

1

5

3

40

1

2

4
4

12

2

2

5

3

2

4

c23-b.qxd

11/3/10

4:07 PM

Page 984

984

CHAP. 23 Graphs. Combinatorial Optimization

8.

8

1
10
3

9.

2

2
2

5

6

5

6
5

23.4

1

8

2

10

2

2

4
15

4

1
3

5

1
6

6

3

3

5

6

4

Shortest Spanning Trees: Greedy Algorithm
So far we have discussed shortest path problems. We now turn to a particularly important
kind of graph, called a tree, along with related optimization problems that arise quite
often in practice.
By definition, a tree T is a graph that is connected and has no cycles. “Connected”
was defined in Sec. 23.3; it means that there is a path from any vertex in T to any other
vertex in T. A cycle is a path s : t of at least three edges that is closed (t ϭ s); see also
Sec. 23.2. Figure 489a shows an example.
CAUTION! The terminology varies; cycles are sometimes also called circuits.
A spanning tree T in a given connected graph G ϭ (V, E ) is a tree containing all the
n vertices of G. See Fig. 489b. Such a tree has n Ϫ 1 edges. (Proof?)
A shortest spanning tree T in a connected graph G (whose edges (i, j) have lengths
l ij Ͼ 0) is a spanning tree for which Sl ij (sum over all edges of T ) is minimum compared
to Sl ij for any other spanning tree in G.

(a)

(b)

Fig. 489. Example of (a) a cycle, (b) a spanning tree in a graph

Trees are among the most important types of graphs, and they occur in various
applications. Familiar examples are family trees and organization charts. Trees can be
used to exhibit, organize, or analyze electrical networks, producer–consumer and other
business relations, information in database systems, syntactic structure of computer
programs, etc. We mention a few specific applications that need no lengthy additional
explanations.
The set of shortest paths from vertex 1 to the vertices 2, Á , n in the last section forms
a spanning tree.
Railway lines connecting a number of cities (the vertices) can be set up in the form of
a spanning tree, the “length” of a line (edge) being the construction cost, and one wants
to minimize the total construction cost. Similarly for bus lines, where “length” may be

c23-b.qxd

11/3/10

4:07 PM

Page 985

SEC. 23.4 Shortest Spanning Trees: Greedy Algorithm

985

the average annual operating cost. Or for steamship lines (freight lines), where “length”
may be profit and the goal is the maximization of total profit. Or in a network of telephone
lines between some cities, a shortest spanning tree may simply represent a selection of
lines that connect all the cities at minimal cost. In addition to these examples we could
mention others from distribution networks, and so on.
We shall now discuss a simple algorithm for the problem of finding a shortest spanning
tree. This algorithm (Table 23.3) is particularly suitable for sparse graphs (graphs with
very few edges; see Sec. 23.1).
Table 23.3 Kruskal’s5 Greedy Algorithm for Shortest Spanning Trees
Proceedings of the American Mathematical Society 7 (1956), 48–50.

ALGORITHM KRUSKAL [G ϭ (V, E), lij for all (i, j) in E]
Given a connected graph G ϭ (V, E) with vertices 1, 2, • • • , n and edges (i, j) having
length l ij Ͼ 0, the algorithm determines a shortest spanning tree T in G.
INPUT: Edges (i, j) of G and their lengths l ij
OUTPUT: Shortest spanning tree T in G
1. Order the edges of G in ascending order of length.
2. Choose them in this order as edges of T, rejecting an edge only if it forms a
cycle with edges already chosen.
If n Ϫ 1 edges have been chosen, then
OUTPUT T (ϭ the set of edges chosen). Stop
End KRUSKAL

EXAMPLE 1

Application of Kruskal’s Algorithm
Using Kruskal’s algorithm, we shall determine a shortest spanning tree in the graph in Fig. 490.
2

1
4

Table 23.4 Solution in Example 1
2

7

3
1
6

Edge

11
8

9

4
6
5

Fig. 490. Graph in Example 1

(3,
(1,
(1,
(4,
(2,
(3,
(5,
(2,

6)
2)
3)
5)
3)
4)
6)
4)

Length

Choice

1
2
4
6
7
8
9
11

1st
2nd
3rd
4th
Reject
5th

Solution. See Table 23.4. In some of the intermediate stages the edges chosen form a disconnected graph
(see Fig. 491); this is typical. We stop after n Ϫ 1 ϭ 5 choices since a spanning tree has n Ϫ 1 edges. In our
problem the edges chosen are in the upper part of the list. This is typical of problems of any size; in general,
᭿
edges farther down in the list have a smaller chance of being chosen.
5

JOSEPH BERNARD KRUSKAL (1928– ), American mathematician who worked at Bell Laboratories.
He is known for his contributions to graph theory and statistics.

c23-b.qxd

11/3/10

4:07 PM

Page 986

986

CHAP. 23 Graphs. Combinatorial Optimization

The efficiency of Kruskal’s method is greatly increased by double labeling of
vertices.
Double Labeling of Vertices.

Each vertex i carries a double label (ri, pi), where

ri ϭ Root of the subtree to which i belongs,
pi ϭ Predecessor of i in its subtree,
pi ϭ 0 for roots.
This simplifies rejecting.
Rejecting. If (i, j) is next in the list to be considered, reject (i, j) if ri ϭ rj (that is, i and
j are in the same subtree, so that they are already joined by edges and (i, j) would thus
create a cycle). If ri rj, include (i, j) in T.
If there are several choices for ri, choose the smallest. If subtrees merge (become a
single tree), retain the smallest root as the root of the new subtree.
For Example 1 the double-label list is shown in Table 23.5. In storing it, at each instant
one may retain only the latest double label. We show all double labels in order to exhibit
the process in all its stages. Labels that remain unchanged are not listed again.
Underscored are the two 1’s that are the common root of vertices 2 and 3, the reason for
rejecting the edge (2, 3). By reading for each vertex the latest label we can read from
this list that 1 is the vertex we have chosen as a root and the tree is as shown in the last
part of Fig. 491.

1

2

3

1
3

4

6

3

4

5

First

Second

Third

Fourth

Fifth

Fig. 491. Choice process in Example 1

Table 23.5 List of Double Labels in Example 1
Vertex
1
2
3
4
5
6

Choice 1
(3, 6)

Choice 2
(1, 2)

Choice 3
(1, 3)

Choice 4
(4, 5)

Choice 5
(3, 4)

(4, 0)
(4, 4)

(1, 3)
(1, 4)

(1, 0)
(1, 1)
(3, 0)

(3, 3)

(1, 1)

(1, 3)

c23-b.qxd

11/3/10

4:07 PM

Page 987

SEC. 23.4 Shortest Spanning Trees: Greedy Algorithm

987

This is made possible by the predecessor label that each vertex carries. Also, for accepting
or rejecting an edge we have to make only one comparison (the roots of the two endpoints
of the edge).
Ordering is the more expensive part of the algorithm. It is a standard process in
data processing for which various methods have been suggested (see Sorting in Ref.
[E25] listed in App. 1). For a complete list of m edges, an algorithm would be
O(m log 2 m), but since the n Ϫ 1 edges of the tree are most likely to be found earlier,
by inspecting the q (Ͻ m) topmost edges, for such a list of q edges one would have
O(q log 2 m).

PROBLEM SET 23.4
KRUSKAL’S GREEDY ALGORITHM

1–6

5.

1.

2

2

5

6.

5

11
4

2

3

6

30
10
5

3

4

12

2

6

4
5

6

7

5

6

2

2

8

3

20
2
4

3

1

11. Apply the algorithm in Prob. 10 to the graph in
Example 1. Compare with the result in Example 1.

7
5

8

13

10. Design an algorithm for obtaining longest spanning
trees.

4

1

7

3

9. Apply the method suggested in Prob. 8 to the graph in
Example 1. Do you get the same tree?

8
2

5

5
12

8. To get a minimum spanning tree, instead of adding
shortest edges, one could think of deleting longest
edges. For what graphs would this be feasible?
Describe an algorithm for this.

1

1

3

4

3

7. CAS PROBLEM. Kruskal’s Algorithm. Write a
corresponding program. (Sorting is discussed in Ref.
[E25] listed in App. 1.)

2
6

6

4.

2

7

10

9
6
12

3

8

3.

3

2

8

3

20

5

4

1

1

4

3

20

2
4

6

2

7

8

2.

7
2
8

2

4

3

1

1
5

Find a shortest spanning tree by Kruskal’s algorithm.
Sketch it.

4

5

12. Forest. A (not necessarily connected) graph without
cycles is called a forest. Give typical examples of
applications in which graphs occur that are forests or
trees.

c23-b.qxd

11/3/10

4:07 PM

Page 988

988

CHAP. 23 Graphs. Combinatorial Optimization

Chicago
Dallas
Denver
Los Angeles
New York

Dallas

Denver

Los Angeles

New York

Washington, DC

800

900
650

1800
1300
850

700
1350
1650
2500

650
1200
1500
2350
200

13. Air cargo. Find a shortest spanning tree in the
complete graph of all possible 15 connections between
the six cities given (distances by airplane, in miles,
rounded). Can you think of a practical application of
the result?
14–20

GENERAL PROPERTIES OF TREES

16. If a graph has no cycles, it must have at least 2 vertices
of degree 1 (definition in Sec. 23.1).
17. A tree with exactly two vertices of degree 1 must be a
path.
18. A tree with n vertices has n Ϫ 1 edges. (Proof by
induction.)

Prove the following. Hint. Use Prob. 14 in proving 15 and
18; use Probs. 16 and 18 in proving 20.

19. If two vertices in a tree are joined by a new edge, a
cycle is formed.

14. Uniqueness. The path connecting any two vertices u
and v in a tree is unique.

20. A graph with n vertices is a tree if and only if it has
n Ϫ 1 edges and has no cycles.

15. If in a graph any two vertices are connected by a unique
path, the graph is a tree.

23.5

Shortest Spanning Trees:
Prim’s Algorithm
Prim’s6 algorithm, shown in Table 23.6, is another popular algorithm for the shortest
spanning tree problem (see Sec. 23.4). This algorithm avoids ordering edges and gives a
tree T at each stage, a property that Kruskal’s algorithm in the last section did not have
(look back at Fig. 491 if you did not notice it).
In Prim’s algorithm, starting from any single vertex, which we call 1, we “grow” the
tree T by adding edges to it, one at a time, according to some rule (in Table 23.6) until
T finally becomes a spanning tree, which is shortest.
We denote by U the set of vertices of the growing tree T and by S the set of its edges.
Thus, initially U ϭ {1} and S ϭ л; at the end, U ϭ V, the vertex set of the given graph
G ϭ (V, E), whose edges (i, j) have length l ij Ͼ 0, as before.

6

ROBERT CLAY PRIM (1921– ), American computer scientist at General Electric, Bell Laboratories, and
Sandia National Laboratories.

c23-b.qxd

11/3/10

4:07 PM

Page 989

SEC. 23.5 Shortest Spanning Trees: Prim’s Algorithm

989

Thus at the beginning (Step 1) the labels
l2, Á , ln

of the vertices

2, Á , n

are the lengths of the edges connecting them to vertex 1 (or ϱ if there is no such edge in
G). And we pick (Step 2) the shortest of these as the first edge of the growing tree T and
include its other end j in U (choosing the smallest j if there are several, to make the process
unique). Updating labels in Step 3 (at this stage and at any later stage) concerns each
vertex k not yet in U. Vertex k has label lk ϭ l i(k),k from before. If l jk Ͻ lk, this means
that k is closer to the new member j just included in U than k is to its old “closest neighbor”
i(k) in U. Then we update the label of k, replacing lk ϭ l i(k),k by lk ϭ l jk and setting
i(k) ϭ j. If, however, l jk м lk (the old label of k), we don’t touch the old label. Thus the
label lk always identifies the closest neighbor of k in U, and this is updated in Step 3 as
U and the tree T grow. From the final labels we can backtrack the final tree, and from their
numeric values we compute the total length (sum of the lengths of the edges) of this tree.
Prim’s algorithm is useful for computer network design, cable, distribution networks,
and transportation networks.
Table 23.6 Prim’s Algorithm for Shortest Spanning Trees
Bell System Technical Journal 36 (1957), 1389–1401.
For an improved version of the algorithm, see Cheriton and Tarjan, SIAM Journal on Computation 5
(1976), 724–742.

ALGORITHM PRIM [G ϭ (V, E), V ϭ {1, • • • , n}, li j for all (i, j) in E]
Given a connected graph G ϭ (V, E) with vertices 1, 2, • • • , n and edges (i, j ) having
length l i j Ͼ 0, this algorithm determines a shortest spanning tree T in G and its length
L(T).
INPUT: n, edges (i, j) of G and their lengths l ij
OUTPUT: Edge set S of a shortest spanning tree T in G; L(T )
[Initially, all vertices are unlabeled.]
1. Initial step
Set i(k) ϭ 1, U ϭ {1}, S ϭ л.
Label vertex k (ϭ 2, • • • , n) with ␭k ϭ l ik [ϭ ϱ if G has no edge (1, k)].
2. Addition of an edge to the tree T
Let ␭ j be the smallest ␭k for vertex k not in U. Include vertex j in U and edge
(i( j), j) in S.
If U ϭ V then compute
L(T) ϭ ͚ l ij (sum over all edges in S)
OUTPUT S, L(T ). Stop
[S is the edge set of a shortest spanning tree T in G.]
Else continue (that is, go to Step 3).
3. Label updating
For every k not in U, if l jk Ͻ ␭k, then set ␭k ϭ l jk and i(k) ϭ j.
Go to Step 2.
End PRIM

c23-b.qxd

11/3/10

4:07 PM

990

Page 990

CHAP. 23 Graphs. Combinatorial Optimization

EXAMPLE 1
2

1
4

Application of Prim’s Algorithm
Find a shortest spanning tree in the graph in Fig. 492 (which is the same as in Example 1, Sec. 23.4,
so that we can compare).

2

7

11
8

3
1
6

9

Solution. The steps are as follows.
4

1. i(k) ϭ 1, U ϭ {1}, S ϭ л, initial labels see Table 23.7.
2. ␭2 ϭ l12 ϭ 2 is smallest, U ϭ {1, 2}, S ϭ {(1, 2)}.

6
5

3. Update labels as shown in Table 23.7, column (I).
2. ␭3 ϭ l13 ϭ 4 is smallest, U ϭ {1, 2, 3}, S ϭ {(1, 2), (1, 3)}.

Fig. 492. Graph in
Example 1

3. Update labels as shown in Table 23.7, column (II).
2. ␭6 ϭ l36 ϭ 1 is smallest, U ϭ {1, 2, 3, 6}, S ϭ {(1, 2), (1, 3), (3, 6)}.
3. Update labels as shown in Table 23.7, column (III).
2. ␭4 ϭ l34 ϭ 8 is smallest, U ϭ {1, 2, 3, 4, 6}, S ϭ {(1, 2), (1, 3), (3, 4), (3, 6)}.
3. Update labels as shown in Table 23.7, column (IV).
2. ␭5 ϭ l45 ϭ 6 is smallest, U ϭ V, S ϭ (1, 2), (1, 3), (3, 4), (3, 6), (4, 5). Stop.
The tree is the same as in Example 1, Sec. 23.4. Its length is 21. You will find it interesting to
᭿
compare the growth process of the present tree with that in Sec. 23.4.

Table 23.7 Labeling of Vertices in Example 1
Relabeling

Vertex

Initial
Label

(I)

(II)

(III)

(IV)

2
3
4
5
6

l 12 ϭ 2
l 13 ϭ 4
ϱ
ϱ
ϱ

—
l 13 ϭ 4
l 24 ϭ 11
ϱ
ϱ

—
—
l 34 ϭ 8
ϱ
l 36 ϭ 1

—
—
l 34 ϭ 8
l 65 ϭ 9
—

—
—
—
l 45 ϭ 6
—

PROBLEM SET 23.5
SHORTEST SPANNING TREES. PRIM’S
ALGORITHM

6–13

Find a shortest spanning tree by Prim’s algorithm.

6.

6

2

1. When will S ϭ E at the end in Prim’s algorithm?

3

2. Complexity. Show that Prim’s algorithm has complexity O(n 2).

1
1

3

14

15
9

10

3. What is the result of applying Prim’s algorithm to a
graph that is not connected?
4. If for a complete graph (or one with very few edges
missing), our data is an n ϫ n distance table (as in Prob.
13, Sec. 23.4), show that the present algorithm [which
is O(n 2)] cannot easily be replaced by an algorithm of
order less than O(n 2).
5. How does Prim’s algorithm prevent the generation of
cycles as you grow T ?

4

7.

20

5

4
2

6

12
3
12
6

1

4

14
8

5

2

6
2

c23-b.qxd

11/3/10

4:07 PM

Page 991

SEC. 23.6 Flows in Networks
8.

8

20

8

7

991
6

6

6
7

8

7

2
1

10
1

2

3

9.

4

8

3

5
5

3

3

4

1

6
4
10

5

2

14

16

4

4

(b) Diameter, Radius, Center. The diameter d(G)
of a graph G ϭ (V, E) is the maximum of d(u, v) as u
and v vary over V, and the radius r(G) is the smallest
eccentricity P(v) of the vertices v. A vertex v with
P(v) ϭ r (G) is called a central vertex. The set of all
central vertices is called the center of G. Find
d(G), r (G), and the center of the graph in Prob. 7.
(c) What are the diameter, radius, and center of the
spanning tree in Example 1 of the text?

2

10. For the graph in Prob. 6, Sec. 23.4.
11. For the graph in Prob. 4, Sec. 23.4.
12. For the graph in Prob. 2, Sec. 23.4.
13. CAS PROBLEM. Prim’s Algorithm. Write a program
and apply it to Probs. 6–9.
14. TEAM PROJECT. Center of a Graph and Related
Concepts. (a) Distance, Eccentricity. Call the length
of a shortest path u : v in a graph G ϭ (V, E) the

23.6

distance d(u, v) from u to v. For fixed u, call the
greatest d(u, v) as v ranges over V the eccentricity P(u)
of u. Find the eccentricity of vertices 1, 2, 3 in the
graph in Prob. 7.

(d) Explain how the idea of a center can be used in setting
up an emergency service facility on a transportation
network. In setting up a fire station, a shopping center.
How would you generalize the concepts in the case of two
or more such facilities?
(e) Show that a tree T whose edges all have length 1
has center consisting of either one vertex or two
adjacent vertices.
(f) Set up an algorithm of complexity O(n) for finding
the center of a tree T.

Flows in Networks
After shortest path problems and problems for trees, as a third large area in combinatorial
optimization we discuss flow problems in networks (electrical, water, communication,
traffic, business connections, etc.), turning from graphs to digraphs (directed graphs; see
Sec. 23.1).
By definition, a network is a digraph G ϭ (V, E) in which each edge (i, j) has assigned
to it a capacity cij Ͼ 0 [ϭ maximum possible flow along (i, j)], and at one vertex, s,
called the source, a flow is produced that flows along the edges of the digraph G to another
vertex, t, called the target or sink, where the flow disappears.
In applications, this may be the flow of electricity in wires, of water in pipes, of cars
on roads, of people in a public transportation system, of goods from a producer to
consumers, of e-mail from senders to recipients over the Internet, and so on.
We denote the flow along a (directed!) edge (i, j) by fij and impose two conditions:
1. For each edge (i, j) in G the flow does not exceed the capacity cij,
(1)

0 Ϲ fij Ϲ cij

(“Edge condition”).

2. For each vertex i, not s or t,
Inflow ϭ Outflow

(“Vertex condition,” “Kirchhoff’s law”);

Page 992

CHAP. 23 Graphs. Combinatorial Optimization

in a formula,
0 if vertex i

s, i

t,

a fki Ϫ a fij ϭ dϪf at the source s,

(2)

k

j

{

992

4:07 PM

{

11/3/10

Inflow

f at the target (sink) t,

Outflow

where f is the total flow (and at s the inflow is zero, whereas at t the outflow is zero).
Figure 493 illustrates the notation (for some hypothetical figures).
f i3 = 1

f1i = 7

1

3

i
f 2i =

2

2

f

i5

=3

fi8 = 5

8

5

Fig. 493. Notation in (2): inflow and outflow for a vertex i (not s or t)

Paths
By a path v1 : vk from a vertex v1 to a vertex vk in a digraph G we mean a sequence
of edges
(v1, v2), (v2, v3), Á , (vk؊1, vk),
regardless of their directions in G, that forms a path as in a graph (see Sec. 23.2). Hence
when we travel along this path from v1 to vk we may traverse some edge in its given
direction—then we call it a forward edge of our path—or opposite to its given direction—
then we call it a backward edge of our path. In other words, our path consists of one-way
streets, and forward edges (backward edges) are those that we travel in the right direction
(in the wrong direction). Figure 494 shows a forward edge (u, v) and a backward edge (w, v)
of a path v1 : vk.
CAUTION! Each edge in a network has a given direction, which we cannot change.
Accordingly, if (u, v) is a forward edge in a path v1 : vk, then (u, v) can become a
backward edge only in another path x 1 : x j in which it is an edge and is traversed in the
opposite direction as one goes from x 1 to x j; see Fig. 495. Keep this in mind, to avoid
misunderstandings.
u

v1

w
v1

...

u

xj

...

...

c23-b.qxd

v

vk

Fig. 494. Forward edge (u, v) and
backward edge (w, v) of a path v1 * vk

x1

...

v

...

vk

Fig. 495. Edge (u, v) as forward edge in the path
v1 * vk and as backward edge in the path x1 * xj

Flow Augmenting Paths
Our goal will be to maximize the flow from the source s to the target t of a given network.
We shall do this by developing methods for increasing an existing flow (including the
special case in which the latter is zero). The idea then is to find a path P: s : t all of

c23-b.qxd

11/3/10

4:07 PM

Page 993

SEC. 23.6 Flows in Networks

993

whose edges are not fully used, so that we can push additional flow through P. This
suggests the following concept.
DEFINITION

Flow Augmenting Path

A flow augmenting path in a network with a given flow fij on each edge (i, j) is a
path P: s : t such that
(i) no forward edge is used to capacity; thus fij Ͻ cij for these;
(ii) no backward edge has flow 0; thus fij Ͼ 0 for these.
EXAMPLE 1

Flow Augmenting Paths
Find flow augmenting paths in the network in Fig. 496, where the first number is the capacity and the second
number a given flow.
20,

5

11, 8

2

4,

s 1
10,

4

4

13,

3

3

6

5, 2
5

7, 4

6 t
3, 3

Fig. 496. Network in Example 1
First number ‫ ؍‬Capacity, Second number ‫ ؍‬Given flow

Solution. In practical problems, networks are large and one needs a systematic method for augmenting
flows, which we discuss in the next section. In our small network, which should help to illustrate and clarify
the concepts and ideas, we can find flow augmenting paths by inspection and augment the existing flow f ϭ 9
in Fig. 496. (The outflow from s is 5 ϩ 4 ϭ 9, which equals the inflow 6 ϩ 3 into t.)
We use the notation
¢ ij ϭ cij Ϫ fij

for forward edges

¢ ij ϭ fij

for backward edges

¢ ϭ min ¢ ij

taken over all edges of a path.

From Fig. 496 we see that a flow augmenting path P1: s : t is P1: 1 Ϫ 2 Ϫ 3 Ϫ 6 (Fig. 497), with
¢ 12 ϭ 20 Ϫ 5 ϭ 15, etc., and ¢ ϭ 3. Hence we can use P1 to increase the given flow 9 to f ϭ 9 ϩ 3 ϭ 12.
All three edges of P1 are forward edges. We augment the flow by 3. Then the flow in each of the edges of P1
is increased by 3, so that we now have f12 ϭ 8 (instead of 5), f23 ϭ 11 (instead of 8), and f36 ϭ 9 (instead of
6). Edge (2, 3) is now used to capacity. The flow in the other edges remains as before.
We shall now try to increase the flow in this network in Fig. 496 beyond f ϭ 12.
There is another flow augmenting path P2: s : t, namely, P2: 1 Ϫ 4 Ϫ 5 Ϫ 3 Ϫ 6 (Fig. 497). It shows how
a backward edge comes in and how it is handled. Edge (3, 5) is a backward edge. It has flow 2, so that ¢ 36 ϭ 2.
We compute ¢ 14 ϭ 10 Ϫ 4 ϭ 6, etc. (Fig. 497) and ¢ ϭ 2. Hence we can use P2 for another augmentation to
get f ϭ 12 ϩ 2 ϭ 14. The new flow is shown in Fig. 498. No further augmentation is possible. We shall confirm
later that f ϭ 14 is maximum.
᭿

Δ 12

= 15

2

Δ23 = 3

3

Δ

36

=7

s 1

Path P1
6 t

3
Δ35 = 2

s 1
Δ

14

=6

4

Δ45 = 3

Δ

36

=4

Path P2
6 t

5

Fig. 497. Flow augmenting paths in Example 1

c23-b.qxd

11/3/10

994

4:07 PM

Page 994

CHAP. 23 Graphs. Combinatorial Optimization

Cut Sets
A cut set is a set of edges in a network. The underlying idea is simple and natural. If we
want to find out what is flowing from s to t in a network, we may cut the network
somewhere between s and t (Fig. 498 shows an example) and see what is flowing in the
edges hit by the cut, because any flow from s to t must sometimes pass through some of
these edges. These form what is called a cut set. [In Fig. 498, the cut set consists of the
edges (2, 3), (5, 2), (4, 5).] We denote this cut set by (S, T ). Here S is the set of vertices
on that side of the cut on which s lies (S ϭ {s, 2, 4} for the cut in Fig. 498) and T is the
set of the other vertices (T ϭ {3, 5, t} in Fig. 498). We say that a cut partitions the vertex
set V into two parts S and T. Obviously, the corresponding cut set (S, T ) consists of all
the edges in the network with one end in S and the other end in T.
Cut
20,

8

2

4,

s 1
10,

6

11, 11

4

7, 6

13,

3

3

11

5, 0
5

6 t
3, 3

Fig. 498. Maximum flow in Example 1

By definition, the capacity cap (S, T) of a cut set (S, T ) is the sum of the capacities of
all forward edges in (S, T ) (forward edges only!), that is, the edges that are directed from
S to T,
(3)

cap (S, T ) ϭ Scij

[sum over the forward edges of (S, T )].

Thus, cap (S, T ) ϭ 11 ϩ 7 ϭ 18 in Fig. 498.

Explanation. This can be seen as follows. Look at Fig. 498. Recall that for each edge
in that figure, the first number denotes capacity and the second number flow. Intuitively,
you can think of the edges as roads, where the capacity of the road is how many cars can
actually be on the road, and the flow denotes how many cars actually are on the road. To
compute capacity cap (S, T ) we are only looking at the first number on the edges. Take
a look and see that the cut physically cuts three edges, that is, (2, 3), (4, 5), and (5, 2).
The cut concerns only forward edges that are being cut, so it concerns edges (2, 3) and
(4, 5) (and does not include edge (5, 2) which is also being cut, but since it goes backwards,
it does not count). Hence (2, 3) contributes 11 and (4, 5) contributes 7 to the capacity cap
(S, T), for a total of 18 in Fig. 498. Hence cap (S, T ) ϭ 18.
The other edges (directed from T to S) are called backward edges of the cut set (S, T ),
and by the net flow through a cut set we mean the sum of the flows in the forward edges
minus the sum of the flows in the backward edges of the cut set.
CAUTION! Distinguish well between forward and backward edges in a cut set and in
a path: (5, 2) in Fig. 498 is a backward edge for the cut shown but a forward edge in the
path 1 Ϫ 4 Ϫ 5 Ϫ 2 Ϫ 3 Ϫ 6.
For the cut in Fig. 498 the net flow is 11 ϩ 6 Ϫ 3 ϭ 14. For the same cut in Fig. 496
(not indicated there), the net flow is 8 ϩ 4 Ϫ 3 ϭ 9. In both cases it equals the flow f.

11/3/10

4:07 PM

Page 995

SEC. 23.6 Flows in Networks

995

We claim that this is not just by chance, but cuts do serve the purpose for which we have
introduced them:

THEOREM 1

Net Flow in Cut Sets

Any given flow in a network G is the net flow through any cut set (S, T ) of G.

By Kirchhoff’s law (2), multiplied by Ϫ1, at a vertex i we have
a fij Ϫ a fli ϭ b

(4)

j

Outflow

l

{

PROOF

{

c23-b.qxd

0

if i

s, t,

f

if i ϭ s.

Inflow

Here we can sum over j and l from 1 to n (ϭ number of vertices) by putting fij ϭ 0 for
j ϭ i and also for edges without flow or nonexisting edges; hence we can write the two
sums as one,
a ( fij Ϫ fji) ϭ b
j

0

if i

s, t,

f

if i ϭ s.

We now sum over all i in S. Since s is in S, this sum equals f :
a a ( fij Ϫ fji) ϭ f.

(5)

iʦS jʦV

We claim that in this sum, only the edges belonging to the cut set contribute. Indeed,
edges with both ends in T cannot contribute, since we sum only over i in S; but edges
(i, j) with both ends in S contribute ϩfij at one end and Ϫfij at the other, a total contribution
of 0. Hence the left side of (5) equals the net flow through the cut set. By (5), this is equal
᭿
to the flow f and proves the theorem.
This theorem has the following consequence, which we shall also need later in this
section.

THEOREM 2

Upper Bound for Flows

A flow f in a network G cannot exceed the capacity of any cut set (S, T ) in G.

PROOF

By Theorem 1 the flow f equals the net flow through the cut set, f ϭ f1 Ϫ f2, where f1
is the sum of the flows through the forward edges and f2 (м 0) is the sum of the flows
through the backward edges of the cut set. Thus f Ϲ f1. Now f1 cannot exceed the sum
of the capacities of the forward edges; but this sum equals the capacity of the cut set, by
definition. Together, f Ϲ cap (S, T ), as asserted.
᭿

c23-b.qxd

11/3/10

4:07 PM

996

Page 996

CHAP. 23 Graphs. Combinatorial Optimization

Cut sets will now bring out the full importance of augmenting paths:
THEOREM 3

Main Theorem. Augmenting Path Theorem for Flows

A flow from s to t in a network G is maximum if and only if there does not exist a
flow augmenting path s : t in G.

PROOF

(a) If there is a flow augmenting path P: s : t, we can use it to push through it an additional
flow. Hence the given flow cannot be maximum.
(b) On the other hand, suppose that there is no flow augmenting path s : t in G.
Let S0 be the set of all vertices i (including s) such that there is a flow augmenting
path s : i, and let T0 be the set of the other vertices in G. Consider any edge (i, j) with
i in S0 and j in T0. Then we have a flow augmenting path s : i since i is in S0, but
s : i : j is not flow augmenting because j is not in S0. Hence we must have

(6)

fij ϭ b

cij
0

if (i, j) is a b

forward
edge of the path s : i : j.
backward

Otherwise we could use (i, j) to get a flow augmenting path s : i : j. Now (S0, T0)
defines a cut set (since t is in T0; why?). Since by (6), forward edges are used to capacity
and backward edges carry no flow, the net flow through the cut set (S0, T0) equals the
sum of the capacities of the forward edges, which is cap (S0, T0) by definition. This
net flow equals the given flow f by Theorem 1. Thus f ϭ cap (S0, T0). We also have
f Ϲ cap (S0, T0) by Theorem 2. Hence f must be maximum since we have reached
᭿
equality.
The end of this proof yields another basic result (by Ford and Fulkerson, Canadian Journal
of Mathematics 8 (1956), 399–404), namely, the so-called
THEOREM 4

Max-Flow Min-Cut Theorem

The maximum flow in any network G equals the capacity of a “minimum cut set”
(ϭ a cut set of minimum capacity) in G.

PROOF

We have just seen that f ϭ cap (S0, T0) for a maximum flow f and a suitable cut set (S0, T0).
Now by Theorem 2 we also have f Ϲ cap (S, T ) for this f and any cut set (S, T) in G.
Together, cap (S0, T0) Ϲ cap (S, T ). Hence (S0, T0) is a minimum cut set.
The existence of a maximum flow in this theorem follows for rational capacities from
the algorithm in the next section and for arbitrary capacities from the Edmonds–Karp BFS
also in that section.
᭿
The two basic tools in connection with networks are flow augmenting paths and cut sets.
In the next section we show how flow augmenting paths can be used in an algorithm for
maximum flows.

c23-b.qxd

11/3/10

4:07 PM

Page 997

SEC. 23.6 Flows in Networks

997

PROBLEM SET 23.6
CUT SETS, CAPACITY

1–6

13.

Find T and cap (S, T ) for:

8,

1. Fig. 498, S ϭ {1, 2, 4, 5}

3

4, 2
6,

1

2

3

4. Fig. 499, S ϭ {1, 2}

14.

5. Fig. 499, S ϭ {1, 2, 4, 5}

7, 1

1
s
9, 4

s 1

6, 1
7,

5

4

6, 5

8, 5
4, 2
5

3

10,

4, 2

5, 2

4

10, 1
4, 2

3

3, 1
5
t

16, 6

8
7 t

2, 1
6

5 t

12, 3

2

8, 5

6. Fig. 498, S ϭ {1, 3, 5}

4

1

1
4,

3. Fig. 498, S ϭ {1, 2, 3}

8, 4

4,

s 1

2. Fig. 499, S ϭ {1, 2, 3}

2

10, 2

2

6,

15.

8, 5
5, 3

10, 3

1
s

Fig. 499. Problems 2, 4, and 5

2

4, 2

1

4

5
t

6, 0
3, 1

1, 1

3

MINIMUM CUT SET

7–8

Find a minimum cut set and its capacity for the network:

MAXIMUM FLOW

16–19

7. In Fig. 499
8. In Fig. 496. Verify that its capacity equals the maximum
flow.
9. Why are backward edges not considered in the
definition of the capacity of a cut set?

Find the maximum flow by inspection:
16. In Prob. 13
17.

10. Incremental network. Sketch the network in Fig. 499,
and on each edge (i, j) write cij Ϫ fij and fij. Do you
recognize that from this “incremental network” one can
more easily see flow augmenting paths?

s 1

6,

5, 2

FLOW AUGMENTING PATHS

12–15

3

3

11, 7

2, 2
5,

2

11. Omission of edges. Which edges could be omitted
from the network in Fig. 499 without decreasing the
maximum flow?

8, 5

4

4, 2

5

13, 9

2

6 t

4, 1

18. In Prob. 12
19.

s 1

10, 7

2

8, 4

4

Find flow augmenting paths:
12.
2, 1

1, 0

2

5, 3
4

2, 1

1
s

7, 1

6
t

4, 2
2, 1

3

1, 0

5

6, 2

8, 5

7, 4

8, 1
3

3, 1

5 t

8, 1

20. Find another maximum flow f ϭ 15 in Prob. 19.

c23-b.qxd

11/3/10

4:07 PM

998

23.7

Page 998

CHAP. 23 Graphs. Combinatorial Optimization

Maximum Flow: Ford–Fulkerson Algorithm
Flow augmenting paths, as discussed in the last section, are used as the basic tool in the
Ford–Fulkerson7 algorithm in Table 23.8 in which a given flow (for instance, zero flow in
all edges) is increased until it is maximum. The algorithm accomplishes the increase by a
stepwise construction of flow augmenting paths, one at a time, until no further such paths
can be constructed, which happens precisely when the flow is maximum.
In Step 1, an initial flow may be given. In Step 3, a vertex j can be labeled if there is
an edge (i, j) with i labeled and
cij Ͼ fij

(“forward edge”)

or if there is an edge ( j, i) with i labeled and
fji Ͼ 0

(“backward edge”).

To scan a labeled vertex i means to label every unlabeled vertex j adjacent to i that can be
labeled. Before scanning a labeled vertex i, scan all the vertices that got labeled before i.
This BFS (Breadth First Search) strategy was suggested by Edmonds and Karp in 1972
(Journal of the Association for Computing Machinery 19, 248–64). It has the effect that one
gets shortest possible augmenting paths.
Table 23.8 Ford–Fulkerson Algorithm for Maximum Flow
Canadian Journal of Mathematics 9 (1957), 210–218

ALGORITHM FORD–FULKERSON
[G ϭ (V, E ), vertices 1 (ϭ s), Á , n (ϭ t), edges (i, j), cij]
This algorithm computes the maximum flow in a network G with source s, sink t, and
capacities cij Ͼ 0 of the edges (i, j).
INPUT: n, s ϭ 1, t ϭ n, edges (i, j) of G, cij
OUTPUT: Maximum flow ƒ in G
1. Assign an initial flow fij (for instance, fij ϭ 0 for all edges), compute ƒ.
2. Label s by л. Mark the other vertices “unlabeled.”
3. Find a labeled vertex i that has not yet been scanned. Scan i as follows. For every
unlabeled adjacent vertex j, if cij Ͼ fij, compute
¢ ij ϭ cij Ϫ fij

and

¢j ϭ b

¢ ij

if i ϭ 1

min (¢ i, ¢ ij)

if i Ͼ 1

and label j with a “forward label” (i ϩ, ¢ j); or if fji Ͼ 0, compute
¢ j ϭ min (¢ i, fji)
and label j by a “backward label” (i ؊, ¢ j).

7
LESTER RANDOLPH FORD Jr. (1927– ) and DELBERT RAY FULKERSON (1924–1976), American
mathematicians known for their pioneering work on flow algorithms.

c23-b.qxd

11/3/10

4:07 PM

Page 999

SEC. 23.7 Maximum Flow: Ford–Fulkerson Algorithm

999

If no such j exists then OUTPUT ƒ. Stop
[ƒ is the maximum flow.]
Else continue (that is, go to Step 4).
4. Repeat Step 3 until t is reached.
[This gives a flow augmenting path P: s * t.]
If it is impossible to reach t then OUTPUT ƒ. Stop
[ƒ is the maximum flow.]
Else continue (that is, go to Step 5).
5. Backtrack the path P, using the labels.
6. Using P, augment the existing flow by ⌬ t. Set f ϭ f ϩ ¢ t.
7. Remove all labels from vertices 2, Á , n. Go to Step 3.
End FORD–FULKERSON

EXAMPLE 1

Ford–Fulkerson Algorithm
Applying the Ford–Fulkerson algorithm, determine the maximum flow for the network in Fig. 500 (which is
the same as that in Example 1, Sec. 23.6, so that we can compare).

Solution. The algorithm proceeds as follows.
1. An initial flow f ϭ 9 is given.
2. Label s (ϭ 1) by л. Mark 2, 3, 4, 5, 6 “unlabeled.”

5
20,

2

4,

s 1
10,

4

11, 8

4

3

3

7, 4

13,

6

5, 2
5

6 t
3, 3

Fig. 500. Network in Example 1 with capacities (first numbers) and given flow
3. Scan 1.
Compute ¢ 12 ϭ 20 Ϫ 5 ϭ 15 ϭ ¢ 2. Label 2 by (1ϩ, 15).
Compute ¢ 14 ϭ 10 Ϫ 4 ϭ 6 ϭ ¢ 4. Label 4 by (1ϩ, 6).
4. Scan 2.
Compute ¢ 23 ϭ 11 Ϫ 8 ϭ 3, ¢ 3 ϭ min (¢ 2, 3) ϭ 3. Label 3 by (2ϩ, 3).
Compute ¢ 5 ϭ min (¢ 2, 3) ϭ 3. Label 5 by (2؊, 3).
Scan 3.
Compute ¢ 36 ϭ 13 Ϫ 6 ϭ 7, ¢ 6 ϭ ¢ t ϭ min (¢ 3, 7) ϭ 3. Label 6 by (3ϩ, 3).
5. P: 1 Ϫ 2 Ϫ 3 Ϫ 6 (ϭ t) is a flow augmenting path.
6. ¢ t ϭ 3. Augmentation gives f12 ϭ 8, f23 ϭ 11, f36 ϭ 9, other fij unchanged. Augmented flow
f ϭ 9 ϩ 3 ϭ 12.
7. Remove labels on vertices 2, Á , 6. Go to Step 3.
3. Scan 1.
Compute ¢ 12 ϭ 20 Ϫ 8 ϭ 12 ϭ ¢ 2. Label 2 by (1ϩ, 12).
Compute ¢ 14 ϭ 10 Ϫ 4 ϭ 6 ϭ ¢ 4. Label 4 by (1ϩ, 6).

c23-b.qxd

11/3/10

4:07 PM

1000

Page 1000

CHAP. 23 Graphs. Combinatorial Optimization
4. Scan 2.
Compute ¢ 5 ϭ min (¢ 2, 3) ϭ 3. Label 5 by (2؊, 3).
Scan 4. [No vertex left for labeling.]
Scan 5.
Compute ¢ 3 ϭ min (¢ 5, 2) ϭ 2. Label 3 by (5؊, 2).
Scan 3.
Compute ¢ 36 ϭ 13 Ϫ 9 ϭ 4, ¢ 6 ϭ min (¢ 3, 4) ϭ 2. Label 6 by (3ϩ, 2).
5. P: 1 Ϫ 2 Ϫ 5 Ϫ 3 Ϫ 6 (ϭ t) is a flow augmenting path.
6. ¢ t ϭ 2. Augmentation gives f12 ϭ 10, f32 ϭ 1, f35 ϭ 0, f36 ϭ 11, other fij unchanged. Augmented
flow f ϭ 12 ϩ 2 ϭ 14.
7. Remove labels on vertices 2, Á , 6. Go to Step 3.
One can now scan 1 and then scan 2, as before, but in scanning 4 and then 5 one finds that no vertex is left for
labeling. Thus one can no longer reach t. Hence the flow obtained (Fig. 501) is maximum, in agreement with
our result in the last section.
᭿
10
20,

2

4,

s 1
10,

4

11, 11

4

7, 4

13,

3

1

11

5, 0
5

6 t
3, 3

Fig. 501. Maximum flow in Example 1

PROBLEM SET 23.7
1. Do the computations indicated near the end of Example 1 in detail.
2. Solve Example 1 by Ford–Fulkerson with initial flow 0.
Is it more work than in Example 1?
3. Which are the “bottleneck” edges by which the flow in
Example 1 is actually limited? Hence which capacities
could be decreased without decreasing the maximum
flow?
4. What is the (simple) reason that Kirchhoff’s law is
preserved in augmenting a flow by the use of a flow
augmenting path?
5. How does Ford–Fulkerson prevent the formation of
cycles?

MAXIMUM FLOW

6–9

Find the maximum flow by Ford-Fulkerson:
6. In Prob. 12, Sec. 23.6
7. In Prob. 15, Sec. 23.6
8. In Prob. 14, Sec. 23.6
9.
5, 3
4, 2

2

4

2, 1

1
s
3, 2

3

3, 2

6, 3

10, 4
3, 1

5

1, 0

6
t

10. Integer flow theorem. Prove that, if the capacities in
a network G are integers, then a maximum flow exists
and is an integer.
11. CAS PROBLEM. Ford–Fulkerson. Write a program
and apply it to Probs. 6–9.
12. How can you see that Ford–Fulkerson follows a BFS
technique?
13. Are the consecutive flow augmenting paths produced
by Ford–Fulkerson unique?
14. If the Ford–Fulkerson algorithm stops without reaching t, show that the edges with one end labeled and the
other end unlabeled form a cut set (S, T) whose capacity
equals the maximum flow.
15. Find a minimum cut set in Fig. 500 and its capacity.
16. Show that in a network G with all cij ϭ 1, the maximum
flow equals the number of edge-disjoint paths s : t.
17. In Prob. 15, the cut set contains precisely all forward
edges used to capacity by the maximum flow (Fig. 501).
Is this just by chance?
18. Show that in a network G with capacities all equal to 1,
the capacity of a minimum cut set (S, T) equals the
minimum number q of edges whose deletion destroys
all directed paths s : t. (A directed path v : w is a
path in which each edge has the direction in which it is
traversed in going from v to w.)

c23-b.qxd

11/3/10

4:07 PM

Page 1001

SEC. 23.8 Bipartite Graphs. Assignment Problems

1001

19. Several sources and sinks. If a network has several
sources s1, Á , sk, show that it can be reduced to the
case of a single-source network by introducing a new
vertex s and connecting s to s1, Á , sk by k edges of
capacity ϱ. Similarly if there are several sinks. Illustrate
this idea by a network with two sources and two sinks.
20. Find the maximum flow in the network in Fig. 502 with
two sources (factories) and two sinks (consumers).

23.8

5

s1 1
4

3
3

s2 2

4

6

5

7 t1

3

4

6

7

3

5

8

6

4

8 t2

Fig. 502. Problem 20

Bipartite Graphs. Assignment Problems
From digraphs we return to graphs and discuss another important class of combinatorial
optimization problems that arises in assignment problems of workers to jobs, jobs to
machines, goods to storage, ships to piers, classes to classrooms, exams to time periods,
and so on. To explain the problem, we need the following concepts.
A bipartite graph G ϭ (V, E ) is a graph in which the vertex set V is partitioned into
two sets S and T (without common elements, by the definition of a partition) such that
every edge of G has one end in S and the other in T. Hence there are no edges in G that
have both ends in S or both ends in T. Such a graph G ϭ (V, E ) is also written
G ϭ (S, T; E ).
Figure 503 shows an illustration. V consists of seven elements, three workers a, b, c,
making up the set S, and four jobs 1, 2, 3, 4, making up the set T. The edges indicate that
worker a can do the jobs 1 and 2, worker b the jobs 1, 2, 3, and worker c the job 4. The
problem is to assign one job to each worker so that every worker gets one job to do. This
suggests the next concept, as follows.

DEFINITION

Maximum Cardinality Matching

A matching in G ϭ (S, T; E ) is a set M of edges of G such that no two of them
have a vertex in common. If M consists of the greatest possible number of edges,
we call it a maximum cardinality matching in G.
For instance, a matching in Fig. 503 is M 1 ϭ {(a, 2), (b, 1)}. Another is M 2 ϭ {(a, 1),
(b, 3), (c, 4)}; obviously, this is of maximum cardinality.
S

T
1

a
2
b
3
c
4

Fig. 503. Bipartite graph in the assignment of a set S ϭ {a, b, c}
of workers to a set T ϭ {1, 2, 3, 4} of jobs

A vertex v is exposed (or not covered) by a matching M if v is not an endpoint of an
edge of M. This concept, which always refers to some matching, will be of interest when
we begin to augment given matchings (below). If a matching leaves no vertex exposed,

c23-b.qxd

11/3/10

4:07 PM

1002

Page 1002

CHAP. 23 Graphs. Combinatorial Optimization

we call it a complete matching. Obviously, a complete matching can exist only if S and
T consist of the same number of vertices.
We now want to show how one can stepwise increase the cardinality of a matching M
until it becomes maximum. Central in this task is the concept of an augmenting path.
An alternating path is a path that consists alternately of edges in M and not in M
(Fig. 504A). An augmenting path is an alternating path both of whose endpoints (a and b
in Fig. 504B) are exposed. By dropping from the matching M the edges that are on an
augmenting path P (two edges in Fig. 504B) and adding to M the other edges of P (three
in the figure), we get a new matching, with one more edge than M. This is how we use
an augmenting path in augmenting a given matching by one edge. We assert that this
will always lead, after a number of steps, to a maximum cardinality matching. Indeed,
the basic role of augmenting paths is expressed in the following theorem.

(A) Alternating path

b
a
(B) Augmenting path P

Fig. 504. Alternating and augmenting paths.
Heavy edges are those belonging to a matching M

THEOREM 1

Augmenting Path Theorem for Bipartite Matching

A matching M in a bipartite graph G ϭ (S, T; E) is of maximum cardinality if and
only if there does not exist an augmenting path P with respect to M.
PROOF

(a) We show that if such a path P exists, then M is not of maximum cardinality. Let P have
q edges belonging to M. Then P has q ϩ 1 edges not belonging to M. (In Fig. 504B we
have q ϭ 2.) The endpoints a and b of P are exposed, and all the other vertices on P are
endpoints of edges in M, by the definition of an alternating path. Hence if an edge of M is
not an edge of P, it cannot have an endpoint on P since then M would not be a matching.
Consequently, the edges of M not on P, together with the q ϩ 1 edges of P not belonging
to M form a matching of cardinality one more than the cardinality of M because we omitted
q edges from M and added q ϩ 1 instead. Hence M cannot be of maximum cardinality.
(b) We now show that if there is no augmenting path for M, then M is of maximum
cardinality. Let M* be a maximum cardinality matching and consider the graph H
consisting of all edges that belong either to M or to M*, but not to both. Then it is possible
that two edges of H have a vertex in common, but three edges cannot have a vertex in
common since then two of the three would have to belong to M (or to M*), violating that
M and M* are matchings. So every v in V can be in common with two edges of H or with
one or none. Hence we can characterize each “component” (ϭ maximal connected subset)
of H as follows.
(A) A component of H can be a closed path with an even number of edges (in the case
of an odd number, two edges from M or two from M* would meet, violating the matching
property). See (A) in Fig. 505.

c23-b.qxd

11/3/10

4:07 PM

Page 1003

SEC. 23.8 Bipartite Graphs. Assignment Problems

1003

(B) A component of H can be an open path P with the same number of edges from M
and edges from M*, for the following reason. P must be alternating, that is, an edge of
M is followed by an edge of M*, etc. (since M and M* are matchings). Now if P had an
edge more from M*, then P would be augmenting for M [see (B2) in Fig. 505],
contradicting our assumption that there is no augmenting path for M. If P had an edge
more from M, it would be augmenting for M* [see (B3) in Fig. 505], violating the
maximum cardinality of M*, by part (a) of this proof. Hence in each component of H, the
two matchings have the same number of edges. Adding to this the number of edges that
belong to both M and M* (which we left aside when we made up H ), we conclude that
M and M* must have the same number of edges. Since M* is of maximum cardinality,
this shows that the same holds for M, as we wanted to prove.
᭿

(A)

(B1)

Edge from M
Edge from M*

(Possible)

(B2)

(Augmenting for M)

(B3)

(Augmenting for M*)

Fig. 505. Proof of the augmenting path theorem for bipartite matching

This theorem suggests the algorithm in Table 23.9 for obtaining augmenting paths, in
which vertices are labeled for the purpose of backtracking paths. Such a label is in
addition to the number of the vertex, which is also retained. Clearly, to get an augmenting
path, one must start from an exposed vertex, and then trace an alternating path until one
arrives at another exposed vertex. After Step 3 all vertices in S are labeled. In Step 4,
the set T contains at least one exposed vertex, since otherwise we would have stopped
at Step 1.
Table 23.9 Bipartite Maximum Cardinality Matching

ALGORITHM MATCHING [G ϭ (S, T; E), M, n]
This algorithm determines a maximum cardinality matching M in a bipartite graph G by
augmenting a given matching in G.
INPUT: Bipartite graph G ϭ (S, T; E) with vertices 1, • • • , n, matching M in G (for
instance, M ϭ л)
OUTPUT: Maximum cardinality matching M in G
1. If there is no exposed vertex in S then
OUTPUT M. Stop
[M is of maximum cardinality in G.]
Else label all exposed vertices in S with л.
2. For each i in S and edge (i, j) not in M, label j with i, unless already labeled.

c23-b.qxd

11/3/10

4:07 PM

1004

Page 1004

CHAP. 23 Graphs. Combinatorial Optimization

3. For each nonexposed j in T, label i with j, where i is the other end
of the unique edge (i, j) in M.
4. Backtrack the alternating path P ending on an exposed vertex in T
by using the labels on the vertices.
5. If no P in Step 4 is augmenting then
OUTPUT M. Stop
[M is of maximum cardinality in G.]
Else augment M by using an augmenting path P.
Remove all labels.
Go to Step 1.
End MATCHING

EXAMPLE 1

Maximum Cardinality Matching
Is the matching M 1 in Fig. 506a of maximum cardinality? If not, augment it until maximum cardinality is reached.

S
Ø 1

T
5 3

S
7 1

T
5

6 2

6 3

6 2

6 3

7 3

7 1

5 3

7 2

Ø 4

8 3

Ø 4

8 3

(a) Given graph
and matching M1

(b) Matching M2
and new labels

Fig. 506. Example 1

Solution. We apply the algorithm.
1. Label 1 and 4 with л.
2. Label 7 with 1. Label 5, 6, 8 with 3.
3. Label 2 with 6, and 3 with 7.
[All vertices are now labeled as shown in Fig. 506a.]
4. P1: 1 ؊ 7 Ϫ 3 Ϫ 5. [By backtracking, P1 is augmenting.]
P2: 1 ؊ 7 Ϫ 3 Ϫ 8. [P2 is augmenting.]
5. Augment M 1 by using P1, dropping (3, 7) from M 1 and including (1, 7) and (3, 5). Remove all labels.
Go to Step 1.
Figure 506b shows the resulting matching M 2 ϭ {(1, 7), (2, 6), (3, 5)}.
1. Label 4 with л.
2. Label 7 with 2. Label 6 and 8 with 3.
3. Label 1 with 7, and 2 with 6, and 3 with 5.
4. P3: 5 ؊ 3 Ϫ 8. [P3 is alternating but not augmenting.]
5. Stop. M 2 is of maximum cardinality (namely, 3).

᭿

c23-b.qxd

11/3/10

4:07 PM

Page 1005

SEC. 23.8 Bipartite Graphs. Assignment Problems

1005

PROBLEM SET 23.8
BIPARTITE OR NOT?

1–7

12.

1

2

3

4

5

6

7

8

If you answer is yes, find S and T:
1.

1

2.

2

3

3.

1

2

3

4

4.

2

3

1

4

4

13–15

1

2

3

5

5.

1

2
3

4

5

6

7

6.

6

2
3
1

8

4
5

7.

1

2

3

6

5

4

8. Can you obtain the answer to Prob. 3 from that to
Prob. 1?
9. Can you obtain a bipartite subgraph in Prob. 4 by
omitting two edges? Any two edges? Any two edges
without a common vertex?

19–25
10–12

MATCHING. AUGMENTING PATHS

Find an augmenting path:
10.

1

4

2

3

11.

1

2

5

3

4

6

5

6

7

MAXIMUM CARDINALITY MATCHING

Using augmenting paths, find a maximum cardinality
matching:
13. In Prob. 11
14. In Prob. 10
15. In Prob. 12
16. Complete bipartite graphs. A bipartite graph
G ϭ (S, T; E ) is called complete if every vertex in S is
joined to every vertex in T by an edge, and is denoted
by K n1,n2, where n 1 and n 2 are the numbers of vertices
in S and T, respectively. How many edges does this
graph have?
17. Planar graph. A planar graph is a graph that can be
drawn on a sheet of paper so that no two edges cross.
Show that the complete graph K 4 with four vertices is
planar. The complete graph K 5 with five vertices is not
planar. Make this plausible by attempting to draw K 5
so that no edges cross. Interpret the result in terms of
a net of roads between five cities.
18. Bipartite graph K 3,3 not planar. Three factories 1,
2, 3 are each supplied underground by water, gas, and
electricity, from points A, B, C, respectively. Show that
this can be represented by K 3,3 (the complete bipartite
graph G ϭ (S, T; E ) with S and T consisting of three
vertices each) and that eight of the nine supply lines
(edges) can be laid out without crossing. Make it
plausible that K 3,3 is not planar by attempting to draw
the ninth line without crossing the others.

VERTEX COLORING

19. Vertex coloring and exam scheduling. What is the
smallest number of exam periods for six subjects a, b,
c, d, e, f if some of the students simultaneously take a,
b, f, some c, d, e, some a, c, e, and some c, e? Solve
this as follows. Sketch a graph with six vertices a, Á , f
and join vertices if they represent subjects simultaneously taken by some students. Color the vertices
so that adjacent vertices receive different colors. (Use
numbers 1, 2, Á instead of actual colors if you want.)
What is the minimum number of colors you need? For
any graph G, this minimum number is called the

c23-b.qxd

11/3/10

4:07 PM

1006

Page 1006

CHAP. 23 Graphs. Combinatorial Optimization

(vertex) chromatic number ␹v(G). Why is this the
answer to the problem? Write down a possible
schedule.
20. Scheduling and matching. Three teachers x 1, x 2, x 3
teach four classes y1, y2, y3, y4 for these numbers of
periods:

x1
x2
x3

y1

y2

y3

y4

1
1
0

0
1
1

1
1
1

1
1
1

Show that this arrangement can be represented by a
bipartite graph G and that a teaching schedule for one
period corresponds to a matching in G. Set up a
teaching schedule with the smallest possible number of
periods.
21. How many colors do you need for vertex coloring any
tree?
22. Harbor management. How many piers does a harbor
master need for accommodating six cruise ships
S1, Á , S6 with expected dates of arrival A and departure
D in July, (A, D) ϭ (10, 13), (13, 15), (14, 17),
(12, 15), (16, 18), (14, 17), respectively, if each pier can

23.
24.

25.

26.

accommodate only one ship, arrival being at 6 am and
departures at 11 pm? Hint. Join Si and Sj by an edge if
their intervals overlap. Then color vertices.
What would be the answer to Prob. 22 if only the five
ships S1, Á , S5 had to be accommodated?
Four- (vertex) color theorem. The famous four-color
theorem states that one can color the vertices of any
planar graph (so that adjacent vertices get different
colors) with at most four colors. It had been conjectured
for a long time and was eventually proved in 1976 by
Appel and Haken [Illinois J. Math 21 (1977), 429–567].
Can you color the complete graph K 5 with four colors?
Does the result contradict the four-color theorem? (For
more details, see Ref. [F1] in App. 1.)
Find a graph, as simple as possible, that cannot be
vertex colored with three colors. Why is this of interest
in connection with Prob. 24?
Edge coloring. The edge chromatic number ␹e(G) of
a graph G is the minimum number of colors needed for
coloring the edges of G so that incident edges get
different colors. Clearly, ␹e(G) м max d(u), where d(u)
is the degree of vertex u. If G ϭ (S, T; E ) is bipartite,
the equality sign holds. Prove this for K n,n the complete
(cf. Sec. 23.1) bipartite graph G ϭ (S, T, E ) with S and
T consisting of n vertices each.

CHAPTER 23 REVIEW QUESTIONS AND PROBLEMS
1. What is a graph, a digraph, a cycle, a tree?
2. State some typical problems that can be modeled and
solved by graphs or digraphs.
3. State from memory how graphs can be handled on
computers.
4. What is a shortest path problem? Give applications.
5. What situations can be handled in terms of the traveling
salesman problem?
6. Give typical applications involving spanning trees.
7. What are the basic ideas and concepts in handling flows?
8. What is combinatorial optimization? Which sections of
this chapter involved it? Explain details.
9. Define bipartite graphs and describe some typical
applications of them.
10. What is BFS? DFS? In what connection did these
concepts occur?
11–16

MATRICES FOR GRAPHS AND DIGRAPHS

Find the adjacency matrix of:
11. 3
2

12.

1

13.

3

2

14–16

1

2

4

3

Sketch the graph whose adjacency matrix is:

0

1

1

1

0

1

0

1

1
14. E
1

0

1

1

0

0

1

1

0

1

1
15. E
0

0

0

1

1

1

1

0

1

1

1

0

0

1

1

1

1

0

0

1

1

0

0

1

1

1

1

0

16. E

U

U

U

17. Vertex incidence list. Make it for the graph in Prob. 15.
4

1

c23-b.qxd

11/3/10

4:07 PM

Page 1007

Summary of Chapter 23

1007

18. Find a shortest path and its length by Moore’s BFS
algorithm, assuming that all the edges have length 1.

22. Find flow augmenting paths and the maximum flow.

4, 2
t

s

2, 1

s 1
3, 2

Problem 18

3

4

10, 4

3, 2
3, 1
5

6, 3

6 t

1, 0

Problem 22

19. Find shortest paths by Dijkstra’s algorithm.
8

3

5, 3

2

23. Using augmenting paths, find a maximum cardinality
matching.

1

3
6

1

2

3

4

5

6

7

8

2
5

4

2

4

Problem 19
20. Find a shortest spanning tree.

5

3

4

1

5
7

Problem 25

1
2

4

3
2

8

24. Find an augmenting path,

2

1

2

3

5

Problem 20
21. Company A has offices in Chicago, Los Angeles, and
New York; Company B in Boston and New York;
Company C in Chicago, Dallas, and Los Angeles.
Represent this by a bipartite graph.

SUMMARY OF CHAPTER

4

Problem 24

23

Graphs. Combinatorial Optimization
Combinatorial optimization concerns optimization problems of a discrete or
combinatorial structure. It uses graphs and digraphs (Sec. 23.1) as basic tools.
A graph G ϭ (V, E ) consists of a set V of vertices v1, v2, Á , vn (often simply
denoted by 1, 2, Á , n) and a set E of edges e1, e2, Á , em, each of which connects
two vertices. We also write (i, j) for an edge with vertices i and j as endpoints. A
digraph (ϭ directed graph) is a graph in which each edge has a direction (indicated
by an arrow). For handling graphs and digraphs in computers, one can use matrices
or lists (Sec. 23.1).
This chapter is devoted to important classes of optimization problems for graphs
and digraphs that all arise from practical applications, and corresponding algorithms,
as follows.

c23-b.qxd

11/3/10

1008

4:07 PM

Page 1008

CHAP. 23 Graphs. Combinatorial Optimization

In a shortest path problem (Sec. 23.2) we determine a path of minimum length
(consisting of edges) from a vertex s to a vertex t in a graph whose edges (i, j) have
a “length” l ij Ͼ 0, which may be an actual length or a travel time or cost or an
electrical resistance [if (i, j) is a wire in a net], and so on. Dijkstra’s algorithm
(Sec. 23.3) or, when all l ij ϭ 1, Moore’s algorithm (Sec. 23.2) are suitable for these
problems.
A tree is a graph that is connected and has no cycles (no closed paths). Trees are
very important in practice. A spanning tree in a graph G is a tree containing all the
vertices of G. If the edges of G have lengths, we can determine a shortest spanning
tree, for which the sum of the lengths of all its edges is minimum, by Kruskal’s
algorithm or Prim’s algorithm (Secs. 23.4, 23.5).
A network (Sec. 23.6) is a digraph in which each edge (i, j) has a capacity
cij Ͼ 0 [ϭ maximum possible flow along (i, j)] and at one vertex, the source s, a
flow is produced that flows along the edges to a vertex t, the sink or target, where
the flow disappears. The problem is to maximize the flow, for instance, by applying
the Ford–Fulkerson algorithm (Sec. 23.7), which uses flow augmenting paths
(Sec. 23.6). Another related concept is that of a cut set, as defined in Sec. 23.6.
A bipartite graph G ϭ (V, E ) (Sec. 23.8) is a graph whose vertex set V consists
of two parts S and T such that every edge of G has one end in S and the other in T,
so that there are no edges connecting vertices in S or vertices in T. A matching in
G is a set of edges, no two of which have an endpoint in common. The problem
then is to find a maximum cardinality matching in G, that is, a matching M that
has a maximum number of edges. For an algorithm, see Sec. 23.8.

c24.qxd

11/3/10

5:12 PM

Page 1009

PART

G

Probability,
Statistics
CHAPTER 24
CHAPTER 25

Data Analysis. Probability Theory
Mathematical Statistics
Probability theory (Chap. 24) provides models of probability distributions (theoretical
models of the observable reality involving chance effects) to be tested by statistical methods,
and it will also supply the mathematical foundation of these methods in Chap. 25.
Modern mathematical statistics (Chap. 25) has various engineering applications, for
instance, in testing materials, control of production processes, quality control of production
outputs, performance tests of systems, robotics, and automatization in general, production
planning, marketing analysis, and so on.
To this we could add a long list of fields of applications, for instance, in agriculture,
biology, computer science, demography, economics, geography, management of natural
resources, medicine, meteorology, politics, psychology, sociology, traffic control, urban
planning, etc. Although these applications are very heterogeneous, we shall see that most
statistical methods are universal in the sense that each of them can be applied in various
fields.

Additional Software for
Probability and Statistics
See also the list of software at the beginning of Part E on Numerical Analysis.
Data Desk. Data Description, Inc., Ithaca, NY. Phone 1-800-573-5121 or (607) 257-1000,
website at www.datadesk.com.
1009

c24.qxd

11/9/10

1010

3:47 PM

Page 1010

PART G Probability, Statistics

MINITAB. Minitab, Inc., State College, PA. Phone 1-800-448-3555 or (814) 238-3280,
website at www.minitab.com.
SAS. SAS Institute, Inc., Cary, NC. Phone 1-800-727-0025 or (919) 677-8000, website
at www.sas.com.
R. website at www.r-project.org. Free software, part of the GNU/Free Software Foundation
project.
SPSS. SPSS, Inc., Chicago, IL. (part of IBM) Phone 1-800-543-2185 or (312) 651-3000,
website at www.spss.com.
STATISTICA. StatSoft, Inc., Tulsa, OK. Phone (918) 749-1119, website at
www.statsoft.com.
TIBCO Spotfire S+. TIBCO Software Inc., Palo Alto, CA; Office for this software:
Somerville, MA. Phone 1-866-240-0491 (toll-free), (617) 702-1602, website at spotfire.
tibco.com/products/s-plus/statistical-analysis-software.aspx

c24.qxd

11/3/10

5:12 PM

Page 1011

CHAPTER

24

Data Analysis.
Probability Theory
We first show how to handle data numerically or in terms of graphs, and how to extract
information (average size, spread of data, etc.) from them. If these data are influenced by
“chance,” by factors whose effect we cannot predict exactly (e.g., weather data, stock
prices, life spans of tires, etc.), we have to rely on probability theory. This theory
originated in games of chance, such as flipping coins, rolling dice, or playing cards.
Nowadays it gives mathematical models of chance processes called random experiments
or, briefly, experiments. In such an experiment we observe a random variable X, that
is, a function whose values in a trial (a performance of an experiment) occur “by chance”
(Sec. 24.3) according to a probability distribution that gives the individual probabilities
with which possible values of X may occur in the long run. (Example: Each of the six
faces of a die should occur with the same probability, 1>6.) Or we may simultaneously
observe more than one random variable, for instance, height and weight of persons or
hardness and tensile strength of steel. This is discussed in Sec. 24.9, which will also give
the basis for the mathematical justification of the statistical methods in Chapter 25.
Prerequisite: Calculus.
References and Answers to Problems: App. 1 Part G, App. 2.

24.1

Data Representation. Average. Spread
Data can be represented numerically or graphically in various ways. For instance, your
daily newspaper may contain tables of stock prices and money exchange rates, curves or
bar charts illustrating economical or political developments, or pie charts showing how
your tax dollar is spent. And there are numerous other representations of data for special
purposes.
In this section we discuss the use of standard representations of data in statistics. (For
these, software packages, such as DATA DESK, R, and MINITAB, are available, and
Maple or Mathematica may also be helpful; see pp. 789 and 1009) We explain corresponding
concepts and methods in terms of typical examples.

EXAMPLE 1

Recording and Sorting
Sample values (observations, measurements) should be recorded in the order in which they occur. Sorting, that
is, ordering the sample values by size, is done as a first step of investigating properties of the sample and graphing
it. Sorting is a standard process on the computer; see Ref. [E35], listed in App. 1.

1011

c24.qxd

11/3/10

5:12 PM

1012

Page 1012

CHAP. 24 Data Analysis. Probability Theory
Super alloys is a collective name for alloys used in jet engines and rocket motors, requiring high temperature
(typically 1800°F), high strength, and excellent resistance to oxidation. Thirty specimens of Hastelloy C (nickelbased steel, investment cast) had the tensile strength (in 1000 lb>sq in.), recorded in the order obtained and
rounded to integer values,
89

77

88

91

88

93

99

79

87

84

86

82

88

89

78

90

91

81

90

83

83

92

87

89

86

89

81

87

84

89

77

78

79

81

81

82

83

83

84

84

86

86

87

87

87

88

88

88

89

89

89

89

89

90

90

91

91

92

93

99

(1)
Sorting gives

᭿

(2)

Graphic Representation of Data
We shall now discuss standard graphic representations used in statistics for obtaining
information on properties of data.
EXAMPLE 2

Stem-and-Leaf Plot (Fig. 507)
This is one of the simplest but most useful representations of data. For (1) it is shown in Fig. 507. The numbers
in (1) range from 78 to 99; see (2). We divide these numbers into 5 groups, 75–79, 80–84, 85–89, 90–94,
95–99. The integers in the tens position of the groups are 7, 8, 8, 9, 9. These form the stem in Fig. 507. The
first leaf is 789, representing 77, 78, 79. The second leaf is 1123344, representing 81, 81, 82, 83, 83, 84, 84.
And so on.
The number of times a value occurs is called its absolute frequency. Thus 78 has absolute frequency 1, the
value 89 has absolute frequency 5, etc. The column to the extreme left in Fig. 507 shows the cumulative absolute
frequencies, that is, the sum of the absolute frequencies of the values up to the line of the leaf. Thus, the number
10 in the second line on the left shows that (1) has 10 values up to and including 84. The number 23 in the next
line shows that there are 23 values not exceeding 89, etc. Dividing the cumulative absolute frequencies by
n (ϭ 30 in Fig. 507) gives the cumulative relative frequencies 0.1, 0.33, 0.76, 0.93, 1.00.
᭿

EXAMPLE 3

Histogram (Fig. 508)
For large sets of data, histograms are better in displaying the distribution of data than stem-and-leaf plots. The
principle is explained in Fig. 508. (An application to a larger data set is shown in Sec. 25.7). The bases of the
rectangles in Fig. 508 are the x-intervals (known as class intervals) 74.5–79.5, 79.5–84.5, 84.5–89.5, 89.5–94.5,
94.5–99.5, whose midpoints (known as class marks) are x ϭ 77, 82, 87, 92, 97, respectively. The height of a
rectangle with class mark x is the relative class frequency frel(x), defined as the number of data values in that
class interval, divided by n (ϭ 30 in our case). Hence the areas of the rectangles are proportional to these
relative frequencies, 0.10, 0.23, 0.43, 0.17, 0.07, so that histograms give a good impression of the distribution
of data.
᭿
frel(x)
0.5

Leaf unit = 1.0
3
10
23
29
30

7
8
8
9
9

789
1123344
6677788899999
001123
9

Fig. 507. Stem-and-leaf plot
of the data in Example 1

0.4
0.3
0.2
0.1
0

77

82

87

92

97

x

Fig. 508. Histogram of the data in
Example 1 (grouped as in Fig. 507)

c24.qxd

11/3/10

5:12 PM

Page 1013

SEC. 24.1 Data Representation. Average. Spread
EXAMPLE 4

1013

Boxplot. Median. Interquartile Range. Outlier
A boxplot of a set of data illustrates the average size and the spread of the values, in many cases the two most
important quantities characterizing the set, as follows.
The average size is measured by the median, or middle quartile, qM. If the number n of values of the set is odd,
then qM is the middlemost of the values when ordered as in (2). If n is even, then qM is the average of the two
middlemost values of the ordered set. In (2) we have n ϭ 30 and thus qM ϭ 12 (x 15 ϩ x 16) ϭ 12 (87 ϩ 88) ϭ 87.5.
(In general, qM will be a fraction if n is even.)
The spread of values can be measured by the range R ϭ x max Ϫ x min, the largest value minus the smallest
one.
Better information on the spread gives the interquartile range IQR ϭ qU Ϫ qL. Here qU is the middlemost
value (or the average of the two middlemost values) in the data above the median; and qL is the middlemost
value (or the average of the two middlemost values) in the data below the median. Hence in (2) we have
qU ϭ x 23 ϭ 89, qL ϭ x 8 ϭ 83, and IQR ϭ 89 Ϫ 83 ϭ 6.
The box in Fig. 509 extends vertically from qL to qU; it has height IQR ϭ 6. The vertical lines below and
above the box extend from x min ϭ 77 to x max ϭ 99, so that they show R ϭ 22.

100

95

90

qU
qM

85

qL
80

75

Data set (1)

Fig. 509. Boxplot of the data set (1)

The line above the box is suspiciously long. This suggests the concept of an outlier, a value that is more
than 1.5 times the IQR away from either end of the box; here 1.5 is purely conventional. An outlier indicates
that something might have gone wrong in the data collection. In (2) we have 89 ϩ 1.5 IQR ϭ 98, and we regard
99 as an outlier.
᭿

Mean. Standard Deviation. Variance.
Empirical Rule
Medians and quartiles are easily obtained by ordering and counting, practically without
calculation. But they do not give full information on data: you can change data values to
some extent without changing the median. Similarly for the quartiles.
The average size of the data values can be measured in a more refined way by the
mean
n

(3)

1
1
x ϭ n a x j ϭ n (x 1 ϩ x 2 ϩ Á ϩ x n).
jϭ1

c24.qxd

11/4/10

1:38 PM

1014

Page 1014

CHAP. 24 Data Analysis. Probability Theory

This is the arithmetic mean of the data values, obtained by taking their sum and dividing
by the data size n. Thus in (1),
1
x ϭ 30
(89 ϩ 77 ϩ Á ϩ 89) ϭ 260
3 Ϸ 86.7.

Every data value contributes, and changing one of them will change the mean.
Similarly, the spread (variability) of the data values can be measured in a more refined
way by the standard deviation s or by its square, the variance
n

(4)

s2 ϭ

1
1
2
2
2
[(x 1 Ϫ x ) ϩ Á ϩ (x n Ϫ x ) ].
(x j Ϫ x ) ϭ
a
nϪ1
n Ϫ 1 jϭ1

Thus, to obtain the variance of the data, take the difference x j Ϫ x of each data value from
the mean, square it, take the sum of these n squares, and divide it by n Ϫ 1 (not n, as we
motivate in Sec. 25.2). To get the standard deviation s, take the square root of s 2.
For example, using x ϭ 260>3, we get for the data (1) the variance
1
s 2 ϭ 29
[(89 Ϫ

260 2
3 )

2006
2
2
Á ϩ (89 Ϫ 260
ϩ (77 Ϫ 260
3 ) ϩ
3 ) ] ϭ 87 Ϸ 23.06

Hence the standard deviation is s ϭ 12006>87 Ϸ 4.802. Note that the standard deviation
has the same dimension as the data values (kg>mm2, see at the beginning), which is an
advantage. On the other hand, the variance is preferable to the standard deviation in
developing statistical methods, as we shall see in Chap. 25.
CAUTION! Your CAS (Maple, for instance) may use 1>n instead of 1>(n Ϫ 1) in (4),
but the latter is better when n is small (see Sec. 25.2).
Mean and standard deviation, introduced to give center and spread, actually give much
more information according to this rule.
Empirical Rule. For any mound-shaped, nearly symmetric distribution of data the intervals
x Ϯ s, x Ϯ 2s, x Ϯ 3s

contain about

68%, 95%, 99.7%,

respectively, of the data points.
EXAMPLE 5

Empirical Rule and Outliers. z-Score
For (1), with x ϭ 86.7 and s ϭ 4.8, the three intervals in the Rule are 81.9 Ϲ x Ϲ 91.5, 77.1 Ϲ x Ϲ 96.3,
72.3 Ϲ x Ϲ 101.1 and contain 73% (22 values remain, 5 are too small, and 5 too large), 93% (28 values,
1 too small, and 1 too large), and 100%, respectively.
If we reduce the sample by omitting the outlier 99, mean and standard deviation reduce to x red ϭ 86.2, sred ϭ 4.3,
approximately, and the percentage values become 67% (5 and 5 values outside), 93% (1 and 1 outside), and 100%.
Finally, the relative position of a value x in a set of mean x and standard deviation s can be measured by the
z-score
z(s) ϭ

xϪx
.
s

This is the distance of x from the mean x measured in multiples of s. For instance, z(83) ϭ (83 Ϫ 86.7)>
4.8 ϭ Ϫ0.77. This is negative because 83 lies below the mean. By the Empirical Rule, the extreme z-values
᭿
are about Ϫ3 and 3.

c24.qxd

11/3/10

5:12 PM

Page 1015

SEC. 24.2 Experiments, Outcomes, Events

1015

PROBLEM SET 24.1
DATA REPRESENTATIONS

1–10

Represent the data by a stem-and-leaf plot, a histogram, and
a boxplot:
1. Length of nails [mm]
19

21

19

20

19

20

21

20

2. Phone calls per minute in an office between 9:00 A.M.
and 9:10 A.M.
6

6

4

2

1

7

0

4

6

158
146

154
156

133
138

141
138

130
149

144
139

137

4. Iron content [%] of 15 specimens of hermatite (Fe 2O3)
72.8
71.5

70.4
69.7

71.2
70.5

69.2
71.3

70.3
69.1

68.9
70.9

71.1
70.6

69.8

5. Weight of filled bags [g] in an automatic filling
203

199

198

201

200

201

201

6. Gasoline consumption [miles per gallon, rounded] of
six cars of the same model under similar conditions
15.0

15.5

14.5

15.0

15.5

15.0

7. Release time [sec] of a relay
1.3
1.6

1.2
1.3

24.2

1.4
1.5

1.5
1.1

1.3
1.4

1.3
1.2

1.4
1.3

1.1
1.5

86
90

86
88

1.5
1.4

1.4
1.4

87
89

89
90

76
88

85
80

82
84

86
89

87
90

85
89

9. Efficiency [%] of seven Voith Francis turbines of
runner diameter 2.3 m under a head range of 185 m
91.8

7

3. Systolic blood pressure of 15 female patients of ages
20–22
156
151

8. Foundrax test of Brinell hardness (2.5 mm steel ball,
62.5 kg load, 30 sec) of 20 copper plates (values in
kg>mm2)

89.1

89.9

92.5

90.7

91.2

91.0

10. Ϫ0.51 0.12 Ϫ0.47 0.95 0.25 Ϫ0.18 Ϫ0.54
11–16

AVERAGE AND SPREAD

Find the mean and compare it with the median. Find the
standard deviation and compare it with the interquartile range.
11. For the data in Prob. 1
12. For the phone call data in Prob. 2
13. For the medical data in Prob. 3
14. For the iron contents in Prob. 4
15. For the release times in Prob. 7
16. For the Brinell hardness data in Prob. 8
17. Outlier, reduced data. Calculate s for the data
4 1 3 10 2. Then reduce the data by deleting
the outlier and calculate s. Comment.
18. Outlier, reduction. Do the same tasks as in Prob. 17
for the hardness data in Prob. 8.
19. Construct the simplest possible data with x ϭ 100 but
qM ϭ 0. What is the point of this problem?
20. Mean. Prove that x must always lie between the
smallest and the largest data values.

Experiments, Outcomes, Events
We now turn to probability theory. This theory has the purpose of providing mathematical
models of situations affected or even governed by “chance effects,” for instance, in weather
forecasting, life insurance, quality of technical products (computers, batteries, steel sheets,
etc.), traffic problems, and, of course, games of chance with cards or dice. And the accuracy
of these models can be tested by suitable observations or experiments—this is a main
purpose of statistics to be explained in Chap. 25.
We begin by defining some standard terms. An experiment is a process of measurement
or observation, in a laboratory, in a factory, on the street, in nature, or wherever; so
“experiment” is used in a rather general sense. Our interest is in experiments that involve
randomness, chance effects, so that we cannot predict a result exactly. A trial is a single
performance of an experiment. Its result is called an outcome or a sample point. n trials
then give a sample of size n consisting of n sample points. The sample space S of an
experiment is the set of all possible outcomes.

c24.qxd

11/3/10

5:12 PM

1016
EXAMPLES 1–6

Page 1016

CHAP. 24 Data Analysis. Probability Theory
Random Experiments. Sample Spaces
(1) Inspecting a lightbulb. S ϭ {Defective, Nondefective}.
(2) Rolling a die. S ϭ {1, 2, 3, 4, 5, 6}.
(3) Measuring tensile strength of wire. S the numbers in some interval.
(4) Measuring copper content of brass. S: 50% to 90%, say.
(5) Counting daily traffic accidents in New York. S the integers in some interval.
(6) Asking for opinion about a new car model. S ϭ {Like, Dislike, Undecided}.

᭿

The subsets of S are called events and the outcomes simple events.
EXAMPLE 7

Events
In (2), events are A ϭ {1, 3, 5} (“Odd number”), B ϭ {2, 4, 6} (“Even number”), C ϭ {5, 6}. etc. Simple
events are {1}, {2}, Á , {6}.
᭿

If, in a trial, an outcome a happens and a ʦ A (a is an element of A), we say that A
happens. For instance, if a die turns up a 3, the event A: Odd number happens. Similarly,
if C in Example 7 happens (meaning 5 or 6 turns up), then, say, D ϭ {4, 5, 6} happens.
Also note that S happens in each trial, meaning that some event of S always happens. All
this is quite natural.

Unions, Intersections, Complements of Events
In connection with basic probability laws we shall need the following concepts and facts
about events (subsets) A, B, C, Á of a given sample space S.
The union A ʜ B of A and B consists of all points in A or B or both.
The intersection A ʝ B of A and B consists of all points that are in both A and B.
If A and B have no points in common, we write
AʝBϭл
where л is the empty set (set with no elements) and we call A and B mutually exclusive
(or disjoint) because, in a trial, the occurrence of A excludes that of B (and conversely)—
if your die turns up an odd number, it cannot turn up an even number in the same trial.
Similarly, a coin cannot turn up Head and Tail at the same time.
Complement Ac of A. This is the set of all the points of S not in A. Thus,
A ʝ Ac ϭ л,

A ʜ Ac ϭ S.

In Example 7 we have Ac ϭ B, hence A ʜ Ac ϭ {1, 2, 3, 4, 5, 6} ϭ S.
Another notation for the complement of A is A (instead of Ac), but we shall not
use this because in set theory A is used to denote the closure of A (not needed in
our work).
Unions and intersections of more events are defined similarly. The union
m

ʜ Aj ϭ A1 ʜ A2 ʜ Á ʜ Am

jϭ1

c24.qxd

11/3/10

5:12 PM

Page 1017

SEC. 24.2 Experiments, Outcomes, Events

1017

of events A1, Á , Am consists of all points that are in at least one Aj. Similarly for the
union A1 ʜ A2 ʜ Á of infinitely many subsets A1, A2, Á of an infinite sample space
S (that is, S consists of infinitely many points). The intersection
m

ʝ Aj ϭ A1 ʝ A2 ʝ Á ʝ Am

jϭ1

of A1, Á , Am consists of the points of S that are in each of these events. Similarly for
the intersection A1 ʝ A2 ʝ Á of infinitely many subsets of S.
Working with events can be illustrated and facilitated by Venn diagrams1 for showing
unions, intersections, and complements, as in Figs. 510 and 511, which are typical
examples that give the idea.
EXAMPLE 8

Unions and Intersections of 3 Events
In rolling a die, consider the events
A:

Number greater than 3,

B:

Number less than 6,

C:

Even number.

Then A ʝ B ϭ {4, 5}, B ʝ C ϭ {2, 4}, C ʝ A ϭ {4, 6}, A ʝ B ʝ C ϭ {4}. Can you sketch a Venn diagram
of this? Furthermore, A ʜ B ϭ S, hence A ʜ B ʜ C ϭ S (why?).
᭿

A

A

B

B
S

S
Union A ∪ B

Intersection A ∩ B

Fig. 510. Venn diagrams showing two events A and B in a sample space S
and their union A ʜ B (colored) and intersection A ʝ B (colored)
3
5

1
A

C
6

2
4
S

Fig. 511. Venn diagram for the experiment of rolling a die, showing S,
A ϭ {1, 3, 5}, C ϭ {5, 6}, A ʜ C ϭ {1, 3, 5, 6}, A ʝ C ϭ {5}

PROBLEM SET 24.2
1–12

SAMPLE SPACES, EVENTS

Graph a sample space for the experiments:
1. Drawing 3 screws from a lot of right-handed and lefthanded screws
2. Tossing 2 coins
1

3. Rolling 2 dice
4. Rolling a die until the first Six appears
5. Tossing a coin until the first Head appears
6. Recording the lifetime of each of 3 lightbulbs

JOHN VENN (1834–1923), English mathematician.

c24.qxd

11/3/10

5:12 PM

Page 1018

1018

CHAP. 24 Data Analysis. Probability Theory

7. Recording the daily maximum temperature X and the
daily maximum air pressure Y at Times Square in New
York
8. Choosing a committee of 2 from a group of 5 people
9. Drawing gaskets from a lot of 10, containing one
defective D, unitil D is drawn, one at a time and
assuming sampling without replacement, that is,
gaskets drawn are not returned to the lot. (More about
this in Sec. 24.6)
10. In rolling 3 dice, are the events A: Sum divisible by 3
and B: Sum divisible by 5 mutually exclusive?
11. Answer the questions in Prob. 10 for rolling 2 dice.
12. List all 8 subsets of the sample space S ϭ {a, b, c}.
13. In Prob. 3 circle and mark the events A: Faces are equal,
B: Sum of faces less than 5, A ʜ B, A ʝ B, Ac, B c.
14. In drawing 2 screws from a lot of right-handed and
left-handed screws, let A, B, C, D mean at a least
1 right-handed, at least 1 left-handed, 2 right-handed,
2 left-handed, respectively. Are A and B mutually
exclusive? C and D?
15–20

G
3
4

5

24.3

1
6

M

P

Problem 15
16. Show that, by the definition of complement, for any
subset A of a sample space S.
S c ϭ л,
лc ϭ S,
(Ac)c ϭ A,
c
c
A ʝ A ϭ л.
A ʜ A ϭ S,
17. Using a Venn diagram, show that A ʕ B if and only if
A ʜ B ϭ B.
18. Using a Venn diagram, show that A ʕ B if and only if
A ʝ B ϭ A.
19. (De Morgan’s laws) Using Venn diagrams, graph and
check De Morgan’s laws

VENN DIAGRAMS

15. In connection with a trip to Europe by some students,
consider the events P that they see Paris, G that they
have a good time, and M that they run out of money,
and describe in words the events 1, Á , 7 in the
diagram.

2
7

(A ʜ B)c ϭ Ac ʝ B c
(A ʝ B)c ϭ Ac ʜ B c.
20. Using Venn diagrams, graph and check the rules
A ʜ (B ʝ C) ϭ (A ʜ B) ʝ (A ʜ C)
A ʝ (B ʜ C) ϭ (A ʝ B) ʜ (A ʝ C).

Probability
The “probability” of an event A in an experiment is supposed to measure how frequently
A is about to occur if we make many trials. If we flip a coin, then heads H and tails T
will appear about equally often—we say that H and T are “equally likely.” Similarly, for
a regularly shaped die of homogeneous material (“fair die”) each of the six outcomes
1, Á , 6 will be equally likely. These are examples of experiments in which the sample
space S consists of finitely many outcomes (points) that for reasons of some symmetry
can be regarded as equally likely. This suggests the following definition.

DEFINITION 1

First Definition of Probability

If the sample space S of an experiment consists of finitely many outcomes (points)
that are equally likely, then the probability P(A) of an event A is
(1)

P(A) ϭ

Number of points in A
Number of points in S

.

c24.qxd

11/3/10

5:12 PM

Page 1019

SEC. 24.3 Probability

1019

From this definition it follows immediately that, in particular,
(2)

EXAMPLE 1

P(S) ϭ 1.

Fair Die
In rolling a fair die once, what is the probability P(A) of A of obtaining a 5 or a 6? The probability of B: “Even
number”?

Solution. The six outcomes are equally likely, so that each has probability 1>6. Thus P(A) ϭ 2>6 ϭ 1>3
᭿
because A ϭ {5, 6} has 2 points, and P(B) ϭ 3>6 ϭ 1>2.
Definition 1 takes care of many games as well as some practical applications, as we shall
see, but certainly not of all experiments, simply because in many problems we do not
have finitely many equally likely outcomes. To arrive at a more general definition of
probability, we regard probability as the counterpart of relative frequency. Recall from
Sec. 24.1 that the absolute frequency f (A) of an event A in n trials is the number of times
A occurs, and the relative frequency of A in these trials is f (A)>n; thus

(3)

Number of times A occurs
f (A)
frel(A) ϭ n ϭ
.
Number of trials

Now if A did not occur, then f (A) ϭ 0. If A always occurred, then f (A) ϭ n. These are
the extreme cases. Division by n gives
(4*)

0 Ϲ frel(A) Ϲ 1.

In particular, for A ϭ S we have f (S) ϭ n because S always occurs (meaning that
some event always occurs; if necessary, see Sec. 24.2, after Example 7). Division
by n gives
(5*)

frel(S) ϭ 1.

Finally, if A and B are mutually exclusive, they cannot occur together. Hence the absolute
frequency of their union A ʜ B must equal the sum of the absolute frequencies of A and
B. Division by n gives the same relation for the relative frequencies,
(6*)

frel(A ʜ B) ϭ frel(A) ϩ frel(B)

(A ʝ B ϭ л).

We are now ready to extend the definition of probability to experiments in which equally
likely outcomes are not available. Of course, the extended definition should include
Definition 1. Since probabilities are supposed to be the theoretical counterpart of relative
frequencies, we choose the properties in (4*), (5*), (6*) as axioms. (Historically, such a
choice is the result of a long process of gaining experience on what might be best and
most practical.)

c24.qxd

11/3/10

5:12 PM

Page 1020

1020

CHAP. 24 Data Analysis. Probability Theory

DEFINITION 2

General Definition of Probability

Given a sample space S, with each event A of S (subset of S) there is associated a
number P(A), called the probability of A, such that the following axioms of
probability are satisfied.
1. For every A in S,
0 Ϲ P(A) Ϲ 1.

(4)

2. The entire sample space S has the probability
P(S) ϭ 1.

(5)

3. For mutually exclusive events A and B (A ʝ B ϭ л; see Sec. 24.2),
P(A ʜ B) ϭ P(A) ϩ P(B)

(6)

(A ʝ B ϭ л).

If S is infinite (has infinitely many points), Axiom 3 has to be replaced by
3 r . For mutually exclusive events A1, A2, Á ,
(6 r )

P(A1 ʜ A2 ʜ Á ) ϭ P(A1) ϩ P(A2) ϩ Á .

In the infinite case the subsets of S on which P(A) is defined are restricted to form a
so-called s-algebra, as explained in Ref. [GenRef6] (not [G6]!) in App. 1. This is of no
practical consequence to us.

Basic Theorems of Probability
We shall see that the axioms of probability will enable us to build up probability theory
and its application to statistics. We begin with three basic theorems. The first of them
is useful if we can get the probability of the complement Ac more easily than P(A)
itself.
THEOREM 1

Complementation Rule

For an event A and its complement Ac in a sample space S,
(7)

PROOF

P(Ac) ϭ 1 Ϫ P(A).

By the definition of complement (Sec. 24.2), we have S ϭ A ʜ Ac and A ʝ Ac ϭ л.
Hence by Axioms 2 and 3,
1 ϭ P(S) ϭ P(A) ϩ P(Ac),

thus

P(Ac) ϭ 1 Ϫ P(A).

᭿

c24.qxd

11/3/10

5:12 PM

Page 1021

SEC. 24.3 Probability
EXAMPLE 2

1021

Coin Tossing
Five coins are tossed simultaneously. Find the probability of the event A: At least one head turns up. Assume
that the coins are fair.

Solution. Since each coin can turn up heads or tails, the sample space consists of 25 ϭ 32 outcomes. Since

the coins are fair, we may assign the same probability (1>32) to each outcome. Then the event Ac (No heads
turn up) consists of only 1 outcome. Hence P(Ac) ϭ 1>32, and the answer is P(A) ϭ 1 Ϫ P(Ac) ϭ 31>32. ᭿

The next theorem is a simple extension of Axiom 3, which you can readily prove by
induction.
THEOREM 2

Addition Rule for Mutually Exclusive Events

For mutually exclusive events A1, Á , Am in a sample space S,
(8)

EXAMPLE 3

P(A1 ʜ A2 ʜ Á Am) ϭ P(A1) ϩ P(A2) ϩ Á ϩ P(Am).

Mutually Exclusive Events
If the probability that on any workday a garage will get 10–20, 21–30, 31–40, over 40 cars to service is 0.20,
0.35, 0.25, 0.12, respectively, what is the probability that on a given workday the garage gets at least 21 cars
to service?

Solution. Since these are mutually exclusive events, Theorem 2 gives the answer 0.35 ϩ 0.25 ϩ 0.12 ϭ 0.72.
Check this by the complementation rule.
᭿
In many cases, events will not be mutually exclusive. Then we have
THEOREM 3

Addition Rule for Arbitrary Events

For events A and B in a sample space,
(9)

PROOF

P(A ʜ B) ϭ P(A) ϩ P(B) Ϫ P(A ʝ B).

C, D, E in Fig. 512 make up A ʜ B and are mutually exclusive (disjoint). Hence by
Theorem 2,
P(A ʜ B) ϭ P(C) ϩ P(D) ϩ P(E).
This gives (9) because on the right P(C) ϩ P(D) ϭ P(A) by Axiom 3 and disjointness;
and P(E) ϭ P(B) Ϫ P(D) ϭ P(B) Ϫ P(A ʝ B), also by Axiom 3 and disjointness.
᭿

C

A

D

E

B

Fig. 512. Proof of Theorem 3

c24.qxd

11/3/10

5:12 PM

1022

Page 1022

CHAP. 24 Data Analysis. Probability Theory

Note that for mutually exclusive events A and B we have A ʝ B ϭ л by definition and,
by comparing (9) and (6),
P(л) ϭ 0.

(10)

(Can you also prove this by (5) and (7)?)
EXAMPLE 4

Union of Arbitrary Events
In tossing a fair die, what is the probability of getting an odd number or a number less than 4?

Solution. Let A be the event “Odd number” and B the event “Number less than 4.” Then Theorem 3 gives
the answer
P(A ʜ B) ϭ 36 ϩ 36 Ϫ 26 ϭ 23

᭿

because A ʝ B ϭ “Odd number less than 4” ϭ {1, 3}.

Conditional Probability. Independent Events
Often it is required to find the probability of an event B under the condition that an event
A occurs. This probability is called the conditional probability of B given A and is denoted
by P(B ƒ A). In this case A serves as a new (reduced) sample space, and that probability is
the fraction of P(A) which corresponds to A ʝ B. Thus
(11)

P(B ƒ A) ϭ

P(A ʝ B)
P(A)

[P(A)

0].

[P(B)

0].

Similarly, the conditional probability of A given B is
(12)

P(A ƒ B) ϭ

P(A ʝ B)
P(B)

Solving (11) and (12) for P(A ʝ B), we obtain
THEOREM 4

Multiplication Rule

If A and B are events in a sample space S and P(A)
(13)

EXAMPLE 5

0, P(B)

0, then

P(A ʝ B) ϭ P(A)P(B ƒ A) ϭ P(B)P(A ƒ B).

Multiplication Rule
In producing screws, let A mean “screw too slim” and B “screw too short.” Let P(A) ϭ 0.1 and let the conditional
probability that a slim screw is also too short be P(B ƒ A) ϭ 0.2. What is the probability that a screw that we pick
randomly from the lot produced will be both too slim and too short?

Solution. P(A ʝ B) ϭ P(A)P(B ƒ A) ϭ 0.1 ⅐ 0.2 ϭ 0.02 ϭ 2%, by Theorem 4.
Independent Events.
(14)

If events A and B are such that
P(A ʝ B) ϭ P(A)P(B),

᭿

c24.qxd

11/3/10

5:12 PM

Page 1023

SEC. 24.3 Probability

1023

they are called independent events. Assuming P(A)
that in this case
P(A ƒ B) ϭ P(A),

0, P(B)

0, we see from (11)–(13)

P(B ƒ A) ϭ P(B).

This means that the probability of A does not depend on the occurrence or nonoccurrence
of B, and conversely. This justifies the term “independent.”
Independence of m Events. Similarly, m events A1, Á , Am are called independent if
(15a)

P(A1 ʝ Á ʝ Am) ϭ P(A1) Á P(Am)

as well as for every k different events Aj1, Aj2, Á , Ajk.
(15b)

P(Aj1 ʝ Aj2 ʝ Á ʝ Ajk) ϭ P(Aj1)P(Aj2) Á P(Ajk)

where k ϭ 2, 3, Á , m Ϫ 1.
Accordingly, three events A, B, C are independent if and only if
P(A ʝ B) ϭ P(A)P(B),
P(B ʝ C) ϭ P(B)P(C),

(16)

P(C ʝ A) ϭ P(C)P(A),
P(A ʝ B ʝ C) ϭ P(A)P(B)P(C).
Sampling. Our next example has to do with randomly drawing objects, one at a time,
from a given set of objects. This is called sampling from a population, and there are
two ways of sampling, as follows.
1. In sampling with replacement, the object that was drawn at random is placed back to
the given set and the set is mixed thoroughly. Then we draw the next object at random.
2. In sampling without replacement the object that was drawn is put aside.
EXAMPLE 6

Sampling With and Without Replacement
A box contains 10 screws, three of which are defective. Two screws are drawn at random. Find the probability
that neither of the two screws is defective.

Solution. We consider the events
A: First drawn screw nondefective.
B: Second drawn screw nondefective.
7
Clearly, P(A) ϭ 10
because 7 of the 10 screws are nondefective and we sample at random, so that each screw
1
has the same probability (10
) of being picked. If we sample with replacement, the situation before the second
7
drawing is the same as at the beginning, and P(B) ϭ 10
. The events are independent, and the answer is

P(A ʝ B) ϭ P(A)P(B) ϭ 0.7 ⅐ 0.7 ϭ 0.49 ϭ 49%.
7
If we sample without replacement, then P(A) ϭ 10
, as before. If A has occurred, then there are 9 screws left
in the box, 3 of which are defective. Thus P(B ƒ A) ϭ 69 ϭ 23 , and Theorem 4 yields the answer
7
P(A ʝ B) ϭ 10
⅐ 23 ϭ 47%.

Is it intuitively clear that this value must be smaller than the preceding one?

᭿

c24.qxd

11/3/10

5:12 PM

1024

Page 1024

CHAP. 24 Data Analysis. Probability Theory

PROBLEM SET 24.3
1. In rolling 3 fair dice, what is the probability of obtaining
a sum not greater than 16?
2. In rolling 2 fair dice, what is the probability of a sum
greater than 3 but not exceeding 6?
3. Three screws are drawn at random from a lot of 100
screws, 10 of which are defective. Find the probability
of the event that all 3 screws drawn are nondefective,
assuming that we draw (a) with replacement, (b) without
replacement.
4. In Prob. 3 find the probability of E: At least 1 defective
(i) directly, (ii) by using complements; in both cases
(a) and (b).
5. If a box contains 10 left-handed and 20 right-handed
screws, what is the probability of obtaining at least
one right-handed screw in drawing 2 screws with
replacement?
6. Will the probability in Prob. 5 increase or decrease if we
draw without replacement. First guess, then calculate.
7. Under what conditions will it make practically no
difference whether we sample with or without
replacement?
8. If a certain kind of tire has a life exceeding 40,000 miles
with probability 0.90, what is the probability that a set
of these tires on a car will last longer than 40,000 miles?
9. If we inspect photocopy paper by randomly drawing 5
sheets without replacement from every pack of 500,
what is the probability of getting 5 clean sheets although
0.4% of the sheets contain spots?
10. Suppose that we draw cards repeatedly and with
replacement from a file of 100 cards, 50 of which refer
to male and 50 to female persons. What is the
probability of obtaining the second “female” card before
the third “male” card?
11. A batch of 200 iron rods consists of 50 oversized rods,
50 undersized rods, and 100 rods of the desired length.
If two rods are drawn at random without replacement,
what is the probability of obtaining (a) two rods of the

24.4

desired length, (b) exactly one of the desired length,
(c) none of the desired length?
12. If a circuit contains four automatic switches and we
want that, with a probability of 99%, during a given
time interval the switches to be all working, what
probability of failure per time interval can we admit
for a single switch?
13. A pressure control apparatus contains 3 electronic
tubes. The apparatus will not work unless all tubes are
operative. If the probability of failure of each tube
during some interval of time is 0.04, what is the
corresponding probability of failure of the apparatus?
14. Suppose that in a production of spark plugs the fraction
of defective plugs has been constant at 2% over a long
time and that this process is controlled every half hour
by drawing and inspecting two just produced. Find the
probabilities of getting (a) no defectives, (b) 1
defective, (c) 2 defectives. What is the sum of these
probabilities?
15. What gives the greater probability of hitting at least
once: (a) hitting with probability 1>2 and firing 1 shot,
(b) hitting with probability 1>4 and firing 2 shots,
(c) hitting with probability 1>8 and firing 4 shots? First
guess.
16. You may wonder whether in (16) the last relation
follows from the others, but the answer is no. To see
this, imagine that a chip is drawn from a box containing
4 chips numbered 000, 011, 101, 110, and let A, B, C
be the events that the first, second, and third digit,
respectively, on the drawn chip is 1. Show that then
the first three formulas in (16) hold but the last one
does not hold.
17. Show that if B is a subset of A, then P(B) Ϲ P(A).
18. Extending Theorem 4, show that P(A ʝ B ʝ C ) ϭ
P(A)P(B ƒ A)P(C ƒ A ʝ B).
19. Make up an example similar to Prob. 16, for instance,
in terms of divisibility of numbers.

Permutations and Combinations
Permutations and combinations help in finding probabilities P(A) ϭ a>k by systematically
counting the number a of points of which an event A consists; here, k is the number of
points of the sample space S. The practical difficulty is that a may often be surprisingly
large, so that actual counting becomes hopeless. For example, if in assembling some
instrument you need 10 different screws in a certain order and you want to draw them

c24.qxd

11/3/10

5:12 PM

Page 1025

SEC. 24.4 Permutations and Combinations

1025

randomly from a box (which contains nothing else) the probability of obtaining them in
the required order is only 1>3,628,800 because there are
10! ϭ 1 ⅐ 2 ⅐ 3 ⅐ 4 ⅐ 5 ⅐ 6 ⅐ 7 ⅐ 8 ⅐ 9 ⅐ 10 ϭ 3,628,800
orders in which they can be drawn. Similarly, in many other situations the numbers of
orders, arrangements, etc. are often incredibly large. (If you are unimpressed, take 20
screws—how much bigger will the number be?)

Permutations
A permutation of given things (elements or objects) is an arrangement of these things in
a row in some order. For example, for three letters a, b, c there are 3! ϭ 1 ⅐ 2 ⅐ 3 ϭ 6
permutations: abc, acb, bac, bca, cab, cba. This illustrates (a) in the following theorem.
THEOREM 1

Permutations

(a) Different things. The number of permutations of n different things taken
all at a time is
(1)

n! ϭ 1 ⅐ 2 ⅐ 3 Á n

(read “n factorial ”).

(b) Classes of equal things. If n given things can be divided into c classes of
alike things differing from class to class, then the number of permutations of
these things taken all at a time is
(2)

n!
n 1!n 2! Á n c!

(n 1 ϩ n 2 ϩ Á ϩ n c ϭ n)

Where n j is the number of things in the jth class.
PROOF

EXAMPLE 1

(a) There are n choices for filling the first place in the row. Then n Ϫ 1 things are still
available for filling the second place, etc.
(b) n 1 alike things in class 1 make n 1! permutations collapse into a single permutation
(those in which class 1 things occupy the same n 1 positions), etc., so that (2) follows
᭿
from (1).
Illustration of Theorem 1(b)
If a box contains 6 red and 4 blue balls, the probability of drawing first the red and then the blue balls is
P ϭ 6!4!>10! ϭ 1>210 Ϸ 0.5%.

᭿

A permutation of n things taken k at a time is a permutation containing only k of the
n given things. Two such permutations consisting of the same k elements, in a different
order, are different, by definition. For example, there are 6 different permutations of the
three letters a, b, c, taken two letters at a time, ab, ac, bc, ba, ca, cb.
A permutation of n things taken k at a time with repetitions is an arrangement obtained
by putting any given thing in the first position, any given thing, including a repetition of the
one just used, in the second, and continuing until k positions are filled. For example, there

c24.qxd

11/3/10

5:12 PM

1026

Page 1026

CHAP. 24 Data Analysis. Probability Theory

are 32 ϭ 9 different such permutations of a, b, c taken 2 letters at a time, namely, the
preceding 6 permutations and aa, bb, cc. You may prove (see Team Project 14):
THEOREM 2

Permutations

The number of different permutations of n different things taken k at a time without
repetitions is
(3a)

n(n Ϫ 1)(n Ϫ 2) Á (n Ϫ k ϩ 1) ϭ

n!
(n Ϫ k)!

and with repetitions is
n k.

(3b)
EXAMPLE 2

Illustration of Theorem 2
In an encrypted message the letters are arranged in groups of five letters, called words. From (3b) we see that
the number of different such words is
265 ϭ 11,881,376.
From (3a) it follows that the number of different such words containing each letter no more than once is
26!>(26 Ϫ 5)! ϭ 26 ⅐ 25 ⅐ 24 ⅐ 23 ⅐ 22 ϭ 7,893,600.

᭿

Combinations
In a permutation, the order of the selected things is essential. In contrast, a combination
of given things means any selection of one or more things without regard to order. There
are two kinds of combinations, as follows.
The number of combinations of n different things, taken k at a time, without
repetitions is the number of sets that can be made up from the n given things, each set
containing k different things and no two sets containing exactly the same k things.
The number of combinations of n different things, taken k at a time, with repetitions
is the number of sets that can be made up of k things chosen from the given n things,
each being used as often as desired.
For example, there are three combinations of the three letters a, b, c, taken two letters
at a time, without repetitions, namely, ab, ac, bc, and six such combinations with
repetitions, namely, ab, ac, bc, aa, bb, cc.
THEOREM 3

Combinations

The number of different combinations of n different things taken, k at a time, without
repetitions, is
(4a)

n
n!
n(n Ϫ 1) Á (n Ϫ k ϩ 1)
a bϭ
ϭ
,
k!(n Ϫ k)!
1⅐2Ák
k

and the number of those combinations with repetitions is
(4b)

a

nϩkϪ1
b.
k

c24.qxd

11/3/10

5:12 PM

Page 1027

SEC. 24.4 Permutations and Combinations

PROOF

EXAMPLE 3

1027

The statement involving (4a) follows from the first part of Theorem 2 by noting that there
are k! permutations of k things from the given n things that differ by the order of the
elements (see Theorem 1), but there is only a single combination of those k things of the
type characterized in the first statement of Theorem 3. The last statement of Theorem 3
᭿
can be proved by induction (see Team Project 14).
Illustration of Theorem 3
The number of samples of five lightbulbs that can be selected from a lot of 500 bulbs is [see (4a)]
a

500
5

bϭ

500!
5!495!

ϭ

500 ⅐ 499 ⅐ 498 ⅐ 497 ⅐ 496
1⅐2⅐3⅐4⅐5

᭿

ϭ 255,244,687,600.

Factorial Function
In (1)–(4) the factorial function is basic. By definition,
0! ϭ 1.

(5)

Values may be computed recursively from given values by
(n ϩ 1)! ϭ (n ϩ 1)n!.

(6)

For large n the function is very large (see Table A3 in App. 5). A convenient approximation
for large n is the Stirling formula2
n

n
n! ϳ 12pn a e b

(7)

(e ϭ 2.718 Á )

where ϳ is read “asymptotically equal” and means that the ratio of the two sides of (7)
approaches 1 as n approaches infinity.
EXAMPLE 4

Stirling Formula
n!

By (7)

Exact Value

Relative Error

4!
10!
20!

23.5
3,598,696
2.42279 ⅐ 1018

24
3,628,800
2,432,902,008,176,640,000

2.1%
0.8%
0.4%

᭿

Binomial Coefficients
The binomial coefficients are defined by the formula
(8)

2

a(a Ϫ 1)(a Ϫ 2) Á (a Ϫ k ϩ 1)
a
a bϭ
k!
k

JAMES STIRLING (1692–1770), Scots mathematician.

(k м 0, integer).

c24.qxd

11/3/10

1028

5:12 PM

Page 1028

CHAP. 24 Data Analysis. Probability Theory

The numerator has k factors. Furthermore, we define
(9)

a
a b ϭ 1,
0

in particular,

0
a b ϭ 1.
0

For integer a ϭ n we obtain from (8)
n
n
a bϭa
b
k
nϪk

(10)

(n м 0, 0 Ϲ k Ϲ n).

Binomial coefficients may be computed recursively, because
(11)

a
a
aϩ1
a bϩa
bϭa
b
k
kϩ1
kϩ1

(k м 0, integer).

mϩkϪ1
Ϫm
b ϭ (Ϫ1)k a
b
k
k

(k м 0, integer)
(m Ͼ 0).

Formula (8) also yields
(12)

a

There are numerous further relations; we mention two important ones,
n؊1

kϩs
nϩk
a a k b ϭ ak ϩ 1b
sϭ0

(13)

(k м 0, n м 1,
both integer)

and
r

(14)

p
q
pϩq
a a k b ar Ϫ kb ϭ a r b
kϭ0

(r м 0, integer).

PROBLEM SET 24.4
Note the large numbers in the answers to some of these
problems, which would make counting cases hopeless!
1. In how many ways can a company assign 10 drivers to
n buses, one driver to each bus and conversely?
2. List (a) all permutations, (b) all combinations without
repetitions, (c) all combinations with repetitions, of 5
letters a, e, i, o, u taken 2 at a time.
3. If a box contains 4 rubber gaskets and 2 plastic gaskets,
what is the probability of drawing (a) first the plastic
and then the rubber gaskets, (b) first the rubber and
then the plastic ones? Do this by using a theorem and
checking it by multiplying probabilities.
4. An urn contains 2 green, 3 yellow, and 5 red balls. We
draw 1 ball at random and put it aside. Then we draw
the next ball, and so on. Find the probability of drawing

5.

6.
7.

8.

at first the 2 green balls, then the 3 yellow ones, and
finally the red ones.
In how many different ways can we select a committee
consisting of 3 engineers, 2 physicists, and 2 computer
scientists from 10 engineers, 5 physicists, and 6
computer scientists? First guess.
How many different samples of 4 objects can we draw
from a lot of 50?
Of a lot of 10 items, 2 are defective. (a) Find the
number of different samples of 4. Find the number of
samples of 4 containing (b) no defectives, (c) 1
defective, (d) 2 defectives.
Determine the number of different bridge hands. (A
bridge hand consists of 13 cards selected from a full
deck of 52 cards.)

c24.qxd

11/3/10

5:12 PM

Page 1029

SEC. 24.5 Random Variables. Probability Distributions

1029

9. In how many different ways can 6 people be seated at
a round table?

findings, arranged in logical order and illustrated with
numeric examples.
14. TEAM PROJECT. Permutations, Combinations.
(a) Prove Theorem 2.
(b) Prove the last statement of Theorem 3.
(c) Derive (11) from (8).
(d) By the binomial theorem,

10. If a cage contains 100 mice, 3 of which are male, what
is the probability that the 3 male mice will be included
if 10 mice are randomly selected?
11. How many automobile registrations may the police
have to check in a hit-and-run accident if a witness
reports KDP7 and cannot remember the last two digits
on the license plate but is certain that all three digits
were different?
12. If 3 suspects who committed a burglary and 6 innocent
persons are lined up, what is the probability that a
witness who is not sure and has to pick three persons
will pick the three suspects by chance? That the witness
picks 3 innocent persons by chance?
13. CAS PROJECT. Stirling formula. (a) Using (7),
compute approximate values of n! for n ϭ 1, Á , 20.
(b) Determine the relative error in (a). Find an
empirical formula for that relative error.
(c) An upper bound for that relative error is
e1>12n Ϫ 1. Try to relate your empirical formula to this.
(d) Search through the literature for further information
on Stirling’s formula. Write a short eassy about your

24.5

n

n
(a ϩ b)n ϭ a a b a kb n؊k,
k
kϭ0
so that a kb n؊k has the coefficient

A nk B . Can you

conclude this from Theorem 3 or is this a mere
coincidence?
(e) Prove (14) by using the binomial theorem.
(f) Collect further formulas for binomial coefficients
from the literature and illustrate them numerically.
15. Birthday problem. What is the probability that in a
group of 20 people (that includes no twins) at least
two have the same birthday, if we assume that the
probability of having birthday on a given day is 1>365
for every day. First guess. Hint. Consider the complementary event.

Random Variables.
Probability Distributions
In Sec. 24.1 we considered frequency distributions of data. These distributions show the
absolute or relative frequency of the data values. Similarly, a probability distribution
or, briefly, a distribution, shows the probabilities of events in an experiment. The quantity
that we observe in an experiment will be denoted by X and called a random variable
(or stochastic variable) because the value it will assume in the next trial depends on
chance, on randomness—if you roll a die, you get one of the numbers from 1 to 6, but
you don’t know which one will show up next. Thus X ϭ Number a die turns up is a
random variable. So is X ϭ Elasticity of rubber (elongation at break). (“Stochastic” means
related to chance.)
If we count (cars on a road, defective screws in a production, tosses until a die shows
the first Six), we have a discrete random variable and distribution. If we measure
(electric voltage, rainfall, hardness of steel), we have a continuous random variable and
distribution. Precise definitions follow. In both cases the distribution of X is determined
by the distribution function
(1)

F(x) ϭ P(X Ϲ x);

this is the probability that in a trial, X will assume any value not exceeding x.
CAUTION! The terminology is not uniform. F(x) is sometimes also called the
cumulative distribution function.

c24.qxd

11/3/10

5:12 PM

1030

Page 1030

CHAP. 24 Data Analysis. Probability Theory

For (1) to make sense in both the discrete and the continuous case we formulate conditions as follows.
DEFINITION

Random Variable

A random variable X is a function defined on the sample space S of an experiment.
Its values are real numbers. For every number a the probability
P(X ϭ a)
with which X assumes a is defined. Similarly, for any interval I the probability
P(X ʦ I )
with which X assumes any value in I is defined.
Although this definition is very general, in practice only a very small number of distributions
will occur over and over again in applications.
From (1) we obtain the fundamental formula for the probability corresponding to an
interval a Ͻ x Ϲ b,
(2)

P(a Ͻ X Ϲ b) ϭ F(b) Ϫ F(a).

This follows because X Ϲ a (“X assumes any value not exceeding a”) and a Ͻ X Ϲ b
(“X assumes any value in the interval a Ͻ x Ϲ b”) are mutually exclusive events, so that
by (1) and Axiom 3 of Definition 2 in Sec. 24.3
F(b) ϭ P(X Ϲ b) ϭ P(X Ϲ a) ϩ P(a Ͻ X Ϲ b)
ϭ F(a) ϩ P(a Ͻ X Ϲ b)
and subtraction of F(a) on both sides gives (2).

Discrete Random Variables and Distributions
By definition, a random variable X and its distribution are discrete if X assumes only finitely
many or at most countably many values x 1, x 2, x 3, Á , called the possible values of X,
with positive probabilities p1 ϭ P(X ϭ x 1), p2 ϭ P(X ϭ x 2), p3 ϭ P(X ϭ x 3), Á ,
whereas the probability P(X ʦ I ) is zero for any interval I containing no possible value.
Clearly, the discrete distribution of X is also determined by the probability function
f (x) of X, defined by
(3)

f (x) ϭ b

pj

if x ϭ x j

0

otherwise

( j ϭ 1, 2, Á ),

From this we get the values of the distribution function F(x) by taking sums,
(4)

F(x) ϭ a f (x j) ϭ a pj
xj Ϲx

xj Ϲx

c24.qxd

11/3/10

5:12 PM

Page 1031

SEC. 24.5 Random Variables. Probability Distributions

1031

where for any given x we sum all the probabilities pj for which x j is smaller than or equal
to that of x. This is a step function with upward jumps of size pj at the possible values
x j of X and constant in between.
EXAMPLE 1

Probability Function and Distribution Function
Figure 513 shows the probability function f (x) and the distribution function F (x) of the discrete random variable
X ϭ Number a fair die turns up.
X has the possible values x ϭ 1, 2, 3, 4, 5, 6 with probability 1>6 each. At these x the distribution function
has upward jumps of magnitude 1>6. Hence from the graph of f (x) we can construct the graph of F(x) and
conversely.
In Figure 513 (and the next one) at each jump the fat dot indicates the function value at the jump!
᭿
f (x)

f (x)

1

1

6

0

x

5

6

0

5

10

12

x

5

10

12

x

F(x)

F(x)

1

1

30
36
20
36

1
2

10
36

0

5

0

x

Fig. 513. Probability function ƒ(x)
and distribution function F(x) of the
random variable X ϭ Number
obtained in tossing a fair die once

EXAMPLE 2

Fig. 514. Probability function ƒ(x) and
distribution function F(x) of the random
variable X ϭ Sum of the two numbers
obtained in tossing two fair dice once

Probability Function and Distribution Function
The random variable X ϭ Sum of the two numbers two fair dice turn up is discrete and has the possible values
2 (ϭ 1 ϩ 1), 3, 4, Á , 12 (ϭ 6 ϩ 6). There are 6 ⅐ 6 ϭ 36 equally likely outcomes (1, 1) (1, 2), Á , (6, 6),
where the first number is that shown on the first die and the second number that on the other die. Each such
outcome has probability 1>36. Now X ϭ 2 occurs in the case of the outcome (1, 1); X ϭ 3 in the case of the
two outcomes (1, 2) and (2, 1); X ϭ 4 in the case of the three outcomes (1, 3), (2, 2), (3, 1); and so on. Hence
f (x) ϭ P(X ϭ x) and F(x) ϭ P(X Ϲ x) have the values

x

2

3

4

5

6

7

8

9

10

11

12

ƒ(x)
F(x)

1/36
1/36

2/36
3/36

3/36
6/36

4/36
10/36

5/36
15/36

6/36
21/36

5/36
26/36

4/36
30/36

3/36
33/36

2/36
35/36

1/36
36/36

Figure 514 shows a bar chart of this function and the graph of the distribution function, which is again a step
function, with jumps (of different height!) at the possible values of X.
᭿

c24.qxd

11/3/10

5:12 PM

1032

Page 1032

CHAP. 24 Data Analysis. Probability Theory

Two useful formulas for discrete distributions are readily obtained as follows. For the
probability corresponding to intervals we have from (2) and (4)

(5)

P(a Ͻ X Ϲ b) ϭ F(b) Ϫ F(a) ϭ

a pj

(X discrete).

aϽxj Ϲb

This is the sum of all probabilities pj for which x j satisfies a Ͻ x j Ϲ b. (Be careful about
Ͻ and Ϲ ! ) From this and P(S) ϭ 1 (Sec. 24.3) we obtain the following formula.
a pj ϭ 1

(6)

EXAMPLE 3

(sum of all probabilities).

j

Illustration of Formula (5)
In Example 2, compute the probability of a sum of at least 4 and at most 8.

᭿

3
23
Solution. P(3 Ͻ X Ϲ 8) ϭ F(8) Ϫ F(3) ϭ 26
36 Ϫ 36 ϭ 36 .

EXAMPLE 4

Waiting Time Problem. Countably Infinite Sample Space
In tossing a fair coin, let X ϭ Number of trials until the first head appears. Then, by independence of events
(Sec. 24.3),
ϭ 12

P(X ϭ 1) ϭ P(H )

ϭ

1
2

P(X ϭ 3) ϭ P(T TH ) ϭ

1
2

P(X ϭ 2) ϭ P(TH )

(H ϭ Head)
⅐

1
2

ؒ

1
2

ؒ

1
2

ϭ

1
4

ϭ

1
8,

(T ϭ Tail)
etc.

and in general P(X ϭ n) ϭ (12 )n, n ϭ 1, 2, Á . Also, (6) can be confirmed by the sum formula for the geometric
series,
1
2

ϩ

1
4

ϩ

1
8

ϩ Á ϭ Ϫ1 ϩ

1
1 Ϫ 12

ϭ Ϫ1 ϩ 2 ϭ 1.

᭿

Continuous Random Variables and Distributions
Discrete random variables appear in experiments in which we count (defectives in a
production, days of sunshine in Chicago, customers standing in a line, etc.). Continuous
random variables appear in experiments in which we measure (lengths of screws, voltage
in a power line, Brinell hardness of steel, etc.). By definition, a random variable X and
its distribution are of continuous type or, briefly, continuous, if its distribution function
F(x) [defined in (1)] can be given by an integral

(7)

F(x) ϭ

Ύ

x

f (v) dv

؊ؕ

c24.qxd

11/3/10

5:12 PM

Page 1033

SEC. 24.5 Random Variables. Probability Distributions

1033

(we write v because x is needed as the upper limit of the integral) whose integrand f (x),
called the density of the distribution, is nonnegative, and is continuous, perhaps except
for finitely many x-values. Differentiation gives the relation of f to F as
f (x) ϭ F r(x)

(8)

for every x at which f (x) is continuous.
From (2) and (7) we obtain the very important formula for the probability corresponding
to an interval:

(9)

P(a Ͻ X Ϲ b) ϭ F(b) Ϫ F(a) ϭ

Ύ

b

f (v) dv.

a

This is the analog of (5).
From (7) and P(S) ϭ 1 (Sec. 24.3) we also have the analog of (6):

Ύ

(10)

ؕ

f (v) dv ϭ 1.

؊ؕ

Continuous random variables are simpler than discrete ones with respect to intervals.
Indeed, in the continuous case the four probabilities corresponding to a Ͻ X Ϲ b,
a Ͻ X Ͻ b, a Ϲ X Ͻ b, and a Ϲ X Ϲ b with any fixed a and b (Ͼ a) are all the same.
Can you see why? (Answer. This probability is the area under the density curve, as in
Fig. 515, and does not change by adding or subtracting a single point in the interval of
integration.) This is different from the discrete case! (Explain.)
The next example illustrates notations and typical applications of our present formulas.
Curve of density
f (x)

P(a < X ≤ b)

a

b

x

Fig. 515. Example illustrating formula (9)

EXAMPLE 5

Continuous Distribution
Let X have the density function f (x) ϭ 0.75(1 Ϫ x 2) if Ϫ1 Ϲ x Ϲ 1 and zero otherwise. Find the distribution
function. Find the probabilities P(Ϫ12 Ϲ X Ϲ 12 ) and P( 14 Ϲ X Ϲ 2). Find x such that P(X Ϲ x) ϭ 0.95.

Solution. From (7) we obtain F(x) ϭ 0 if x Ϲ Ϫ1,
F(x) ϭ 0.75

Ύ

x

(1 Ϫ v2) dv ϭ 0.5 ϩ 0.75x Ϫ 0.25x 3

if Ϫ1 Ͻ x Ϲ 1,

؊1

and F(x) ϭ 1 if x Ͼ 1. From this and (9) we get
P(Ϫ12 Ϲ X Ϲ 12 ) ϭ F(12 ) Ϫ F(Ϫ12 ) ϭ 0.75

Ύ

1>2

؊1>2

(1 Ϫ v2) dv ϭ 68.75%

c24.qxd

11/3/10

1034

5:12 PM

Page 1034

CHAP. 24 Data Analysis. Probability Theory
(because P(Ϫ12 Ϲ X Ϲ 12 ) ϭ P(Ϫ12 Ͻ X Ϲ 12 ) for a continuous distribution) and
P(14 Ϲ X Ϲ 2) ϭ F(2) Ϫ F(14 ) ϭ 0.75

Ύ

1

(1 Ϫ v2) dv ϭ 31.64%.

1>4

(Note that the upper limit of integration is 1, not 2. Why?) Finally,
P(X Ϲ x) ϭ F(x) ϭ 0.5 ϩ 0.75x Ϫ 0.25x 3 ϭ 0.95.
Algebraic simplification gives 3x Ϫ x 3 ϭ 1.8. A solution is x ϭ 0.73, approximately.
Sketch f (x) and mark x ϭ Ϫ12 , 12 , 14 , and 0.73, so that you can see the results (the probabilities) as areas under
the curve. Sketch also F(x).
᭿

Further examples of continuous distributions are included in the next problem set and in
later sections.

PROBLEM SET 24.5
1. Graph the probability function f (x) ϭ kx 2 (x ϭ 1, 2, 3,
4, 5; k suitable) and the distribution function.
2. Graph the density function f (x) ϭ kx 2 (0 Ϲ x Ϲ 5;
k suitable) and the distribution function.
3. Uniform distribution. Graph f and F when the density
of X is f (x) ϭ k ϭ const if Ϫ2 Ϲ x Ϲ 2 and 0 elsewhere. Find P(0 Ϲ X Ϲ 2).
4. In Prob. 3 find c and ෂ
c such that P(Ϫc Ͻ X Ͻ c) ϭ
95% and P(0 Ͻ X Ͻ ෂ
c ) ϭ 95%.
5. Graph f and F when f (Ϫ2) ϭ f (2) ϭ 18, f (Ϫ1) ϭ
f (1) ϭ 38. Can f have further positive values?
6. A box contains 4 right-handed and 6 left-handed
screws. Two screws are drawn at random without
replacement. Let X be the number of left-handed
screws drawn. Find the probabilities P(X ϭ 0),
P(X ϭ 1), P(X ϭ 2), P(1 Ͻ X Ͻ 2), P(X Ϲ 1),
P(X м 1), P(X Ͼ 1), and P(0.5 Ͻ X Ͻ 10).
7. Let X be the number of years before a certain kind of
pump needs replacement. Let X have the probability
function f (x) ϭ kx 3, x ϭ 0, 1, 2, 3, 4, Find k. Sketch f
and F.
8. Graph the distribution function F(x) ϭ 1 Ϫ e؊3x if
x Ͼ 0, F(x) ϭ 0 if x Ϲ 0, and the density f (x). Find x
such that F(x) ϭ 0.9.
9. Let X [millimeters] be the thickness of washers.
Assume that X has the density f (x) ϭ kx if
0.9 Ͻ x Ͻ 1.1 and 0 otherwise. Find k. What is the
probability that a washer will have thickness between
0.95 mm and 1.05 mm?

10. If the diameter X of axles has the density f (x) ϭ k if
119.9 Ϲ x Ϲ 120.1 and 0 otherwise, how many
defectives will a lot of 500 axles approximately contain
if defectives are axles slimmer than 119.91 or thicker
than 120.09?
11. Find the probability that none of three bulbs in a traffic
signal will have to be replaced during the first 1500
hours of operation if the lifetime X of a bulb is a random
variable with the density f (x) ϭ 630.25 Ϫ (x Ϫ 1.5)24
when 1 Ϲ x Ϲ 2 and f (x) ϭ 0 otherwise, where x is
measured in multiples of 1000 hours.
12 Let X be the ratio of sales to profits of some company.
Assume that X has the distribution function F(x) ϭ 0 if
x Ͻ 2, F(x) ϭ (x 2 Ϫ 4)>5 if 2 Ϲ x Ͻ 3, F(x) ϭ 1 if
x м 3. Find and sketch the density. What is the probability
that X is between 2.5 (40% profit) and 5 (20% profit)?
13. Suppose that in an automatic process of filling oil
cans, the content of a can (in gallons) is Y ϭ 100 ϩ X,
where X is a random variable with density
f (x) ϭ 1 Ϫ ƒ x ƒ when ƒ x ƒ Ϲ 1 and 0 when ƒ x ƒ Ͼ 1.
Sketch f (x) and F(x). In a lot of 1000 cans, about how
many will contain 100 gallons or more? What is the
probability that a can will contain less than 99.5
gallons? Less than 99 gallons?
14. Find the probability function of X ϭ Number of times
a fair die is rolled until the first Six appears and show
that it satisfies (6).
15. Let X be a random variable that can assume every real
value. What are the complements of the events X Ϲ b,
X Ͻ b, X м c, X Ͼ c, b Ϲ X Ϲ c, b Ͻ X Ϲ c?

c24.qxd

11/3/10

5:12 PM

Page 1035

SEC. 24.6 Mean and Variance of a Distribution

24.6

1035

Mean and Variance of a Distribution
The mean ␮ and variance s2 of a random variable X and of its distribution are the theoretical
counterparts of the mean x and variance s 2 of a frequency distribution in Sec. 24.1 and
serve a similar purpose. Indeed, the mean characterizes the central location and the variance
the spread (the variability) of the distribution. The mean ␮ (mu) is defined by
(a) ␮ ϭ a x j f (x j)

(Discrete distribution)

j

(1)

Ύ

(b) ␮ ϭ

ؕ

(Continuous distribution)

x f (x) dx

؊ؕ

and the variance s2 (sigma square) by
(a) s2 ϭ a (x j Ϫ ␮)2f (x j)

(Discrete distribution)

j

(2)
(b) s2 ϭ

Ύ

ؕ

(x Ϫ ␮)2f (x) dx

(Continuous distribution).

؊ؕ

s (the positive square root of s2) is called the standard deviation of X and its distribution.
f is the probability function or the density, respectively, in (a) and (b).
The mean ␮ is also denoted by E(X ) and is called the expectation of X because it gives
the average value of X to be expected in many trials. Quantities such as ␮ and s2 that
measure certain properties of a distribution are called parameters. ␮ and s2 are the two
most important ones. From (2) we see that
s2 Ͼ 0

(3)

(except for a discrete “distribution” with only one possible value, so that s2 ϭ 0). We
assume that ␮ and s2 exist (are finite), as is the case for practically all distributions that
are useful in applications.
EXAMPLE 1

Mean and Variance
The random variable X ϭ Number of heads in a single toss of a fair coin has the possible values X ϭ 0 and
X ϭ 1 with probabilities P(X ϭ 0) ϭ 12 and P(X ϭ 1) ϭ 12 . From (la) we thus obtain the mean
␮ ϭ 0 ⅐ 12 ϩ 1 ⅐ 12 ϭ 12 , and (2a) yields the variance
s2 ϭ (0 Ϫ 12 )2 ⅐ 12 ϩ (1 Ϫ 12 )2 ⅐

EXAMPLE 2

1
2

ϭ 14 .

Uniform Distribution. Variance Measures Spread
The distribution with the density
f (x) ϭ

1
bϪa

if

aϽxϽb

᭿

c24.qxd

11/3/10

5:12 PM

1036

Page 1036

CHAP. 24 Data Analysis. Probability Theory
and f ϭ 0 otherwise is called the uniform distribution on the interval a Ͻ x Ͻ b. From (1b) (or from Theorem 1,
below) we find that ␮ ϭ (a ϩ b)>2, and (2b) yields the variance
s2 ϭ

Ύ

b

a

ax Ϫ

aϩb
2

2

b

1
bϪa

(b Ϫ a)2

dx ϭ

12

.

᭿

Figure 516 illustrates that the spread is large if and only if s2 is large.
f (x)

0

f (x)

΂σ 2 = 121 ΃

1

1

x

–1

0

F(x)

F(x)

1

1

0

1

΂σ 2 = 34 ΃

1

x

–1

0

1

2

1

2

Fig. 516. Uniform distributions having the same mean (0.5) but different variances s

x

x
2

Symmetry. We can obtain the mean ␮ without calculation if a distribution is symmetric.
Indeed, you may prove
THEOREM 1

Mean of a Symmetric Distribution

If a distribution is symmetric with respect to x ϭ c, that is, f (c Ϫ x) ϭ f (c ϩ x),
then ␮ ϭ c. (Examples 1 and 2 illustrate this.)

Transformation of Mean and Variance
Given a random variable X with mean ␮ and variance s2, we want to calculate the mean
and variance of X* ϭ a1 ϩ a2X, where a1 and a2 are given constants. This problem is
important in statistics, where it often appears.
THEOREM 2

Transformation of Mean and Variance

(a) If a random variable X has mean ␮ and variance s2, then the random
variable
(4)

X* ϭ a1 ϩ a2X

(a2 Ͼ 0)

has the mean ␮* and variance s*2, where
(5)

␮* ϭ a1 ϩ a2␮

and

s*2 ϭ a 22s2.

c24.qxd

11/3/10

5:12 PM

Page 1037

SEC. 24.6 Mean and Variance of a Distribution

1037

(b) In particular, the standardized random variable Z corresponding to X,
given by
Zϭ

(6)

XϪ␮
s

has the mean 0 and the variance 1.

PROOF

We prove (5) for a continuous distribution. To a small interval I of length ¢x on the
x-axis there corresponds the probability f (x)¢x [approximately; the area of a rectangle
of base ¢x and height f (x)]. Then the probability f (x)¢x must equal that for the
corresponding interval on the x*-axis, that is, f *(x*)¢x*, where f * is the density of X*
and ¢x* is the length of the interval on the x*-axis corresponding to I. Hence for
differentials we have f *(x*) dx* ϭ f (x) dx. Also, x* ϭ a1 ϩ a2x by (4), so that (1b)
applied to X* gives
␮* ϭ

Ύ

ؕ

x*f *(x*) dx*

؊ؕ

ϭ

Ύ

ؕ

(a1 ϩ a2x) f (x) dx

؊ؕ

ϭ a1

Ύ

ؕ

f (x) dx ϩ a2

؊ؕ

Ύ

ؕ

x f (x) dx.

؊ؕ

On the right the first integral equals 1, by (10) in Sec. 24.5. The second intergral is ␮.
This proves (5) for ␮*. It implies
x* Ϫ ␮* ϭ (a1 ϩ a2x) Ϫ (a1 ϩ a2␮) ϭ a2(x Ϫ ␮).
From this and (2) applied to X*, again using f *(x*) dx* ϭ f (x) dx, we obtain the second
formula in (5),
s*2 ϭ

Ύ

ؕ

(x* Ϫ ␮*) f *(x*) dx* ϭ
2

؊ؕ

a 22

Ύ

ؕ

(x Ϫ ␮)2f (x) dx ϭ a 22s2.

؊ؕ

For a discrete distribution the proof of (5) is similar.
Choosing a1 ϭ Ϫ␮>s and a2 ϭ 1>s we obtain (6) from (4), writing X* ϭ Z. For these
a1, a2 formula (5) gives ␮* ϭ 0 and s*2 ϭ 1, as claimed in (b).
᭿

Expectation, Moments
Recall that (1) defines the expectation (the mean) of X, the value of X to be expected on
the average, written ␮ ϭ E(X ). More generally, if g(x) is nonconstant and continuous for
all x, then g(X ) is a random variable. Hence its mathematical expectation or, briefly, its

c24.qxd

11/3/10

1038

5:12 PM

Page 1038

CHAP. 24 Data Analysis. Probability Theory

expectation E(g(X)) is the value of g(X ) to be expected on the average, defined [similarly
to (1)] by
(7)

E(g(X)) ϭ a g(x j) f (x j)

E(g(X)) ϭ

or

Ύ

ؕ

g(x) f (x) dx.

؊ؕ

j

In the first formula, f is the probability function of the discrete random variable X. In the
second formula, f is the density of the continuous random variable X. Important special
cases are the kth moment of X (where k ϭ 1, 2, Á )
E(X k) ϭ a x kj f (x j)

(8)

Ύ

or

ؕ

x kf (x) dx

؊ؕ

j

and the kth central moment of X (k ϭ 1, 2, Á )
(9)

E([X Ϫ ␮]k) ϭ a (x j Ϫ ␮)kf (x j)

or

Ύ

ؕ

(x Ϫ ␮)kf (x) dx.

؊ؕ

j

This includes the first moment, the mean of X
(10)

␮ ϭ E(X)

[(8) with k ϭ 1].

It also includes the second central moment, the variance of X
(11)

s2 ϭ E([X Ϫ ␮]2)

[(9) with k ϭ 2].

For later use you may prove
(12)

E(1) ϭ 1.

PROBLEM SET 24.6
1–8

MEAN, VARIANCE

Find the mean and variance of the random variable X with
probability function or density f (x).
1. f (x) ϭ k x (0 Ϲ x Ϲ 2, k suitable)
2. X ϭ Number a fair die turns up
3. Uniform distribution on [0, 2p]
4. Y ϭ 13(X Ϫ ␮)> p with X as in Prob. 3
5. f (x) ϭ 4e ؊4x (x м 0)
6. f (x) ϭ k(1 Ϫ x 2) if Ϫ1 Ϲ x Ϲ 1 and 0 otherwise
7. f (x) ϭ Ce؊x>2 (x ϭ 0)
8. X ϭ Number of times a fair coin is flipped until the
first Head appears. (Calculate ␮ only.)
9. If the diameter X [cm] of certain bolts has the density
f (x) ϭ k(x Ϫ 0.9)(1.1 Ϫ x) for 0.9 Ͻ x Ͻ 1.1 and 0
for other x, what are k, ␮, and s2? Sketch f (x).

10. If, in Prob. 9, a defective bolt is one that deviates from
1.00 cm by more than 0.06 cm, what percentage of
defectives should we expect?
11. For what choice of the maximum possible deviation
from 1.00 cm shall we obtain 10% defectives in Probs. 9
and 10?
12. What total sum can you expect in rolling a fair die
20 times? Do the experiment. Repeat it a number of
times and record how the sum varies.
13. What is the expected daily profit if a store sells X air
conditioners per day with probability f (10) ϭ 0.1,
f (11) ϭ 0.3, f (12) ϭ 0.4, f (13) ϭ 0.2 and the profit
per conditioner is $55?
14. Find the expectation of g(X ) ϭ X 2, where X is uniformly
distributed on the interval Ϫ1 Ϲ x Ϲ 1.

c24.qxd

11/3/10

5:12 PM

Page 1039

SEC. 24.7 Binomial, Poisson, and Hypergeometric Distributions

(b) Prove (10)–(12).
(c) Find all the moments of the uniform distribution
on an interval a Ϲ x Ϲ b.
(d) The skewness g of a random variable X is defined
by

15. A small filling station is supplied with gasoline every
Saturday afternoon. Assume that its volume X of sales
in ten thousands of gallons has the probability density
f (x) ϭ 6x(1 Ϫ x) if 0 Ϲ x Ϲ 1 and 0 otherwise.
Determine the mean, the variance, and the standardized
variable.
16. What capacity must the tank in Prob. 15 have in order
that the probability that the tank will be emptied in a
given week be 5%?
17. James rolls 2 fair dice, and Harry pays k cents to James,
where k is the product of the two faces that show on
the dice. How much should James pay to Harry for
each game to make the game fair?
18. What is the mean life of a lightbulb whose life X [hours]
has the density f (x) ϭ 0.001e؊0.001x (x м 0)?
19. Let X be discrete with probability function f (0) ϭ f (3) ϭ
1
3
3
8 , f (1) ϭ f (2) ϭ 8 . Find the expectation of X .
20. TEAM PROJECT. Means, Variances, Expectations.
(a) Show that E(X Ϫ ␮) ϭ 0, s2 ϭ E(X 2) Ϫ ␮2.

24.7

1039

gϭ

(13)

1
s3

E([X Ϫ ␮]3).

Show that for a symmetric distribution (whose third
central moment exists) the skewness is zero.
(e) Find the skewness of the distribution with density
f (x) ϭ xe؊x when x Ͼ 0 and f (x) ϭ 0 otherwise.
Sketch f (x).
(f) Calculate the skewness of a few simple discrete
distributions of your own choice.
(g) Find a nonsymmetric discrete distribution with
3 possible values, mean 0, and skewness 0.

Binomial, Poisson, and Hypergeometric
Distributions
These are the three most important discrete distributions, with numerous applications.

Binomial Distribution
The binomial distribution occurs in games of chance (rolling a die, see below, etc.),
quality inspection (e.g., counting of the number of defectives), opinion polls (counting
number of employees favoring certain schedule changes, etc.), medicine (e.g., recording
the number of patients who recovered on a new medication), and so on. The conditions
of its occurrence are as follows.
We are interested in the number of times an event A occurs in n independent trials. In
each trial the event A has the same probability P(A) ϭ p. Then in a trial, A will not occur
with probability q ϭ 1 Ϫ p. In n trials the random variable that interests us is
X ϭ Number of times the event A occurs in n trials.
X can assume the values 0, 1, Á , n, and we want to determine the corresponding
probabilities. Now X ϭ x means that A occurs in x trials and in n Ϫ x trials it does not
occur. This may look as follows.
(1)

A AÁA

B Á B.

}

}
x times

B

n Ϫ x times

Here B ϭ Ac is the complement of A, meaning that A does not occur (Sec. 24.2). We now
use the assumption that the trials are independent, that is, they do not influence each other.
Hence (1) has the probability (see Sec. 24.3 on independent events)

c24.qxd

11/3/10

1040

5:12 PM

Page 1040

CHAP. 24 Data Analysis. Probability Theory

(1*)

}

qq Á q ϭ p xq n؊x.

}

ؒ

pp Á p

n Ϫ x times

x times

Now (1) is just one order of arranging x A’s and n Ϫ x B’s. We now use Theorem 1(b)
in Sec. 24.4, which gives the number of permutations of n things (the n outcomes of the
n trials) consisting of 2 classes, class 1 containing the n 1 ϭ x A’s and class 2 containing
the n Ϫ n 1 ϭ n Ϫ x B’s. This number is
n
n!
ϭa b.
x!(n Ϫ x)!
x
Accordingly, (1*), multiplied by this binomial coefficient, gives the probability P(X ϭ x)
of X ϭ x, that is, of obtaining A precisely x times in n trials. Hence X has the probability
function
n
f (x) ϭ a b p xq n؊x
x

(2)

(x ϭ 0, 1, Á , n)

and f (x) ϭ 0 otherwise. The distribution of X with probability function (2) is called the
binomial distribution or Bernoulli distribution. The occurrence of A is called success
(regardless of what it actually is; it may mean that you miss your plane or lose your watch)
and the nonoccurrence of A is called failure. Figure 517 shows typical examples. Numeric
values can be obtained from Table A5 in App. 5 or from your CAS.
The mean of the binomial distribution is (see Team Project 16)
␮ ϭ np

(3)

and the variance is (see Team Project 16)
s2 ϭ npq.

(4)

For the symmetric case of equal chance of success and failure (p ϭ q ϭ 12) this gives the
mean n>2, the variance n>4, and the probability function
n
1 n
f (x) ϭ a b a b
2
x

(2*)

(x ϭ 0, 1, Á , n).

0.5

0
0

5
p = 0.1

0

5
p = 0.2

0

5
p = 0.5

0

5
p = 0.8

0

5
p = 0.9

Fig. 517. Probability function (2) of the binomial distribution for n ϭ 5 and various values of p

c24.qxd

11/3/10

5:12 PM

Page 1041

SEC. 24.7 Binomial, Poisson, and Hypergeometric Distributions
EXAMPLE 1

1041

Binomial Distribution
Compute the probability of obtaining at least two “Six” in rolling a fair die 4 times.

Solution. p ϭ P(A) ϭ P(“Six”) ϭ 16, q ϭ 56, n ϭ 4. The event “At least two ‘Six’” occurs if we obtain 2 or
3 or 4 “Six.” Hence the answer is
2

2

3

4

1
5
1
5
1
4
4
4
P ϭ f (2) ϩ f (3) ϩ f (4) ϭ a b a b a b ϩ a b a b a b ϩ a b a b
2
3
4
6
6
6
6
6
ϭ

1
64

(6 ⅐ 25 ϩ 4 ⅐ 5 ϩ 1) ϭ

171
1296

᭿

ϭ 13.2%.

Poisson Distribution
The discrete distribution with infinitely many possible values and probability function
f (x) ϭ

(5)

␮x
x!

e؊␮

(x ϭ 0, 1, Á )

is called the Poisson distribution, named after S. D. Poisson (Sec. 18.5). Figure 518
shows (5) for some values of ␮. It can be proved that this distribution is obtained as a
limiting case of the binomial distribution, if we let p : 0 and n : ϱ so that the mean
␮ ϭ np approaches a finite value. (For instance, ␮ ϭ np may be kept constant.) The
Poisson distribution has the mean ␮ and the variance (see Team Project 16)
s2 ϭ ␮.

(6)

Figure 518 gives the impression that, with increasing mean, the spread of the distribution
increases, thereby illustrating formula (6), and that the distribution becomes more and
more (approximately) symmetric.

0.5

0

5

μ = 0.5

0

5

μ=1

0

5

μ= 2

0

5

10

μ=5

Fig. 518. Probability function (5) of the Poisson distribution for various values of ␮

EXAMPLE 2

Poisson Distribution
If the probability of producing a defective screw is p ϭ 0.01, what is the probability that a lot of 100 screws
will contain more than 2 defectives?

Solution. The complementary event is Ac: Not more than 2 defectives. For its probability we get, from the
binomial distribution with mean ␮ ϭ np ϭ 1, the value [see (2)]
P(Ac) ϭ a

100
100
100
b 0.99100 ϩ a
b 0.01 ⅐ 0.9999 ϩ a
b 0.012 ⅐ 0.9998.
0
1
2

c24.qxd

11/3/10

5:12 PM

1042

Page 1042

CHAP. 24 Data Analysis. Probability Theory
Since p is very small, we can approximate this by the much more convenient Poisson distribution with mean
␮ ϭ np ϭ 100 ⅐ 0.01 ϭ 1, obtaining [see (5)]
P(Ac) Ϸ e؊1 (1 ϩ 1 ϩ 12 )
ϭ 91.97%.
Thus P(A) ϭ 8.03%. Show that the binomial distribution gives P(A) ϭ 7.94%, so that the Poisson approximation
is quite good.
᭿

EXAMPLE 3

Parking Problems. Poisson Distribution
If on the average, 2 cars enter a certain parking lot per minute, what is the probability that during any given
minute 4 or more cars will enter the lot?

Solution. To understand that the Poisson distribution is a model of the situation, we imagine the minute to
be divided into very many short time intervals, let p be the (constant) probability that a car will enter the lot
during any such short interval, and assume independence of the events that happen during those intervals. Then
we are dealing with a binomial distribution with very large n and very small p, which we can approximate by
the Poisson distribution with
␮ ϭ np ϭ 2,
because 2 cars enter on the average. The complementary event of the event “4 cars or more during a given
minute” is “3 cars or fewer enter the lot” and has the probability
f (0) ϩ f (1) ϩ f (2) ϩ f (3) ϭ e؊2 a

20
0!

ϩ

21
1!

ϩ

22
2!

ϩ

23
3!

b

ϭ 0.857.
Answer: 14.3%.

(Why did we consider that complement?)

᭿

Sampling with Replacement
This means that we draw things from a given set one by one, and after each trial we
replace the thing drawn (put it back to the given set and mix) before we draw the next
thing. This guarantees independence of trials and leads to the binomial distribution.
Indeed, if a box contains N things, for example, screws, M of which are defective, the
probability of drawing a defective screw in a trial is p ϭ M>N. Hence the probability of
drawing a nondefective screw is q ϭ 1 Ϫ p ϭ 1 Ϫ M>N, and (2) gives the probability of
drawing x defectives in n trials in the form

(7)

n
M x
M n؊x
f (x) ϭ a b a b a1 Ϫ b
N
N
x

(x ϭ 0, 1, Á , n).

Sampling without Replacement.
Hypergeometric Distribution
Sampling without replacement means that we return no screw to the box. Then we no
longer have independence of trials (why?), and instead of (7) the probability of drawing
x defectives in n trials is

c24.qxd

11/3/10

5:12 PM

Page 1043

SEC. 24.7 Binomial, Poisson, and Hypergeometric Distributions

1043

M NϪM
a ba
b
x
nϪx
f (x) ϭ
N
a b
n

(8)

(x ϭ 0, 1, Á , n).

The distribution with this probability function is called the hypergeometric distribution
(because its moment generating function (see Team Project 16) can be expressed by the
hypergeometric function defined in Sec. 5.4, a fact that we shall not use).
Derivation of (8). By (4a) in Sec. 24.4 there are
N
(a) a b different ways of picking n things from N,
n
M
(b) a b different ways of picking x defectives from M,
x
(c) a

NϪM
b different ways of picking n Ϫ x nondefectives from N Ϫ M,
nϪx

and each way in (b) combined with each way in (c) gives the total number of mutually
exclusive ways of obtaining x defectives in n drawings without replacement. Since (a) is
the total number of outcomes and we draw at random, each such way has the probability
N
1^a b . From this, (8) follows.
᭿
n
The hypergeometric distribution has the mean (Team Project 16)
␮ϭn

(9)

M
N

and the variance
(10)

EXAMPLE 4

s2 ϭ

nM(N Ϫ M)(N Ϫ n)
N 2(N Ϫ 1)

.

Sampling with and without Replacement
We want to draw random samples of two gaskets from a box containing 10 gaskets, three of which are defective.
Find the probability function of the random variable X ϭ Number of defectives in the sample.

Solution. We have N ϭ 10, M ϭ 3, N Ϫ M ϭ 7, n ϭ 2. For sampling with replacement, (7) yields
3 x 7 2؊x
2
f (x) ϭ a b a b a b ,
x
10
10

f (0) ϭ 0.49, f (1) ϭ 0.42, f (2) ϭ 0.09.

For sampling without replacement we have to use (8), finding
3
7
10
f (x) ϭ a b a
b^a b ,
x 2Ϫx
2

f (0) ϭ f (1) ϭ

21
45

Ϸ 0.47, f (2) ϭ

3
45

Ϸ 0.07.

᭿

c24.qxd

11/3/10

1044

5:12 PM

Page 1044

CHAP. 24 Data Analysis. Probability Theory

If N, M, and N Ϫ M are large compared with n, then it does not matter too much whether
we sample with or without replacement, and in this case the hypergeometric distribution
may be approximated by the binomial distribution (with p ϭ M>N), which is somewhat
simpler.
Hence, in sampling from an indefinitely large population (“infinite population”), we
may use the binomial distribution, regardless of whether we sample with or without
replacement.

PROBLEM SET 24.7
1. Mark the positions of ␮ in Fig. 517. Comment.
2. Graph (2) for n ϭ 8 as in Fig. 517 and compare with
Fig. 517.
3. In Example 3, if 5 cars enter the lot on the average,
what is the probability that during any given minute 6
or more cars will enter? First guess. Compare with
Example 3.
4. How do the probabilities in Example 4 of the text
change if you double the numbers: drawing 4 gaskets
from 20, 6 of which are defective? First guess.
5. Five fair coins are tossed simultaneously. Find the
probability function of the random variable X ϭ Number
of heads and compute the probabilities of obtaining no
heads, precisely 1 head, at least 1 head, not more than
4 heads.
6. Suppose that 4% of steel rods made by a machine are
defective, the defectives occurring at random during
production. If the rods are packaged 100 per box, what
is the Poisson approximation of the probability that a
given box will contain x ϭ 0, 1, Á , 5 defectives?
7. Let X be the number of cars per minute passing a certain
point of some road between 8 A.M. and 10 A.M. on a
Sunday. Assume that X has a Poisson distribution with
mean 5. Find the probability of observing 4 or fewer
cars during any given minute.
8. Suppose that a telephone switchboard of some
company on the average handles 300 calls per hour,
and that the board can make at most 10 connections
per minute. Using the Poisson distribution, estimate the
probability that the board will be overtaxed during a
given minute. (Use Table A6 in App. 5 or your CAS.)
9. Rutherford–Geiger experiments. In 1910, E.
Rutherford and H. Geiger showed experimentally that
the number of alpha particles emitted per second in a
radioactive process is a random variable X having a
Poisson distribution. If X has mean 0.5, what is the
probability of observing two or more particles during
any given second?
10. Let p ϭ 2% be the probability that a certain type of
lightbulb will fail in a 24-hour test. Find the probability

11.
12.

13.

14.

15.

16.

that a sign consisting of 15 such bulbs will burn 24
hours with no bulb failures.
Guess how much less the probability in Prob. 10 would
be if the sign consisted of 100 bulbs. Then calculate.
Suppose that a certain type of magnetic tape contains,
on the average, 2 defects per 100 meters. What is the
probability that a roll of tape 300 meters long will
contain (a) x defects, (b) no defects?
Suppose that a test for extrasensory perception consists
of naming (in any order) 3 cards randomly drawn from
a deck of 13 cards. Find the probability that by chance
alone, the person will correctly name (a) no cards, (b) 1
card, (c) 2 cards, (d) 3 cards.
If a ticket office can serve at most 4 customers per
minute and the average number of customers is 120 per
hour, what is the probability that during a given minute
customers will have to wait? (Use the Poisson
distribution, Table 6 in Appendix 5.)
Suppose that in the production of 60-ohm radio
resistors, nondefective items are those that have a
resistance between 58 and 62 ohms and the probability
of a resistor’s being defective is 0.1%. The resistors
are sold in lots of 200, with the guarantee that all
resistors are nondefective. What is the probability that
a given lot will violate this guarantee? (Use the Poisson
distribution.)
TEAM PROJECT. Moment Generating Function.
The moment generating function G(t) is defined by
G(t) ϭ E(etXj) ϭ a etxjf (x j)
j

or
G(t) ϭ E(etX) ϭ

Ύ

ؕ

etxf (x) dx

؊ؕ

where X is a discrete or continuous random variable,
respectively.
(a) Assuming that termwise differentiation and differentiation under the integral sign are permissible, show

c24.qxd

11/3/10

5:12 PM

Page 1045

SEC. 24.8 Normal Distribution

1045
A1, Á , Ak with probabilities p1, Á , pk, respectively,
where p1 ϩ Á ϩ pk ϭ 1. Suppose that n independent
trials are performed. Show that the probability of
getting x 1 A1’s, Á , x k Ak’s is

that E(X k) ϭ G(k)(0), where G(k) ϭ d kG>dt k, in
particular, ␮ ϭ G r(0).
(b) Show that the binomial distribution has the
moment generating function
n
n
n
n
G(t) ϭ a etx a b p xq n؊x ϭ a a b ( pet)xq n؊x
x
x
xϭ0
xϭ0

ϭ (pet ϩ q)n.
(c) Using (b), prove (3).
(d) Prove (4).
(e) Show that the Poisson distribution
has the moment
t
generating function G(t) ϭ e؊␮e␮e and prove (6).
M
MϪ1
(f) Prove x a b ϭ M a
b.
x
xϪ1
Using this, prove (9).
17. Multinomial distribution. Suppose a trial can result
in precisely one of k mutually exclusive events

24.8

n!
p x11 Á p xkk
x! Á x k!

f (x 1, Á , x k) ϭ

where 0 Ϲ x j Ϲ n, j ϭ 1, Á , k, and x 1 ϩ Á ϩ
x k ϭ n. The distribution having this probability
function is called the multinomial distribution.
18. A process of manufacturing screws is checked every
hour by inspecting n screws selected at random from
that hour’s production. If one or more screws are
defective, the process is halted and carefully examined.
How large should n be if the manufacturer wants the
probability to be about 95% that the process will be
halted when 10% of the screws being produced are
defective? (Assume independence of the quality of any
screw from that of the other screws.)

Normal Distribution
Turning from discrete to continuous distributions, in this section we discuss the normal
distribution. This is the most important continuous distribution because in applications many
random variables are normal random variables (that is, they have a normal distribution)
or they are approximately normal or can be transformed into normal random variables in a
relatively simple fashion. Furthermore, the normal distribution is a useful approximation of
more complicated distributions, and it also occurs in the proofs of various statistical tests.
The normal distribution or Gauss distribution is defined as the distribution with the
density
1
1 xϪ␮
f (x) ϭ
exp c Ϫ a
b
2
s
s12p

2

(1)

d

(s Ͼ 0)

where exp is the exponential function with base e ϭ 2.718 Á . This is simpler than it may
at first look. f (x) has these features (see also Fig. 519).
1. ␮ is the mean and s the standard deviation.
2. 1>(s12p) is a constant factor that makes the area under the curve of f (x) from Ϫϱ
to ϱ equal to 1, as it must be by (10), Sec. 24.5.
3. The curve of f (x) is symmetric with respect to x ϭ ␮ because the exponent is
quadratic. Hence for ␮ ϭ 0 it is symmetric with respect to the y-axis x ϭ 0 (Fig. 519,
“bell-shaped curves”).
4. The exponential function in (1) goes to zero very fast—the faster the smaller the
standard deviation s is, as it should be (Fig. 519).

c24.qxd

11/3/10

1046

5:12 PM

Page 1046

CHAP. 24 Data Analysis. Probability Theory
f (x)
1.5

σ = 0.25

1.0
σ = 0.5

0.5

σ = 1.0
–2

–1

0

2

1

x

Fig. 519. Density (1) of the normal distribution with ␮ ϭ 0 for various values of s

Distribution Function F(x)
From (7) in Sec. 24.5 and (1) we see that the normal distribution has the distribution
function

(2)

F(x) ϭ

1
s12p

1 vϪ␮
exp c Ϫ a
b
2
s
؊ϱ

Ύ

x

2

d dv.

Here we needed x as the upper limit of integration and wrote v (instead of x) in the integrand.
For the corresponding standardized normal distribution with mean 0 and standard
deviation 1 we denote F(x) by £(z). Then we simply have from (2)
£(z) ϭ

(3)

1
12p

Ύ

z

e؊u

>2

2

du.

؊ϱ

This integral cannot be integrated by one of the methods of calculus. But this is no serious
handicap because its values can be obtained from Table A7 in App. 5 or from your CAS.
These values are needed in working with the normal distribution. The curve of £(z) is
S-shaped. It increases monotone (why?) from 0 to 1 and intersects the vertical axis at 12
(why?), as shown in Fig. 520.
Relation Between F(x) and ≥(z). Although your CAS will give you values of F(x) in
(2) with any ␮ and s directly, it is important to comprehend that and why any such an
F(x) can be expressed in terms of the tabulated standard £(z), as follows.
y
Φ(x)

1.0
0.8
0.6
0.4
0.2

–3

–2

–1

0

1

2

3

x

Fig. 520. Distribution function £(z) of the normal distribution with mean 0 and variance 1

c24.qxd

11/3/10

5:12 PM

Page 1047

SEC. 24.8 Normal Distribution

THEOREM 1

1047

Use of the Normal Table A7 in App. 5

The distribution function F(x) of the normal distribution with any ␮ and s [see (2)]
is related to the standardized distribution function £(z) in (3) by the formula
F(x) ϭ £a

(4)

PROOF

xϪ␮
s b.

Comparing (2) and (3) we see that we should set
uϭ

vϪ␮
s .

Then v ϭ x gives

uϭ

xϪ␮
s

as the new upper limit of integration. Also v Ϫ ␮ ϭ su, thus dv ϭ s du. Together, since
s drops out,
1
F(x) ϭ
s12p

Ύ

(x؊␮)>s

e؊u

>2

2

s du ϭ £a

؊ؕ

xϪ␮
s b.

᭿

Probabilities corresponding to intervals will be needed quite frequently in statistics in
Chap. 25. These are obtained as follows.
THEOREM 2

Normal Probabilities for Intervals

The probability that a normal random variable X with mean ␮ and standard
deviation s assume any value in an interval a Ͻ x Ϲ b is
(5)

PROOF

P(a Ͻ X Ϲ b) ϭ F(b) Ϫ F(a) ϭ £a

bϪ␮
aϪ␮
b
Ϫ
£a
s
s b.

Formula (2) in Sec. 24.5 gives the first equality in (5), and (4) in this section gives the
second equality.
᭿

Numeric Values
In practical work with the normal distribution it is good to remember that about 23 of all values
of X to be observed will lie between ␮ Ϯ s, about 95% between ␮ Ϯ 2s, and practically all
between the three-sigma limits ␮ Ϯ 3s. More precisely, by Table A7 in App. 5,

(6)

(a)

P(␮ Ϫ s Ͻ X Ϲ ␮ ϩ s) Ϸ 68%

(b)

P(␮ Ϫ 2s Ͻ X Ϲ ␮ ϩ 2s) Ϸ 95.5%

(c)

P(␮ Ϫ 3s Ͻ X Ϲ ␮ ϩ 3s) Ϸ 99.7%.

Formulas (6a) and (6b) are illustrated in Fig. 521.

c24.qxd

11/3/10

5:12 PM

1048

Page 1048

CHAP. 24 Data Analysis. Probability Theory

The formulas in (6) show that a value deviating from ␮ by more than s, 2s, or 3s will
occur in one of about 3, 20, and 300 trials, respectively.
95.5%

68%
16%

16%

μ -- σ

μ

2.25%

μ+σ

2.25%

μ -- 2σ

(a)

μ

μ + 2σ

(b)

Fig. 521. Illustration of formula (6)

In tests (Chap. 25) we shall ask, conversely, for the intervals that correspond to certain
given probabilities; practically most important are the probabilities of 95%, 99%, and
99.9%. For these, Table A8 in App. 5 gives the answers ␮ Ϯ 2s, ␮ Ϯ 2.6s, and
␮ Ϯ 3.3s, respectively. More precisely,

(7)

(a)

P(␮ Ϫ 1.96s Ͻ X Ϲ ␮ ϩ 1.96s) ϭ 95%

(b)

P(␮ Ϫ 2.58s Ͻ X Ϲ ␮ ϩ 2.58s) ϭ 99%

(c)

P(␮ Ϫ 3.29s Ͻ X Ϲ ␮ ϩ 3.29s) ϭ 99.9%.

Working with the Normal Tables A7 and A8 in App. 5
There are two normal tables in App. 5, Tables A7 and A8. If you want probabilities, use
Table A7. If probabilities are given and corresponding intervals or x-values are wanted,
use Table A8. The following examples are typical. Do them with care, verifying all values,
and don’t just regard them as dull exercises for your software. Make sketches of the density
to see whether the results look reasonable.
EXAMPLE 1

Reading Entries from Table A7
If X is standardized normal (so that ␮ ϭ 0, s ϭ 1), then
P(X Ϲ 2.44) ϭ 0.9927 Ϸ 9914 %
P(X Ϲ Ϫ1.16) ϭ 1 Ϫ £(1.16) ϭ 1 Ϫ 0.8770 ϭ 0.1230 ϭ 12.3%
P(X м 1) ϭ 1 Ϫ P(X Ϲ 1) ϭ 1 Ϫ 0.8413 ϭ 0.1587) by (7), Sec. 24.3
P(1.0 Ϲ X Ϲ 1.8) ϭ £(1.8) Ϫ £(1.0) ϭ 0.9641 Ϫ 0.8413 ϭ 0.1228.

EXAMPLE 2

᭿

Probabilities for Given Intervals, Table A7
Let X be normal with mean 0.8 and variance 4 (so that s ϭ 2). Then by (4) and (5)
P(X Ϲ 2.44) ϭ F(2.44) ϭ £ a

2.44 Ϫ 0.80
2

b ϭ £(0.82) ϭ 0.7939 Ϸ 80%

or, if you like it better, (similarly in the other cases)
P(X Ϲ 2.44) ϭ P a

X Ϫ 0.80
2

Ϲ

2.44 Ϫ 0.80
2

P(X м 1) ϭ 1 Ϫ P(X Ϲ 1) ϭ 1 Ϫ £a

b ϭ P(Z Ϲ 0.82) ϭ 0.7939

1 Ϫ 0.8
2

b ϭ 1 Ϫ 0.5398 ϭ 0.4602

P(1.0 Ϲ X Ϲ 1.8) ϭ £(0.5) Ϫ £(0.1) ϭ 0.6915 Ϫ 0.5398 ϭ 0.1517.

᭿

c24.qxd

11/3/10

5:12 PM

Page 1049

SEC. 24.8 Normal Distribution
EXAMPLE 3

1049

Unknown Values c for Given Probabilities, Table A8
Let X be normal with mean 5 and variance 0.04 (hence standard deviation 0.2). Find c or k corresponding to
the given probability
P(X Ϲ c) ϭ 95%,

£a

cϪ5
0.2

P(5 Ϫ k Ϲ X Ϲ 5 ϩ k) ϭ 90%,
P(X м c) ϭ 1%,

EXAMPLE 4

cϪ5

b ϭ 95%,

0.2

ϭ 1.645,

5 ϩ k ϭ 5.329

(as before; why?)
cϪ5

thus P(X Ϲ c) ϭ 99%,

c ϭ 5.329

0.2

ϭ 2.326,

c ϭ 5.465.

᭿

Defectives
In a production of iron rods let the diameter X be normally distributed with mean 2 in. and standard deviation
0.008 in.
(a) What percentage of defectives can we expect if we set the tolerance limits at 2 Ϯ 0.02 in.?
(b) How should we set the tolerance limits to allow for 4% defectives?

Solution. (a) 114 % because from (5) and Table A7 we obtain for the complementary event the probability
P(1.98 Ϲ X Ϲ 2.02) ϭ £a

2.02 Ϫ 2.00
0.008

b Ϫ £a

1.98 Ϫ 2.00
0.008

b

ϭ £(2.5) Ϫ £(Ϫ2.5)
ϭ 0.9938 Ϫ (1 Ϫ 0.9938)
ϭ 0.9876
ϭ 9834 %.
(b) 2 Ϯ 0.0164 because, for the complementary event, we have
0.96 ϭ P(2 Ϫ c Ϲ X Ϲ 2 ϩ c)
or
0.98 ϭ P(X Ϲ 2 ϩ c)
so that Table A8 gives
0.98 ϭ £ a

2ϩcϪ2
0.008
2ϩcϪ2
0.008

b,
ϭ 2.054,

c ϭ 0.0164.

᭿

Normal Approximation of the Binomial Distribution
The probability function of the binomial distribution is (Sec. 24.7)
(8)

n
f (x) ϭ a b p xq n؊x
x

(x ϭ 0, 1, Á , n).

If n is large, the binomial coefficients and powers become very inconvenient. It is of great
practical (and theoretical) importance that, in this case, the normal distribution provides
a good approximation of the binomial distribution, according to the following theorem,
one of the most important theorems in all probability theory.

c24.qxd

11/3/10

5:12 PM

1050

THEOREM 3

Page 1050

CHAP. 24 Data Analysis. Probability Theory

Limit Theorem of De Moivre and Laplace

For large n,
f (x) ϳ f *(x)

(9)

(x ϭ 0, 1, Á , n).

Here f is given by (8). The function
f *(x) ϭ

(10)

2
1
e؊z >2,
12p 1npq

zϭ

x Ϫ np
1npq

is the density of the normal distribution with mean ␮ ϭ np and variance s2 ϭ npq
(the mean and variance of the binomial distribution). The symbol ϳ (read
asymptotically equal) means that the ratio of both sides approaches 1 as n approaches
ϱ . Furthermore, for any nonnegative integers a and b (Ͼ a),
b
n
P(a Ϲ X Ϲ b) ϭ a a x b p xq n؊x ϳ £(b) Ϫ £(a),
xϭa

(11)
aϭ

a Ϫ np Ϫ 0.5
1npq

,

bϭ

b Ϫ np ϩ 0.5
1npq

.

A proof of this theorem can be found in [G3] listed in App. 1. The proof shows that the term
0.5 in a and b is a correction caused by the change from a discrete to a continuous distribution.

PROBLEM SET 24.8
1. Let X be normal with mean 10 and variance 4. Find
P(X Ͼ 12), P(X Ͻ 10), P(X Ͻ 11), P(9 Ͻ X Ͻ 13).
2. Let X be normal with mean 105 and variance 25. Find
P(X Ϲ 112.5), P(x Ͼ 100), P(110.5 Ͻ X Ͻ 111.25).
3. Let X be normal with mean 50 and variance 9.
Determine c such that P(X Ͻ c) ϭ 5%, P(X Ͼ c) ϭ
1%, P(50 Ϫ c Ͻ X Ͻ 50 ϩ c) ϭ 50%.
4. Let X be normal with mean 3.6 and variance 0.01. Find
c such that P(X Ϲ c) ϭ 50%, P(X Ͼ c) ϭ 10%,
P(Ϫc Ͻ X Ϫ 3.6 Ϲ c) ϭ 99.9%.
5. If the lifetime X of a certain kind of automobile battery
is normally distributed with a mean of 5 years and a
standard deviation of 1 year, and the manufacturer wishes
to guarantee the battery for 4 years, what percentage of
the batteries will he have to replace under the guarantee?
6. If the standard deviation in Prob. 5 were smaller, would
that percentage be larger or smaller?
7. A manufacturer knows from experience that the
resistance of resistors he produces is normal with mean

␮ ϭ 150 ⍀ and standard deviation s ϭ 5 ⍀. What
percentage of the resistors will have resistance between
148 ⍀ and 152 ⍀? Between 140 ⍀ and 160 ⍀?
8. The breaking strength X [kg] of a certain type of plastic
block is normally distributed with a mean of 1500 kg
and a standard deviation of 50 kg. What is the maximum
load such that we can expect no more than 5% of the
blocks to break?
9. If the mathematics scores of the SAT college entrance
exams are normal with mean 480 and standard deviation
100 (these are about the actual values over the past
years) and if some college sets 500 as the minimum
score for new students, what percent of students would
not reach that score?
10. A producer sells electric bulbs in cartons of 1000 bulbs.
Using (11), find the probability that any given carton
contains not more than 1% defective bulbs, assuming
the production process to be a Bernoulli experiment
with p ϭ 1%(ϭ probability that any given bulb will be
defective). First guess. Then calculate.

c24.qxd

11/3/10

5:12 PM

Page 1051

SEC. 24.9 Distributions of Several Random Variables
11. If sick-leave time X used by employees of a company
in one month is (very roughly) normal with mean 1000
hours and standard deviation 100 hours, how much
time t should be budgeted for sick leave during the next
month if t is to be exceeded with probability of only
20%?
12. If the monthly machine repair and maintenance cost X
in a certain factory is known to be normal with mean
$12,000 and standard deviation $2000, what is the
probability that the repair cost for the next month will
exceed the budgeted amount of $15,000?
13. If the resistance X of certain wires in an electrical
network is normal with mean 0.01 ⍀ and standard
deviation 0.001 ⍀, how many of 1000 wires will meet
the specification that they have resistance between
0.009 and 0.011 ⍀?
14. TEAM PROJECT. Normal Distribution. (a) Derive
the formulas in (6) and (7) from the appropriate normal
table.
(b) Show that £(Ϫz) ϭ 1 Ϫ £(z). Give an example.
(c) Find the points of inflection of the curve of (1).
(d) Considering £ 2(ϱ) and introducing polar coordinates in the double integral (a standard trick worth
remembering), prove

24.9

1051

(12)

£(ϱ) ϭ

Ύ
12p
1

ؕ

e؊u

>2

2

du ϭ 1.

؊ؕ

(e) Show that s in (1) is indeed the standard deviation
of the normal distribution. [Use (12).]
(f ) Bernoulli’s law of large numbers. In an experiment
let an event A have probability p (0 Ͻ p Ͻ 1), and let X
be the number of times A happens in n independent trials.
Show that for any given P Ͼ 0,
X
P a ` n Ϫ p ` Ϲ Pb : 1

as n : ϱ.

(g) Transformation. If X is normal with mean ␮ and
variance s2, show that X* ϭ c1X ϩ c2 (c1 Ͼ 0) is
normal with mean ␮* ϭ c1␮ ϩ c2 and variance
s*2 ϭ c12s2.
15. WRITING PROJECT. Use of Tables, Use of CAS.
Give a systematic discussion of the use of Tables A7 and
A8 for obtaining P(X Ͻ b), P(X Ͼ a), P(a Ͻ X Ͻ b),
P(X Ͻ c) ϭ k, P(X Ͼ c) ϭ k, as well as P(␮ Ϫ c Ͻ
X Ͻ ␮ ϩ c) ϭ k; include simple examples. If you have
a CAS, describe to what extent it makes the use of those
tables superfluous; give examples.

Distributions of Several Random Variables
Distributions of two or more random variables are of interest for two reasons:
1. They occur in experiments in which we observe several random variables, for
example, carbon content X and hardness Y of steel, amount of fertilizer X and yield of
corn Y, height X1, weight X2, and blood pressure X3 of persons, and so on.
2. They will be needed in the mathematical justification of the methods of statistics in
Chap. 25.
In this section we consider two random variables X and Y or, as we also say, a twodimensional random variable (X, Y ). For (X, Y ) the outcome of a trial is a pair of numbers
X ϭ x, Y ϭ y, briefly (X, Y ) ϭ (x, y), which we can plot as a point in the XY-plane.
The two-dimensional probability distribution of the random variable (X, Y) is given
by the distribution function
(1)

F(x, y) ϭ P(X Ϲ x, Y Ϲ y).

This is the probability that in a trial, X will assume any value not greater than x and in
the same trial, Y will assume any value not greater than y. This corresponds to the blue
region in Fig. 522, which extends to Ϫϱ to the left and below. F(x, y) determines the

c24.qxd

11/3/10

5:12 PM

1052

Page 1052

CHAP. 24 Data Analysis. Probability Theory
(x, y)

Y

X

Fig. 522. Formula (1)

probability distribution uniquely, because in analogy to formula (2) in Sec. 24.5, that is,
P(a Ͻ X Ϲ b) ϭ F(b) Ϫ F(a), we now have for a rectangle (see Prob. 16)
(2)

P(a1 Ͻ X Ϲ b1,

a2 Ͻ Y Ϲ b2) ϭ F(b1, b2) Ϫ F(a1, b2) Ϫ F(b1, a2) ϩ F(a1, a2).

As before, in the two-dimensional case we shall also have discrete and continuous
random variables and distributions.

Discrete Two-Dimensional Distributions
In analogy to the case of a single random variable (Sec. 24.5), we call (X, Y ) and its
distribution discrete if (X, Y ) can assume only finitely many or at most countably infinitely
many pairs of values (x 1, y1), (x 2, y2), Á with positive probabilities, whereas the probability
for any domain containing none of those values of (X, Y ) is zero.
Let (x i, yj) be any of those pairs and let P(X ϭ x i, Y ϭ yj) ϭ pij (where we admit that
pij may be 0 for certain pairs of subscripts i, j). Then we define the probability function
f (x, y) of (X, Y ) by
(3)

f (x, y) ϭ pij

if x ϭ x i, y ϭ yj

f (x, y) ϭ 0

and

otherwise;

here, i ϭ 1, 2, Á and j ϭ 1, 2, Á independently. In analogy to (4), Sec. 24.5, we now have
for the distribution function the formula

(4)

F(x, y) ϭ a

a f (x i, yj).

xi Ϲx yj Ϲy

Instead of (6) in Sec. 24.5 we now have the condition
(5)

a a f (x i, yj) ϭ 1.
i

EXAMPLE 1

j

Two-Dimensional Discrete Distribution
If we simultaneously toss a dime and a nickel and consider
X ϭ Number of heads the dime turns up,
Y ϭ Number of heads the nickel turns up,
then X and Y can have the values 0 or 1, and the probability function is
f (0, 0) ϭ f (1, 0) ϭ f (0, 1) ϭ f (1, 1) ϭ 14 ,

f (x, y) ϭ 0 otherwise.

᭿

c24.qxd

11/3/10

5:12 PM

Page 1053

SEC. 24.9 Distributions of Several Random Variables

1053

Y
b2

a2
a1

b1

X

Fig. 523. Notion of a two-dimensional distribution

Continuous Two-Dimensional Distributions
In analogy to the case of a single random variable (Sec. 24.5) we call (X, Y ) and its
distribution continuous if the corresponding distribution function F(x, y) can be given by
a double integral
F(x, y) ϭ

(6)

y

x

؊ؕ

؊ؕ

Ύ Ύ

f (x*, y*) dx* dy*

whose integrand f, called the density of (X, Y ), is nonnegative everywhere, and is
continuous, possibly except on finitely many curves.
From (6) we obtain the probability that (X, Y ) assume any value in a rectangle (Fig. 523)
given by the formula
b2

(7)

P(a1 Ͻ X Ϲ b1,

a2 Ͻ Y Ϲ b2) ϭ

Ύ Ύ
a2

EXAMPLE 2

b1

f (x, y) dx dy.

a1

Two-Dimensional Uniform Distribution in a Rectangle
Let R be the rectangle a1 Ͻ x Ϲ b1, a2 Ͻ y Ϲ b2. The density (see Fig. 524)
f (x, y) ϭ 1>k if (x, y) is in R,

(8)

f (x, y) ϭ 0 otherwise

defines the so-called uniform distribution in the rectangle R; here k ϭ (b1 Ϫ a1)(b2 Ϫ a2) is the area of R.
᭿
The distribution function is shown in Fig. 525.

1

y
y

β2

x

x

β2
β1

β1
α1

α2
0

Fig. 524. Density function (8) of the
uniform distribution

α1

α2
0

Fig. 525. Distribution function of the
uniform distribution defined by (8)

Marginal Distributions of a Discrete Distribution
This is a rather natural idea, without counterpart for a single random variable. It amounts
to being interested only in one of the two variables in (X, Y ), say, X, and asking for its
distribution, called the marginal distribution of X in (X, Y ). So we ask for the probability

c24.qxd

11/3/10

5:12 PM

1054

Page 1054

CHAP. 24 Data Analysis. Probability Theory

P(X ϭ x, Y arbitrary). Since (X, Y ) is discrete, so is X. We get its probability function,
call it f1(x), from the probability function f (x, y) of (X, Y ) by summing over y:
f1(x) ϭ P(X ϭ x, Y arbitrary) ϭ a f (x, y)

(9)

y

where we sum all the values of f (x, y) that are not 0 for that x.
From (9) we see that the distribution function of the marginal distribution of X is
F1(x) ϭ P(X Ϲ x, Y arbitrary) ϭ a f1(x*).

(10)

x*Ϲx

Similarly, the probability function
f2( y) ϭ P(X arbitrary, Y Ϲ y) ϭ a f (x, y)

(11)

x

determines the marginal distribution of Y in (X, Y). Here we sum all the values of f (x, y) that
are not zero for the corresponding y. The distribution function of this marginal distribution is
F2( y) ϭ P(X arbitrary, Y Ϲ y) ϭ a f2( y*).

(12)

y*Ϲy

EXAMPLE 3

Marginal Distributions of a Discrete Two-Dimensional Random Variable
In drawing 3 cards with replacement from a bridge deck let us consider
(X, Y ),

X ϭ Number of queens,

Y ϭ Number of kings or aces.

The deck has 52 cards. These include 4 queens, 4 kings, and 4 aces. Hence in a single trial a queen has probability
4
1
8
2
52 ϭ 13 and a king or ace 52 ϭ 13 . This gives the probability function of (X, Y ),
f (x, y) ϭ

3!
x!y!(3 Ϫ x Ϫ y)!

a

1
13

x

b a

2
13

y

b a

10
13

3؊x؊y

b

(x ϩ y Ϲ 3)

and f (x, y) ϭ 0 otherwise. Table 24.1 shows in the center the values of f (x, y) and on the right and lower margins
the values of the probability functions f1(x) and f2(y) of the marginal distributions of X and Y, respectively. ᭿

Table 24.1 Values of the Probability Functions ƒ(x, y), ƒ1(x), ƒ2(y) in Drawing
Three Cards with Replacement from a Bridge Deck, where X is the Number
of Queens Drawn and Y is the Number of Kings or Aces Drawn
x
0
1
2
3
ƒ2(y)

y

0

1

2

3

ƒ1(x)

1000
_

600
_

120
_

8
_

1728
_

300
_

120
_

12
_

30
_

6
_

2197
2197

2197

1
_

2197
2197

2197

2197

2197

0

2197

0
0

2197

0

0

0

1331
_

726
_

132
_

8
_

2197

2197

2197

2197

2197

432
_
2197

36
_

2197
1
_
2197

c24.qxd

11/3/10

5:12 PM

Page 1055

SEC. 24.9 Distributions of Several Random Variables

1055

Marginal Distributions of a Continuous Distribution
This is conceptually the same as for discrete distributions, with probability functions and
sums replaced by densities and integrals. For a continuous random variable (X, Y ) with
density f (x, y) we now have the marginal distribution of X in (X, Y ), defined by the
distribution function
(13)

F1(x) ϭ P(X Ϲ x, Ϫϱ Ͻ Y Ͻ ϱ) ϭ

Ύ

x

f1(x*) dx*

؊ؕ

with the density f1 of X obtained from f (x, y) by integration over y,
f1(x) ϭ

(14)

Ύ

ؕ

f (x, y) dy.

؊ؕ

Interchanging the roles of X and Y, we obtain the marginal distribution of Y in (X, Y )
with the distribution function
(15)

F2(y) ϭ P(Ϫϱ Ͻ X Ͻ ϱ, Y Ϲ y) ϭ

Ύ

y

f2(y*) dy*

؊ؕ

and density
(16)

f2(y) ϭ

Ύ

ؕ

f (x, y) dx.

؊ؕ

Independence of Random Variables
X and Y in a (discrete or continuous) random variable (X, Y ) are said to be independent if
(17)

F(x, y) ϭ F1(x)F2(y)

holds for all (x, y). Otherwise these random variables are said to be dependent. These
definitions are suggested by the corresponding definitions for events in Sec. 24.3.
Necessary and sufficient for independence is
(18)

f (x, y) ϭ f1(x)f2(y)

for all x and y. Here the f ’s are the above probability functions if (X, Y ) is discrete or
those densities if (X, Y ) is continuous. (See Prob. 20.)
EXAMPLE 4

Independence and Dependence
In tossing a dime and a nickel, X ϭ Number of heads on the dime, Y ϭ Number of heads on the nickel may
᭿
assume the values 0 or 1 and are independent. The random variables in Table 24.1 are dependent.

c24.qxd

11/3/10

1056

5:12 PM

Page 1056

CHAP. 24 Data Analysis. Probability Theory

Extension of Independence to n-Dimensional Random Variables. This will be needed
throughout Chap. 25. The distribution of such a random variable X ϭ (X1, Á , Xn) is
determined by a distribution function of the form
F(x 1, Á , x n) ϭ P(X1 Ϲ x 1, Á , Xn Ϲ x n).
The random variables X1, Á , Xn are said to be independent if
(19)

F(x 1, Á , x n) ϭ F1(x 1)F2(x 2) Á Fn(x n)

for all (x 1, Á , x n). Here Fj(x j) is the distribution function of the marginal distribution of
Xj in X, that is,
Fj(x j) ϭ P(Xj Ϲ x j, Xk arbitrary, k ϭ 1, Á , n, k

j).

Otherwise these random variables are said to be dependent.

Functions of Random Variables
When n ϭ 2, we write X1 ϭ X, X2 ϭ Y, x 1 ϭ x, x 2 ϭ y. Taking a nonconstant continuous
function g(x, y) defined for all x, y, we obtain a random variable Z ϭ g(X, Y ). For example,
if we roll two dice and X and Y are the numbers the dice turn up in a trial, then Z ϭ X ϩ Y
is the sum of those two numbers (see Fig. 514 in Sec. 24.5).
In the case of a discrete random variable (X, Y ) we may obtain the probability function
f (z) of Z ϭ g(X, Y ) by summing all f (x, y) for which g(x, y) equals the value of z
considered; thus
(20)

f (z) ϭ P(Z ϭ z) ϭ aa f (x, y).
g(x,y)ϭz

Hence the distribution function of Z is
(21)

F(z) ϭ P(Z Ϲ z) ϭ aa f (x, y)
g(x,y)Ϲz

where we sum all values of f (x, y) for which g(x, y) Ϲ z.
In the case of a continuous random variable (X, Y ) we similarly have

(22)

F(z) ϭ P(Z Ϲ z) ϭ

ΎΎ f (x, y) dx dy

g(x,y)Ϲz

where for each z we integrate the density f (x, y) of (X, Y ) over the region g(x, y) Ϲ z in
the xy-plane, the boundary curve of this region being g(x, y) ϭ z.

c24.qxd

11/3/10

5:12 PM

Page 1057

SEC. 24.9 Distributions of Several Random Variables

1057

Addition of Means
The number
a a g(x, y) f (x, y)
(23)

x

E(g(X, Y )) ϭ e

ؕ

ؕ

؊ؕ

؊ؕ

Ύ Ύ

[(X, Y ) discrete]

y

g(x, y) f (x, y) dx dy

[(X, Y ) continuous]

is called the mathematical expectation or, briefly, the expectation of g(X, Y ). Here it is
assumed that the double series converges absolutely and the integral of ƒ g(x, y) ƒ f (x, y)
over the xy-plane exists (is finite). Since summation and integration are linear processes,
we have from (23)
(24)

E(ag(X, Y ) ϩ bh(X, Y )) ϭ aE(g(X, Y )) ϩ bE(h(X, Y )).

An important special case is
E(X ϩ Y ) ϭ E(X ) ϩ E(Y ),
and by induction we have the following result.
THEOREM 1

Addition of Means

The mean (expectation) of a sum of random variables equals the sum of the means
(expectations), that is,
(25)

E(X1 ϩ X2 ϩ Á ϩ Xn) ϭ E(X1) ϩ E(X2) ϩ Á ϩ E(Xn).

Furthermore, we readily obtain
THEROEM 2

Multiplication of Means

The mean (expectation) of the product of independent random variables equals the
product of the means (expectations), that is,
E(X1X2 Á Xn) ϭ E(X1)E(X2) Á E(Xn).

(26)

PROOF

If X and Y are independent random variables (both discrete or both continuous), then
E(XY ) ϭ E(X )E(Y ). In fact, in the discrete case we have
E(XY ) ϭ a a xyf (x, y) ϭ a xf1(x) a yf2( y) ϭ E(X )E(Y ),
x

y

x

y

c24.qxd

11/3/10

5:12 PM

1058

Page 1058

CHAP. 24 Data Analysis. Probability Theory

and in the continuous case the proof of the relation is similar. Extension to n independent
random variables gives (26), and Theorem 2 is proved.
᭿

Addition of Variances
This is another matter of practical importance that we shall need. As before, let Z ϭ X ϩ Y
and denote the mean and variance of Z by ␮ and s2. Then we first have (see Team Project
20(a) in Problem Set 24.6)
s2 ϭ E([Z Ϫ ␮]2) ϭ E(Z 2) Ϫ [E(Z )]2.
From (24) we see that the first term on the right equals
E(Z 2) ϭ E(X 2 ϩ 2XY ϩ Y 2) ϭ E(X 2) ϩ 2E(XY ) ϩ E(Y 2).
For the second term on the right we obtain from Theorem 1
[E(Z )]2 ϭ [E(X ) ϩ E(Y )]2 ϭ [E(X )]2 ϩ 2E(X )E(Y ) ϩ [E(Y )]2.
By substituting these expressions into the formula for s2 we have
s2 ϭ E(X 2) Ϫ [E(X )]2 ϩ E(Y 2) Ϫ [E(Y )]2
ϩ 2[E(XY ) Ϫ E(X )E(Y )].
From Team Project 20, Sec. 24.6, we see that the expression in the first line on the right
is the sum of the variances of X and Y, which we denote by s12 and s22, respectively. The
quantity in the second line (except for the factor 2) is
(27)

sXY ϭ E(XY ) Ϫ E(X )E(Y )

and is called the covariance of X and Y. Consequently, our result is
(28)

s2 ϭ s21 ϩ s22 ϩ 2sXY.

If X and Y are independent, then
E(XY ) ϭ E(X )E(Y );
hence sXY ϭ 0, and
(29)

s2 ϭ s21 ϩ s22.

Extension to more than two variables gives the basic
THEOREM 3

Addition of Variances

The variance of the sum of independent random variables equals the sum of the
variances of these variables.

c24.qxd

11/3/10

5:12 PM

Page 1059

SEC. 24.9 Distributions of Several Random Variables

1059

CAUTION! In the numerous applications of Theorems 1 and 3 we must always
remember that Theorem 3 holds only for independent variables.
This is the end of Chap. 24 on probability theory. Most of the concepts, methods, and
special distributions discussed in this chapter will play a fundamental role in the next
chapter, which deals with methods of statistical inference, that is, conclusions from
samples to populations, whose unknown properties we want to know and try to discover
by looking at suitable properties of samples that we have obtained.

PROBLEM SET 24.9
1. Let f (x, y) ϭ k when 8 Ϲ x Ϲ 12 and 0 Ϲ y Ϲ 2 and
zero elsewhere. Find k. Find P(X Ϲ 11, 1 Ϲ Y Ϲ 1.5)
and P(9 Ϲ X Ϲ 13, Y Ϲ 1).
2. Find P(X Ͼ 4, Y Ͼ 4) and P(X Ϲ 1, Y Ϲ 1) if (X, Y)
1
has the density f (x, y) ϭ 32
if x м 0, y м 0, x ϩ y Ϲ 8.
3. Let f (x, y) ϭ k if x Ͼ 0, y Ͼ 0, x ϩ y Ͻ 3 and 0 otherwise. Find k. Sketch f (x, y). Find P(X ϩ Y Ϲ 1), P(Y Ͼ X ).
4. Find the density of the marginal distribution of X in
Prob. 2.
5. Find the density of the marginal distribution of Y in
Fig. 524.
6. If certain sheets of wrapping paper have a mean weight
of 10 g each, with a standard deviation of 0.05 g, what
are the mean weight and standard deviation of a pack
of 10,000 sheets?
7. What are the mean thickness and the standard deviation
of transformer cores each consisting of 50 layers of
sheet metal and 49 insulating paper layers if the metal
sheets have mean thickness 0.5 mm each with a
standard deviation of 0.05 mm and the paper layers
have mean 0.05 mm each with a standard deviation of
0.02 mm?
8. Let X [cm] and Y [cm] be the diameters of a pin and
hole, respectively. Suppose that (X, Y) has the density
f (x, y) ϭ 625 if 0.98 Ͻ x Ͻ 1.02,

1.00 Ͻ y Ͻ 1.04

and 0 otherwise. (a) Find the marginal distributions.
(b) What is the probability that a pin chosen at random
will fit a hole whose diameter is 1.00?
9. Using Theorems 1 and 3, obtain the formulas for the
mean and the variance of the binomial distribution.
10. Using Theorem 1, obtain the formula for the mean of
the hypergeometric distribution. Can you use Theorem
3 to obtain the variance of that distribution?
11. A 5-gear assembly is put together with spacers between
the gears. The mean thickness of the gears is 5.020 cm
with a standard deviation of 0.003 cm. The mean
thickness of the spacers is 0.040 cm with a standard
deviation of 0.002 cm. Find the mean and standard
deviation of the assembled units consisting of 5 randomly
selected gears and 4 randomly selected spacers.

12. If the mean weight of certain (empty) containers is 5 lb
the standard deviation is 0.2 lb, and if the filling of the
containers has mean weight 100 lb and standard
deviation 0.5 lb, what are the mean weight and the
standard deviation of filled containers?
13. Find P(X Ͼ Y ) when (X, Y ) has the density
f (x, y) ϭ 0.25e؊0.5(xϩy)

if x м 0, y м 0

and 0 otherwise.
14. An electronic device consists of two components. Let
X and Y [years] be the times to failure of the first and
second components, respectively. Assume that (X, Y )
has the density f (x, y) ϭ 4e؊2(xϩy) if x Ͼ 0 and y Ͼ 0
and 0 otherwise. (a) Are X and Y dependent or
independent? (b) Find the densities of the marginal
distributions. (c) What is the probability that the first
component will have a lifetime of 2 years or longer?
15. Give an example of two different discrete distributions
that have the same marginal distributions.
16. Prove (2).
17. Let (X, Y ) have the probability function
f (0, 0) ϭ f (1, 1) ϭ 18,
f (0, 1) ϭ f (1, 0) ϭ 38.
Are X and Y independent?
18. Let (X, Y ) have the density
f (x, y) ϭ k if x 2 ϩ y 2 Ͻ 1
and 0 otherwise. Determine k. Find the densities of the
marginal distributions. Find the probability
P(X 2 ϩ Y 2 Ͻ 14).
19. Show that the random variables with the densities
f (x, y) ϭ x ϩ y
and
g(x, y) ϭ (x ϩ 12 )(y ϩ 12 )
if 0 Ϲ x Ϲ 1, 0 Ϲ y Ϲ 1 and f (x, y) ϭ 0 and
g(x, y) ϭ 0 elsewhere, have the same marginal
distribution.
20. Prove the statement involving (18).

c24.qxd

11/3/10

1060

5:12 PM

Page 1060

CHAP. 24 Data Analysis. Probability Theory

CHAPTER 24 REVIEW QUESTIONS AND PROBLEMS
1. What are stem-and-leaf plots? Boxplots? Histograms?
Compare their advantages.
2. What properties of data are measured by the mean? The
median? The standard deviation? The variance?
3. What do we mean by an experiment? An outcome? An
event? Give examples.
4. What is a random variable? Its distribution function?
Its probability function or density?
5. State the definition of probability from memory. Give
simple examples.
6. What is sampling with and without replacement? What
distributions are involved?
7. When is the Poisson distribution a good approximation
of the binomial distribution? The normal distribution?
8. Explain the use of the tables of the normal distribution.
If you have a CAS, how would you proceed without
the tables?
9. State the main theorems on probability. Illustrate them
by simple examples.
10. State the most important facts about distributions of
two random variables and their marginal distributions.
11. Make a stem-and-leaf plot, histogram, and boxplot of the
data 110, 113, 109, 118, 110, 115, 104, 111, 116, 113.
12. Same task as in Prob. 11. for the data 13.5, 13.2, 12.1,
13.6, 13.3.
13. Find the mean, standard deviation, and variance in
Prob. 11.
14. Find the mean, standard deviation, and variance in
Prob. 12.

SUMMARY OF CHAPTER

15. Show that the mean always lies between the smallest
and the largest data value.
16. What are the outcomes in the sample space of the
experiment of simultaneously tossing three coins?
17. Plot a histogram of the data 8, 2, 4, 10 and guess x and s
by inspecting the histogram. Then calculate x, s 2, and s.
18. Using a Venn diagram, show that A ʕ B if and only if
A ʝ B ϭ A.
19. Suppose that 3% of bolts made by a machine are
defective, the defectives occurring at random during
production. If the bolts are packaged 50 per box, what
is the binomial approximation of the probability that a
given box will contain x ϭ 0, 1, Á , 5 defectives?
20. Of a lot of 12 items, 3 are defective. (a) Find the number
of different samples of 3 items. Find the number of
samples of 3 items containing (b) no defectives, (c) 1
defective, (d) 2 defectives, (e) 3 defectives.
21. Find the probability function of X ϭ Number of times
of tossing a fair coin until the first head appears.
22. If the life of ball bearings has the density f (x) ϭ ke؊x
if 0 Ϲ x Ϲ 2 and 0 otherwise, what is k? What is the
probability P(X м 1)?
23. Find the mean and variance of a discrete random variable
X having the probability function f (0) ϭ 14 , f (1) ϭ 12 ,
f (2) ϭ 14 .
24. Let X be normal with mean 14 and variance 4. Determine
c such that P(X Ϲ c) ϭ 95%, P(X Ϲ c) ϭ 5%,
P(X Ϲ c) ϭ 99.5%.
25. Let X be normal with mean 80 and variance 9. Find
P(X Ͼ 83), P(X Ͻ 81), P(X Ͻ 80), and P(78 Ͻ X Ͻ 82).

24

Data Analysis. Probability Theory
A random experiment, briefly called experiment, is a process in which the result
(“outcome”) depends on “chance” (effects of factors unknown to us). Examples are
games of chance with dice or cards, measuring the hardness of steel, observing weather
conditions, or recording the number of accidents in a city. (Thus the word “experiment”
is used here in a much wider sense than in common language.) The outcomes are
regarded as points (elements) of a set S, called the sample space, whose subsets are
called events. For events E we define a probability P(E) by the axioms (Sec. 24.3)
0 Ϲ P(E) Ϲ 1
(1)

P(S) ϭ 1
Á
P(E 1 ʜ E 2 ʜ
) ϭ P(E 1) ϩ P(E 2) ϩ Á

(E j ʝ E k ϭ л ).

c24.qxd

11/3/10

5:12 PM

Page 1061

Summary of Chapter 24

1061

These axioms are motivated by properties of frequency distributions of data
(Sec. 24.1).
The complement E c of E has the probability
P(E c) ϭ 1 Ϫ P(E).

(2)

The conditional probability of an event B under the condition that an event A
happens is (Sec. 24.3)
P(B ƒ A) ϭ

(3)

P(A ʝ B)

[P(A) Ͼ 0].

P(A)

Two events A and B are called independent if the probability of their simultaneous
appearance in a trial equals the product of their probabilities, that is, if
P(A ʝ B) ϭ P(A)P(B).

(4)

With an experiment we associate a random variable X. This is a function defined
on S whose values are real numbers; furthermore, X is such that the probability
P(X ϭ a) with which X assumes any value a, and the probability P(a Ͻ X Ϲ b) with
which X assumes any value in an interval a Ͻ X Ϲ b are defined (Sec. 24.5). The
probability distribution of X is determined by the distribution function
F(x) ϭ P(X Ϲ x).

(5)

In applications there are two important kinds of random variables: those of the
discrete type, which appear if we count (defective items, customers in a bank, etc.)
and those of the continuous type, which appear if we measure (length, speed,
temperature, weight, etc.).
A discrete random variable has a probability function
f (x) ϭ P(X ϭ x).

(6)

Its mean ␮ and variance s2 are (Sec. 24.6)
(7)

␮ ϭ a x j f (x j)

and

s2 ϭ a (x j Ϫ ␮)2f (x j)

j

j

where the x j are the values for which X has a positive probability. Important discrete
random variables and distributions are the binomial, Poisson, and hypergeometric
distributions discussed in Sec. 24.7.
A continuous random variable has a density
f (x) ϭ F r (x)

(8)

[see (5)].

Its mean and variance are (Sec. 24.6)
(9)

␮ϭ

Ύ

ؕ

؊ؕ

x f (x) dx

and

s2 ϭ

Ύ

ؕ

(x Ϫ ␮)2f (x) dx.

؊ؕ

c24.qxd

11/3/10

1062

5:12 PM

Page 1062

CHAP. 24 Data Analysis. Probability Theory

Very important is the normal distribution (Sec. 24.8), whose density is
(10)

1
1 xϪ␮
f (x) ϭ
exp c Ϫ a
b
2
s
s12p

2

d

and whose distribution function is (Sec. 24.8; Tables A7, A8 in App. 5)
(11)

F(x) ϭ £a

xϪ␮
s b.

A two-dimensional random variable (X, Y ) occurs if we simultaneously observe
two quantities (for example, height X and weight Y of adults). Its distribution function
is (Sec. 24.9)
(12)

F(x, y) ϭ P(X Ϲ x, Y Ϲ y).

X and Y have the distribution functions (Sec. 24.9)
(13)

F1(x) ϭ P(X Ϲ x, Y arbitrary)

and

F2(y) ϭ P(x arbitrary, Y Ϲ y)

respectively; their distributions are called marginal distributions. If both X and Y
are discrete, then (X, Y ) has a probability function
f (x, y) ϭ P(X ϭ x, Y ϭ y).
If both X and Y are continuous, then (X, Y ) has a density f (x, y).

c25.qxd

11/3/10

6:21 PM

Page 1063

CHAPTER

25

Mathematical Statistics
In probability theory we set up mathematical models of processes that are affected by
“chance.” In mathematical statistics or, briefly, statistics, we check these models against
the observable reality. This is called statistical inference. It is done by sampling, that
is, by drawing random samples, briefly called samples. These are sets of values from a
much larger set of values that could be studied, called the population. An example is
10 diameters of screws drawn from a large lot of screws. Sampling is done in order to
see whether a model of the population is accurate enough for practical purposes. If this
is the case, the model can be used for predictions, decisions, and actions, for instance, in
planning productions, buying equipment, investing in business projects, and so on.
Most important methods of statistical inference are estimation of parameters (Secs. 25.2),
determination of confidence intervals (Sec. 25.3), and hypothesis testing (Sec. 25.4, 25.7,
25.8), with application to quality control (Sec. 25.5) and acceptance sampling (Sec. 25.6).
In the last section (25.9) we give an introduction to regression and correlation analysis,
which concern experiments involving two variables.
Prerequisite: Chap. 24.
Sections that may be omitted in a shorter course: 25.5, 25.6, 25.8.
References, Answers to Problems, and Statistical Tables: App. 1 Part G, App. 2, App. 5.

25.1

Introduction.

Random Sampling

Mathematical statistics consists of methods for designing and evaluating random
experiments to obtain information about practical problems, such as exploring the relation
between iron content and density of iron ore, the quality of raw material or manufactured
products, the efficiency of air-conditioning systems, the performance of certain cars, the
effect of advertising, the reactions of consumers to a new product, etc.
Random variables occur more frequently in engineering (and elsewhere) than one
would think. For example, properties of mass-produced articles (screws, lightbulbs, etc.)
always show random variation, due to small (uncontrollable!) differences in raw material
or manufacturing processes. Thus the diameter of screws is a random variable X and we
have nondefective screws, with diameter between given tolerance limits, and defective
screws, with diameter outside those limits. We can ask for the distribution of X, for the
percentage of defective screws to be expected, and for necessary improvements of the
production process.
Samples are selected from populations—20 screws from a lot of 1000, 100 of 5000
voters, 8 beavers in a wildlife conservation project—because inspecting the entire
population would be too expensive, time-consuming, impossible or even senseless (think
1063

c25.qxd

11/3/10

6:21 PM

1064

Page 1064

CHAP. 25 Mathematical Statistics

of destructive testing of lightbulbs or dynamite). To obtain meaningful conclusions,
samples must be random selections. Each of the 1000 screws must have the same chance
of being sampled (of being drawn when we sample), at least approximately. Only then
will the sample mean x ϭ (x1 ϩ Á ϩ x 20)>20 (Sec. 24.1) of a sample of size n ϭ 20
(or any other n) be a good approximation of the population mean ␮ (Sec. 24.6); and the
accuracy of the approximation will generally improve with increasing n, as we shall see.
Similarly for other parameters (standard deviation, variance, etc.).
Independent sample values will be obtained in experiments with an infinite sample
space S (Sec. 24.2), certainly for the normal distribution. This is also true in sampling with
replacement. It is approximately true in drawing small samples from a large finite population
(for instance, 5 or 10 of 1000 items). However, if we sample without replacement from a
small population, the effect of dependence of sample values may be considerable.
Random numbers help in obtaining samples that are in fact random selections. This
is sometimes not easy to accomplish because there are many subtle factors that can bias
sampling (by personal interviews, by poorly working machines, by the choice of
nontypical observation conditions, etc.). Random numbers can be obtained from a
random number generator in Maple, Mathematica, or other systems listed on p. 789.
(The numbers are not truly random, as they would be produced in flipping coins or
rolling dice, but are calculated by a tricky formula that produces numbers that do have
practically all the essential features of true randomness. Because these numbers
eventually repeat, they must not be used in cryptography, for example, where true
randomness is required.)
EXAMPLE 1

Random Numbers from a Random Number Generator
To select a sample of size n ϭ 10 from 80 given ball bearings, we number the bearings from 1 to 80. We then
let the generator randomly produce 10 of the integers from 1 to 80 and include the bearings with the numbers
obtained in our sample, for example.
44

55

53

03

52

61

67

78

39

54

or whatever.
Random numbers are also contained in (older) statistical tables.

᭿

Representing and processing data were considered in Sec. 24.1 in connection with
frequency distributions. These are the empirical counterparts of probability distributions
and helped motivating axioms and properties in probability theory. The new aspect in this
chapter is randomness: the data are samples selected randomly from a population.
Accordingly, we can immediately make the connection to Sec. 24.1, using stem-and-leaf
plots, box plots, and histograms for representing samples graphically.
Also, we now call the mean x in (5), Sec. 24.1, the sample mean
n

1
1
x ϭ n a x j ϭ n (x1 ϩ x 2 ϩ Á ϩ x n).

(1)

jϭ1

We call n the sample size, the variance s 2 in (6), Sec. 24.1, the sample variance
n

(2)

1
1
2
2
Á ϩ (x n Ϫ x)2],
s ϭ
a (x j Ϫ x) ϭ n Ϫ 1 [(x 1 Ϫ x) ϩ
n Ϫ 1 jϭ1
2

c25.qxd

11/3/10

6:21 PM

Page 1065

SEC. 25.2 Point Estimation of Parameters

1065

and its positive square root s the sample standard deviation. x, s 2, and s are called
parameters of a sample; they will be needed throughout this chapter.

25.2

Point Estimation of Parameters
Beginning in this section, we shall discuss the most basic practical tasks in statistics and
corresponding statistical methods to accomplish them. The first of them is point estimation
of parameters, that is, of quantities appearing in distributions, such as p in the binomial
distribution and ␮ and s in the normal distribution.
A point estimate of a parameter is a number (point on the real line), which is computed
from a given sample and serves as an approximation of the unknown exact value of the
parameter of the population. An interval estimate is an interval (“confidence interval”)
obtained from a sample; such estimates will be considered in the next section. Estimation
of parameters is of great practical importance in many applications.
As an approximation of the mean ␮ of a population we may take the mean x of a
corresponding sample. This gives the estimate ␮ˆ ϭ x for ␮, that is,

(1)

1
␮ˆ ϭ x ϭ n (x1 ϩ Á ϩ xn)

ˆ 2 for the variance of a population is
where n is the sample size. Similarly, an estimate s
2
the variance s of a corresponding sample, that is,
n

(2)

ˆ 2 ϭ s2 ϭ
s

1
2
a (x j Ϫ x) .
n Ϫ 1 jϭ1

Clearly, (1) and (2) are estimates of parameters for distributions in which ␮ or s2
appear explicity as parameters, such as the normal and Poisson distributions. For the
binomial distribution, p ϭ ␮>n [see (3) in Sec. 24.7]. From (1) we thus obtain for p
the estimate

(3)

x
pˆ ϭ n .

We mention that (1) is a special case of the so-called method of moments. In this
method the parameters to be estimated are expressed in terms of the moments of the
distribution (see Sec. 24.6). In the resulting formulas, those moments of the distribution
are replaced by the corresponding moments of the sample. This gives the estimates. Here
the kth moment of a sample x1, Á , xn is
n

1
m k ϭ n a x kj .
jϭ1

c25.qxd

11/3/10

6:21 PM

1066

Page 1066

CHAP. 25 Mathematical Statistics

Maximum Likelihood Method
Another method for obtaining estimates is the so-called maximum likelihood method of
R. A. Fisher [Messenger Math. 41 (1912), 155–160]. To explain it, we consider a discrete
(or continuous) random variable X whose probability function (or density) f (x) depends
on a single parameter u. We take a corresponding sample of n independent values
x1, Á , xn. Then in the discrete case the probability that a sample of size n consists
precisely of those n values is
l ϭ f (x1) f (x 2) Á f (x n).

(4)

In the continuous case the probability that the sample consists of values in the small
intervals x j Ϲ x Ϲ x j ϩ ¢x ( j ϭ 1, 2, Á , n) is
f (x 1)¢x f (x 2)¢x Á f (x n)¢x ϭ l(¢x)n.

(5)

Since f (x j) depends on u, the function l in (5) given by (4) depends on x1, Á , xn and u.
We imagine x1, Á , xn to be given and fixed. Then l is a function of u, which is called
the likelihood function. The basic idea of the maximum likelihood method is quite simple,
as follows. We choose that approximation for the unknown value of u for which l is as
large as possible. If l is a differentiable function of u, a necessary condition for l to have
a maximum in an interval (not at the boundary) is
0l
ϭ 0.
0u

(6)

(We write a partial derivative, because l depends also on x1, Á , xn.) A solution of (6)
depending on x1, Á , xn is called a maximum likelihood estimate for u. We may replace
(6) by
0 ln l
ϭ 0,
0u

(7)

because f (x j) Ͼ 0, a maximum of l is in general positive, and ln l is a monotone increasing
function of l. This often simplifies calculations.
Several Parameters. If the distribution of X involves r parameters u1, Á , ur, then instead
of (6) we have the r conditions 0l>0u1 ϭ 0, Á , 0l>0ur ϭ 0, and instead of (7) we have
0 ln l
ϭ 0,
0u1

(8)

EXAMPLE 1

Á,

0 ln l
ϭ 0.
0ur

Normal Distribution
Find maximum likelihood estimates for u1 ϭ ␮ and u2 ϭ s in the case of the normal distribution.

Solution. From (1), Sec. 24.8, and (4) we obtain the likelihood function
lϭa

1 n 1 n ؊h
b a b e
s
12p

where

hϭ

1 n
2
a (x j Ϫ ␮) .
2s2 jϭ1

c25.qxd

11/3/10

6:21 PM

Page 1067

SEC. 25.2 Point Estimation of Parameters

1067

Taking logarithms, we have
ln l ϭ Ϫn ln 12p Ϫ n ln s Ϫ h.
The first equation in (8) is 0(ln l)>0␮ ϭ 0, written out
0 ln l
0h
1 n
ϭϪ
ϭ
a (x j Ϫ ␮) ϭ 0.
0␮
0␮
s2 jϭ1

n

hence

a x j Ϫ n␮ ϭ 0.
jϭ1

The solution is the desired estimate ␮ˆ for ␮: we find
␮ˆ ϭ

1 n
x ϭ x.
n a j
jϭ1

The second equation in (8) is 0(ln l)>0s ϭ 0, written out
0 ln l
n
0h
1
1 n
2
ϭϪ Ϫ
ϭϪ ϩ
a (x j Ϫ ␮) ϭ 0.
s
s
0s
0s
s3 jϭ1
Replacing ␮ by ␮ˆ and solving for s2, we obtain the estimate
n
ෂ2 ϭ 1
s
(x Ϫ x)2
n a j
jϭ1

which we shall use in Sec. 25.7. Note that this differs from (2). We cannot discuss criteria for the goodness of
estimates but want to mention that for small n, formula (2) is preferable.
᭿

PROBLEM SET 25.2
1. Normal distribution. Apply the maximum likelihood
method to the normal distribution with ␮ ϭ 0.
2. Find the maximum likelihood estimate for the
parameter ␮ of a normal distribution with known
variance s2 ϭ s20 ϭ 16.
3. Poisson distribution. Derive the maximum likelihood
estimator for ␮. Apply it to the sample (10, 25, 26, 17,
10, 4), giving numbers of minutes with 0–10, 11–20,
21–30, 31–40, 41–50, more than 50 fliers per minute,
respectively, checking in at some airport check-in.
4. Uniform distribution. Show that, in the case of the
parameters a and b of the uniform distribution (see
Sec. 24.6), the maximum likelihood estimate cannot be
obtained by equating the first derivative to zero. How
can we obtain maximum likelihood estimates in this
case, more or less by using common sense?
5. Binomial distribution. Derive a maximum likelihood
estimate for p.
6. Extend Prob. 5 as follows. Suppose that m times n trials
were made and in the first n trials A happened k 1 times,
in the second n trials A happened k 2 times, Á , in the
mth n trials A happened k m times. Find a maximum
likelihood estimate of p based on this information.

7. Suppose that in Prob. 6 we made 3 times 4 trials and
A happened 2, 3, 2 times, respectively. Estimate p.
8. Geometric distribution. Let X ϭ Number of independent trials until an event A occurs. Show that X has
a geometric distribution, defined by the probability
function f (x) ϭ pq x؊1, x ϭ 1, 2, Á , where p is the
probability of A in a single trial and q ϭ 1 Ϫ p. Find
the maximum likelihood estimate of p corresponding to
a sample x1, x 2, Á , x n of observed values of X.
9. In Prob. 8, show that f (1) ϩ f (2) ϩ Á ϭ 1 (as it
should be!). Calculate independently of Prob. 8 the
maximum likelihood of p in Prob. 8 corresponding to
a single observed value of X.
10. In rolling a die, suppose that we get the first “Six” in
the 7th trial and in doing it again we get it in the 6th
trial. Estimate the probability p of getting a “Six” in
rolling that die once.
11. Find the maximum likelihood estimate of u in the
density f (x) ϭ ue؊ux if x м 0 and f (x) ϭ 0 if x Ͻ 0.
12. In Prob. 11, find the mean ␮, substitute it in f (x), find
the maximum likelihood estimate of ␮, and show that
it is identical with the estimate for ␮ which can be
obtained from that for u in Prob. 11.

c25.qxd

11/3/10

6:21 PM

1068

Page 1068

CHAP. 25 Mathematical Statistics

13. Compute uˆ in Prob. 11 from the sample 1.9, 0.4, 0.7, 0.6,
1.4. Graph the sample distribution function Fˆ (x) and the
distribution function F(x) of the random variable, with
u ϭ uˆ, on the same axes. Do they agree reasonably well?
(We consider goodness of fit systematically in Sec. 25.7.)
14. Do the same task as in Prob. 13 if the given sample is
0.4, 0.7, 0.2, 1.1, 0.1.

25.3

15. CAS EXPERIMENT. Maximum Likelihood
Estimates. (MLEs). Find experimentally how much
MLEs can differ depending on the sample size. Hint.
Generate many samples of the same size n, e.g., of the
standardized normal distribution, and record x and s 2.
Then increase n.

Confidence Intervals
Confidence intervals1 for an unknown parameter u of some distribution (e.g., u ϭ ␮) are
intervals u1 Ϲ u Ϲ u2 that contain u, not with certainty but with a high probability g,
which we can choose (95% and 99% are popular). Such an interval is calculated from a
1
sample. g ϭ 95% means probability 1 Ϫ g ϭ 5% ϭ 20
of being wrong—one of about
20 such intervals will not contain u. Instead of writing u1 Ϲ u Ϲ u2, we denote this more
distinctly by writing
(1)

CONFg {u1 Ϲ u Ϲ u2}.

Such a special symbol, CONF, seems worthwhile in order to avoid the misunderstanding
that u must lie between u1 and u2.
g is called the confidence level, and u1 and u2 are called the lower and upper
confidence limits. They depend on g. The larger we choose g, the smaller is the error
probability 1 Ϫ g, but the longer is the confidence interval. If g : 1, then its length goes
to infinity. The choice of g depends on the kind of application. In taking no umbrella, a
5% chance of getting wet is not tragic. In a medical decision of life or death, a 5% chance
of being wrong may be too large and a 1% chance of being wrong (g ϭ 99%) may be
more desirable.
Confidence intervals are more valuable than point estimates (Sec. 25.2). Indeed, we can
take the midpoint of (1) as an approximation of u and half the length of (1) as an “error bound”
(not in the strict sense of numerics, but except for an error whose probability we know).
u1 and u2 in (1) are calculated from a sample x1, Á , xn. These are n observations of a
random variable X. Now comes a standard trick. We regard x1, Á , xn as single
observations of n random variables X1, Á , Xn (with the same distribution, namely, that
of X ). Then u1 ϭ u1(x1, Á , xn) and u2 ϭ u2(x1, Á , xn) in (1) are observed values of two
random variables ⍜1 ϭ ⍜1(X1, Á , Xn) and ⍜2 ϭ ⍜2(X1, Á , Xn). The condition (1)
involving g can now be written
(2)

P(⍜1 Ϲ u Ϲ ⍜2) ϭ g.

Let us see what all this means in concrete practical cases.
In each case in this section we shall first state the steps of obtaining a confidence interval
in the form of a table, then consider a typical example, and finally justify those steps
theoretically.
1
JERZY NEYMAN (1894–1981), American statistician, developed the theory of confidence intervals (Annals
of Mathematical Statistics 6 (1935), 111–116).

c25.qxd

11/3/10

6:21 PM

Page 1069

SEC. 25.3 Confidence Intervals

1069

Confidence Interval for ␮ of the Normal Distribution
with Known s2
Table 25.1 Determination of a Confidence Interval for the Mean ␮
of a Normal Distribution with Known Variance ␴ 2
Step 1. Choose a confidence level ␥ (95%, 99%, or the like).
Step 2. Determine the corresponding c:

␥

0.90

0.95

0.99

0.999

c

1.645

1.960

2.576

3.291

Step 3. Compute the mean x of the sample x1, Á , x n.
Step 4. Compute k ϭ cs> 1n. The confidence interval for ␮ is

(3)

EXAMPLE 1

CONFg {x Ϫ k Ϲ ␮ Ϲ x ϩ k}.

Confidence Interval for ␮ of the Normal Distribution with Known ␴ 2
Determine a 95% confidence interval for the mean of a normal distribution with variance s2 ϭ 9, using a sample
of n ϭ 100 values with mean x ϭ 5.

Solution. Step 1. g ϭ 0.95 is required. Step 2. The corresponding c equals 1.960; see Table 25.1.
Step 3. x ϭ 5 is given. Step 4. We need k ϭ 1.960 ؒ 3> 1100 ϭ 0.588. Hence x Ϫ k ϭ 4.412, x ϩ k ϭ 5.588
and the confidence interval is CONF0.95 {4.412 Ϲ ␮ Ϲ 5.588}.
This is sometimes written ␮ ϭ 5 Ϯ 0.588, but we shall not use this notation, which can be misleading.
With your CAS you can determine this interval more directly. Similarly for the other examples in this section. ᭿
Theory for Table 25.1.

THEOREM 1

The method in Table 25.1 follows from the basic

Sum of Independent Normal Random Variables

Let X1, Á , Xn be independent normal random variables each of which has mean
␮ and variance s2. Then the following holds.
(a) The sum X1 ϩ Á ϩ Xn is normal with mean n␮ and variance ns2.
(b) The following random variable X is normal with mean ␮ and variance s2>n.
(4)

1
X ϭ n (X1 ϩ Á ϩ Xn)

(c) The following random variable Z is normal with mean 0 and variance 1.
(5)

Zϭ

XϪ␮
s> 1n

c25.qxd

11/3/10

6:21 PM

1070

Page 1070

CHAP. 25 Mathematical Statistics

PROOF

The statements about the mean and variance in (a) follow from Theorems 1 and 3 in
Sec. 24.9. From this, and Theorem 2 in Sec. 24.6, we see that X has the mean (1>n)n␮ ϭ ␮
and the variance (1>n)2ns2 ϭ s2>n. This implies that Z has the mean 0 and variance 1,
by Theorem 2(b) in Sec. 24.6. The normality of X1 ϩ Á ϩ Xn is proved in Ref. [G3]
᭿
listed in App. 1. This implies the normality of (4) and (5).

Derivation of (3) in Table 25.1. Sampling from a normal distribution gives independent
sample values (see Sec. 25.1), so that Theorem 1 applies. Hence we can choose g and
then determine c such that

(6)

P(Ϫc Ϲ Z Ϲ c) ϭ P aϪc Ϲ

XϪ␮
s> 1n

Ϲ cb ϭ £(c) Ϫ £(Ϫc) ϭ g.

For the value g ϭ 0.95 we obtain z(D) ϭ 1.960 from Table A8 in App. 5, as used in
Example 1. For g ϭ 0.9, 0.99, 0.999 we get the other values of c listed in Table 25.1.
Finally, all we have to do is to convert the inequality in (6) into one for ␮ and insert
observed values obtained from the sample. We multiply Ϫc Ϲ Z Ϲ c by Ϫ1 and then by
s> 1n, writing cs> 1n ϭ k (as in Table 25.1),
P(Ϫc Ϲ Z Ϲ c) ϭ P(c м ϪZ м Ϫc) ϭ P ac м

␮ϪX
s> 1n

м Ϫcb

ϭ P(k м ␮ Ϫ X м Ϫk) ϭ g.
Adding X gives P(X ϩ k м ␮ м X Ϫ k) ϭ g or
(7)

P(X Ϫ k Ϲ ␮ Ϲ X ϩ k) ϭ g.

Inserting the observed value x of X gives (3). Here we have regarded x1, Á , x n as single
observations of X1, Á , Xn (the standard trick!), so that x1 ϩ Á ϩ xn is an observed value
of X1 ϩ Á ϩ Xn and x is an observed value of X. Note further that (7) is of the form (2)
᭿
with ⍜1 ϭ X Ϫ k and ⍜2 ϭ X ϩ k.

EXAMPLE 2

Sample Size Needed for a Confidence Interval of Prescribed Length
How large must n be in Example 1 if we want to obtain a 95% confidence interval of length L ϭ 0.4?

Solution. The interval (3) has the length L ϭ 2k ϭ 2cs> 1n. Solving for n, we obtain
n ϭ (2cs>L)2.
In the present case the answer is n ϭ (2 ؒ 1.960 ؒ 3>0.4)2 Ϸ 870.
Figure 526 shows how L decreases as n increases and that for g ϭ 99% the confidence interval is substantially
᭿
longer than for g ϭ 95% (and the same sample size n).

c25.qxd

11/3/10

6:21 PM

Page 1071

SEC. 25.3 Confidence Intervals

1071
0.6

γ = 99%
0.4

γ = 95%

L/σ
σ
0.2

0
0

500
n

Fig. 526. Length of the confidence interval (3) (measured in multiples of s)
as a function of the sample size n for g ϭ 95% and g ϭ 99%

Confidence Interval for ␮ of the Normal Distribution
with Unknown s2
In practice s2 is frequently unknown. Then the method in Table 25.1 does not help and
the whole theory changes, although the steps of determining a confidence interval for ␮
remain quite similar. They are shown in Table 25.2. We see that k differs from that in
Table 25.1, namely, the sample standard deviation s has taken the place of the unknown
standard deviation s of the population. And c now depends on the sample size n and must
be determined from Table A9 in App. 5 or from your CAS. That table lists values z for
given values of the distribution function (Fig. 527)
F(z) ϭ K m

(8)

Ύ

x

؊ϱ

u2
a1 ϩ m b

؊(mϩ1)>2

du

of the t-distribution. Here, m (ϭ 1, 2, Á ) is a parameter, called the number of degrees
of freedom of the distribution (abbreviated d.f.). In the present case, m ϭ n Ϫ 1; see
Table 25.2. The constant K m is such that F(ϱ) ϭ 1. By integration it turns out that
K m ϭ ⌫(12 m ϩ 12)>3 1mp ⌫(12 m)4, where ⌫ is the gamma function (see (24) in App. A3.1).
Table 25.2 Determination of a Confidence Interval for the Mean ␮
of a Normal Distribution with Unknown Variance ␴ 2
Step 1. Choose a confidence level g (95%, 99%, or the like).
Step 2. Determine the solution c of the equation

(9)

F(c) ϭ 12 (1 ϩ g)

from the table of the t-distribution with n Ϫ 1 degrees of freedom
(Table A9 in App. 5; or use a CAS; n ϭ sample size).
Step 3. Compute the mean x and the variance s2 of the sample x1, Á , x n.
Step 4. Compute k ϭ cs/͙nෆ. The confidence interval is

(10)

CONFg {x Ϫ k Ϲ ␮ Ϲ x ϩ k}.

c25.qxd

11/3/10

6:21 PM

1072

Page 1072

CHAP. 25 Mathematical Statistics

Figure 528 compares the curve of the density of the t-distribution with that of the normal
distribution. The latter is steeper. This illustrates that Table 25.1 (which uses more
information, namely, the known value of s2) yields shorter confidence intervals than
Table 25.2. This is confirmed in Fig. 529, which also gives an idea of the gain by increasing
the sample size.
y
3 d.f.
1.0

y
1 d.f.

0.8

0.4

0.6

0.3

0.4

0.2

0.2

0.1

3 d.f.

1 d.f.

–3

–2

–1

0

1

2

3

–3

x

Fig. 527. Distribution functions of the
t-distribution with 1 and 3 d.f. and of the
standardized normal distribution (steepest curve)

–2

–1

0

1

2

3

x

Fig. 528. Densities of the t-distribution
with 1 and 3 d.f. and of the standardized
normal distribution

2

L' /L 1.5

γ = 99%
γ = 95%

1
0

10

n

20

Fig. 529. Ratio of the lengths L r and L of the confidence
intervals (10) and (3) with g ϭ 95% and g ϭ 99% as a function
of the sample size n for equal s and s

EXAMPLE 3

Confidence Interval for ␮ of the Normal Distribution with Unknown ␴ 2
Five independent measurements of the point of inflammation (flash point) of Diesel oil (D-2) gave the values
(in °F) 144 147 146 142 144. Assuming normality, determine a 99% confidence interval for the mean.

Solution. Step 1. g ϭ 0.99 is required.
Step 2. F(c) ϭ 12 (1 ϩ g) ϭ 0.995, and Table A9 in App. 5 with n Ϫ 1 ϭ 4 d.f. gives c ϭ 4.60.
Step 3. x ϭ 144.6, s 2 ϭ 3.8.
Step 4. k ϭ 13.8 ؒ 4.60> 15 ϭ 4.01. The confidence interval is CONF0.99 {140.5 Ϲ ␮ Ϲ 148.7}.
If the variance s2 were known and equal to the sample variance s 2, thus s2 ϭ 3.8, then Table 25.1 would
give k ϭ cs> 1n ϭ 2.576 13.8> 15 ϭ 2.25 and CONF0.99 {142.35 Ϲ ␮ Ϲ 146.85}. We see that the present
interval is almost twice as long as that obtained from Table 25.1 (with s2 ϭ 3.8). Hence for small samples the
difference is considerable! See also Fig. 529.
᭿

c25.qxd

11/3/10

6:21 PM

Page 1073

SEC. 25.3 Confidence Intervals

1073

Theory for Table 25.2. For deriving (10) in Table 25.2 we need from Ref. [G3]

THEOREM 2

Student’s t-Distribution

Let X1, Á , Xn be independent normal random variables with the same mean ␮ and
the same variance s2. Then the random variable

Tϭ

(11)

XϪ␮
S> 1n

has a t-distribution [see (8)] with n Ϫ 1 degrees of freedom (d.f.); here X is given
by (4) and
n

(12)

S2 ϭ

1
2
a (Xj Ϫ X) .
n Ϫ 1 jϭ1

Derivation of (10). This is similar to the derivation of (3). We choose a number g
between 0 and 1 and determine a number c from Table A9 in App. 5 with n Ϫ 1 d.f. (or
from a CAS) such that
(13)

P(Ϫc Ϲ T Ϲ c) ϭ F(c) Ϫ F(Ϫc) ϭ g.

Since the t-distribution is symmetric, we have
F(Ϫc) ϭ 1 Ϫ F(c),
and (13) assumes the form (9). Substituting (11) into (13) and transforming the result as
before, we obtain
(14)

P(X Ϫ K Ϲ ␮ Ϲ X ϩ K) ϭ g

where
K ϭ cS> 1n.
By inserting the observed values x of X and s 2 of S 2 into (14) we finally obtain (10). ᭿

Confidence Interval for the Variance s2
of the Normal Distribution
Table 25.3 shows the steps, which are similar to those in Tables 25.1 and 25.2.

c25.qxd

11/3/10

6:21 PM

1074

Page 1074

CHAP. 25 Mathematical Statistics
Table 25.3 Determination of a Confidence Interval for the Variance
␴ 2 of a Normal Distribution, Whose Mean Need Not Be Known
Step 1. Choose a confidence level g (95%, 99%, or the like).
Step 2. Determine solutions c1 and c2 of the equations
F(c1) ϭ 12 (1 Ϫ g),

(15)

F(c2) ϭ 12 (1 ϩ g)

from the table of the chi-square distribution with n Ϫ 1 degrees of
freedom (Table A10 in App. 5; or use a CAS; n ϭ sample size).
Step 3. Compute (n Ϫ 1)s 2, where s 2 is the variance of the sample
x 1, Á , x n.
Step 4. Compute k 1 ϭ (n Ϫ 1)s 2>c1 and k 2 ϭ (n Ϫ 1)s 2>c2. The
confidence interval is
CONFg {k 2 Ϲ s2 Ϲ k 1}.

(16)

EXAMPLE 4

Confidence Interval for the Variance of the Normal Distribution
Determine a 95% confidence interval (16) for the variance, using Table 25.3 and a sample (tensile strength of
sheet steel in kg>mm2, rounded to integer values)
89 84

87

81

89

86

91

90

78

89

87

99

83

89.

Solution. Step 1. g ϭ 0.95 is required.
Step 2. For n Ϫ 1 ϭ 13 we find
c1 ϭ 5.01 and

c2 ϭ 24.74.

Step 3. 13s ϭ 326.9.
2

Step 4. 13s 2>c1 ϭ 65.25, 13s 2>c2 ϭ 13.21.
The confidence interval is
CONF0.95 {13.21 Ϲ s2 Ϲ 65.25}.
This is rather large, and for obtaining a more precise result, one would need a much larger sample.

᭿

Theory for Table 25.3. In Table 25.1 we used the normal distribution, in Table 25.2
the t-distribution, and now we shall use the ␹ 2-distribution (chi-square distribution),
whose distribution function is F(z) ϭ 0 if z Ͻ 0 and
z

F(z) ϭ Cm

Ύe

؊u>2 (m؊2)>2

u

du

if z м 0

(Fig. 530).

0

The parameter m (ϭ 1, 2, Á ) is called the number of degrees of freedom (d.f.), and
Cm ϭ 1>[2m>2⌫(12 m)].
Note that the distribution is not symmetric (see also Fig. 531).

c25.qxd

11/3/10

6:21 PM

Page 1075

SEC. 25.3 Confidence Intervals

1075

For deriving (16) in Table 25.3 we need the following theorem.
y
1
2 d.f.

0.8

3 d.f.

0.6

0.4
5 d.f.
0.2

0

2

4

6

8

10

x

Fig. 530. Distribution function of the chi-square distribution with 2, 3, 5 d.f.

THEOREM 3

Chi-Square Distribution

Under the assumptions in Theorem 2 the random variable

Y ϭ (n Ϫ 1)

(17)

S2
s2

with S 2 given by (12) has a chi-square distribution with n Ϫ 1 degrees of freedom.

Proof in Ref. [G3], listed in App. 1.
y
0.5

0.4

2 d.f.

0.3

0.2

3 d.f.
5 d.f.

0.1

0

2

4

6

8

10

x

Fig. 531. Density of the chi-square distribution with 2, 3, 5 d.f.

Derivation of (16). This is similar to the derivation of (3) and (10). We choose a number
g between 0 and 1 and determine c1 and c2 from Table A10, App. 5, such that [see (15)]
P(Y Ϲ c1) ϭ F(c1) ϭ 12 (1 Ϫ g),

P(Y Ϲ c2) ϭ F(c2) ϭ 12 (1 ϩ g).

c25.qxd

11/3/10

6:21 PM

1076

Page 1076

CHAP. 25 Mathematical Statistics

Subtraction yields
P(c1 Ϲ Y Ϲ c2) ϭ P(Y Ϲ c2) Ϫ P(Y Ϲ c1) ϭ F(c2) Ϫ F(c1) ϭ g.
Transforming c1 Ϲ Y Ϲ c2 with Y given by (17) into an inequality for s2, we obtain
nϪ1 2
nϪ1 2
2
c2 S Ϲ s Ϲ c1 S .
᭿

By inserting the observed value s 2 of S 2 we obtain (16).

Confidence Intervals for Parameters
of Other Distributions
The methods in Tables 25.1–25.3 for confidence intervals for ␮ and s2 are designed for
the normal distribution. We now show that they can also be applied to other distributions
if we use large samples.
We know that if X1, Á , Xn are independent random variables with the same mean ␮
and the same variance s2, then their sum Yn ϭ X1 ϩ Á ϩ Xn has the following properties.
(A) Yn has the mean n␮ and the variance ns2 (by Theorems 1 and 3 in Sec. 24.9).
(B) If those variables are normal, then Yn is normal (by Theorem 1).
If those random variables are not normal, then (B) is not applicable. However, for large
n the random variable Yn is still approximately normal. This follows from the central limit
theorem, which is one of the most fundamental results in probability theory.
THEOREM 4

Central Limit Theorem

Let X1, Á , Xn, Á be independent random variables that have the same distribution
function and therefore the same mean ␮ and the same variance s2. Let
Yn ϭ X1 ϩ Á ϩ Xn. Then the random variable
(18)

Zn ϭ

Yn Ϫ n␮
s1n

is asymptotically normal with mean 0 and variance 1; that is, the distribution
function Fn(x) of Zn satisfies
1
lim Fn(x) ϭ £(x) ϭ
n:ϱ
12p

Ύ

x

e؊u

>2

2

du.

؊ϱ

A proof can be found in Ref. [G3] listed in App. 1.
Hence, when applying Tables 25.1–25.3 to a nonnormal distribution, we must use
sufficiently large samples. As a rule of thumb, if the sample indicates that the skewness
of the distribution (the asymmetry; see Team Project 20(d), Problem Set 24.6) is small,
use at least n ϭ 20 for the mean and at least n ϭ 50 for the variance.

c25.qxd

11/3/10

6:21 PM

Page 1077

SEC. 25.4 Testing of Hypotheses. Decisions

1077

PROBLEM SET 25.3
1. Why are interval estimates generally more useful than
point estimates?
2–6

MEAN (VARIANCE KNOWN)

2. Find a 95% confidence interval for the mean of a
normal population with standard deviation 4.00 from
the sample 39, 51, 49, 43, 57, 59. Does that interval
get longer or shorter if we take g ϭ 0.99 instead of
0.95? By what factor?
3. By what factor does the length of the interval in Prob. 2
change if we double the sample size?
4. Determine a 95% confidence interval for the mean ␮
of a normal population with variance s2 ϭ 16, using
a sample of size 200 with mean 74.81.
5. What sample size would be needed for obtaining a 95%
confidence interval (3) of length 2s? Of length s?
6. What sample size is needed to obtain a 99% confidence
interval of length 2.0 for the mean of a normal population
with variance 25? Use Fig. 526. Check by calculation.

MEAN (VARIANCE UNKNOWN)
7. Find a 95% confidence interval for the percentage of
cars on a certain highway that have poorly adjusted
brakes, using a random sample of 800 cars stopped at
a roadblock on that highway, 126 of which had poorly
adjusted brakes.
8. K. Pearson result. Find a 99% confidence interval for
p in the binomial distribution from a classical result by
K. Pearson, who in 24,000 trials of tossing a coin obtained
12,012 Heads. Do you think that the coin was fair?
9–11
Find a 99% confidence interval for the mean of
a normal population from the sample:
9. Copper content (%) of brass 66, 66, 65, 64, 66, 67, 64,
65, 63, 64
10. Melting point (°C) of aluminum 660, 667, 654, 663, 662
11. Knoop hardness of diamond 9500, 9800, 9750, 9200,
9400, 9550

25.4

12. CAS EXPERIMENT. Confidence Intervals. Obtain
100 samples of size 10 of the standardized normal
distribution. Calculate from them and graph the
corresponding 95% confidence intervals for the mean
and count how many of them do not contain 0. Does
the result support the theory? Repeat the whole
experiment, compare and comment.
13–17

VARIANCE

Find a 95% confidence interval for the variance of a normal
population from the sample:
13. Length of 20 bolts with sample mean 20.2 cm and
sample variance 0.04 cm2
14. Carbon monoxide emission (grams per mile) of a
certain type of passenger car (cruising at 55 mph): 17.3,
17.8, 18.0, 17.7, 18.2, 17.4, 17.6, 18.1
15. Mean energy (keV) of delayed neutron group (Group 3,
half-life 6.2 s) for uranium U 235 fission: a sample of
100 values with mean 442.5 and variance 9.3
16. Ultimate tensile strength (k psi) of alloy steel
(Maraging H) at room temperature: 251, 255, 258, 253,
253, 252, 250, 252, 255, 256
17. The sample in Prob. 9
18. If X1 and X2 are independent normal random variables
with mean 14 and 8 and variance 2 and 5, respectively,
what distribution does 3 X1 Ϫ X2 have? Hint. Use Team
Project 14(g) in Sec. 24.8.
19. A machine fills boxes weighing Y lb with X lb of salt,
where X and Y are normal with mean 100 lb and 5 lb
and standard deviation 1 lb and 0.5 lb, respectively.
What percent of filled boxes weighing between 104 lb
and 106 lb are to be expected?
20. If the weight X of bags of cement is normally
distributed with a mean of 40 kg and a standard
deviation of 2 kg, how many bags can a delivery truck
carry so that the probability of the total load exceeding
2000 kg will be 5%?

Testing of Hypotheses. Decisions
The ideas of confidence intervals and of tests2 are the two most important ideas in modern
statistics. In a statistical test we make inference from sample to population through testing a
hypothesis, resulting from experience or observations, from a theory or a quality requirement,
and so on. In many cases the result of a test is used as a basis for a decision, for instance, to
2

Beginning around 1930, a systematic theory of tests was developed by NEYMAN (see Sec. 25.3) and EGON
SHARPE PEARSON (1895–1980), English statistician, the son of Karl Pearson (see the footnote on p. 1086).

c25.qxd

11/3/10

6:21 PM

1078

Page 1078

CHAP. 25 Mathematical Statistics

buy (or not to buy) a certain model of car, depending on a test of the fuel efficiency (miles>gal)
(and other tests, of course), to apply some medication, depending on a test of its effect; to
proceed with a marketing strategy, depending on a test of consumer reactions, etc.
Let us explain such a test in terms of a typical example and introduce the corresponding
standard notions of statistical testing.
EXAMPLE 1

Test of a Hypothesis. Alternative. Significance Level A
We want to buy 100 coils of a certain kind of wire, provided we can verify the manufacturer’s claim that the
wire has a breaking limit ␮ ϭ ␮0 ϭ 200 lb (or more). This is a test of the hypothesis (also called null hypothesis)
␮ ϭ ␮0 ϭ 200. We shall not buy the wire if the (statistical) test shows that actually ␮ ϭ ␮1 Ͻ ␮0, the wire is
weaker, the claim does not hold. ␮1 is called the alternative (or alternative hypothesis) of the test. We shall
accept the hypothesis if the test suggests that it is true, except for a small error probability a, called the
significance level of the test. Otherwise we reject the hypothesis. Hence a is the probability of rejecting a
hypothesis although it is true. The choice of a is up to us. 5% and 1% are popular values.
For the test we need a sample. We randomly select 25 coils of the wire, cut a piece from each coil, and
determine the breaking limit experimentally. Suppose that this sample of n ϭ 25 values of the breaking limit
has the mean x ϭ 197 lb (somewhat less than the claim!) and the standard deviation s ϭ 6 lb.
At this point we could only speculate whether this difference 197 Ϫ 200 ϭ Ϫ3 is due to randomness, is a
chance effect, or whether it is significant, due to the actually inferior quality of the wire. To continue beyond
speculation requires probability theory, as follows.
We assume that the breaking limit is normally distributed. (This assumption could be tested by the method
in Sec. 25.7. Or we could remember the central limit theorem (Sec. 25.3) and take a still larger sample.) Then
Tϭ

X Ϫ ␮0
S> 1n

in (11), Sec. 25.3, with ␮ ϭ ␮0 has a t-distribution with n Ϫ 1 degrees of freedom (n Ϫ 1 ϭ 24 for our sample).
Also x ϭ 197 and s ϭ 6 are observed values of X and S to be used later. We can now choose a significance
level, say, a ϭ 5%. From Table A9 in App. 5 or from a CAS we then obtain a critical value c such that
P(T Ϲ c) ϭ a ϭ 5%. For P(T Ϲ ෂ
c ) ϭ 1 Ϫ a ϭ 95% the table gives ෂ
c ϭ 1.71, so that c ϭ Ϫcෂ ϭ Ϫ1.71
because of the symmetry of the distribution (Fig. 532).
We now reason as follows—this is the crucial idea of the test. If the hypothesis is true, we have a chance
of only a (ϭ 5%) that we observe a value t of T (calculated from a sample) that will fall between Ϫϱ and
Ϫ1.71. Hence, if we nevertheless do observe such a t, we assert that the hypothesis cannot be true and we reject
it. Then we accept the alternative. If, however, t м c, we accept the hypothesis.
A simple calculation finally gives t ϭ (197 Ϫ 200)>(6> 125) ϭ Ϫ2.5 as an observed value of T. Since
Ϫ2.5 Ͻ Ϫ1.71, we reject the hypothesis (the manufacturer’s claim) and accept the alternative ␮ ϭ ␮1 Ͻ 200,
the wire seems to be weaker than claimed.
᭿
Reject hypothesis

Do not reject hypothesis

95%

α = 5%
c = –1.71

0

t

Fig. 532. t-distribution in Example 1

This example illustrates the steps of a test:
1. Formulate the hypothesis u ϭ u0 to be tested. (u0 ϭ ␮0 in the example.)
2. Formulate an alternative u ϭ u1. (u1 ϭ ␮1 in the example.)
3. Choose a significance level a (5%, 1%, 0.1%).
ˆ ϭ g(X1, Á , Xn) whose distribution depends on the
4. Use a random variable ⍜
hypothesis and on the alternative, and this distribution is known in both cases. Determine

c25.qxd

11/3/10

6:21 PM

Page 1079

SEC. 25.4 Testing of Hypotheses. Decisions

1079

ˆ , assuming the hypothesis to be true. (In the
a critical value c from the distribution of ⍜
ˆ ϭ T, and c is, obtained from P(T Ϲ c) ϭ a.)
example, ⍜
ˆ.
5. Use a sample x 1, Á , x n to determine an observed value uˆ ϭ g(x1, Á , x n) of ⍜
(t in the example.)
6. Accept or reject the hypothesis, depending on the size of uˆ relative to c. (t Ͻ c in
the example, rejection of the hypothesis.)
Two important facts require further discussion and careful attention. The first is the
choice of an alternative. In the example, ␮1 Ͻ ␮0, but other applications may require
␮1 Ͼ ␮0 or ␮1 ␮0. The second fact has to do with errors. We know that a (the
significance level of the test) is the probability of rejecting a true hypothesis. And we
shall discuss the probability b of accepting a false hypothesis.

One-Sided and Two-Sided Alternatives (Fig. 533)
Let u be an unknown parameter in a distribution, and suppose that we want to test the
hypothesis u ϭ u0. Then there are three main kinds of alternatives, namely,
(1)

u Ͼ u0

(2)

u Ͻ u0

(3)

u

u0.

(1) and (2) are one-sided alternatives, and (3) is a two-sided alternative.
We call rejection region (or critical region) the region such that we reject the
hypothesis if the observed value in the test falls in this region. In ① the critical c lies to
the right of u0 because so does the alternative. Hence the rejection region extends to
the right. This is called a right-sided test. In ② the critical c lies to the left of u0 (as
in Example 1), the rejection region extends to the left, and we have a left-sided test
(Fig. 533, middle part). These are one-sided tests. In ③ we have two rejection regions.
This is called a two-sided test (Fig. 533, lower part).
Acceptance Region
Do not reject hypothesis
(Accept hypothesis)
1

Rejection Region
(Critical Region)
Reject hypothesis

θ0

c
Acceptance Region
Do not reject hypothesis
(Accept hypothesis)

Rejection Region
(Critical Region)
Reject hypothesis
2
c

θ0

Acceptance Region
Do not reject
hypothesis
(Accept hypothesis)

Rejection Region
(Critical Region)
Reject hypothesis
3
c1

θ0

Rejection Region
(Critical Region)
Reject hypothesis

c2

Fig. 533. Test in the case of alternative (1) (upper part of the figure), alternative
(2) (middle part), and alternative (3)

c25.qxd

11/3/10

1080

6:21 PM

Page 1080

CHAP. 25 Mathematical Statistics

All three kinds of alternatives occur in practical problems. For example, (1) may arise
if u0 is the maximum tolerable inaccuracy of a voltmeter or some other instrument.
Alternative (2) may occur in testing strength of material, as in Example 1. Finally, u0 in
(3) may be the diameter of axle-shafts, and shafts that are too thin or too thick are equally
undesirable, so that we have to watch for deviations in both directions.

Errors in Tests
Tests always involve risks of making false decisions:
(I) Rejecting a true hypothesis (Type I error).
a ϭ Probability of making a Type I error.
(II) Accepting a false hypothesis (Type II error).
b ϭ Probability of making a Type II error.
Clearly, we cannot avoid these errors because no absolutely certain conclusions about
populations can be drawn from samples. But we show that there are ways and means of
choosing suitable levels of risks, that is, of values a and b. The choice of a depends on the
nature of the problem (e.g., a small risk a ϭ 1% is used if it is a matter of life or death).
Let us discuss this systematically for a test of a hypothesis u ϭ u0 against an alternative
that is a single number u1, for simplicity. We let u1 Ͼ u0, so that we have a right-sided
test. For a left-sided or a two-sided test the discussion is quite similar.
We choose a critical c Ͼ u0 (as in the upper part of Fig. 533, by methods discussed
below). From a given sample x 1, Á , x n we then compute a value
uˆ ϭ g(x 1, Á , x n)
with a suitable g (whose choice will be a main point of our further discussion; for instance,
take g ϭ (x 1 ϩ Á ϩ x n)>n in the case in which u is the mean). If uˆ Ͼ c, we reject the
hypothesis. If uˆ Ϲ c, we accept it. Here, the value uˆ can be regarded as an observed value
of the random variable
(4)

ˆ ϭ g(X , Á , X )
⍜
1
n

because x j may be regarded as an observed value of Xj, j ϭ 1, Á , n. In this test there are
two possibilities of making an error, as follows.
Type I Error (see Table 25.4). The hypothesis is true but is rejected (hence the
alternative is accepted) because ⍜ assumes a value uˆ Ͼ c. Obviously, the probability of
making such an error equals
(5)

ˆ Ͼ c)
P(⍜
uϭu0 ϭ a.

a is called the significance level of the test, as mentioned before.
ˆ
Type II Error (see Table 25.4). The hypothesis is false but is accepted because ⍜
assumes a value uˆ Ϲ c. The probability of making such an error is denoted by b; thus
(6)

ˆ Ϲ c)
P(⍜
uϭu1 ϭ b.

11/3/10

6:21 PM

Page 1081

SEC. 25.4 Testing of Hypotheses. Decisions

1081

h ϭ 1 Ϫ b is called the power of the test. Obviously, the power h is the probability of
avoiding a Type II error.
Table 25.4 Type I and Type II Errors in Testing a Hypothesis
␪ ‫␪ ؍‬0 Against an Alternative ␪ ‫␪ ؍‬1
Unknown Truth

Accepted

c25.qxd

u ϭ u0

u ϭ u1

u ϭ u0

True decision
Pϭ1Ϫ␣

Type II error
Pϭ␤

u ϭ u1

Type 1 error
Pϭ␣

True decision
Pϭ1Ϫ␤

Formulas (5) and (6) show that both a and b depend on c, and we would like to choose
c so that these probabilities of making errors are as small as possible. But the important
Figure 534 shows that these are conflicting requirements because to let a decrease we must
shift c to the right, but then b increases. In practice we first choose a (5%, sometimes 1%),
then determine c, and finally compute b. If b is large so that the power h ϭ 1 Ϫ b is small,
we should repeat the test, choosing a larger sample, for reasons that will appear shortly.
^
Density of Θ if
the alternative
is true

^ if
Density of Θ
the hypothesis
is true

β

θ0

α

θ1

c

Acceptance region

Rejection region (Critical region)

Fig. 534. Illustration of Type I and II errors in testing a hypothesis
␪ ϭ ␪0 against an alternative ␪ ϭ ␪1 (Ͼ ␪0, right-sided test)

If the alternative is not a single number but is of the form (1)–(3), then b becomes a
function of u. This function b(u) is called the operating characteristic (OC) of the test
and its curve the OC curve. Clearly, in this case h ϭ 1 Ϫ b also depends on u. This
function h(u) is called the power function of the test. (Examples will follow.)
Of course, from a test that leads to the acceptance of a certain hypothesis u0, it does
not follow that this is the only possible hypothesis or the best possible hypothesis. Hence
the terms “not reject” or “fail to reject” are perhaps better than the term “accept.”

Test for ␮ of the Normal Distribution with Known s2
The following example explains the three kinds of hypotheses.
EXAMPLE 2

Test for the Mean of the Normal Distribution with Known Variance
Let X be a normal random variable with variance s2 ϭ 9. Using a sample of size n ϭ 10 with mean x, test the
hypothesis ␮ ϭ ␮0 ϭ 24 against the three kinds of alternatives, namely,
(a)

␮ Ͼ ␮0

(b)

␮ Ͻ ␮0

(c)

␮

␮0.

c25.qxd

11/3/10

1082

6:21 PM

Page 1082

CHAP. 25 Mathematical Statistics

Solution. We choose the significance level a ϭ 0.05. An estimate of the mean will be obtained from
Xϭ

1
(X ϩ Á ϩ Xn).
n 1

If the hypothesis is true, X is normal with mean ␮ ϭ 24 and variance s2>n ϭ 0.9, see Theorem 1, Sec. 25.3.
Hence we may obtain the critical value c from Table A8 in App. 5.
Case (a).

Right-Sided Test. We determine c from P(X Ͼ c)␮ϭ24 ϭ a ϭ 0.05, that is,
P(X Ϲ c)␮ϭ24 ϭ £ a

c Ϫ 24
10.9

b ϭ 1 Ϫ a ϭ 0.95.

Table A8 in App. 5 gives (c Ϫ 24)> 10.9 ϭ 1.645, and c ϭ 25.56, which is greater than ␮0, as in the upper
part of Fig. 533. If x Ϲ 25.56, the hypothesis is accepted. If x Ͼ 25.56, it is rejected. The power function of the
test is (Fig. 535)

␩(␮)
1.0
0.8
0.6
0.4
0.2

20

␮0

22

26

28 ␮

Fig. 535. Power function h(␮) in Example 2, case (a) (dashed) and case (c)

h(␮) ϭ P(X Ͼ 25.56)␮ ϭ 1 Ϫ P(X Ϲ 25.56)␮
(7)

Case (b).

ϭ 1 Ϫ £a

25.56 Ϫ ␮
10.9

b ϭ 1 Ϫ £(26.94 Ϫ 1.05␮)

Left-Sided Test. The critical value c is obtained from the equation
P(X Ϲ c)␮ϭ24 ϭ £ a

c Ϫ 24
10.9

b ϭ a ϭ 0.05.

Table A8 in App. 5 yields c ϭ 24 Ϫ 1.56 ϭ 22.44. If x м 22.44, we accept the hypothesis. If x Ͻ 22.44, we
reject it. The power function of the test is

(8)

h(␮) ϭ P(X Ϲ 22.44)␮ ϭ £ a

22.44 Ϫ ␮
10.9

b ϭ £(23.65 Ϫ 1.05␮).

Case (c). Two-Sided Test. Since the normal distribution is symmetric, we choose c1 and c2 equidistant from
␮ ϭ 24, say, c1 ϭ 24 Ϫ k and c2 ϭ 24 ϩ k, and determine k from
P(24 Ϫ k Ϲ X Ϲ 24 ϩ k)␮ϭ24 ϭ £ a

k
10.9

b Ϫ £ aϪ

k
10.9

b ϭ 1 Ϫ a ϭ 0.95.

c25.qxd

11/3/10

6:21 PM

Page 1083

SEC. 25.4 Testing of Hypotheses. Decisions

1083

Table A8 in App. 5 gives k> 10.9 ϭ 1.960, hence k ϭ 1.86. This gives the values c1 ϭ 24 Ϫ 1.86 ϭ 22.14 and
c2 ϭ 24 ϩ 1.86 ϭ 25.86. If x is not smaller than c1 and not greater than c2, we accept the hypothesis. Otherwise
we reject it. The power function of the test is (Fig. 535)
h(␮) ϭ P(X Ͻ 22.14)␮ ϩ P(X Ͼ 25.86)␮ ϭ P(X Ͻ 22.14)␮ ϩ 1 Ϫ P(X Ϲ 25.86)␮
(9)

ϭ 1 ϩ £a

22.14 Ϫ ␮
10.9

b Ϫ £a

25.86 Ϫ ␮
10.9

b

ϭ 1 ϩ £(23.34 Ϫ 1.05␮) Ϫ £(27.26Ϫ1.05␮).
Consequently, the operating characteristic b(␮) ϭ 1 Ϫ h(␮) (see before) is (Fig. 536)
b(␮) ϭ £(27.26 Ϫ 1.05␮) Ϫ £(23.34 Ϫ 1.05␮).
If we take a larger sample, say, of size n ϭ 100 (instead of 10), then s2>n ϭ 0.09 (instead of 0.9) and the
critical values are c1 ϭ 23.41 and c2 ϭ 24.59, as can be readily verified. Then the operating characteristic of
the test is
b(␮) ϭ £ a

24.59 Ϫ ␮
10.09

b Ϫ £a

23.41 Ϫ ␮
10.09

b

ϭ £(81.97 Ϫ 3.33␮) Ϫ £(78.03 Ϫ 3.33␮).
Figure 536 shows that the corresponding OC curve is steeper than that for n ϭ 10. This means that the increase
of n has led to an improvement of the test. In any practical case, n is chosen as small as possible but so
large that the test brings out deviations between ␮ and ␮0 that are of practical interest. For instance, if
deviations of Ϯ2 units are of interest, we see from Fig. 536 that n ϭ 10 is much too small because when
␮ ϭ 24 Ϫ 2 ϭ 22 or ␮ ϭ 24 ϩ 2 ϭ 26 b is almost 50%. On the other hand, we see that n ϭ 100 is sufficient
for that purpose.
᭿

␤(␮)
1.0
0.8
0.6
0.4
n = 10
0.2
n = 100
20

22

␮0

26

28 ␮

Fig. 536. Curves of the operating characteristic (OC curves) in
Example 2, case (c), for two different sample sizes n

Test for ␮ When s2 Is Unknown, and for s2
EXAMPLE 3

Test for the Mean of the Normal Distribution with Unknown Variance
The tensile strength of a sample of n ϭ 16 manila ropes (diameter 3 in.) was measured. The sample mean was
x ϭ 4482 kg, and the sample standard deviation was s ϭ 115 kg (N. C. Wiley, 41st Annual Meeting of the
American Society for Testing Materials). Assuming that the tensile strength is a normal random variable, test
the hypothesis ␮0 ϭ 4500 kg against the alternative ␮1 ϭ 4400 kg. Here ␮0 may be a value given by the
manufacturer, while ␮1 may result from previous experience.

c25.qxd

11/3/10

6:21 PM

1084

Page 1084

CHAP. 25 Mathematical Statistics

Solution. We choose the significance level a ϭ 5%. If the hypothesis is true, it follows from Theorem 2
in Sec. 25.3, that the random variable
Tϭ

X Ϫ ␮0
S> 1n

ϭ

X Ϫ 4500
S>4

has a t-distribution with n Ϫ 1 ϭ 15 d.f. The test is left-sided. The critical value c is obtained from
P(T Ͻ c)␮0 ϭ a ϭ 0.05. Table A9 in App. 5 gives c ϭ Ϫ1.75. As an observed value of T we obtain from the
sample t ϭ (4482 Ϫ 4500)>(115>4) ϭ Ϫ0.626. We see that t Ͼ c and accept the hypothesis. For obtaining
numeric values of the power of the test, we would need tables called noncentral Student t-tables; we shall not
discuss this question here.
᭿

EXAMPLE 4

Test for the Variance of the Normal Distribution
Using a sample of size n ϭ 15 and sample variance s 2 ϭ 13 from a normal population, test the hypothesis
s2 ϭ s20 ϭ 10 against the alternative s2 ϭ s21 ϭ 20.

Solution. We choose the significance level a ϭ 5%. If the hypothesis is true, then
Y ϭ (n Ϫ 1)

S2
s20

ϭ 14

S2
10

ϭ 1.4S 2

has a chi-square distribution with n Ϫ 1 ϭ 14 d.f. by Theorem 3, Sec. 25.3. From
P(Y Ͼ c) ϭ a ϭ 0.05,

that is,

P(Y Ϲ c) ϭ 0.95,

and Table A10 in App. 5 with 14 degrees of freedom we obtain c ϭ 23.68. This is the critical value of Y. Hence
to S 2 ϭ s20Y>(n Ϫ 1) ϭ 0.714Y there corresponds the critical value c* ϭ 0.714 ؒ 23.68 ϭ 16.91. Since s 2 Ͻ c*,
we accept the hypothesis.
If the alternative is true, the random variable Y1 ϭ 14S 2>s21 ϭ 0.7S 2 has a chi-square distribution with 14
d.f. Hence our test has the power
h ϭ P(S 2 Ͼ c*)s2 ϭ20 ϭ P(Y1 Ͼ 0.7c*)s2 ϭ20 ϭ 1 Ϫ P(Y1 Ϲ 11.84)s2 ϭ20.
From a more extensive table of the chi-square distribution (e.g. in Ref. [G3] or [G8]) or from your CAS, you
see that h Ϸ 62%. Hence the Type II risk is very large, namely, 38%. To make this risk smaller, we would
have to increase the sample size.
᭿

Comparison of Means and Variances
EXAMPLE 5

Comparison of the Means of Two Normal Distributions
Using a sample x 1, Á , x n1 from a normal distribution with unknown mean ␮x and a sample y1, Á , yn2 from
another normal distribution with unknown mean ␮y, we want to test the hypothesis that the means are equal,
␮x ϭ ␮y, against an alternative, say, ␮x Ͼ ␮y. The variances need not be known but are assumed to be equal.3
Two cases of comparing means are of practical importance:

Case A. The samples have the same size. Furthermore, each value of the first sample corresponds to precisely
one value of the other, because corresponding values result from the same person or thing (paired comparison)—
for example, two measurements of the same thing by two different methods or two measurements from the two
eyes of the same person. More generally, they may result from pairs of similar individuals or things, for example,
identical twins, pairs of used front tires from the same car, etc. Then we should form the differences of
corresponding values and test the hypothesis that the population corresponding to the differences has mean 0,
using the method in Example 3. If we have a choice, this method is better than the following.
3
This assumption of equality of variances can be tested, as shown in the next example. If the test shows that
they differ significantly, choose two samples of the same size n1 ϭ n2 ϭ n (not too small, Ͼ 30, say), use the
test in Example 2 together with the fact that (12) is an observed value of an approximately standardized normal
random variable.

c25.qxd

11/3/10

6:21 PM

Page 1085

SEC. 25.4 Testing of Hypotheses. Decisions

1085

Case B. The two samples are independent and not necessarily of the same size. Then we may proceed
as follows. Suppose that the alternative is ␮x Ͼ ␮y. We choose a significance level a. Then we compute the
sample means x and y as well as (n 1 Ϫ 1)s 2x and (n 2 Ϫ 1)s 2y, where s 2x and s 2y are the sample variances. Using
Table A9 in App. 5 with n 1 ϩ n 2 Ϫ 2 degrees of freedom, we now determine c from
P(T Ϲ c) ϭ 1 Ϫ a.

(10)
We finally compute
t0 ϭ

(11)

n 1n 2(n 1 ϩ n 2 Ϫ 2)

xϪy

n1 ϩ n 2

2(n 1 Ϫ 1)s x2 ϩ (n 2 Ϫ 1)s y2

B

.

It can be shown that this is an observed value of a random variable that has a t-distribution with n 1 ϩ n 2 Ϫ 2
degrees of freedom, provided the hypothesis is true. If t 0 Ϲ c, the hypothesis is accepted. If t 0 Ͼ c, it is rejected.
If the alternative is ␮x ␮y, then (10) must be replaced by
P(T Ϲ c1) ϭ 0.5a,

(10*)

P(T Ϲ c2) ϭ 1 Ϫ 0.5a.

Note that for samples of equal size n 1 ϭ n 2 ϭ n, formula (11) reduces to
t 0 ϭ 1n

(12)

xϪy
2s 2x ϩ s 2y

.

To illustrate the computations, let us consider the two samples (x 1, Á , x n1) and ( y1, Á , yn2) given by
105

108

86

103

103

107

124

105

89

92

84

97

103

107

111

97

and

showing the relative output of tin plate workers under two different working conditions [J. J. B. Worth, Journal
of Industrial Engineering 9, 249–253). Assuming that the corresponding populations are normal and have the
same variance, let us test the hypothesis ␮x ϭ ␮y against the alternative ␮x ␮y. (Equality of variances will
be tested in the next example.)

Solution. We find
x ϭ 105.125,

y ϭ 97.500,

s x2 ϭ 106.125.

s y2 ϭ 84.000.

We choose the significance level a ϭ 5%. From (10*) with 0.5a ϭ 2.5%, 1 Ϫ 0.5a ϭ 97.5% and Table A9
in App. 5 with 14 degrees of freedom we obtain c1 ϭ Ϫ2.14 and c2 ϭ 2.14. Formula (12) with n ϭ 8 gives the
value
t 0 ϭ 18 ؒ 7.625> 1190.125 ϭ 1.56.
Since c1 Ϲ t 0 Ϲ c2, we accept the hypothesis ␮x ϭ ␮y that under both conditions the mean output is the same.
Case A applies to the example because the two first sample values correspond to a certain type of work, the
next two were obtained in another kind of work, etc. So we may use the differences
16

16

2

6

0

0

13

8

of corresponding sample values and the method in Example 3 to test the hypothesis ␮ ϭ 0, where ␮ is the mean
of the population corresponding to the differences. As a logical alternative we take ␮ 0. The sample mean is
d ϭ 7.625, and the sample variance is s 2 ϭ 45.696. Hence
t ϭ 18 (7.625 Ϫ 0)> 145.696 ϭ 3.19.
From P(T Ϲ c1) ϭ 2.5%, P(T Ϲ c2) ϭ 97.5% and Table A9 in App. 5 with n Ϫ 1 ϭ 7 degrees of freedom we
obtain c1 ϭ Ϫ2.36, c2 ϭ 2.36 and reject the hypothesis because t ϭ 3.19 does not lie between c1 and c2. Hence
our present test, in which we used more information (but the same samples), shows that the difference in output
is significant.
᭿

c25.qxd

11/3/10

6:21 PM

1086
EXAMPLE 6

Page 1086

CHAP. 25 Mathematical Statistics
Comparison of the Variance of Two Normal Distributions
Using the two samples in the last example, test the hypothesis s2x ϭ s2y; assume that the corresponding
populations are normal and the nature of the experiment suggests the alternative s2x Ͼ s2y.

Solution. We find s 2x ϭ 106.125, s 2y ϭ 84.000. We choose the significance level a ϭ 5%. Using
P(V Ϲ c) ϭ 1 Ϫ a ϭ 95% and Table A11 in App. 5, with (n 1 Ϫ 1, n 2 Ϫ 1) ϭ (7, 7) degrees of freedom, we
determine c ϭ 3.79. We finally compute v0 ϭ s 2x>s 2y ϭ 1.26. Since v0 Ϲ c, we accept the hypothesis. If v0 Ͻ c,
we would reject it.
This test is justified by the fact that v0 is an observed value of a random variable that has a so-called
F-distribution with (n 1 Ϫ 1, n 2 Ϫ 1) degrees of freedom, provided the hypothesis is true. (Proof in Ref. [G3]
listed in App. 1.) The F-distribution with (m, n) degrees of freedom was introduced by R. A. Fisher4 and has
the distribution function F(z) ϭ 0 if z Ͻ 0 and
(13)

F(z) ϭ K mn

Ύ

z

t (m؊2)>2(mt ϩ n)؊(mϩn)>2 dt

(z м 0),

0

where K mn ϭ m m>2n n>2⌫(12 m ϩ 12 n)>⌫(12 m)⌫(12 n). (For ⌫ see App. A3.1.)

᭿

This long section contained the basic ideas and concepts of testing, along with typical
applications and you may perhaps want to review it quickly before going on, because the
next sections concern an adaptation of these ideas to tasks of great practical importance
and resulting tests in connection with quality control, acceptance (or rejection) of goods
produced, and so on.

PROBLEM SET 25.4
1. From memory: Make a list of the three types of
alternatives, each with a typical example of your own.
2. Make a list of methods in this section, each with the
distribution needed in testing.
3. Test ␮ ϭ 0 against ␮ Ͼ 0, assuming normality and
using the sample 0, 1, Ϫ1, 3, Ϫ8, 6, 1 (deviations of the
azimuth [multiples of 0.01 radian] in some revolution
of a satellite). Choose a ϭ 5%.
4. In one of his classical experiments Buffon obtained 2048
heads in tossing a coin 4040 times. Was the coin fair?
5. Do the same test as in Prob. 4, using a result by K.
Pearson, who obtained 6019 heads in 12,000 trials.
6. Assuming normality and known variance s2 ϭ 9,
test the hypothesis ␮ ϭ 60.0 against the alternative
␮ ϭ 57.0 using a sample of size 20 with mean x ϭ 58.50
and choosing a ϭ 5%.
7. How does the result in Prob. 6 change if we use a smaller sample, say, of size 5, the other data (x ϭ 58.05,
a ϭ 5%, etc.) remaining as before?

4

8. Determine the power of the test in Prob. 6.
9. What is the rejection region in Prob. 6 in the case of a
two-sided test with a ϭ 5% ?
10. CAS EXPERIMENT. Tests of Means and Variances.
(a) Obtain 100 samples of size 10 each from the normal
distribution with mean 100 and variance 25. For each
sample, test the hypothesis ␮0 ϭ 100 against the
alternative ␮1 Ͼ 100 at the level of a ϭ 10%. Record
the number of rejections of the hypothesis. Do the whole
experiment once more and compare.
(b) Set up a similar experiment for the variance of a
normal distribution and perform it 100 times.
11. A firm sells oil in cans containing 5000 g oil per can
and is interested to know whether the mean weight
differs significantly from 5000 g at the 5% level, in
which case the filling machine has to be adjusted. Set
up a hypothesis and an alternative and perform the test,
assuming normality and using a sample of 50 fillings
with mean 4990 g and standard deviation 20 g.

After the pioneering work of the English statistician and biologist, KARL PEARSON (1857–1936), the
founder of the English school of statistics, and WILLIAM SEALY GOSSET (1876–1937), who discovered the
t-distribution (and published under the name “Student”), the English statistician Sir RONALD AYLMER
FISHER (1890–1962), professor of eugenics in London (1933–1943) and professor of genetics in Cambridge,
England (1943–1957) and Adelaide, Australia (1957–1962), had great influence on the further development of
modern statistics.

c25.qxd

11/3/10

6:21 PM

Page 1087

SEC. 25.5 Quality Control
12. If a sample of 25 tires of a certain kind has a mean life
of 37,000 miles and a standard deviation of 5000 miles,
can the manufacturer claim that the true mean life of
such tires is greater than 35,000 miles? Set up and test
a corresponding hypothesis at the 5% level, assuming
normality.
13. If simultaneous measurements of electric voltage by
two different types of voltmeter yield the differences
(in volts) 0.4, Ϫ0.6, 0.2, 0.0, 1.0, 1.4, 0.4, 1.6, can we
assert at the 5% level that there is no significant
difference in the calibration of the two types of
instruments? Assume normality.
14. If a standard medication cures about 75% of patients
with a certain disease and a new medication cured 310
of the first 400 patients on whom it was tried, can we
conclude that the new medication is better? Choose
a ϭ 5%. First guess. Then calculate.
15. Suppose that in the past the standard deviation of
weights of certain 100.0-oz packages filled by a
machine was 0.8 oz. Test the hypothesis H0: s ϭ 0.8
against the alternative H1: s Ͼ 0.8 (an undesirable
increase), using a sample of 20 packages with standard
deviation 1.0 oz and assuming normality. Choose
a ϭ 5%.
16. Suppose that in operating battery-powered electrical
equipment, it is less expensive to replace all batteries at fixed intervals than to replace each battery
individually when it breaks down, provided the
standard deviation of the lifetime is less than a certain

25.5

1087

17.

18.

19.

20.

limit, say, less than 5 hours. Set up and apply a suitable
test, using a sample of 28 values of lifetimes with
standard deviation s ϭ 3.5 hours and assuming
normality: choose a ϭ 5%.
Brand A gasoline was used in 16 similar automobiles
under identical conditions. The corresponding sample
of 16 values (miles per gallon) had mean 19.6 and
standard deviation 0.4. Under the same conditions,
high-power brand B gasoline gave a sample of 16
values with mean 20.2 and standard deviation 0.6. Is
the mileage of B significantly better than that of A?
Test at the 5% level; assume normality. First guess.
Then calculate.
The two samples 70, 80, 30, 70, 60, 80 and 140, 120,
130, 120, 120, 130, 120 are values of the differences of
temperatures (°C) of iron at two stages of casting, taken
from two different crucibles. Is the variance of the first
population larger than that of the second? Assume
normality. Choose a ϭ 5%.
Show that for a normal distribution the two types of
errors in a test of a hypothesis H0: ␮ ϭ ␮0 against an
alternative H1: ␮ ϭ ␮1 can be made as small as one
pleases (not zero!) by taking the sample sufficiently
large.
Test for equality of population means against the
alternative that the means are different assuming
normality, choosing a ϭ 5% and using two samples of
sizes 12 and 18, with mean 10 and 14, respectively,
and equal standard deviation 3.

Quality Control
The ideas on testing can be adapted and extended in various ways to serve basic practical
needs in engineering and other fields. We show this in the remaining sections for some
of the most important tasks solvable by statistical methods. As a first such area of problems,
we discuss industrial quality control, a highly successful method used in various industries.
No production process is so perfect that all the products are completely alike. There is
always a small variation that is caused by a great number of small, uncontrollable factors
and must therefore be regarded as a chance variation. It is important to make sure that the
products have required values (for example, length, strength, or whatever property may
be essential in a particular case). For this purpose one makes a test of the hypothesis that
the products have the required property, say, ␮ ϭ ␮0, where ␮0 is a required value. If
this is done after an entire lot has been produced (for example, a lot of 100,000 screws),
the test will tell us how good or how bad the products are, but it it obviously too late to
alter undesirable results. It is much better to test during the production run. This is done
at regular intervals of time (for example, every hour or half-hour) and is called quality
control. Each time a sample of the same size is taken, in practice 3 to 10 times. If the
hypothesis is rejected, we stop the production and look for the cause of the trouble.

1088

6:21 PM

Page 1088

CHAP. 25 Mathematical Statistics

If we stop the production process even though it is progressing properly, we make a
Type I error. If we do not stop the process even though something is not in order, we
make a Type II error (see Sec. 25.4). The result of each test is marked in graphical form
on what is called a control chart. This was proposed by W. A. Shewhart in 1924 and
makes quality control particularly effective.

Control Chart for the Mean
An illustration and example of a control chart is given in the upper part of Fig. 537. This
control chart for the mean shows the lower control limit LCL, the center control line
CL, and the upper control limit UCL. The two control limits correspond to the critical
values c1 and c2 in case (c) of Example 2 in Sec. 25.4. As soon as a sample mean falls
outside the range between the control limits, we reject the hypothesis and assert that the
4.20
0.5%

Mean

11/3/10

4.15

UCL

4.10

CL

4.05

LCL

99%

0.5%
4.00
5

Sample no.

10
1%

0.04
0.0365

UCL

0.03
Standard deviation

c25.qxd

0.02
99%

0.01

0
Sample no.

5

10

Fig. 537. Control charts for the mean (upper part of figure) and
the standard deviation in the case of the samples on p. 1089

c25.qxd

11/3/10

6:21 PM

Page 1089

SEC. 25.5 Quality Control

1089

production process is “out of control”; that is, we assert that there has been a shift in
process level. Action is called for whenever a point exceeds the limits.
If we choose control limits that are too loose, we shall not detect process shifts. On the
other hand, if we choose control limits that are too tight, we shall be unable to run the
process because of frequent searches for nonexistent trouble. The usual significance level
is a ϭ 1%. From Theorem 1 in Sec. 25.3 and Table A8 in App. 5 we see that in the case
of the normal distribution the corresponding control limits for the mean are
LCL ϭ ␮0 Ϫ 2.58

(1)

s
,
1n

UCL ϭ ␮0 ϩ 2.58

s
.
1n

Here s is assumed to be known. If s is unknown, we may compute the standard deviations
of the first 20 or 30 samples and take their arithmetic mean as an approximation of s.
The broken line connecting the means in Fig. 537 is merely to display the results.
Additional, more subtle controls are often used in industry. For instance, one observes
the motions of the sample means above and below the centerline, which should happen
frequently. Accordingly, long runs (conventionally of length 7 or more) of means all above
(or all below) the centerline could indicate trouble.
Table 25.5 Twelve Samples of Five Values Each
(Diameter of Small Cylinders, Measured in Millimeters)
Sample
Number

Sample Values

x

s

R

1
2
3
4
5

4.06
4.10
4.06
4.06
4.08

4.08
4.10
4.06
4.08
4.10

4.08
4.12
4.08
4.08
4.12

4.08
4.12
4.10
4.10
4.12

4.10
4.12
4.12
4.12
4.12

4.080
4.112
4.084
4.088
4.108

0.014
0.011
0.026
0.023
0.018

0.04
0.02
0.06
0.06
0.04

6
7
8
9
10

4.08
4.06
4.08
4.06
4.06

4.10
4.08
4.08
4.08
4.08

4.10
4.08
4.10
4.10
4.10

4.10
4.10
4.10
4.12
4.12

4.12
4.12
4.12
4.14
4.16

4.100
4.088
4.096
4.100
4.104

0.014
0.023
0.017
0.032
0.038

0.04
0.06
0.04
0.08
0.10

11
12

4.12
4.14

4.14
4.14

4.14
4.16

4.14
4.16

4.16
4.16

4.140
4.152

0.014
0.011

0.04
0.02

Control Chart for the Variance
In addition to the mean, one often controls the variance, the standard deviation, or the range.
To set up a control chart for the variance in the case of a normal distribution, we may employ
the method in Example 4 of Sec. 25.4 for determining control limits. It is customary to use only
one control limit, namely, an upper control limit. Now from Example 4 of Sec. 25.4 we have
S 2 ϭ s20Y>(n Ϫ 1), where, because of our normality assumption, the random variable Y has a
chi-square distribution with n Ϫ 1 degrees of freedom. Hence the desired control limit is
(2)

UCL ϭ

s2c
nϪ1

c25.qxd

11/3/10

1090

6:21 PM

Page 1090

CHAP. 25 Mathematical Statistics

where c is obtained from the equation
P(Y Ͼ c) ϭ a,

P(Y Ϲ c) ϭ 1 Ϫ a

that is,

and the table of the chi-square distribution (Table A10 in App. 5) with n Ϫ 1 degrees of
freedom (or from your CAS); here a (5% or 1%, say) is the probability that in a properly
running process an observed value s 2 of S 2 is greater than the upper control limit.
If we wanted a control chart for the variance with both an upper control limit UCL and
a lower control limit LCL, these limits would be

(3)

LCL ϭ

s2c1

UCL ϭ

and

nϪ1

s2c2
nϪ1

,

where c1 and c2 are obtained from Table A10 with n Ϫ 1 d.f. and the equations
(4)

P(Y Ϲ c1) ϭ

a
2

P(Y Ϲ c2) ϭ 1 Ϫ

and

a
.
2

Control Chart for the Standard Deviation
To set up a control chart for the standard deviation, we need an upper control limit

(5)

UCL ϭ

s 1c
1n Ϫ 1

obtained from (2). For example, in Table 25.5 we have n ϭ 5. Assuming that the
corresponding population is normal with standard deviation s ϭ 0.02 and choosing
a ϭ 1%, we obtain from the equation
P(Y Ϲ c) ϭ 1 Ϫ a ϭ 99%
and Table A10 in App. 5 with 4 degrees of freedom the critical value c ϭ 13.28 and from
(5) the corresponding value
UCL ϭ

0.02113.28
14

ϭ 0.0365,

which is shown in the lower part of Fig. 537.
A control chart for the standard deviation with both an upper and a lower control limit
is obtained from (3).

Control Chart for the Range
Instead of the variance or standard deviation, one often controls the range R (ϭ largest
sample value minus smallest sample value). It can be shown that in the case of the normal
distribution, the standard deviation s is proportional to the expectation of the random

c25.qxd

11/3/10

6:21 PM

Page 1091

SEC. 25.5 Quality Control

1091

variable R* for which R is an observed value, say, s ϭ lnE(R*) where the factor of
proportionality ln depends on the sample size n and has the values
n

2

ln ϭ s>E(R*) 0.89

3

4

5

6

7

8

9

10

0.59

0.49

0.43

0.40

0.37

0.35

0.34

0.32

n

12

14

16

18

20

30

40

50

ln ϭ s>E(R*)

0.31

0.29

0.28

0.28

0.27

0.25

0.23

0.22

Since R depends on two sample values only, it gives less information about a sample
than s does. Clearly, the larger the sample size n is, the more information we lose in using
R instead of s. A practical rule is to use s when n is larger than 10.

PROBLEM SET 25.5
1. Suppose a machine for filling cans with lubricating
oil is set so that it will generate fillings which form
a normal population with mean 1 gal and standard
deviation 0.02 gal. Set up a control chart of the
type shown in Fig. 537 for controlling the mean, that
is, find LCL and UCL, assuming that the sample size
is 4.
2. Three-sigma control chart. Show that in Prob. 1, the
requirement of the significance level a ϭ 0.3% leads
to LCL ϭ ␮ Ϫ 3s> 1n and UCL ϭ ␮ ϩ 3s> 1n, and
find the corresponding numeric values.

7. Graph the ranges of the samples in Prob. 6 on a control
chart for ranges.
8. Graph ln ϭ s>E(R*) as a function of n. Why is ln a
monotone decreasing function of n?
9. Eight samples of size 2 were taken from a lot of screws.
The values (length in inches) are
Sample No.
Length

1

2

3

4

5

6

7

8

3.50 3.51 3.49 3.52 3.53 3.49 3.48 3.52
3.51 3.48 3.50 3.50 3.49 3.50 3.47 3.49

3. What sample size should we choose in Prob. 1 if we
want LCL and UCL somewhat closer together, say,
UCL Ϫ LCL ϭ 0.02, without changing the significance level?

Assuming that the population is normal with mean
3.500 and variance 0.0004 and using (1), set up a
control chart for the mean and graph the sample means
on the chart.

4. What effect on UCL Ϫ LCL does it have if we double
the sample size? If we switch from a ϭ 1% to
a ϭ 5%?

10. Attribute control charts. Fifteen samples of size 100
were taken from a production of containers. The
numbers of defectives (leaking containers) in those
samples (in the order observed) were

5. How should we change the sample size in controlling
the mean of a normal population if we want
UCL Ϫ LCL to decrease to half its original value?
6. Graph the means of the following 10 samples
(thickness of gaskets, coded values) on a control chart
for means, assuming that the population is normal with
mean 5 and standard deviation 1.16.
Time
Sample
values

10:00
5
2
5
6

11:00
7
5
4
4

12:00
7
3
6
5

13:00
4
4
3
6

1 4 5 4 9 7 0 5 6 13 0 2 1 12 8
From previous experience it was known that the
average fraction defective is p ϭ 4% provided that
the process of production is running properly. Using
the binomial distribution, set up a fraction defective
chart (also called a p-chart), that is, choose the
14:00
5
6
4
6

15:00
6
4
6
4

16:00
5
5
6
4

17:00
5
2
5
3

18:00
3
4
8
4

19:00
3
6
6
8

c25.qxd

11/3/10

6:21 PM

1092

Page 1092

CHAP. 25 Mathematical Statistics

LCL ϭ 0 and determine the UCL for the fraction
defective (in percent) by the use of 3-sigma limits,
where s2 is the variance of the random variable
X ϭ Fraction defective in a sample of size 100.
Is the process under control?
11. Number of defectives. Find formulas for the UCL, CL,
and LCL (corresponding to 3s-limits) in the case of a
control chart for the number of defectives, assuming
that, in a state of statistical control, the fraction of
defectives is p.
12. CAS PROJECT. Control Charts. (a) Obtain 100
samples of 4 values each from the normal distribution
with mean 8.0 and variance 0.16 and their means,
variances, and ranges.
(b) Use these samples for making up a control chart
for the mean.
(c) Use them on a control chart for the standard
deviation.
(d) Make up a control chart for the range.
(e) Describe quantitative properties of the samples
that you can see from those charts (e.g., whether the

25.6

corresponding process is under control, whether the
quantities observed vary randomly, etc.).
13. Since the presence of a point outside control limits for
the mean indicates trouble, how often would we be
making the mistake of looking for nonexistent trouble
if we used (a) 1-sigma limits, (b) 2-sigma limits?
Assume normality.
14. What LCL and UCL should we use instead of (1) if,
instead of x, we use the sum x 1 ϩ Á ϩ x n of the
sample values? Determine these limits in the case of
Fig. 537.
15. Number of defects per unit. A so-called c-chart or
defects-per-unit chart is used for the control of the
number X of defects per unit (for instance, the number
of defects per 100 meters of paper, the number of
missing rivets in an airplane wing, etc.). (a) Set up
formulas for CL and LCL, UCL corresponding to
␮ Ϯ 3s, assuming that X has a Poisson distribution.
(b) Compute CL, LCL, and UCL in a control process
of the number of imperfections in sheet glass; assume
that this number is 3.6 per sheet on the average when
the process is in control.

Acceptance Sampling
Acceptance sampling is usually done when products leave the factory (or in some cases
even within the factory). The standard situation in acceptance sampling is that a producer
supplies to a consumer (a buyer or wholesaler) a lot of N items (a carton of screws, for
instance). The decision to accept or reject the lot is made by determining the number x
of defectives (ϭ defective items) in a sample of size n from the lot. The lot is accepted
if x Ϲ c, where c is called the acceptance number, giving the allowable number of
defectives. If x Ͼ c, the consumer rejects the lot. Clearly, producer and consumer must
agree on a certain sampling plan giving n and c.
From the hypergeometric distribution we see that the event A: “Accept the lot” has
probability (see Sec. 24.7)
c

(1)

M
NϪM
N
P(A) ϭ P(X Ϲ c) ϭ a a b a
b^a b
x
nϪx
n
xϭ0

where M is the number of defectives in a lot of N items. In terms of the fraction defective
u ϭ M>N we can write (1) as
c

(2)

Nu
N Ϫ Nu
N
P(A; u) ϭ a a b a
b^a b .
x
n
Ϫ
x
n
xϭ0

P(A; u) can assume n ϩ 1 values corresponding to u ϭ 0, 1>N, 2>N, Á , N>N; here, n and
c are fixed. A monotone smooth curve through these points is called the operating
characteristic curve (OC curve) of the sampling plan considered.

c25.qxd

11/3/10

6:21 PM

Page 1093

SEC. 25.6 Acceptance Sampling
EXAMPLE 1

1093

Sampling Plan
Suppose that certain tool bits are packaged 20 to a box, and the following sampling plan is used. A sample of
two tool bits is drawn, and the corresponding box is accepted if and only if both bits in the sample are good.
In this case, N ϭ 20, n ϭ 2, c ϭ 0, and (2) takes the form (a factor 2 drops out)
P(A; u) ϭ a
ϭ

20 u 20 Ϫ 20 u
20
ba
b^a b
0
2
2

(20 Ϫ 20 u)(19 Ϫ 20 u)
380

.

The values of P(A, u) for u ϭ 0, 1>20, 2>20, Á , 20>20 and the resulting OC curve are shown in Fig. 538.
᭿
(Verify!)
1

1

P(A; θ ) 0.5

P(A; θ ) 0.5

0
0

0.5

1

θ

Fig. 538. OC curve of the sampling plan with n ϭ 2
and c ϭ 0 for lots of size N ϭ 20

0
0

0.2

θ

Fig. 539. OC curve in Example 2

In most practical cases u will be small (less than 10%). Then if we take small samples
compared to N, we can approximate (2) by the Poisson distribution (Sec. 24.7); thus
c

(3)

P(A; u) ϳ e؊␮ a
xϭ0

EXAMPLE 2

␮x
x!

(␮ ϭ nu).

Sampling Plan. Poisson Distribution
Suppose that for large lots the following sampling plan is used. A sample of size n ϭ 20 is taken. If it contains
not more than one defective, the lot is accepted. If the sample contains two or more defectives, the lot is rejected.
In this plan, we obtain from (3)
P(A; u) ϳ e؊20 u(1 ϩ 20 u),
The corresponding OC curve is shown in Fig. 539.

᭿

Errors in Acceptance Sampling
We show how acceptance sampling fits into general test theory (Sec. 25.4) and what this
means from a practical point of view. The producer wants the probability a of rejecting

c25.qxd

11/3/10

1094

6:21 PM

Page 1094

CHAP. 25 Mathematical Statistics
P(A; θ )
95%
Producer's risk
α = 5%

50%

15%

Consumer's risk
β = 15%

θ1
0 θ0
= 1%
= 5%
Good
Indifference
Poor
material
zone
material

Fig. 540. OC curve, producer’s and consumer’s risks

an acceptable lot (a lot for which u does not exceed a certain number u0 on which the
two parties agree) to be small. u0 is called the acceptable quality level (AQL). Similarly,
the consumer (the buyer) wants the probability b of accepting an unacceptable lot (a lot
for which u is greater than or equal to some u1) to be small. u1 is called the lot tolerance
percent defective (LTPD) or the rejectable quality level (RQL). a is called producer’s
risk. It corresponds to a Type I error in Sec. 25.4. b is called consumer’s risk and
corresponds to a Type II error. Figure 540 shows an example. We see that the points
(u0, 1 Ϫ a) and (u1, b) lie on the OC curve. It can be shown that for large lots we can
choose u0, u1 (Ͼ u0), a, b and then determine n and c such that the OC curve runs very
close to those prescribed points. Table 25.6 shows the analogy between acceptance
sampling and hypothesis testing in Sec. 25.4.
Table 25.6 Acceptance Sampling and Hypothesis Testing
Acceptance Sampling
Acceptable quality level (AQL) u ϭ u0
Lot tolerance percent defectives (LTPD)
u ϭ u1
Allowable number of defectives c
Producer’s risk ␣ of rejecting a lot
with u Ϲ u0
Consumer’s risk ␤ of accepting a lot
with u м u1

Hypothesis Testing
Hypothesis u ϭ u0
Alternative u ϭ u1
Critical value c
Probability ␣ of making a Type I error
(significance level)
Probability ␤ of making a Type II error

Rectification
Rectification of a rejected lot means that the lot is inspected item by item and all defectives
are removed and replaced by nondefective items. (This may be too expensive if the lot is
cheap; in this case the lot may be sold at a cut-rate price or scrapped.) If a production
turns out 100u% defectives, then in K lots of size N each, KNu of the KN items are

c25.qxd

11/3/10

6:21 PM

Page 1095

SEC. 25.6 Acceptance Sampling

1095

defectives. Now KP(A; u) of these lots are accepted. These contain KPNu defectives,
whereas the rejected and rectified lots contain no defectives, because of the rectification.
Hence after the rectification the fraction defective in all K lots equals KPNu>KN. This is
called the average outgoing quality (AOQ); thus
AOQ(u) ϭ uP(A; u).

(4)

Figure 541 shows an example. Since AOQ(0) ϭ 0 and P(A; 1) ϭ 0, the AOQ curve has
a maximum at some u ϭ u*, giving the average outgoing quality limit (AOQL). This is
the worst average quality that may be expected to be accepted under rectification.

1

OC curve

0.5

AOQL
0
0

AOQ curve

θ∗

0.5

θ

1

Fig. 541. OC curve and AOQ curve for the sampling plan in Fig. 538

PROBLEM SET 25.6
1. Lots of kitchen knives are inspected by a sampling plan
that uses a sample of size 20 and the acceptance number
c ϭ 1. What is the probability of accepting a lot with
1%, 2%, 10% defectives (knives with dull blades)?
Use Table A6 of the Poisson distribution in App. 5.
Graph the OC curve.
2. What happens in Prob. 1 if the sample size is increased
to 50? First guess. Then calculate. Graph the OC curve
and compare.
3. How will the probabilities in Prob. 1 with n ϭ 20
change (up or down) if we decrease c to zero? First
guess.
4. What are the producer’s and consumer’s risks in
Prob. 1 if the AQL is 2% and the RQL is 15%?
5. Lots of copper pipes are inspected according to a
sample plan that uses sample size 25 and acceptance
number 1. Graph the OC curve of the plan, using the

Poisson approximation. Find the producer’s risk if the
AQL is 1.5%.
6. Graph the AOQ curve in Prob. 5. Determine the AOQL,
assuming that rectification is applied.
7. In Example 1 in the text, what are the producer’s and
consumer’s risks if the AQL is 0.1 and the RQL is 0.6?
8. What happens in Example 1 in the text if we increase
the sample size to n ϭ 3, leaving the other data as
before? Compute P(A; 0.1) and P(A; 0.2) and compare
with Example 1.
9. Graph and compare sampling plans with c ϭ 1 and
increasing values of n, say, n ϭ 2, 3, 4. (Use the
binomial distribution.)
10. Find the binomial approximation of the hypergeometric
distribution in Example 1 in the text and compare the
approximate and the accurate values.

c25.qxd

11/3/10

6:21 PM

1096

Page 1096

CHAP. 25 Mathematical Statistics

11. Samples of 3 fuses are drawn from lots and a lot is
accepted if in the corresponding sample we find no
more than 1 defective fuse. Criticize this sampling plan.
In particular, find the probability of accepting a lot
that is 50% defective. (Use the binomial distribution
(7), Sec. 24.7.)
12. If in a sampling plan for large lots of spark plugs, the
sample size is 100 and we want the AQL to be 5% and
the producer’s risk 2%, what acceptance number c
should we choose? (Use the normal approximation of
the binomial distribution in Sec. 24.8.)

25.7

13. What is the consumer’s risk in Prob. 12 if we want the
RQL to be 12%? Use c ϭ 9 from the answer of
Prob. 12.
14. A lot of batteries for wrist watches is accepted if and
only if a sample of 20 contains at most 1 defective.
Graph the OC and AOQ curves. Find AOQL. [Use (3).]
15. Graph the OC curve and the AOQ curve for the single
sampling plan for large lots with n ϭ 5 and c ϭ 0, and
find the AOQL.

Goodness of Fit. ␹2 -Test
To test for goodness of fit means that we wish to test that a certain function F(x) is the
distribution function of a distribution from which we have a sample x 1, Á , x n. Then we
ෂ
test whether the sample distribution function F (x) defined by
ෂ
F (x) ϭ Sum of the relative frequencies of all sample values x j not exceeding x
fits F(x) “sufficiently well.” If this is so, we shall accept the hypothesis that F(x) is the
distribution function of the population; if not, we shall reject the hypothesis.
This test is of considerable practical importance, and it differs in character from the
tests for parameters (␮, s2, etc.) considered so far.
ෂ
To test in that fashion, we have to know how much F (x) can differ from F(x) if the
hypothesis is true. Hence we must first introduce a quantity that measures the deviation
ෂ
of F (x) from F(x), and we must know the probability distribution of this quantity under
the assumption that the hypothesis is true. Then we proceed as follows. We determine
a number c such that, if the hypothesis is true, a deviation greater than c has a small
preassigned probability. If, nevertheless, a deviation greater than c occurs, we have reason
to doubt that the hypothesis is true and we reject it. On the other hand, if the deviation
ෂ
does not exceed c, so that F (x) approximates F(x) sufficiently well, we accept the
hypothesis. Of course, if we accept the hypothesis, this means that we have insufficient
evidence to reject it, and this does not exclude the possibility that there are other functions
that would not be rejected in the test. In this respect the situation is quite similar to that
in Sec. 25.4.
Table 25.7 shows a test of that type, which was introduced by R. A. Fisher. This
test is justified by the fact that if the hypothesis is true, then ␹20 is an observed value
of a random variable whose distribution function approaches that of the chi-square
distribution with K Ϫ 1 degrees of freedom (or K Ϫ r Ϫ 1 degrees of freedom if r
parameters are estimated) as n approaches infinity. The requirement that at least five
sample values lie in each interval in Table 25.7 results from the fact that for finite
n that random variable has only approximately a chi-square distribution. A proof can
be found in Ref. [G3] listed in App. 1. If the sample is so small that the requirement
cannot be satisfied, one may continue with the test, but then use the result with
caution.

c25.qxd

11/3/10

6:21 PM

Page 1097

SEC. 25.7 Goodness of Fit. ␹ 2 -Test

1097

Table 25.7 Chi-square Test for the Hypothesis That F (x) is the Distribution Function
of a Population from Which a Sample x1 , • • • , xn is Taken
Step 1. Subdivide the x-axis into K intervals I1, I2, Á , IK such that each interval contains
at least 5 values of the given sample x 1, Á , x n. Determine the number bj of sample
values in the interval Ij, where j ϭ 1, Á , K. If a sample value lies at a common
boundary point of two intervals, add 0.5 to each of the two corresponding bj.
Step 2. Using F(x), compute the probability pj that the random variable X under
consideration assumes any value in the interval Ij, where j ϭ 1, Á , K. Compute
ej ϭ npj.
(This is the number of sample values theoretically expected in Ij if the hypothesis
is true.)
Step 3. Compute the deviation
K

␹20 ϭ a

(1)

jϭ1

(bj Ϫ ej)2
.
ej

Step 4. Choose a significance level (5%, 1%, or the like).
Step 5. Determine the solution c of the equation
P(␹2 Ϲ c) ϭ 1 Ϫ a
from the table of the chi-sqare distribution with K Ϫ 1 degrees of freedom (Table
A10 in App. 5). If r parameters of F(x) are unknown and their maximum likelihood
estimates (Sec. 25.2) are used, then use K Ϫ r Ϫ 1 degrees of freedom (instead
of K Ϫ 1). If ␹20 Ϲ c, accept the hypothesis. If ␹20 Ͼ c, reject the hypothesis.

Table 25.8 Sample of 100 Values of the Splitting Tensile Strength (lb/in.2)
of Concrete Cylinders
320
350
370
320
400
420
390
360
370
340

380
340
390
350
360
400
330
390
400
360

340
350
390
360
350
350
360
350
360
390

410
360
440
340
390
370
380
370
350
400

380
370
330
340
400
330
350
370
380
370

340
350
390
350
350
320
330
350
380
410

360
380
330
350
360
390
360
390
360
360

350
370
360
390
340
380
300
370
340
400

320
300
400
380
370
400
360
370
330
340

370
420
370
340
420
370
360
340
370
360

D. L. IVEY, Splitting tensile tests on structural lightweight aggregate concrete. Texas Transportation
Institute, College Station, Texas.

EXAMPLE 1

Test of Normality
Test whether the population from which the sample in Table 25.8 was taken is normal.

Solution. Table 25.8 shows the values (column by column) in the order obtained in the experiment. Table
25.9 gives the frequency distribution and Fig. 542 the histogram. It is hard to guess the outcome of the test—
does the histogram resemble a normal density curve sufficiently well or not?

c25.qxd

11/3/10

1098

6:21 PM

Page 1098

CHAP. 25 Mathematical Statistics
ෂ 2 ϭ 712.9. The computation in
The maximum likelihood estimates for ␮ and s2 are ␮ˆ ϭ x ϭ 364.7 and s
Table 25.10 yields ␹20 ϭ 2.688. It is very interesting that the interval 375 Á 385 contributes over 50% of ␹20.
From the histogram we see that the corresponding frequency looks much too small. The second largest
contribution comes from 395 Á 405, and the histogram shows that the frequency seems somewhat too large,
which is perhaps not obvious from inspection.

Table 25.9 Frequency Table of the Sample in Table 25.8
1
Tensile
Strength
x
[lb/in.2]

2
Absolute
Frequency

300
310
320
330
340

2
0
4
6
11

350
360
370
380
390
400
410
420
430
440

3
Relative
Frequency

4
Cumulative
Absolute
Frequency

5
Cumulative
Relative
Frequency
ෂ
F (x)

0.02
0.00
0.04
0.06
0.11

2
2
6
12
23

0.02
0.02
0.06
0.12
0.23

14
16
15
8
10

0.14
0.16
0.15
0.08
0.10

37
53
68
76
86

0.37
0.53
0.68
0.76
0.86

8
2
3
0
1

0.08
0.02
0.03
0.00
0.01

94
96
99
99
100

0.94
0.96
0.99
0.99
1.00

ෂ
ƒ(x)

We choose a ϭ 5%. Since K ϭ 10 and we estimated r ϭ 2 parameters we have to use Table A10 in App. 5
with K Ϫ r Ϫ 1 ϭ 7 degrees of freedom. We find c ϭ 14.07 as the solution of P(␹2 Ϲ c) ϭ 95%. Since ␹20 Ͻ c,
we accept the hypothesis that the population is normal.
᭿

0.20

0.15
~
f (x)
0.10

0.05

0
250

300

350

400

450

2

[lb./in. ]

Fig. 542. Frequency histogram of the sample in Table 25.8

c25.qxd

11/3/10

6:21 PM

Page 1099

SEC. 25.7 Goodness of Fit. ␹ 2 -Test

1099

Table 25.10 Computations in Example 1
x j Ϫ 364.7

Ϫϱ
325
335
345
355
365
375
385
395
405

•••
•••
•••
•••
•••
•••
•••
•••
•••
•••

325
335
345
355
365
375
385
395
405
ϱ

x j Ϫ 364.7

26.7

£a

Ϫϱ • • • Ϫ1.49
Ϫ1.49 • • • Ϫ1.11
Ϫ1.11 • • • Ϫ0.74
Ϫ0.74 • • • Ϫ0.36
Ϫ0.36 • • • 0.01
0.01 • • • 0.39
0.39 • • • 0.76
0.76 • • • 1.13
1.13 • • • 1.51
1.51 • • • ϱ

0.0000
0.0681
0.1335
0.2296
0.3594
0.5040
0.6517
0.7764
0.8708
0.9345

xj

26.7
•••
•••
•••
•••
•••
•••
•••
•••
•••
•••

b

0.0681
0.1335
0.2296
0.3594
0.5040
0.6517
0.7764
0.8708
0.9345
1.0000

ej

bj

6.81
6.54
9.61
12.98
14.46
14.77
12.47
9.44
6.37
6.55

6
6
11
14
16
15
8
10
8
6

Term in (1)
0.096
0.045
0.201
0.080
0.164
0.0004
1.602
0.033
0.417
0.046
␹20 ϭ 2.688

PROBLEM SET 25.7
1. Verify the calculations in Example 1 of the text.
2. If it is known that 25% of certain steel rods produced
by a standard process will break when subjected to a
load of 5000 lb, can we claim that a new, less expensive
process yields the same breakage rate if we find that in
a sample of 80 rods produced by the new process, 27
rods broke when subjected to that load? (Use a ϭ 5%.)
3. If 100 flips of a coin result in 40 heads and 60 tails,
can we assert on the 5% level that the coin is fair?
4. If in 10 flips of a coin we get the same ratio as in Prob. 3
(4 heads and 6 tails), is the conclusion the same as in
Prob. 3? First conjecture, then compute.
5. Can you claim, on a 5% level, that a die is fair if 60
trials give 1, Á , 6 with absolute frequencies 10, 13, 9,
11, 9, 8?
6. Solve Prob. 5 if rolling a die 180 times gives 33, 27,
29, 35, 25, 31.
7. If a service station had served 60, 49, 56, 46, 68, 39
cars from Monday through Friday between 1 P.M. and
2 P.M., can one claim on a 5% level that the differences
are due to randomness? First guess. Then calculate.
8. A manufacturer claims that in a process of producing
drill bits, only 2.5% of the bits are dull. Test the claim
against the alternative that more than 2.5% of the bits
are dull, using a sample of 400 bits containing 17 dull
ones. Use a ϭ 5%.
9. In a table of properly rounded function values, even
and odd last decimals should appear about equally
often. Test this for the 90 values of J1(x) in Table A1
in App. 5.

10. TEAM PROJECT. Difficulty with Random
Selection. 77 students were asked to choose 3 of the
integers 11, 12, 13, Á , 30 completely arbitrarily. The
amazing result was as follows.
Number 11 12 13 14 15 16 17 18 19 20
Frequ.

11 10 20

8 13

9 21

9 16

8

Number 21 22 23 24 25 26 27 28 29 30
Frequ.

12

8 15 10 10

9 12

8 13

9

If the selection were completely random, the following
hypotheses should be true.
(a) The 20 numbers are equally likely.
(b) The 10 even numbers together are as likely as the
10 odd numbers together.
(c) The 6 prime numbers together have probability 0.3
and the 14 other numbers together have probability 0.7.
Test these hypotheses, using a ϭ 5%. Design further
experiments that illustrate the difficulties of random
selection.
11. CAS EXPERIMENT. Random Number Generator.
Check your generator experimentally by imitating
results of n trials of rolling a fair die, with a convenient
n (e.g., 60 or 300 or the like). Do this many times and
see whether you can notice any “nonrandomness”
features, for example, too few Sixes, too many even
numbers, etc., or whether your generator seems to work
properly. Design and perform other kinds of checks.
12. Test for normality at the 1% level using a sample of
n ϭ 79 (rounded) values x (tensile strength [kg>mm2]

c25.qxd

11/3/10

6:21 PM

1100

Page 1100

CHAP. 25 Mathematical Statistics

of steel sheets of 0.3 mm thickness). a ϭ a(x) ϭ
absolute frequency. (Take the first two values together,
also the last three, to get K ϭ 5.)
x

57

58

59

60

61

62

63

64

a

4

10

17

27

8

9

3

1

the weeks, and more than 2 accidents in no week?
Choose a ϭ 5%.
15. Radioactivity. Rutherford-Geiger experiments.
Using the given sample, test that the corresponding
population has a Poisson distribution. x is the number
of alpha particles per 7.5-s intervals observed by
E. Rutherford and H. Geiger in one of their classical
experiments in 1910, and a(x) is the absolute frequency
(ϭ number of time periods during which exactly x
particles were observed). Use a ϭ 5%.

13. Mendel’s pathbreaking experiments. In a famous
plant-crossing experiment, the Austrian Augustinian
father Gregor Mendel (1822–1884) obtained 355
yellow and 123 green peas. Test whether this agrees
with Mendel’s theory according to which the ratio
should be 3:1.
14. Accidents in a foundry. Does the random variable
X ϭ Number of accidents per week have a Poisson
distribution if, within 50 weeks, 33 were accident-free,
1 accident occurred in 11 of the 50 weeks, 2 in 6 of

25.8

x

0

1

2

3

4

5

6

a

57

203

383

525

532

408

273

x

7

8

9

10

11

12

м13

a

139

45

27

10

4

2

0

Nonparametric Tests
Nonparametric tests, also called distribution-free tests, are valid for any distribution.
Hence they are used in cases when the kind of distribution is unknown, or is known but
such that no tests specifically designed for it are available. In this section we shall explain
the basic idea of these tests, which are based on “order statistics” and are rather simple.
If there is a choice, then tests designed for a specific distribution generally give better
results than do nonparametric tests. For instance, this applies to the tests in Sec. 25.4 for
the normal distribution.
We shall discuss two tests in terms of typical examples. In deriving the distributions
used in the test, it is essential that the distributions, from which we sample, are continuous.
(Nonparametric tests can also be derived for discrete distributions, but this is slightly more
complicated.)

EXAMPLE 1

Sign Test for the Median
ෂ of the equation F(x) ϭ 0.5, where F is the distribution function
A median of the population is a solution x ϭ ␮
of the population.
Suppose that eight radio operators were tested, first in rooms without air-conditioning and then in air-conditioned
rooms over the same period of time, and the difference of errors (unconditioned minus conditioned) were
9

4

0

6

4

0

7

11.

ෂ ϭ 0 (that is, air-conditioning has no effect) against the alternative ␮ Ͼ 0 (that is, inferior
Test the hypothesis ␮
performance in unconditioned rooms).

Solution. We choose the significance level a ϭ 5%. If the hypothesis is true, the probability p of a positive
difference is the same as that of a negative difference. Hence in this case, p ϭ 0.5, and the random variable
X ϭ Number of positive values among n values
has a binomial distribution with p ϭ 0.5. Our sample has eight values. We omit the values 0, which do not
contribute to the decision. Then six values are left, all of which are positive. Since
6
P(X ϭ 6) ϭ a b (0.5)6(0.5)0
6
ϭ 0.0156
ϭ 1.56%

c25.qxd

11/3/10

6:21 PM

Page 1101

SEC. 25.8 Nonparametric Tests

1101

we have observed an event whose probability is very small if the hypothesis is true; in fact 1.56% Ͻ a ϭ 5%.
ෂ Ͼ 0 is true. That is, the number of errors made in unconditioned rooms
Hence we assert that the alternative ␮
is significantly higher, so that installation of air conditioning should be considered.
᭿

EXAMPLE 2

Test for Arbitrary Trend
A certain machine is used for cutting lengths of wire. Five successive pieces had the lengths
29

31

28

30

32.

Using this sample, test the hypothesis that there is no trend, that is, the machine does not have the tendency to
produce longer and longer pieces or shorter and shorter pieces. Assume that the type of machine suggests the
alternative that there is positive trend, that is, there is the tendency of successive pieces to get longer.

Solution. We count the number of transpositions in the sample, that is, the number of times a larger value
precedes a smaller value:
29 precedes 28

(1 transposition),

31 precedes 28 and 30

(2 transpositions).

The remaining three sample values follow in ascending order. Hence in the sample there are 1 ϩ 2 ϭ 3
transpositions. We now consider the random variable
T ϭ Number of transpositions.
If the hypothesis is true (no trend), then each of the 5! ϭ 120 permutations of five elements 1 2 3 4 5 has the
same probability (1>120). We arrange these permutations according to their number of transpositions:

Tϭ0
1

2

3

4

Tϭ1
5

1
1
1
2

2
2
3
1

3
4
2
3

5
3
4
4

Tϭ2
4
5
5
5

1
1
1
1
1
2
2
2
3

2
2
3
3
4
1
1
3
1

4
5
2
4
2
3
4
1
2

5
3
5
2
3
5
3
4
4

Tϭ3
3
4
4
5
5
4
5
5
5

1
1
1
1
1
1
2
2
2
2
2
3
3
3
4

2
3
3
4
4
5
1
1
3
3
4
1
1
2
1

5
4
5
2
3
2
4
5
1
4
1
2
4
1
2

4
5
2
5
2
3
5
3
5
1
3
5
2
4
3

3
2
4
3
5
4
3
4 etc.
4
5
5
4
5
5
5

From this we obtain
1
4
9
15
29
P(T Ϲ 3) ϭ 120
ϩ 120
ϩ 120
ϩ 120
ϭ 120
ϭ 24%.

We accept the hypothesis because we have observed an event that has a relatively large probability (certainly
much more than 5%) if the hypothesis is true.
Values of the distribution function of T in the case of no trend are shown in Table A12, App. 5. For instance,
if n ϭ 3, then F(0) ϭ 0.167, F(1) ϭ 0.500, F(2) ϭ 1 Ϫ 0.167. If n ϭ 4, then F(0) ϭ 0.042, F(1) ϭ 0.167,
F(2) ϭ 0.375, F(3) ϭ 1 Ϫ 0.375, F(4) ϭ 1 Ϫ 0.167, and so on.

c25.qxd

11/3/10

6:21 PM

1102

Page 1102

CHAP. 25 Mathematical Statistics
Our method and those values refer to continuous distributions. Theoretically, we may then expect that all the
values of a sample are different. Practically, some sample values may still be equal, because of rounding: If m
values are equal, add m(m Ϫ 1)>4 (ϭ mean value of the transpositions in the case of the permutations of m
elements), that is, 12 for each pair of equal values, 32 for each triple, etc.
᭿

PROBLEM SET 25.8
1. What would change in Example 1 had we observed
only 5 positive values? Only 4?
ෂ ϭ 0 against ␮
ෂ Ͼ 0, using 1, Ϫ1, 1, 3, Ϫ8, 6, 0
2. Test ␮

9. Assuming that the populations corresponding to the
samples in Prob. 8 are normal, apply a suitable test for
the normal distribution.

(deviations of the azimuth [multiples of 0.01 radian] in
some revolution of a satellite).

10. Test whether a thermostatic switch is properly set to
50°C against the alternative that its setting is too low.
Use a sample of 9 values, 8 of which are less than 50°C
and 1 is greater.

3. Are oil filters of type A better than type B filters if in
11 trials, A gave cleaner oil than B in 7 cases, B gave
cleaner oil than A in 1 case, whereas in 3 of the trials
the results for A and B were practically the same?
4. Does a process of producing stainless steel pipes of
length 20 ft for nuclear reactors need adjustment if, in a
sample, 4 pipes have the exact length and 15 are shorter
and 3 longer than 20 ft? Use the normal approximation
of the binomial distribution.
5. Do the computations in Prob. 4 without the use of the
DeMoivre–Laplace limit theorem in Sec. 24.8.
6. Thirty new employees were grouped into 15 pairs of
similar intelligence and experience and were then
instructed in data processing by an old method (A)
applied to one (randomly selected) person of each pair,
and by a new presumably better method (B) applied to
the other person of each pair. Test for equality of
methods against the alternative that (B) is better than
(A), using the following scores obtained after the end
of the training period.
A 60 70 80 85 75 40 70 45 95 80 90 60 80 75 65

11. How would you proceed in the sign test if the
ෂϭ␮
ෂ (any number) instead of ␮
ෂ ϭ 0?
hypothesis is ␮
0
12. Test the hypothesis that, for a certain type of voltmeter,
readings are independent of temperature T [°C] against
the alternative that they tend to increase with T. Use
a sample of values obtained by applying a constant
voltage:

Temperature T [°C] 10
Reading V [volts]

20

30

40

50

99.5 101.1 100.4 100.8 101.6

13. Does the amount of fertilizer increase the yield of
wheat X [kg>plot]? Use a sample of values ordered
according to increasing amounts of fertilizer:
33.4 35.3 31.6 35.0 36.1 37.6 36.5 38.7.
14. Apply the test explained in Example 2 to the following
data (x ϭ diastolic blood pressure [mm Hg], y ϭ
weight of heart [in grams] of 10 patients who died of
cerebral hemorrhage).

B 65 85 85 80 95 65 100 60 90 85 100 75 90 60 80

7. Assuming normality, solve Prob. 6 by a suitable test
from Sec. 25.4.
8. In a clinical experiment, each of 10 patients were given
two different sedatives A and B. The following table
shows the effect (increase of sleeping time, measured
in hours). Using the sign test, find out whether the
difference is significant.
A
B

1.9 0.8 1.1 0.1 Ϫ0.1 4.4 5.5 1.6 4.6 3.4
0.7 Ϫ1.6 Ϫ0.2 Ϫ1.2 Ϫ0.1 3.4 3.7 0.8 0.0 2.0

Difference 1.2

2.4

1.3

1.3

0.0 1.0 1.8 0.8 4.6 1.4

x 121 120 95 123 140 112 92 100 102 91
y 521 465 352 455 490 388 301 395 375 418
15. Does an increase in temperature cause an increase of
the yield of a chemical reaction from which the
following sample was taken?

Temperature [°C]

10

20

30

40

60

80

Yield [kg/min]

0.6

1.1

0.9

1.6

1.2

2.0

c25.qxd

11/3/10

6:21 PM

Page 1103

SEC. 25.9 Regression. Fitting Straight Lines. Correlation

25.9

1103

Regression. Fitting Straight Lines.
Correlation
So far we were concerned with random experiments in which we observed a single quantity
(random variable) and got samples whose values were single numbers. In this section we
discuss experiments in which we observe or measure two quantities simultaneously, so
that we get samples of pairs of values (x 1, y1), (x 2, y2), Á , (x n, yn). Most applications
involve one of two kinds of experiments, as follows.
1. In regression analysis one of the two variables, call it x, can be regarded as an
ordinary variable because we can measure it without substantial error or we can
even give it values we want. x is called the independent variable, or sometimes
the controlled variable because we can control it (set it at values we choose). The
other variable, Y, is a random variable, and we are interested in the dependence of
Y on x. Typical examples are the dependence of the blood pressure Y on the age x
of a person or, as we shall now say, the regression of Y on x, the regression of the
gain of weight Y of certain animals on the daily ration of food x, the regression of
the heat conductivity Y of cork on the specific weight x of the cork, etc.
2. In correlation analysis both quantities are random variables and we are interested
in relations between them. Examples are the relation (one says “correlation”)
between wear X and wear Y of the front tires of cars, between grades X and Y of
students in mathematics and in physics, respectively, between the hardness X of
steel plates in the center and the hardness Y near the edges of the plates, etc.

Regression Analysis
In regression analysis the dependence of Y on x is a dependence of the mean ␮ of Y on
x, so that ␮ ϭ ␮(x) is a function in the ordinary sense. The curve of ␮(x) is called the
regression curve of Y on x.
In this section we discuss the simplest case, namely, that of a straight regression line
␮(x) ϭ ␬0 ϩ ␬1x.

(1)

Then we may want to graph the sample values as n points in the xY-plane, fit a straight
line through them, and use it for estimating ␮(x) at values of x that interest us, so that we
know what values of Y we can expect for those x. Fitting that line by eye would not be
good because it would be subjective; that is, different persons’ results would come out
differently, particularly if the points are scattered. So we need a mathematical method that
gives a unique result depending only on the n points. A widely used procedure is the method
of least squares by Gauss and Legendre. For our task we may formulate it as follows.
Least Squares Principle

The straight line should be fitted through the given points so that the sum of the
squares of the distances of those points from the straight line is minimum, where
the distance is measured in the vertical direction (the y-direction). (Formulas below.)

c25.qxd

11/3/10

1104

6:21 PM

Page 1104

CHAP. 25 Mathematical Statistics

To get uniqueness of the straight line, we need some extra condition. To see this, take
the sample (0, 1), (0, Ϫ1). Then all the lines y ϭ k 1x with any k 1 satisfy the principle.
(Can you see it?) The following assumption will imply uniqueness, as we shall find out.
General Assumption (A1)

The x-values x 1, Á , x n in our sample (x 1, y1), Á , (x n, yn) are not all equal.

From a given sample (x 1, y1), Á , (x n, yn) we shall now determine a straight line by
least squares. We write the line as
y ϭ k 0 ϩ k 1x

(2)

and call it the sample regression line because it will be the counterpart of the population
regression line (1).
Now a sample point (x j, yj) has the vertical distance (distance measured in the
y-direction) from (2) given by
ƒ yj Ϫ (k 0 ϩ k1x j) ƒ

y

(see Fig. 543).

y = yj

y = k0 + k1xj

x

xj

Fig. 543. Vertical distance of a point (xj , yj) from a straight line y ϭ k0 ϩ k1x

Hence the sum of the squares of these distances is
n

(3)

q ϭ a (yj Ϫ k 0 Ϫ k 1x j)2.
jϭ1

In the method of least squares we now have to determine k 0 and k 1 such that q is minimum.
From calculus we know that a necessary condition for this is
0q
(4)
0k 0

ϭ0

0q
and
0k 1

ϭ 0.

We shall see that from this condition we obtain for the sample regression line the formula
(5)

y Ϫ y ϭ k 1(x Ϫ x).

c25.qxd

11/3/10

6:21 PM

Page 1105

SEC. 25.9 Regression. Fitting Straight Lines. Correlation

1105

Here x and y are the means of the x- and the y-values in our sample, that is,
(a)

1
x ϭ n (x 1 ϩ Á ϩ x n)

(6)
1
(b) y ϭ n ( y1 ϩ Á ϩ yn).
The slope k 1 in (5) is called the regression coefficient of the sample and is given by
k1 ϭ

(7)

sxy
s 2x

.

Here the “sample covariance” sxy is
(8) sxy ϭ

n
1
1
a (x j Ϫ x)( yj Ϫ y) ϭ n Ϫ 1
n Ϫ 1 jϭ1

n
1 n
x
y
Ϫ
x
¢
c a j j n a i≤ ¢ a yj≤ d
n

jϭ1

iϭ1

jϭ1

and s 2x is given by
(9a)

s 2x ϭ

n
1
1
2
a (x j Ϫ x) ϭ n Ϫ 1
n Ϫ 1 jϭ1

n
1
2
x
Ϫ
c a j n ¢ a x j≤

2

n

jϭ1

jϭ1

d.

From (5) we see that the sample regression line passes through the point (x, y), by which
it is determined, together with the regression coefficient (7). We may call s 2x the variance
of the x-values, but we should keep in mind that x is an ordinary variable, not a random
variable.
We shall soon also need
n
n
1
1
1 n
2
2
(
y
Ϫ
y)
ϭ
y
Ϫ
c a j n ¢ a yj≤ d .
a j
n Ϫ 1 jϭ1
n Ϫ 1 jϭ1
jϭ1
2

(9b)

s 2y ϭ

Derivation of (5) and (7).
0q
0k 0
0q
0k 1

Differentiating (3) and using (4), we first obtain
ϭ Ϫ2 a ( yj Ϫ k 0 Ϫ k 1x j) ϭ 0,
ϭ Ϫ2 a x j( yj Ϫ k 0 Ϫ k 1x j) ϭ 0

where we sum over j from 1 to n. We now divide by 2, write each of the two sums as
three sums, and take the sums containing yj and x jyj over to the right. Then we get the
“normal equations”
k 0 n ϩ k 1 a x j ϭ a yj
(10)
k 0 a x j ϩ k 1 a x 2j ϭ a x jyj.

c25.qxd

11/3/10

6:21 PM

1106

Page 1106

CHAP. 25 Mathematical Statistics

This is a linear system of two equations in the two unknowns k 0 and k 1. Its coefficient
determinant is [see (9)]

†

n

a xj

a xj

2
a xj

† ϭ

2

Ϫ a a x j b ϭ n(n Ϫ 1)s 2x ϭ n a (x j Ϫ x)2

n a x 2j

and is not zero because of Assumption (A1). Hence the system has a unique solution.
Dividing the first equation of (10) by n and using (6), we get k 0 ϭ y Ϫ k 1x. Together
with y ϭ k 0 ϩ k 1x in (2) this gives (5). To get (7), we solve the system (10) by Cramer’s
rule (Sec. 7.6) or elimination, finding
k1 ϭ

(11)

n a x jyj Ϫ a x i a yj
n(n Ϫ 1)s 2x

.

This gives (7)–(9) and completes the derivation. [The equality of the two expressions in
(8) and in (9) may be shown by the student].
᭿
EXAMPLE 1

Regression Line
The decrease of volume y [%] of leather for certain fixed values of high pressure x [atmospheres] was measured.
The results are shown in the first two columns of Table 25.11. Find the regression line of y on x.

Solution. We see that n ϭ 4 and obtain the values x ϭ 28000>4 ϭ 7000, y ϭ 19.0>4 ϭ 4.75, and from (9)
and (8)

Table 25.11 Regression of the Decrease of Volume y [%]
of Leather on the Pressure x [Atmospheres]
Given Values

Auxiliary Values
x 2j

xj

yj

4000
6000
8000
10,000

2.3
4.1
5.7
6.9

16,000,000
36,000,000
64,000,000
100,000,000

9200
24,600
45,600
69,000

28,000

19.0

216,000,000

148,400

s 2x ϭ
sxy ϭ

1
3
1
3

a216,000,000 Ϫ
a148,400 Ϫ

28,0002
4

28,000 ⅐ 19
4

bϭ

bϭ

x jyj

20,000,000
3
15,400
3

.

Hence k 1 ϭ 15,400>20,000,000 ϭ 0.00077 from (7), and the regression line is
y Ϫ 4.75 ϭ 0.00077(x Ϫ 7000)

or

y ϭ 0.00077x Ϫ 0.64.

Note that y(0) ϭ Ϫ0.64, which is physically meaningless, but typically indicates that a linear relation is merely
an approximation valid on some restricted interval.
᭿

c25.qxd

11/3/10

6:21 PM

Page 1107

SEC. 25.9 Regression. Fitting Straight Lines. Correlation

1107

Confidence Intervals in Regression Analysis
If we want to get confidence intervals, we have to make assumptions about the distribution
of Y (which we have not made so far; least squares is a “geometric principle,” nowhere
involving probabilities!). We assume normality and independence in sampling:
Assumption (A2)

For each fixed x the random variable Y is normal with mean (1), that is,
␮(x) ϭ ␬0 ϩ ␬1x

(12)

and variance s2 independent of x.
Assumption (A3)

The n performances of the experiment by which we obtain a sample
(x 1, y1),

(x 2, y2),

Á,

(x n, yn)

are independent.
␬1 in (12) is called the regression coefficient of the population because it can be shown
that, under Assumptions (A1)–(A3), the maximum likelihood estimate of ␬1 is the sample
regression coefficient k 1 given by (11).
Under Assumptions (A1)–(A3), we may now obtain a confidence interval for ␬1, as
shown in Table 25.12.
Table 25.12 Determination of a Confidence Interval for ␬1 in (1) under Assumptions (A1)–(A3)
Step 1. Choose a confidence level ␥ (95%, 99%, or the like).
Step 2. Determine the solution c of the equation

F(c) ϭ 12 (1 ϩ g)

(13)

from the table of the t-distribution with n Ϫ 2 degrees of freedom (Table A9 in
App. 5; n ϭ sample size).
Step 3. Using a sample (x 1, y1), Á , (x n, yn), compute (n Ϫ 1)s 2x from (9a), (n Ϫ 1)sxy
from (8), k 1 from (7),
2

n
1 n
(n Ϫ 1)s 2y ϭ a y 2j Ϫ n ¢ a yj≤

(14)

jϭ1

jϭ1

[as in (9b)], and

(15)
Step 4. Compute

q0 ϭ (n Ϫ 1)(s 2y Ϫ k 21s 2x).
Kϭc

q0
2
B (n Ϫ 2)(n Ϫ 1)s x

.

The confidence interval is

(16)

CONFg {k 1 Ϫ K Ϲ ␬1 Ϲ k 1 ϩ K}.

c25.qxd

11/3/10

6:21 PM

1108
EXAMPLE 2

Page 1108

CHAP. 25 Mathematical Statistics
Confidence Interval for the Regression Coefficient
Using the sample in Table 25.11, determine a confidence interval for ␬1 by the method in Table 25.12.

Solution. Step 1. We choose g ϭ 0.95.
Step 2. Equation (13) takes the form F(c) ϭ 0.975, and Table A9 in App. 5 with n Ϫ 2 ϭ 2 degrees of freedom
gives c ϭ 4.30.
Step 3.

From Example 1 we have 3s 2x ϭ 20,000,000 and k 1 ϭ 0.00077. From Table 25.11 we compute
3s 2y ϭ 102.0 Ϫ

192
4

ϭ 11.95.
q0 ϭ 11.95 Ϫ 20,000,000 ⅐ 0.000772
ϭ 0.092.
Step 4.

We thus obtain
K ϭ 4.30 10.092>(2 ⅐ 20,000,000)
ϭ 0.000206

and
CONF 0.95 {0.00056 Ϲ ␬1 Ϲ 0.00098}.

᭿

Correlation Analysis
We shall now give an introduction to the basic facts in correlation analysis; for proofs see
Ref. [G2] or [G8] in App. 1.
Correlation analysis is concerned with the relation between X and Y in a twodimensional random variable (X, Y ) (Sec. 24.9). A sample consists of n ordered pairs of
values (x 1, y1), Á , (x n, yn), as before. The interrelation between the x and y values in the
sample is measured by the sample covariance sxy in (8) or by the sample correlation
coefficient
(17)

sxy
rϭs s
x y

with sx and sy given in (9). Here r has the advantage that it does not change under a
multiplication of the x and y values by a factor (in going from feet to inches, etc.).
THEOREM 1

Sample Correlation Coefficient

The sample correlation coefficient r satisfies Ϫ1 Ϲ r Ϲ 1. In particular, r ϭ Ϯ 1
if and only if the sample values lie on a straight line. (See Fig. 544.)

The theoretical counterpart of r is the correlation coefficient r of X and Y,

(18)

sXY
rϭs s
X Y

c25.qxd

11/3/10

6:21 PM

Page 1109

SEC. 25.9 Regression. Fitting Straight Lines. Correlation

1109

r=1

r=0

10

0

10

0

10

0

20

0

r = 0.98

r = –0.3

0

10

0

20

0

r = 0.6

0

10

20

r = –0.9
10

10

0

20

10

10

0

10

10

0

20

0

10

20

Fig. 544. Samples with various values of the correlation coefficient r
2
2
where ␮X ϭ E(X ), ␮Y ϭ E(Y ), sX
ϭ E([X Ϫ ␮X]2), sY
ϭ E([Y Ϫ ␮Y]2) (the means
and variances of the marginal distributions of X and Y; see Sec. 24.9), and sXY is the
covariance of X and Y given by (see Sec. 24.9)

(19)

sXY ϭ E([X Ϫ ␮X][Y Ϫ ␮Y]) ϭ E(XY ) Ϫ E(X)E(Y ).

The analog of Theorem 1 is
THEOREM 2

Correlation Coefficient

The correlation coefficient r satisfies Ϫ1 Ϲ r Ϲ 1. In particular, r ϭ Ϯ1 if and
only if X and Y are linearly related, that is, Y ϭ gX ϩ d, X ϭ g*Y ϩ d*.
X and Y are called uncorrelated if r ϭ 0.
THEOREM 3

Independence. Normal Distribution

(a) Independent X and Y (see Sec. 24.9) are uncorrelated.
(b) If (X, Y) is normal (see below), then uncorrelated X and Y are
independent.
Here the two-dimensional normal distribution can be introduced by taking two independent
standardized normal random variables X*, Y*, whose joint distribution thus has the density
(20)

f *(x*, y*) ϭ

1 ؊(x*2 ϩy*2)>2
e
2p

c25.qxd

11/3/10

6:21 PM

1110

Page 1110

CHAP. 25 Mathematical Statistics

(representing a surface of revolution over the x*y*-plane with a bell-shaped curve as cross
section) and setting
X ϭ ␮X ϩ sXX*
Y ϭ ␮Y ϩ rsYX* ϩ 21 Ϫ r2sYY*.
This gives the general two-dimensional normal distribution with the density
f (x, y) ϭ

(21a)

1
2psXsY 21 Ϫ r

2

e؊h(x,y)>2

where
x Ϫ ␮X
x Ϫ ␮X y Ϫ ␮Y
y Ϫ ␮Y
(21b) h(x, y) ϭ
a
b
Ϫ
2r
a
b
a
b
ϩ
a
c
sX
sX
sY
sY b d .
1 Ϫ r2
2

1

2

In Theorem 3(b), normality is important, as we can see from the following example.
EXAMPLE 3

Uncorrelated But Dependent Random Variables
If X assumes Ϫ1, 0, 1 with probability

1
3

and Y ϭ X 2, then E(X) ϭ 0 and in (3)

sXY ϭ E(XY) ϭ E(X 3) ϭ (Ϫ1)3 ⅐ 13 ϩ 03 ⅐

1
3

ϩ 13 ⅐ 13 ϭ 0,

so that r ϭ 0 and X and Y are uncorrelated. But they are certainly not independent since they are even functionally
related.
᭿

Test for the Correlation Coefficient r
Table 25.13 shows a test for r in the case of the two-dimensional normal distribution. t is
an observed value of a random variable that has a t-distribution with n Ϫ 2 degrees of
freedom. This was shown by R. A. Fisher (Biometrika 10 (1915), 507–521).
Table 25.13 Test of the Hypothesis ␳ ‫ ؍‬0 Against the Alternative ␳ Ͼ 0 in the Case
of the Two-Dimensional Normal Distribution
Step 1. Choose a significance level ␣ (5%, 1%, or the like).
Step 2. Determine the solution c of the equation

P(T Ϲ c) ϭ 1 Ϫ a
from the t-distribution (Table A9 in App. 5) with n Ϫ 2 degrees of freedom.
Step 3. Compute r from (17), using a sample (x 1, y1), Á , (x n, yn).
Step 4. Compute

tϭra

nϪ2
2
B1 Ϫ r

b.

If t Ϲ c, accept the hypothesis. If t Ͼ c, reject the hypothesis.

c25.qxd

11/3/10

6:21 PM

Page 1111

Chapter 25 Review Questions and Problems

1111

Test for the Correlation Coefficient ␳

EXAMPLE 4

Test the hypothesis r ϭ 0 (independence of X and Y, because of Theorem 3) against the alternative r Ͼ 0, using
the data in the lower left corner of Fig. 544, where r ϭ 0.6 (manual soldering errors on 10 two-sided circuit
boards done by 10 workers; x ϭ front, y ϭ back of the boards).

Solution. We choose a ϭ 5%; thus 1 Ϫ a ϭ 95%. Since n ϭ 10, n Ϫ 2 ϭ 8, the table gives c ϭ 1.86. Also,

t ϭ 0.6 18>0.64 ϭ 2.12 Ͼ c. We reject the hypothesis and assert that there is a positive correlation. A worker making
few (many) errors on the front side also tends to make few (many) errors on the reverse side of the board.
᭿

PROBLEM SET 25.9
1–10
SAMPLE REGRESSION LINE
Find and graph the sample regression line of y on x and the
given data as points on the same axes. Show the details of
your work.
1. (0, 1.0), (2, 2.1), (4, 2.9), (6, 3.6), (8, 5.2)
2. (Ϫ2, 3.5), (1, 2.6), (3, 1.3), (5, 0.4)
3. x ϭ Revolutions per minute, y ϭ Power of a Diesel
engine [hp]
x

400

500

600

700

750

y

5800

10,300

14,200

18,800

21,000

4. x ϭ Deformation of a certain steel [mm], y ϭ Brinell
hardness [kg>mm2]
x

6

9

11

13

22

26

28

33

35

y

68

67

65

53

44

40

37

34

32

5. x ϭ Brinell hardness, y ϭ Tensile strength [in 1000 psi
(pounds per square inch)] of steel with 0.45% C
tempered for 1 hour

x

200

300

400

8. Hooke’s law (Sec. 2.4). x ϭ Force [lb], y ϭ Extension
[in] of a spring. Also find the spring modulus.

x

2

4

6

8

y
4.1
7.8
12.3
15.8
9. Thermal conductivity of water. x ϭ Temperature
[°F], y ϭ Conductivity [Btu>(hr ⅐ ft ⅐ °F)]. Also find y
at room temperature 66°F.

x

32

50

100

150

212

y

0.337

0.345

0.365

0.380

0.395

10. Stopping distance of a car. x ϭ Speed [mph]. y ϭ
Stopping distance [ft]. Also find y at 35 mph.

x

30

40

50

60

y
160
240
330
435
11. CAS EXPERIMENT. Moving Data. Take a sample,
for instance, that in Prob. 4, and investigate and graph
the effect of changing y-values (a) for small x, (b) for
large x, (c) in the middle of the sample.

500

y
110
150
190
280
6. Abrasion of quenched and tempered steel S620.
x ϭ Sliding distance [km], y ϭ Wear volume [mm3]

x

1.1

3.2

3.4

4.5

5.6

y

40

65

120

150

190

7. Ohm’s law (Sec. 2.9). x ϭ Voltage [V], y ϭ Current
[A]. Also find the resistance R [⍀].

12–15

CONFIDENCE INTERVALS

Find a 95% confidence interval for the regression
coefficient ␬1, assuming (A2) and (A3) hold and using the
sample.
12. In Prob. 2
13. In Prob. 3
14. In Prob. 4
15. x ϭ Humidity of air [%], y ϭ Expansion of gelatin [%],

x

40

40

80

80

110

110

x

10

20

30

40

y

5.1

4.8

0.0

10.3

13.0

12.7

y

0.8

1.6

2.3

2.8

CHAPTER 25 REVIEW QUESTIONS AND PROBLEMS
1. What is a sample? A population? Why do we sample
in statistics?
2. If we have several samples from the same population,
do they have the same sample distribution function?
The same mean and variance?

3. Can we develop statistical methods without using
probability theory? Apply the methods without using a
sample?
4. What is the idea of the maximum likelihood method?
Why do we say “likelihood” rather than “probability”?

c25.qxd

11/3/10

1112

6:21 PM

Page 1112

CHAP. 25 Mathematical Statistics

5. Couldn’t we make the error of interval estimation zero
simply by choosing the confidence level 1?
6. What is testing? Why do we test? What are the errors
involved?
7. When did we use the t-distribution? The F-distribution?
8. What is the chi-square (␹2) test? Give a sample
example from memory.
9. What are one-sided and two-sided tests? Give typical
examples.
10. How do we test in quality control? In acceptance
sampling?
11. What is the power of a test? What could you perhaps
do when it is low?
12. What is Gauss’s least squares principle (which he found
at age 18)?
13. What is the difference between regression and
correlation?
14. Find the mean, variance, and standard derivation of the
sample 21.0 21.6 19.9 19.6 15.6 20.6 22.1 22.2.
15. Assuming normality, find the maximum likelihood
estimates of mean and variance from the sample in
Prob. 14.
16. Determine a 95% confidence interval for the mean ␮
of a normal population with variance s2 ϭ 25, using
a sample of size 500 with mean 22.
17. Determine a 99% confidence interval for the mean of
a normal population, using the sample 32, 33, 32, 34,
35, 29, 29, 27.

SUMMARY OF CHAPTER

18. Assuming normality, find a 95% confidence interval for
the variance from the sample 145.3, 145.1, 145.4, 146.2.
19. Using a sample of 10 values with mean 14.5 from
a normal population with variance s2 ϭ 0.25, test
the hypothesis ␮0 ϭ 15.0 against the alternative
␮1 ϭ 14.5 on the 5% level. Find the power.
20. Three specimens of high-quality concrete had
compressive strength 357, 359, 413 [kg>cm2], and for
three specimens of ordinary concrete the values were
346, 358, 302. Test for equality of the population means,
␮1 ϭ ␮2, against the alternative ␮1 Ͼ ␮2. Assume
normality and equality of variance. Choose a ϭ 5%.
21. Assume the thickness X of washers to be normal with
mean 2.75 mm and variance 0.00024 mm2. Set up
a control chart for ␮ and graph the means of the five
samples (2.74, 2.76), (2.74, 2.74), (2.79, 2.81), (2.78,
2.76), (2.71, 2.75) on the chart.
22. The OC curve in acceptance sampling cannot have a
strictly vertical portion. Why?
23. Find the risks in the sampling plan with n ϭ 6 and
c ϭ 0, assuming that the AQL is u0 ϭ 1% and the
RQL is u1 ϭ 15%. How do the risks change if we
increase n?
24. Does a process of producing plastic rods of length
ෂ ϭ 2 meters need adjustment if in a sample, 2 rods
␮
have the exact length and 15 are shorter and 3 longer
than 2 meters? (Use the sign test.)
25. Find the regression line of y on x for the data
(x, y) ϭ (0, 4), (2, 0), (4, Ϫ5), (6, Ϫ9), (8, Ϫ10).

25

Mathematical Statistics
We recall from Chap. 24 that, with an experiment in which we observe some quantity
(number of defectives, height of persons, etc.), there is associated a random variable
X whose probability distribution is given by a distribution function
(1)

F(x) ϭ P(X Ϲ x)

(Sec. 24.5)

which for each x gives the probability that X assumes any value not exceeding x.
In statistics we take random samples x 1, Á , x n of size n by performing that
experiment n times (Sec. 25.1) and draw conclusions from properties of samples
about properties of the distribution of the corresponding X. We do this by calculating
point estimates or confidence intervals or by performing a test for parameters (␮
and s2 in the normal distribution, p in the binomial distribution, etc.) or by a test
for distribution functions.

c25.qxd

11/3/10

6:21 PM

Page 1113

Summary of Chapter 25

1113

A point estimate (Sec. 25.2) is an approximate value for a parameter in the
distribution of X obtained from a sample. Notably, the sample mean (Sec. 25.1)
n

1
1
x ϭ n a x j ϭ n (x 1 ϩ Á ϩ x n)

(2)

jϭ1

is an estimate of the mean ␮ of X, and the sample variance (Sec. 25.1)
n

(3)

s2 ϭ

1
1
(x j Ϫ x)2 ϭ
[(x 1 Ϫ x)2 ϩ Á ϩ (x n Ϫ x)2]
a
n Ϫ 1 jϭ1
nϪ1

is an estimate of the variance s2 of X. Point estimation can be done by the basic
maximum likelihood method (Sec. 25.2).
Confidence intervals (Sec. 25.3) are intervals u1 Ϲ u Ϲ u2 with endpoints
calculated from a sample such that, with a high probability g, we obtain an interval
that contains the unknown true value of the parameter u in the distribution of X.
Here, g is chosen at the beginning, usually 95% or 99%. We denote such an interval
by CONFg {u1 Ϲ u Ϲ u2}.
In a test for a parameter we test a hypothesis u ϭ u0 against an alternative u ϭ u1
and then, on the basis of a sample, accept the hypothesis, or we reject it in favor of
the alternative (Sec. 25.4). Like any conclusion about X from samples, this may
involve errors leading to a false decision. There is a small probability a (which we
can choose, 5% or 1%, for instance) that we reject a true hypothesis, and there is a
probability b (which we can compute and decrease by taking larger samples) that
we accept a false hypothesis. a is called the significance level and 1 Ϫ b the power
of the test. Among many other engineering applications, testing is used in quality
control (Sec. 25.5) and acceptance sampling (Sec. 25.6).
If not merely a parameter but the kind of distribution of X is unknown, we can
use the chi-square test (Sec. 25.7) for testing the hypothesis that some function
F(x) is the unknown distribution function of X. This is done by determining the
ෂ
discrepancy between F(x) and the distribution function F (x) of a given sample.
“Distribution-free” or nonparametric tests are tests that apply to any distribution,
since they are based on combinatorial ideas. These tests are usually very simple.
Two of them are discussed in Sec. 25.8.
The last section deals with samples of pairs of values, which arise in an experiment
when we simultaneously observe two quantities. In regression analysis, one of the
quantities, x, is an ordinary variable and the other, Y, is a random variable whose
mean ␮ depends on x, say, ␮(x) ϭ ␬0 ϩ ␬1x. In correlation analysis the relation
between X and Y in a two-dimensional random variable (X, Y ) is investigated,
notably in terms of the correlation coefficient r.

ffirs.qxd

11/4/10

10:50 AM

Page iv

bapp01.qxd

11/4/10

5:55 PM

Page A1

APPENDIX

1

References
Software see at the beginning of Chaps. 19
and 24.
General References
[GenRef1] Abramowitz, M. and I. A. Stegun (eds.),
Handbook of Mathematical Functions. 10th printing,
with corrections. Washington, DC: National Bureau
of Standards. 1972 (also New York: Dover, 1965). See
also [W1]
[GenRef2] Cajori, F., History of Mathematics. 5th ed.
Reprinted. Providence, RI: American Mathematical
Society, 2002.
[GenRef3] Courant, R. and D. Hilbert, Methods of
Mathematical Physics. 2 vols. Hoboken, NJ: Wiley,
1989.
[GenRef4] Courant, R., Differential and Integral
Calculus. 2 vols. Hoboken, NJ: Wiley, 1988.
[GenRef5] Graham, R. L. et al., Concrete Mathematics.
2nd ed. Reading, MA: Addison-Wesley, 1994.
[GenRef6] Ito, K. (ed.), Encyclopedic Dictionary of
Mathematics. 4 vols. 2nd ed. Cambridge, MA: MIT
Press, 1993.
[GenRef7] Kreyszig, E., Introductory Functional
Analysis with Applications. New York: Wiley, 1989.
[GenRef8] Kreyszig, E., Differential Geometry. Mineola,
NY: Dover, 1991.
[GenRef9] Kreyszig, E. Introduction to Differential
Geometry and Riemannian Geometry. Toronto:
University of Toronto Press, 1975.
[GenRef10] Szegö, G., Orthogonal Polynomials. 4th ed.
Reprinted. New York: American Mathematical Society,
2003.
[GenRef11] Thomas, G. et al., Thomas’ Calculus, Early
Transcendentals Update. 10th ed. Reading, MA:
Addison-Wesley, 2003.

Part A. Ordinary Differential Equations
(ODEs) (Chaps. 1–6)
See also Part E: Numeric Analysis
[A1] Arnold, V. I., Ordinary Differential Equations. 3rd
ed. New York: Springer, 2006.
[A2] Bhatia, N. P. and G. P. Szego, Stability Theory of
Dynamical Systems. New York: Springer, 2002.
[A3] Birkhoff, G. and G.-C. Rota, Ordinary Differential
Equations. 4th ed. New York: Wiley, 1989.

[A4] Brauer, F. and J. A. Nohel, Qualitative Theory of
Ordinary Differential Equations. Mineola, NY: Dover,
1994.
[A5] Churchill, R. V., Operational Mathematics. 3rd ed.
New York: McGraw-Hill, 1972.
[A6] Coddington, E. A. and R. Carlson, Linear Ordinary
Differential Equations. Philadelphia: SIAM, 1997.
[A7] Coddington, E. A. and N. Levinson, Theory of
Ordinary Differential Equations. Malabar, FL: Krieger,
1984.
[A8] Dong, T.-R. et al., Qualitative Theory of Differential
Equations. Providence, RI: American Mathematical
Society, 1992.
[A9] Erdélyi, A. et al., Tables of Integral Transforms.
2 vols. New York: McGraw-Hill, 1954.
[A10] Hartman, P., Ordinary Differential Equations. 2nd
ed. Philadelphia: SIAM, 2002.
[A11] Ince, E. L., Ordinary Differential Equations. New
York: Dover, 1956.
[A12] Schiff, J. L., The Laplace Transform: Theory and
Applications. New York: Springer, 1999.
[A13] Watson, G. N., A Treatise on the Theory of Bessel
Functions. 2nd ed. Reprinted. New York: Cambridge
University Press, 1995.
[A14] Widder, D. V., The Laplace Transform. Princeton,
NJ: Princeton University Press, 1941.
[A15] Zwillinger, D., Handbook of Differential Equations.
3rd ed. New York: Academic Press, 1998.

Part B. Linear Algebra, Vector Calculus
(Chaps. 7–10)
For books on numeric linear algebra, see also
Part E: Numeric Analysis.
[B1] Bellman, R., Introduction to Matrix Analysis. 2nd
ed. Philadelphia: SIAM, 1997.
[B2] Chatelin, F., Eigenvalues of Matrices. New York:
Wiley-Interscience, 1993.
[B3] Gantmacher, F. R., The Theory of Matrices. 2 vols.
Providence, RI: American Mathematical Society, 2000.
[B4] Gohberg, I. P. et al., Invariant Subspaces of Matrices
with Applications. New York: Wiley, 2006.
[B5] Greub, W. H., Linear Algebra. 4th ed. New York:
Springer, 1975.
[B6] Herstein, I. N., Abstract Algebra. 3rd ed. New York:
Wiley, 1996.

A1

bapp01.qxd

11/4/10

A2

5:55 PM

Page A2

APP. 1 References

[B7] Joshi, A. W., Matrices and Tensors in Physics. 3rd
ed. New York: Wiley, 1995.
[B8] Lang, S., Linear Algebra. 3rd ed. New York:
Springer, 1996.
[B9] Nef, W., Linear Algebra. 2nd ed. New York: Dover,
1988.
[B10] Parlett, B., The Symmetric Eigenvalue Problem.
Philadelphia: SIAM, 1998.

Part C. Fourier Analysis and PDEs
(Chaps. 11–12)
For books on numerics for PDEs see also Part
E: Numeric Analysis.
[C1] Antimirov, M. Ya., Applied Integral Transforms.
Providence, RI: American Mathematical Society, 1993.
[C2] Bracewell, R., The Fourier Transform and Its
Applications. 3rd ed. New York: McGraw-Hill, 2000.
[C3] Carslaw, H. S. and J. C. Jaeger, Conduction of Heat
in Solids. 2nd ed. Reprinted. Oxford: Clarendon, 2000.
[C4] Churchill, R. V. and J. W. Brown, Fourier Series
and Boundary Value Problems. 6th ed. New York:
McGraw-Hill, 2006.
[C5] DuChateau, P. and D. Zachmann, Applied Partial
Differential Equations. Mineola, NY: Dover, 2002.
[C6] Hanna, J. R. and J. H. Rowland, Fourier Series,
Transforms, and Boundary Value Problems. 2nd ed.
New York: Wiley, 2008.
[C7] Jerri, A. J., The Gibbs Phenomenon in Fourier
Analysis, Splines, and Wavelet Approximations. Boston:
Kluwer, 1998.
[C8] John, F., Partial Differential Equations. 4th edition
New York: Springer, 1982.
[C9] Tolstov, G. P., Fourier Series. New York: Dover, 1976.
[C10] Widder, D. V., The Heat Equation. New York:
Academic Press, 1975.
[C11] Zauderer, E., Partial Differential Equations of
Applied Mathematics. 3rd ed. New York: Wiley, 2006.
[C12] Zygmund, A. and R. Fefferman, Trigonometric Series.
3rd ed. New York: Cambridge University Press, 2002.

Part D. Complex Analysis (Chaps. 13–18)
[D1] Ahlfors, L. V., Complex Analysis. 3rd ed. New
York: McGraw-Hill, 1979.
[D2] Bieberbach, L., Conformal Mapping. Providence,
RI: American Mathematical Society, 2000.
[D3] Henrici, P., Applied and Computational Complex
Analysis. 3 vols. New York: Wiley, 1993.
[D4] Hille, E., Analytic Function Theory. 2 vols. 2nd ed.
Providence, RI: American Mathematical Society,
Reprint V1 1983, V2 2005.
[D5] Knopp, K., Elements of the Theory of Functions.
New York: Dover, 1952.

[D6] Knopp, K., Theory of Functions. 2 parts. New York:
Dover, Reprinted 1996.
[D7] Krantz, S. G., Complex Analysis: The Geometric
Viewpoint. Washington, DC: The Mathematical
Association of America, 1990.
[D8] Lang, S., Complex Analysis. 4th ed. New York:
Springer, 1999.
[D9] Narasimhan, R., Compact Riemann Surfaces. New
York: Springer, 1996.
[D10] Nehari, Z., Conformal Mapping. Mineola, NY:
Dover, 1975.
[D11] Springer, G., Introduction to Riemann Surfaces.
Providence, RI: American Mathematical Society, 2001.

Part E. Numeric Analysis (Chaps. 19–21)
[E1] Ames, W. F., Numerical Methods for Partial
Differential Equations. 3rd ed. New York: Academic
Press, 1992.
[E2] Anderson, E., et al., LAPACK User’s Guide. 3rd ed.
Philadelphia: SIAM, 1999.
[E3] Bank, R. E., PLTMG. A Software Package for
Solving Elliptic Partial Differential Equations: Users’
Guide 8.0. Philadelphia: SIAM, 1998.
[E4] Constanda, C., Solution Techniques for Elementary
Partial Differential Equations. Boca Raton, FL: CRC
Press, 2002.
[E5] Dahlquist, G. and A. Björck, Numerical Methods.
Mineola, NY: Dover, 2003.
[E6] DeBoor, C., A Practical Guide to Splines. Reprinted.
New York: Springer, 2001.
[E7] Dongarra, J. J. et al., LINPACK Users Guide.
Philadelphia: SIAM, 1979. (See also at the beginning of
Chap. 19.)
[E8] Garbow, B. S. et al., Matrix Eigensystem Routines:
EISPACK Guide Extension. Reprinted. New York:
Springer, 1990.
[E9] Golub, G. H. and C. F. Van Loan, Matrix
Computations. 3rd ed. Baltimore, MD: Johns Hopkins
University Press, 1996.
[E10] Higham, N. J., Accuracy and Stability of Numerical
Algorithms. 2nd ed. Philadelphia: SIAM, 2002.
[E11] IMSL (International Mathematical and Statistical
Libraries), FORTRAN Numerical Library. Houston, TX:
Visual Numerics, 2002. (See also at the beginning of
Chap. 19.)
[E12] IMSL, IMSL for Java. Houston, TX: Visual
Numerics, 2002.
[E13] IMSL, C Library. Houston, TX: Visual Numerics,
2002.
[E14] Kelley, C. T., Iterative Methods for Linear and
Nonlinear Equations. Philadelphia: SIAM, 1995.
[E15] Knabner, P. and L. Angerman, Numerical Methods for
Partial Differential Equations. New York: Springer, 2003.

bapp01.qxd

11/9/10

2:40 PM

Page A3

APP. 1 References
[E16] Knuth, D. E., The Art of Computer Programming.
3 vols. 3rd ed. Reading, MA: Addison-Wesley, 1997–
2009.
[E17] Kreyszig, E., Introductory Functional Analysis with
Applications. New York: Wiley, 1989.
[E18] Kreyszig, E., On methods of Fourier analysis in
multigrid theory. Lecture Notes in Pure and Applied
Mathematics 157. New York: Dekker, 1994, pp. 225–242.
[E19] Kreyszig, E., Basic ideas in modern numerical
analysis and their origins. Proceedings of the Annual
Conference of the Canadian Society for the History and
Philosophy of Mathematics. 1997, pp. 34–45.
[E20] Kreyszig, E., and J. Todd, QR in two dimensions.
Elemente der Mathematik 31 (1976), pp. 109–114.
[E21] Mortensen, M. E., Geometric Modeling. 2nd ed.
New York: Wiley, 1997.
[E22] Morton, K. W., and D. F. Mayers, Numerical Solution
of Partial Differential Equations: An Introduction. New
York: Cambridge University Press, 1994.
[E23] Ortega, J. M., Introduction to Parallel and Vector
Solution of Linear Systems. New York: Plenum Press,
1988.
[E24] Overton, M. L., Numerical Computing with IEEE
Floating Point Arithmetic. Philadelphia: SIAM, 2004.
[E25] Press, W. H. et al., Numerical Recipes in C: The Art
of Scientific Computing. 2nd ed. New York: Cambridge
University Press, 1992.
[E26] Shampine, L. F., Numerical Solutions of Ordinary
Differential Equations. New York: Chapman and Hall,
1994.
[E27] Varga, R. S., Matrix Iterative Analysis. 2nd ed. New
York: Springer, 2000.
[E28] Varga, R. S., Gersˇgorin and His Circles. New York:
Springer, 2004.
[E29] Wilkinson, J. H., The Algebraic Eigenvalue
Problem. Oxford: Oxford University Press, 1988.

Part F. Optimization, Graphs (Chaps. 22–23)
[F1] Bondy, J. A. and U.S.R. Murty, Graph Theory with
Applications. Hoboken, NJ: Wiley-Interscience, 1991.
[F2] Cook, W. J. et al., Combinatorial Optimization. New
York: Wiley, 1997.
[F3] Diestel, R., Graph Theory. 4th ed. New York:
Springer, 2006.
[F4] Diwekar, U. M., Introduction to Applied Optimization.
2nd ed. New York: Springer, 2008.
[F5] Gass, S. L., Linear Programming. Method and
Applications. 3rd ed. New York: McGraw-Hill, 1969.
[F6] Gross, J. T. and J.Yellen (eds.), Handbook of Graph
Theory and Applications. 2nd ed. Boca Raton, FL: CRC
Press, 2006.
[F7] Goodrich, M. T., and R. Tamassia, Algorithm
Design: Foundations, Analysis, and Internet Examples.
Hoboken, NJ: Wiley, 2002.

A3
[F8] Harary, F., Graph Theory. Reprinted. Reading, MA:
Addison-Wesley, 2000.
[F9] Merris, R., Graph Theory. Hoboken, NJ: WileyInterscience, 2000.
[F10] Ralston, A., and P. Rabinowitz, A First Course in
Numerical Analysis. 2nd ed. Mineola, NY: Dover, 2001.
[F11] Thulasiraman, K., and M. N. S. Swamy, Graph
Theory and Algorithms. New York: Wiley-Interscience,
1992.
[F12] Tucker, A., Applied Combinatorics. 5th ed.
Hoboken, NJ: Wiley, 2007.

Part G. Probability and Statistics
(Chaps. 24–25)
[G1] American Society for Testing Materials, Manual on
Presentation of Data and Control Chart Analysis. 7th
ed. Philadelphia: ASTM, 2002.
[G2] Anderson, T. W., An Introduction to Multivariate
Statistical Analysis. 3rd ed. Hoboken, NJ: Wiley,
2003.
[G3] Cramér, H., Mathematical Methods of Statistics.
Reprinted. Princeton, NJ: Princeton University Press,
1999.
[G4] Dodge, Y., The Oxford Dictionary of Statistical
Terms. 6th ed. Oxford: Oxford University Press,
2006.
[G5] Gibbons, J. D. and S. Chakraborti, Nonparametric
Statistical Inference. 4th ed. New York: Dekker, 2003.
[G6] Grant, E. L. and R. S. Leavenworth, Statistical
Quality Control. 7th ed. New York: McGraw-Hill,
1996.
[G7] IMSL, Fortran Numerical Library. Houston, TX:
Visual Numerics, 2002.
[G8] Kreyszig, E., Introductory Mathematical Statistics.
Principles and Methods. New York: Wiley, 1970.
[G9] O’Hagan, T. et al., Kendall’s Advanced Theory of
Statistics 3-Volume Set. Kent, U.K.: Hodder Arnold,
2004.
[G10] Rohatgi, V. K. and A. K. MD. E. Saleh, An
Introduction to Probability and Statistics. 2nd ed.
Hoboken, NJ: Wiley-Interscience, 2001.

Web References
[W1] upgraded version of [GenRef1] online at
http://dlmf.nist.gov/. Hardcopy and CD-Rom: Oliver,
W. J. et al. (eds.), NIST Handbook of Mathematical
Functions. Cambridge; New York: Cambridge University
Press, 2010.
[W2] O’Connor, J. and E. Robertson, MacTutor History
of Mathematics Archive. St. Andrews, Scotland:
University of St. Andrews, School of Mathematics and
Statistics. Online at http://www-history.mcs.st-andrews.
ac.uk. (Biographies of mathematicians, etc.).

bapp02_a.qxd

11/9/10

4:38 PM

Page A4

APPENDIX

2

Answers to
Odd-Numbered Problems
Problem Set 1.1, page 8
1

1. y ϭ

p

cos 2px ϩ c

1
sinh 5.13x ϩ c
5.13
y ϭ 1.65e؊4x ϩ 0.35
11. y ϭ (x ϩ 12)ex
؊x
y ϭ 1>(1 ϩ 3e )
15. y ϭ 0 and y ϭ 1 because y r ϭ 0 for these y
1
؊11
11
#
exp(Ϫ1.4 10 t) ϭ 2, t ϭ 10 (ln 2)>1.4 [sec]
Integrate y s ϭ g twice, y r (t) ϭ gt ϩ v0, y r (0) ϭ v0 ϭ 0 (start from rest), then
y(t) ϭ 12 gt 2 ϩ y0, where y(0) ϭ y0 ϭ 0

5. y ϭ 2e؊x(sin x Ϫ cos x) ϩ c
9.
13.
17.
19.

3. y ϭ cex
7. y ϭ

Problem Set 1.2, page 11
11. Straight lines parallel to the x-axis 13. y ϭ x
15. mv r ϭ mg Ϫ bv2, v r ϭ 9.8 Ϫ v2, v(0) ϭ 10, v r ϭ 0 gives the limit>9.8 ϭ 3.1
[meter>sec]
17. Errors of steps 1, 5, 10: 0.0052, 0.0382, 0.1245, approximately
19. x 5 ϭ 0.0286 (error 0.0093), x 10 ϭ 0.2196 (error 0.0189)

Problem Set 1.3, page 18
1. If you add a constant later, you may not get a solution.
Example: y r ϭ y, ln ƒ y ƒ ϭ x ϩ c, y ϭ exϩc ϭ ෂcex but not ex ϩ c (with c 0)
3. cos2 y dy ϭ dx, 12 y ϩ 14 sin 2y ϩ c ϭ x
5. y 2 ϩ 36x 2 ϭ c, ellipses
7. y ϭ x arctan (x 2 ϩ c)
9. y ϭ x>(c Ϫ x)
11. y ϭ 24>x, hyperbola
13. dy>sin2 y ϭ dx>cosh2 x, Ϫcot y ϭ tanh x ϩ c, c ϭ 0, y ϭ Ϫarccot (tanh x)
15. y 2 ϩ 4x 2 ϭ c ϭ 25
17. y ϭ x arctan (x 3 Ϫ 1)
19. y0ekt ϭ 2y0, ek ϭ 2 (1 week), e2k ϭ 22 (2 weeks), e4k ϭ 24
21. 69.6% of y0
23. PV ϭ c ϭ const
25. T ϭ 22 Ϫ 17e؊0.5 306t ϭ 21.9 3°C4 when t ϭ 9.68 min
27.
29.
31.
33.

A4

1
e؊k 10 ϭ 12, k ϭ 10
, ln 12, e؊kt0 ϭ 0.01, t ϭ (ln 100)>k ϭ 66 [min]
No. Use Newton’s law of cooling.
y ϭ ax, y r ϭ g(y>x) ϭ a ϭ const, independent of the point (x, y)
¢S ϭ 0.15S¢␾, dS>d␾ ϭ 0.15S, S ϭ S0e0.15␾ ϭ 1000S0,
␾ ϭ (1>0.15) ln 1000 ϭ 7.3 # 2p. Eight times.

#

bapp02.qxd

11/4/10

7:48 AM

Page A5

App. 2 Answers to Odd-Numbered Problems

A5

Problem Set 1.4, page 26
1.
5.
9.
11.
13.
15.

Exact, 2x ϭ 2x, x 2y ϭ c, y ϭ c>x 2
3. Exact, y ϭ arccos (c>cos x)
2
2
2
Not exact, y ϭ 2x ϩ cx
7. F ϭ ex , ex tan y ϭ c
Exact, u ϭ e2x cos y ϩ k(y), u y ϭ Ϫe2x sin y ϩ k r, k r ϭ 0. Ans. e2x cos y ϭ 1
F ϭ sinh x, sinh2 x cos y ϭ c
u ϭ ex ϩ k( y), u y ϭ k r ϭ Ϫ1 ϩ ey, k ϭ Ϫy ϩ ey. Ans. ex Ϫ y ϩ ey ϭ c
b ϭ k, ax 2 ϩ 2kxy ϩ ly 2 ϭ c

Problem Set 1.5, page 34
y ϭ cex Ϫ 5.2
5. y ϭ (x ϩ c)eϪkx
y ϭ x 2(c ϩ ex)
9. y ϭ (x Ϫ 2.5>e)ecos x
y ϭ 2 ϩ c sin x
13. Separate. y Ϫ 2.5 ϭ c cosh4 1.5x
(y1 ϩ y2) r ϩ p(y1 ϩ y2) ϭ (y1r ϩ py1) ϩ (y2r ϩ py2) ϭ 0 ϩ 0 ϭ 0
(y1 ϩ y2) r ϩ p(y1 ϩ y2) ϭ (y1r ϩ py1) ϩ (y2r ϩ py2) ϭ r ϩ 0 ϭ r
Solution of cy1r ϩ pcy1 ϭ c(y 1r ϩ py1) ϭ cr
y ϭ uy*, y r ϩ py ϭ u r y* ϩ uy* r ϩ puy* ϭ u r y* ϩ u(y* r ϩ py*) ϭ u r y* ϩ u # 0
ϭ r, u r ϭ r>y* ϭ re ͐p dx, u ϭ ͐ e ͐p dx r dx ϩ c . Thus, y ϭ uyh gives (4). We shall
see that this method extends to higher-order ODEs (Secs. 2.10 and 3.3).
2
23. y 2 ϭ 1 ϩ 8e؊x
25. y ϭ 1>u, u ϭ ce؊3.2x ϩ 10>3.2
27. dx>dy ϭ 6ey Ϫ 2x, x ϭ ce ؊2y ϩ 2ey
31. T ϭ 240ekt ϩ 60, T(10) ϭ 200, k ϭ Ϫ0.0539, t ϭ 102 min
33. y r ϭ A Ϫ ky, y(0) ϭ 0, y ϭ A(1 Ϫ e ؊kt)>k
35. y r ϭ 175(0.0001 Ϫ y>450), y(0) ϭ 450 ؒ 0.0004 ϭ 0.18,
y ϭ 0.135e؊0.3889t ϩ 0.045 ϭ 0.18>2,
e؊0.3889t ϭ (0.09 Ϫ 0.045)>0.135 ϭ 1>3,
t ϭ (ln 3)>0.3889 ϭ 2.82. Ans. About 3 years
37. y r ϭ y Ϫ y 2 Ϫ 0.2y, y ϭ 1>(1.25 Ϫ 0.75e؊0.8t), limit 0.8, limit 1
39. y r ϭ By 2 Ϫ Ay ϭ By(y Ϫ A>B), A Ͼ 0, B Ͼ 0. Constant solutions y ϭ 0,
y ϭ A>B, y r Ͼ 0 if y Ͼ A>B (unlimited growth), y r Ͻ 0 if 0 Ͻ y Ͻ A>B
(extinction). y ϭ A>(ceAt ϩ B), y(0) Ͼ A>B if c Ͻ 0, y(0) Ͻ A>B if c Ͼ 0.
3.
7.
11.
15.
17.
19.
21.

Problem Set 1.6, page 38
1. x 2>(c2 ϩ 9) ϩ y 2>c2 Ϫ 1 ϭ 0
3. y Ϫ cosh (x Ϫ c) Ϫ c ϭ 0
y r ϭ Ϫx>yෂ, ෂ
y2 ϩ x2 ϭ ෂ
c , circles
5. y>x ϭ c, y r >x ϭ y>x 2, y r ϭ y>x, ෂ
ෂ2
2
2
ෂ
ෂ
ෂ
ce y
7. 2y Ϫ x ϭ c
9. y r ϭ Ϫ2xy, y r ϭ 1>(2xyෂ), x ϭ ෂ
y ϭෂ
cx
11. ෂ
y r ϭ 9yෂ>4x, ෂ
y ϭෂ
c x 9>4 (cෂ Ͼ 0).
13. y r ϭ Ϫ4x>9y. Trajectories ෂ
Sketch or graph these curves.
y r ϭ u yෂ>u x . Now
15. u ϭ c, u x dx ϩ u y dy ϭ 0, y r ϭ Ϫu x>u y. Trajectories ෂ
vϭෂ
c , vx dx ϩ vy dy ϭ 0, y r ϭ Ϫvx>vy. This agrees with the trajectory ODE
in u if u x ϭ vy (equal denominators) and u y ϭ Ϫvx (equal numerators). But these
are just the Cauchy–Riemann equations.

bapp02.qxd

11/4/10

A6

7:48 AM

Page A6

App. 2 Answers to Odd-Numbered Problems

Problem Set 1.7, page 42
1. y r ϭ f (x, y) ϭ r(x) Ϫ p(x)y; hence 0f> 0y ϭ Ϫp(x) is continuous and is thus
bounded in the closed interval ƒ x Ϫ x 0 ƒ Ϲ a.
3. In ƒ x Ϫ x 0 ƒ Ͻ a; just take b in a ϭ b>K large, namely, b ϭ aK.
5. R has sides 2a and 2b and center (1, 1) since y(1) ϭ 1. In R,
f ϭ 2y 2 Ϲ 2(b ϩ 1)2 ϭ K, a ϭ b>K ϭ b>(2(b ϩ 1)2), da>db ϭ 0 gives b ϭ 1,
and aopt ϭ b>K ϭ 18. Solution by dy>y 2 ϭ 2 dx, etc., y ϭ 1>(3 Ϫ 2x).
7. ƒ 1 ϩ y 2 ƒ Ϲ K ϭ 1 ϩ b 2, a ϭ b>K, da>db ϭ 0, b ϭ 1, a ϭ 12.
9. No. At a common point (x 1, y1) they would both satisfy the “initial condition”
y(x 1) ϭ y1, violating uniqueness.

Chapter 1 Review Questions and Problems, page 43
11.
15.
17.
19.
23.
27.
29.

y ϭ ceϪ2x
13. y ϭ 1>(ceϪ4x ϩ 4)
y ϭ ceϪx ϩ 0.01 cos 10x ϩ 0.1 sin 10x
y ϭ ceϪ2.5x ϩ 0.640 x Ϫ 0.256
25y 2 Ϫ 4x 2 ϭ c
21. F ϭ x, x 3ey ϩ x 2y ϭ c
1
y ϭ sin ( x ϩ 4 p)
25. 3 sin x ϩ 13 sin y ϭ 0
ek ϭ 1.25, (ln 2)>ln 1.25 ϭ 3.1, (ln 3)>ln 1.25 ϭ 4.9 [days]
ek ϭ 0.9, 6.6 days. 43.7 days from ekt ϭ 0.5, ekt ϭ 0.01

Problem Set 2.1, page 53
1.
5.
7.
9.
13.
17.

F(x, z, z r ) ϭ 0
3. y ϭ c1eϪx ϩ c2
Ϫ1>2
y ϭ (c1x ϩ c2)
(dz>dy)z ϭ Ϫz 3 sin y, Ϫ1>z ϭ Ϫdx>dy ϭ cos y ϩ ෂ
c 1, x ϭ Ϫsin y ϩ c1y ϩ c2
y2 ϭ x 3 ln x
11. y ϭ c1e2x ϩ c2
y(t) ϭ c1e ؊t ϩ kt ϩ c2
15. y ϭ 3 cos 2.5x Ϫ sin 2.5x
y ϭ Ϫ0.75x 3>2 Ϫ 2.25x ؊1>2
19. y ϭ 15eϪx Ϫ sin x

Problem Set 2.2, page 59
1.
5.
9.
13.
17.
21.
25.
29.
33.
35.
37.

y ϭ c1e ؊2.5x ϩ c2e2.5x
3. y ϭ c1e ؊2.8x ϩ c2e ؊3.2x
y ϭ (c1 ϩ c2x)e ؊ px
7. y ϭ c1 ϩ c2e ؊4.5x
؊2.6x
0.8x
y ϭ c1e
ϩ c2e
11. y ϭ c1e ؊x>2 ϩ c2e3x>2
5x>3
y ϭ (c1 ϩ c2x)e
15. y ϭ e ؊0.27x (A cos ( 1p x) ϩ B sin (1p x))
y s ϩ 215y r ϩ 5y ϭ 0
19. y s ϩ 4y r ϩ 5y ϭ 0
y ϭ 4.6 cos 5x Ϫ 0.24 sin 5x
23. y ϭ 6e2x ϩ 4e ؊3x
؊x
y ϭ 2e
27. y ϭ (4.5 Ϫ x)e ؊ px
1 ؊0.27x
yϭ
e
sin (1p x)
31. Independent
1p
c1x 2 ϩ c2x 2 ln x ϭ 0 with x ϭ 1 gives c1 ϭ 0; then c2 ϭ 0 for x ϭ 2, say.
Hence independent
Dependent since sin 2x ϭ 2 sin x cos x
y1 ϭ e ؊x, y2 ϭ 0.001ex ϩ e ؊x

bapp02.qxd

11/4/10

7:48 AM

Page A7

App. 2 Answers to Odd-Numbered Problems

A7

Problem Set 2.3, page 61
1.
3.
5.
7.
9.
11.
15.

4e2x, Ϫe ؊x ϩ 8e2x, Ϫcos x Ϫ 2 sin x
0, 0, (D Ϫ 2I )(Ϫ4e ؊2x) ϭ 8e ؊2x ϩ 8e ؊2x
0, 5e2x, 0
(2D Ϫ I)(2D ϩ I), y ϭ c1e0.5x ϩ c2e ؊0.5x
(D Ϫ 2.1I)2, y ϭ (c1 ϩ c2x)e2.1x
(D Ϫ 1.6I)(D Ϫ 2.4I ), y ϭ c1e1.6x ϩ c2e2.4x
Combine the two conditions to get L(cy ϩ kw) ϭ L(cy) ϩ L(kw) ϭ cLy ϩ kLw.
The converse is simple.

Problem Set 2.4, page 69
y r ϭ y0 cos v0t ϩ (v0>v0) sin v0t. At integer t (if v0 ϭ p), because of periodicity.
(i) Lower by a factor 12, (ii) higher by 12
0.3183, 0.4775, 1(k 1 ϩ k 2)>m>(2p) ϭ 0.5 738
mLu s ϭ Ϫmg sin u Ϸ Ϫmgu (tangential component of W ϭ mg),
u s ϩ v02 u ϭ 0, v0>(2p) ϭ 1g>L >(2p)
9. my s ϭ Ϫaෂgy, where m ϭ 1 kg, ay ϭ p # 0.012 # 2y meter 3 is the volume of the
water that causes the restoring force agy with g ϭ 9800 nt (ϭ weight>meter 3).
y s ϩ v02 y ϭ 0, v02 ϭ ag>m ϭ ag ϭ 0.000 628g. Frequency v0>2p ϭ 0.4 3sec؊14.
13. y ϭ [y 0 ϩ (v0 ϩ ay0) t]eϪat, y ϭ [1 ϩ (v0 ϩ 1)t]eϪt;
(ii) v0 ϭ Ϫ2, Ϫ32, Ϫ43, Ϫ54, Ϫ65
15. v* ϭ 3v02 Ϫ c2>(4m 2)41>2 ϭ v031 Ϫ c2>(4mk)41>2 Ϸ v0(1 Ϫ c2>8mk) ϭ 2.9 583
17. The positive solutions of tan t ϭ 1, that is, p>4 (max), 5p>4 (min). etc
19. 0.0 231 ϭ (ln 2)>30 3kg>sec4 from exp (Ϫ10 ؒ 3c>2m) ϭ 12.
1.
3.
5.
7.

˛

Problem Set 2.5, page 73
3.
7.
11.
13.
17.

y ϭ (c1 ϩ c2 ln x) x ؊1.8
5. 1x (c1 cos (ln x) ϩ c2 sin (ln x))
y ϭ c1x 2 ϩ c2x 3
9. y ϭ (c1 ϩ c2 ln x) x 0.6
2
y ϭ x (c1 cos ( 16 ln x) ϩ c2 sin (16 ln x))
y ϭ x Ϫ3>2
15. y ϭ (3.6 ϩ 4.0 ln x)>x
y ϭ cos (ln x) ϩ sin (ln x)
19. y ϭ Ϫ0.525x 5 ϩ 0.625x Ϫ3

Problem Set 2.6, page 79
3.
9.
11.
13.
15.

W ϭ Ϫ2.2e ؊3x
5. W ϭ Ϫx 4
7. W ϭ a
y s ϩ 25y ϭ 0, W ϭ 5, y ϭ 3 cos 5x Ϫ sin 5x
y s ϩ 5y ϩ 6.34 ϭ 0, W ϭ 0.3e Ϫ5x, 3e Ϫ2.5 cos 0.3x
y s ϩ 2y r ϭ 0, W ϭ Ϫ2e ؊2x, y ϭ 0.5(1 ϩ e ؊2x)
y s Ϫ 3.24y ϭ 0, W ϭ 1.8, y ϭ 14.2 cosh 1.8x ϩ 9.1 sinh 1.8x

Problem Set 2.7, page 84
1.
5.
9.
11.

y ϭ c1e ؊x ϩ c2e ؊4x Ϫ 5e ؊3x
3. y ϭ c1e ؊2x ϩ c2e ؊x ϩ 6x 2 Ϫ 18x ϩ 21
1 ؊x
؊2x
y ϭ (c1 ϩ c2x) e
ϩ 2 e sin x 7. y ϭ c1e ؊x>2 ϩ c2e ؊3x>2 ϩ 45 ex ϩ 6x Ϫ 16
4x
Ϫ4x
y ϭ c1e ϩ c2e
ϩ 1.2 xe4x Ϫ 2ex
2
y ϭ cos( 13x) ϩ 6x Ϫ 4

bapp02.qxd

11/4/10

A8

7:48 AM

Page A8

App. 2 Answers to Odd-Numbered Problems

13. y ϭ ex>4 Ϫ 2ex>2 ϩ 15 e؊x ϩ ex
15. y ϭ ln x
17. y ϭ e ؊0.1x (1.5 cos 0.5x Ϫ sin 0.5x) ϩ 2e0.5x

Problem Set 2.8, page 91
3.
5.
7.
9.
11.
13.
15.
17.
19.
25.

yp ϭ 1.0625 cos 2t ϩ 3.1875 sin 2t
yp ϭ Ϫ1.28 cos 4.5t ϩ 0.36 sin 4.5t
yp ϭ 25 ϩ 43 cos 3t ϩ sin 3t
y ϭ e ؊1.5t(A cos t ϩ B sin t) ϩ 0.8 cos t ϩ 0.4 sin t
y ϭ A cos 12t ϩ B sin 12t ϩ t (sin 12t Ϫ cos 12t)>(2 12)
y ϭ A cos t ϩ B sin t Ϫ (cos vt)>(v2 Ϫ 1)
y ϭ e ؊2t (A cos 2t ϩ B sin 2t) ϩ 14 sin 2t
1
1
y ϭ 13 sin t Ϫ 15
sin 3t Ϫ 105
sin 5t
y ϭ e ؊t(0.4 cos t ϩ 0.8 sin t) ϩ eϪt>2(Ϫ0.4 cos 12 t ϩ 0.8 sin 12 t)
CAS Experiment. The choice of v needs experimentation, inspection of the curves
obtained, and then changes on a trail-and-error basis. It is interesting to see how in
the case of beats the period gets increasingly longer and the maximum amplitude gets
increasingly larger as v>(2p) approaches the resonance frequency.

Problem Set 2.9, page 98
1.
3.
5.
7.
9.
13.
15.
17.
19.

RI r ϩ I>C ϭ 0, I ϭ ce؊t>(RC)
LI r ϩ RI ϭ E, I ϭ (E>R) ϩ ce؊Rt>L ϭ 4.8 ϩ ce؊40t
I ϭ 2 (cos t Ϫ cos 20t)>399
I0 is maximum when S ϭ 0; thus, C ϭ 1>(v2L).
Iϭ0
11. I ϭ 5.5 cos 10t ϩ 16.5 sin 10t A
I ϭ e ؊5t(A cos 10t ϩ B sin 10t) Ϫ 400 cos 25t ϩ 200 sin 25t A
R Ͼ Rcrit ϭ 22L>C is Case I, etc.
E(0) ϭ 600, I r (0) ϭ 600, I ϭ e ؊3t (Ϫ100 cos 4t ϩ 75 sin 4t) ϩ 100 cos t
1
F, E ϭ 4.4 sin 10t V
R ϭ 2 ⍀, L ϭ 1 H, C ϭ 12

Problem Set 2.10, page 102
1.
3.
7.
11.

y ϭ A cos 3x ϩ B sin 3x ϩ 19 (cos 3x) ln ƒ cos 3x ƒ ϩ 13 x sin 3x
y ϭ c1x ϩ c2x 2 Ϫ x sin x
5. y ϭ A cos x ϩ B sin x ϩ 12 x (cos x ϩ sin x)
2x
؊2 2x
9. y ϭ (c1 ϩ c2x)ex ϩ 4x 7>2 ex
y ϭ (c1 ϩ c2x) e ϩ x e
2
3
4
13. y ϭ c1x Ϫ3 ϩ c2x 3 ϩ 3x 5
y ϭ c1x ϩ c2x ϩ 1>(2x )

Chapter 2 Review Questions and Problems, page 102
7.
11.
15.
19.
23.

y ϭ c1e؊4.5x ϩ c2e؊3.5x
9. y ϭ eϪ3x (A cos 5x ϩ B sin 5x)
y ϭ (c1 ϩ c2x)e0.8x
13. y ϭ c1x ؊4 ϩ c2x 3
2x
؊x>2
2
y ϭ c1e ϩ c2e
Ϫ 3x ϩ x
17. y ϭ (c1 ϩ c2x)e1.5x ϩ 0.25x 2e1.5x
3
x
y ϭ 5 cos 4x Ϫ 4 sin 4x ϩ e
21. y ϭ Ϫ4x ϩ 2x 3 ϩ 1>x
I ϭ Ϫ0.01093 cos 415t ϩ 0.05273 sin 415t A

bapp02.qxd

11/4/10

7:48 AM

Page A9

App. 2 Answers to Odd-Numbered Problems

A9

1
25. I ϭ 73
(50 sin 4t Ϫ 110 cos 4t) A
27. RLC - circuit with R ϭ 20 ⍀, L ϭ 4 H, C ϭ 0.1 F, E ϭ Ϫ25 cos 4t V
29. v ϭ 3.1 is close to v0 ϭ 2k>m ϭ 3, y ϭ 25 (cos 3t Ϫ cos 3.1t).

Problem Set 3.1, page 111
9. Linearly independent
13. Linearly independent

11. Linearly independent
15. Linearly dependent

Problem Set 3.2, page 116
1.
5.
7.
9.
13.

y ϭ c1 ϩ c2 cos 5x ϩ c3 sin 5x
3. y ϭ c1 ϩ c2x ϩ c3 cos 2x ϩ c4 sin 2x
y ϭ A1 cos x ϩ B1 sin x ϩ A2 cos 3x ϩ B2 sin 3x
y ϭ 2.398 ϩ e ؊1.6x (1.002 cos 1.5x Ϫ 1.998 sin 1.5x)
y ϭ 4e ؊x ϩ 5e ؊x>2 cos 3x
11. y ϭ cosh 5x Ϫ cos 4x
y ϭ e0.25x ϩ 4.3eϪ0.7x ϩ 12.1 cos 0.1x Ϫ 0.6 sin 0.1x

Problem Set 3.3, page 122
1.
3.
5.
7.
9.
13.

y ϭ (c1 ϩ c2x ϩ c3x 2)e؊x ϩ 18 ex Ϫ x ϩ 2
y ϭ c1 cos x ϩ c2 sin x ϩ c3 cos 3x ϩ c4 sin 3x ϩ 0.1 sinh 2x
1 Ϫ2
y ϭ c1x 2 ϩ c2x ϩ c3x Ϫ1 Ϫ 12
x
y ϭ (c1 ϩ c2x ϩ c3x 2)e3x Ϫ 14 (cos 3x Ϫ sin 3x)
11. y ϭ eϪ3x (Ϫ1.4 cos x Ϫ sin x)
y ϭ cos x ϩ 12 sin 4x
y ϭ 2 Ϫ 2 sin x ϩ cos x

Chapter 3 Review Questions and Problems, page 122
7.
9.
11.
15.
19.

y ϭ c1 ϩ e ؊2x (A cos 3x ϩ B sin 3x)
y ϭ c1 cosh 2x ϩ c2 sinh 2x ϩ c3 cos 2x ϩ c4 sin 2x ϩ cosh x
y ϭ (c1 ϩ c2x ϩ c3x 2)e ؊1.5x
13. y ϭ (c1 ϩ c2x ϩ c3x 2)e ؊2x ϩ x 2 Ϫ 3x ϩ 3
10
1>2
3>2
17. y ϭ 2eϪ2x cos 4x ϩ 0.05 x Ϫ 0.06
y ϭ c1x ϩ c2x
ϩ c3x
Ϫ 3
؊4x
؊5x
y ϭ 4e
ϩ 5e

Problem Set 4.1, page 136
1.
5.
7.
11.
13.
15.

Yes
y1r ϭ 0.02(Ϫy1 ϩ y2), y2r ϭ 0.02( y1 Ϫ 2y2 ϩ y3), y3r ϭ 0.02( y2 Ϫ y3)
9. c1 ϭ 10, c2 ϭ 5
c1 ϭ 1, c2 ϭ Ϫ5
y1r ϭ y2, y2r ϭ y1 ϩ ϩ 15
y
,
y
ϭ
c1[1 4]T e4t ϩ c2 [1 Ϫ14 ]T e ؊t>4
2
4
y1r ϭ y2, y2r ϭ 24y1 Ϫ 2y2, y1 ϭ c1e4t ϩ c2e؊6t ϭ y, y2 ϭ y r
(a) For example, C ϭ 1000 gives Ϫ2.39993, Ϫ0.000167. (b) Ϫ2.4, 0.
(d) a22 ϭ Ϫ4 ϩ 216.4 ϭ 1.05964 gives the critical case. C about 0.18506.

bapp02.qxd

11/4/10

A10

7:48 AM

Page A10

App. 2 Answers to Odd-Numbered Problems

Problem Set 4.3, page 147
1. y1 ϭ c1e؊2t ϩ c2e2t, y2 ϭ Ϫ3c1e؊2t ϩ c2e2t
3. y1 ϭ 2c1e2t ϩ 2c2, y2 ϭ c1e2t Ϫ c2
5. y1 ϭ 5c1 ϩ 2c2e14.5t
y2 ϭ Ϫ2c1 ϩ 5c2e14.5t
7. y1 ϭ Ϫc2 cos 12t ϩ c3 sin 12t ϩ c1
y2 ϭ c2 12 sin 12t ϩ c3 12 cos 12t
y3 ϭ c2 cos 12t Ϫ c3 sin 12t ϩ c1
9. y1 ϭ 12 c1e؊18t ϩ 2c2e9t Ϫ c3e18t
y2 ϭ c1e؊18t ϩ c2e9t ϩ c3e18t
y3 ϭ c1e؊18t Ϫ 2c2e9t Ϫ 12 c3e18t
11. y1 ϭ Ϫ20et ϩ 8e؊t>2
y2 ϭ 4et Ϫ 4e؊t>2
13. y1 ϭ 2 sinh t, y2 ϭ 2 cosh t
15. y1 ϭ 12 et
y2 ϭ 12 et
17. y2 ϭ y1r ϩ y1, y2r ϭ y1s ϩ y1r ϭ Ϫy1 Ϫ y2 ϭ Ϫy1 Ϫ ( y1r ϩ y1),
y1s ϩ 2y1r ϩ 2y1 ϭ 0, y1 ϭ e؊t(A cos t ϩ B sin t),
y2 ϭ y1r ϩ y1 ϭ e؊t(B cos t Ϫ A sin t). Note that r 2 ϭ y 21 ϩ y 22 ϭ e؊2t(A2 ϩ B 2).
19. I1 ϭ c1e؊t ϩ 3c2e؊3t, I2 ϭ Ϫ3c1e؊t Ϫ c2e؊3t

Problem Set 4.4, page 151
1.
3.
5.
7.
9.
11.
15.
17.

Unstable improper node, y1 ϭ c1et, y2 ϭ c2e2t
Center, always stable, y1 ϭ A cos 3t ϩ B sin 3t, y2 ϭ 3B cos 3t Ϫ 3A sin 3t
Stable spiral, y1 ϭ e؊2t(A cos 2t ϩ B sin 2t), y2 ϭ e؊2t(B cos 2t Ϫ A sin 2t)
Saddle point, always unstable, y1 ϭ c1e؊t ϩ c2e3t, y2 ϭ Ϫc1e؊t ϩ c2e3t
Unstable node, y1 ϭ c1e6t ϩ c2e2t, y2 ϭ 2c1e6t Ϫ 2c2e2t
y ϭ e؊t (A cos t ϩ B sin t). Stable and attractive spirals
p ϭ 0.2 0 (was 0), ¢ Ͻ0, spiral point, unstable.
For instance, (a) Ϫ2, (b) Ϫ1, (c) ϭ Ϫ 12, (d) ϭ1, (e) 4.

Problem Set 4.5, page 159
5. Center at (0, 0). At (2, 0) set y1 ϭ 2 ϩ ~y 1. Then ~y 2r ϭ ~y 1 . Saddle point at (2, 0).
7. (0, 0), y1r ϭ Ϫy1 ϩ y2, y2r ϭ Ϫy1 Ϫ y2, stable and attractive spiral point; (Ϫ2, 2),
y1 ϭ Ϫ2 ϩ ~y 1, y2 ϭ 2 ϩ ~y 2, ~y 1r ϭ Ϫy~1 Ϫ 3y~2, ~y 2r ϭ Ϫy~1 Ϫ ~y 2, saddle point
9. (0, 0) saddle point, (Ϫ3, 0) and (3, 0) centers
11. (12p Ϯ 2np, 0) saddle points; (Ϫ12 p Ϯ 2np, 0) centers.
~ ) Ϸ Ϯy~ .
Use Ϫcos (Ϯ12 p ϩ ~y 1) ϭ sin (Ϯy
1
1
13. (Ϯ2np, 0) centers; y1 ϭ (2n ϩ 1)p ϩ ~y 1r , (p Ϯ 2np, 0) saddle points
15. By multiplication, y2 y2r ϭ (4y1 Ϫ y 31)y1r . By integration,
y 22 ϭ 4y 21 Ϫ 12 y 41 ϩ c* ϭ 12 (c ϩ 4 Ϫ y 21)(c Ϫ 4 ϩ y 21), where c* ϭ 12 c2 Ϫ 8.

Problem Set 4.6, page 163
3. y1 ϭ c1e؊t ϩ c2et, y2 ϭ Ϫc1e؊t ϩ c2et Ϫ e3t
5. y1 ϭ c1e5t ϩ c2e2t Ϫ 0.43t Ϫ 0.24, y2 ϭ c1e5t Ϫ 2c2e2t ϩ 1.12t ϩ 0.53

bapp02.qxd

11/4/10

7:48 AM

Page A11

App. 2 Answers to Odd-Numbered Problems

A11

7. y1 ϭ c1et ϩ 4c2e2t Ϫ 3t Ϫ 4 Ϫ 2e؊t, y2 ϭ Ϫc1et Ϫ 5c2e2t ϩ 5t ϩ 7.5 ϩ e؊t
9. The formula for v shows that these various choices differ by multiples of the eigenvector for l ϭ Ϫ2, which can be absorbed into, or taken out of, c1 in the general
solution y (h).
2t
11. y1 ϭ Ϫ83 cosh t Ϫ 43 sinh t ϩ 11
y2 ϭ Ϫ83 sinh t Ϫ 43 cosh t ϩ 43 e2t
3 e ,
13. y1 ϭ cos 2t ϩ sin 2t ϩ 4 cos t, y2 ϭ 2 cos 2t Ϫ 2 sin 2t ϩ sin t
15. y1 ϭ 4e؊t Ϫ 4et ϩ e2t, y2 ϭ Ϫ4e؊t ϩ t
17. I1 ϭ 2c1el1t ϩ 2c2el2t ϩ 100,
I2 ϭ (1.1 ϩ 10.41)c1el1t ϩ (1.1 Ϫ 10.41) c2el2t,
l1 ϭ Ϫ0.9 ϩ 10.41, l2 ϭ Ϫ0.9 Ϫ 10.41
19. c1 ϭ 17.948, c2 ϭ Ϫ67.948

Chapter 4 Review Questions and Problems, page 164
11. y1 ϭ c1e4t ϩ c2e؊4t, y2 ϭ 2c1e4t Ϫ 2c2e؊4t. Saddle point
13. y1 ϭ e؊4t(A cos t ϩ B sin t), y2 ϭ 15 e؊4t[(B Ϫ 2A) cos t Ϫ (A ϩ 2B) sin t];
asymptotically stable spiral point
15. y1 ϭ c1e؊5t ϩ c2e؊t, y2 ϭ c1e؊5t Ϫ c2e؊t. Stable node
17. y1 ϭ e؊t(A cos 2t ϩ B sin 2t), y2 ϭ e؊t(B cos 2t Ϫ A sin 2t). Stable and attractive
spiral point
19. Unstable spiral point
21. y1 ϭ c1e؊4t ϩ c2e4t Ϫ 1 Ϫ 8t 2, y2 ϭ Ϫc1e؊4t ϩ c2e4t Ϫ 4t
23. y1 ϭ 2c1e؊t ϩ 2c2e3t ϩ cos t Ϫ sin t, y2 ϭ Ϫc1e؊t ϩ c2e3t
25. I1r ϩ 2.5(I1 Ϫ I2) ϭ 169 sin t, 2.5(I2r Ϫ I1r ) ϩ 25I2 ϭ 0,
I1 ϭ (19 ϩ 32.5t)e؊5t Ϫ 19 cos t ϩ 62.5 sin t,
I2 ϭ (Ϫ6 Ϫ 32.5t)e؊5t ϩ 6 cos t ϩ 2.5 sin t
27. (0, 0) saddle point; (Ϫ1, 0), (1, 0) centers
29. (np, 0) center when n is even and saddle point when n is odd

Problem Set 5.1, page 174
2ƒ k ƒ
23>2
2
y ϭ a0(1 Ϫ x 2 ϩ x 4>2! Ϫ x 6>3! ϩ Ϫ Á ) ϭ a0e؊x
y ϭ a0 ϩ a1x Ϫ 12 a0 x 2 Ϫ 16 a1 x 3 ϩ Á ϭ a0 cos x ϩ a1 sin x
1 4
1 5
1 4
1 5
a0(1 Ϫ 12
x Ϫ 60
x Ϫ Á ) ϩ a1(x ϩ 12 x 2 ϩ 16 x 3 ϩ 24
x Ϫ 24
x Ϫ Á)
1 4
13 6
1 5
5
a0(1 Ϫ 12 x 2 Ϫ 24
x ϩ 720
x ϩ Á ) ϩ a1(x Ϫ 16 x 3 Ϫ 24
x ϩ 1008
x7 ϩ Á )
ؕ (m ϩ 1)(m ϩ 2)
ؕ (m Ϫ 4)2
m
m
15. a
x
,
a (m Ϫ 3)! x
2
(m
ϩ
1)
ϩ
1
mϭ1
mϭ5
3.
5.
7.
9.
11.
13.

5
17. s ϭ 1 ϩ x Ϫ x 2 Ϫ 56 x 3 ϩ 23 x 4 ϩ 11
24 x ,
1 3
3x

1
30

19. s ϭ 4 Ϫ x Ϫ
ϩ x , s (2) ϭ
values. Exact: y ϭ (x Ϫ 2)2ex
2

5

˛

Ϫ85;

s (12) ϭ 923
768
˛

but x ϭ 2 is too large to give good

Problem Set 5.2, page 179
1
5. P6(x) ϭ 16
(231x 6 Ϫ 315x 4 ϩ 105x 2 Ϫ 5),
1
P7(x) ϭ 16 (429x 7 Ϫ 693x 5 ϩ 315x 3 Ϫ 35x)

bapp02.qxd

11/4/10

A12

7:48 AM

Page A12

App. 2 Answers to Odd-Numbered Problems

11. Set x ϭ az. y ϭ c1Pn(x>a) ϩ c2Q n(x>a)
15. P11 ϭ 21 Ϫ x 2, P21 ϭ 3x21 Ϫ x 2, P22 ϭ 3(1 Ϫ x 2),
P42 ϭ (1 Ϫ x 2)(105x 2 Ϫ15)>2

Problem Set 5.3, page 186
x2
x4
sin x
1
x
x3
cos x
ϩ
Ϫ ϩ Á ϭ x , y2 ϭ x Ϫ
ϩ
Ϫϩ Á ϭ x
3!
5!
2!
4!
b0 ϭ 1, c0 ϭ 0, r 2 ϭ 0, y1 ϭ e؊x, y2 ϭ e؊x ln x
1 4
1
1
y1 ϭ 1 ϩ 12 x 2 Ϫ 16 x 3 ϩ 24
x Ϫ 30
x 5 ϩ 144
x6 Ϫ Á ,
1 4
1
1
y2 ϭ x ϩ 16 x 3 Ϫ 12
x ϩ 120
x 5 Ϫ 120
x6 ϩ Á
y1 1x, y2 ϭ 1 ϩ x
y1 ϭ ex, y2 ϭ ex>x
y1 ϭ ex, y2 ϭ ex ln x
y ϭ AF(1, 1, Ϫ12; x) ϩ Bx 3>2F(52, 52, 52; x)
2
3>4 7
y ϭ A(1 Ϫ 8x ϩ 32
F(4, Ϫ54, 74; x)
5 x ) ϩ Bx
1
y ϭ c1F(2, Ϫ2, Ϫ2; t Ϫ 2) ϩ c2(t Ϫ 2)3>2F(72, Ϫ12, 52; t Ϫ 2)

3. y1 ϭ 1 Ϫ
5.
7.
9.
11.
13.
15.
17.
19.

Problem Set 5.4, page 195
3.
5.
7.
9.
13.

15.

19.
21.
23.
25.

c1J0(1x)
c1J␯(lx) ϩ c2J؊␯(lx), ␯ 0, Ϯ1, Ϯ2, Á
c1J1>2(12 x) ϩ c2J؊1>2(12 x) ϭ x ؊1>2(cෂ1 sin 12 x ϩ ෂ
c2 cos 12 x)
؊␯
x (c1J␯(x) ϩ c2J؊␯(x)), ␯ 0, Ϯ1, Ϯ2, Á
Jn(x 1) ϭ Jn(x 2) ϭ 0 implies x 1؊nJn(x 1) ϭ x 2؊nJn(x 2) ϭ 0 and
[x ؊nJn(x)] r ϭ 0 somewhere between x 1 and x 2 by Rolle’s theorem.
Now use (21b) to get Jnϩ1(x) ϭ 0 there. Conversely, Jnϩ1(x 3) ϭ Jnϩ1(x 4) ϭ 0,
thus x 3nϩ1Jnϩ1(x 3) ϭ x 4nϩ1Jnϩ1(x 4) ϭ 0 implies Jn(x) ϭ 0 in between by Rolle’s
theorem and (21a) with ␯ ϭ n ϩ 1.
By Rolle, J 0r ϭ 0 at least once between two zeros of J0. Use J 0r ϭ ϪJ1 by (21b)
with ␯ ϭ 0. Together J1 ϭ 0 at least once between two zeros of J0. Also use
(xJ1) r ϭ xJ0 by (21a) with ␯ ϭ 1 and Rolle.
Use (21b) with ␯ ϭ 0, (21a) with ␯ ϭ 1, (21d) with ␯ ϭ 2, respectively.
Integrate (21a).
Use (21a) with ␯ ϭ 1, partial integration, (21b) with ␯ ϭ 0, partial integration.
Use (21d) to get

Ύ J (x) dx ϭ Ϫ2J (x) ϩ Ύ J (x) dx ϭ Ϫ2J (x) Ϫ 2J (x) ϩ Ύ J (x) dx
5

4

3

4

ϭ Ϫ2J4(x) Ϫ 2J2(x) Ϫ J0(x) ϩ c.

Problem Set 5.5, page 200
1. c1J4 (x) ϩ c2Y4(x)
3. c1J2>3(x 2) ϩ c2Y2>3(x 2)
5. c1J0 (1x) ϩ c2Y0(1x)

2

1

bapp02.qxd

11/4/10

7:48 AM

Page A13

App. 2 Answers to Odd-Numbered Problems

7.
9.
11.
13.

A13

1x (c1J1>4 (12 kx 2) ϩ c2Y1>4 (12 kx 2))
x 3(c1J3(x) ϩ c2Y3 (x))
Set H (1) ϭ kH (2) and use (10).
Use (20) in Sec. 5.4.

Chapter 5 Review Questions and Problems, page 200
11.
13.
15.
17.
19.

cos 2x, sin 2x
(x Ϫ 1)؊5, (x Ϫ 1)7; Euler–Cauchy with x Ϫ 1 instead of x
J25 (x), J؊25(x)
ex, 1 ϩ x
1x J1(1x), 1x Y1(1x)

Problem Set 6.1, page 210
1. 3>s 2 ϩ 12>s
5. 1>((s Ϫ 2)2 Ϫ 1)
1
e؊s Ϫ 1
9. ϩ
s
s2
Ϫs 2
(1 Ϫ e )
13.
s
19. Use e

at

3. s>(s 2 ϩ p2)
7. (v cos u ϩ s sin u)>(s 2 ϩ v2)
1 Ϫ e؊bs
be؊bs
11.
Ϫ
2
s
s
15.

e؊s Ϫ 1
2s

2

e؊s
2s

ϩ

1
s

ϭ cosh at ϩ sinh at.

23. Set ct ϭ p. Then l( f (ct)) ϭ

Ύ

ϱ

ϱ

e؊stf (ct) dt ϭ

0

1

27.

29. 2t 3 Ϫ 1.9t 5

31. l؊1 a

2

(s ϩ 3)3
37. pte؊pt
41. e؊5pt sinh pt
45. (k 0 ϩ k 1t)e؊at

Ύe

؊(s>c)p

f ( p) dp>c ϭ F(s>c)>c.

0

25. 0.2 cos 1.8t ϩ sin 1.8t

33.

Ϫ

35.

L2

cos

npt
L

4
3
Ϫ
b ϭ 4e2t Ϫ 3e؊t
sϪ2
sϩ1
0.5 # 2p

(s ϩ 4.5)2 ϩ 4p2
39. 72 t 3e؊t22
43. e3t(2 cos 3t ϩ 53 sin 3t)

Problem Set 6.2, page 216
1. y ϭ 1.25e؊5.2t Ϫ 1.25 cos 2t ϩ 3.25 sin 2t
3. (s Ϫ 3)(s ϩ 2) ϭ 11s ϩ 28 Ϫ 11 ϭ 11s ϩ 17, Y ϭ 10>(s Ϫ 3) ϩ 1>(s ϩ 2),
y ϭ 10e3t ϩ e؊2t
5. (s 2 Ϫ 14)Y ϭ 12s, y ϭ 12 cosh 12 t
7. y ϭ 12 e3t ϩ 52 e؊4t ϩ 12 e؊3t
9. y ϭ et Ϫ e3t ϩ 2t
11. (s ϩ 1.5)2Y ϭ s ϩ 31.5 ϩ 3 ϩ 54>s 4 ϩ 64>s,
Y ϭ 1>(s ϩ 1.5) ϩ 1>(s ϩ 1.5) 2 ϩ 24>s 4 Ϫ 32>s 3 ϩ 32>s 2,
y ϭ (1 ϩ t)e؊1.5t ϩ 4t 3 Ϫ 16t 2 ϩ 32t
ෂ
13. t ϭ ෂ
t Ϫ 1, Y ϭ 4>(s Ϫ 6), ෂ
y ϭ 4e6t, y ϭ 4e6(tϩ1)

bapp02.qxd

11/4/10

A14

7:48 AM

Page A14

App. 2 Answers to Odd-Numbered Problems

ෂ
15. t ϭ ෂt ϩ 1.5, (s Ϫ 1)(s ϩ 4)Y ϭ 4s ϩ 17 ϩ 6>(s Ϫ 2), y ϭ 3et؊1.5 ϩ e2(t؊1.5)
1
2v2
17.
19.
(s ϩ a)2
s(s 2 ϩ 4v2)
21. l( f r ) ϭ l(sinh 2t) ϭ sl( f ) Ϫ 1. Answer: (s 2 Ϫ 2)>(s 3 Ϫ 4s)
23. 12(1 Ϫ e؊t>4)
25. (1 Ϫ cos vt)>v2
1
t
27. 19 (1 ϩ t Ϫ cos 3t Ϫ 13 sin 3t)
29. 2 (e؊at Ϫ 1) ϩ a
a

Problem Set 6.3, page 223

3. l((t Ϫ 2)u(t Ϫ 2)) ϭ e؊2s>s 2
1
1
5. aet a1 Ϫ uat Ϫ p bbb ϭ
(1 Ϫ e؊ps>2ϩp>2)
2
sϪ1
1
7.
(e؊2(sϩp) Ϫ e؊4(sϩp))
sϩp

11.
15.
19.
23.
25.
27.
29.
31.
33.
35.
37.

9
4

ϩ 2ϩ b
s
s3
s
13. 2[1 ϩ u(t Ϫ p)] sin 3t
(se؊ps>2 ϩ e؊ps)>(s 2 ϩ 1)
3
17. e؊t cos t (0 Ͻ t Ͻ 2p)
(t Ϫ 3) u(t Ϫ 3)>6
1 t
3 ؊5t
21. sin 3t ϩ sin t (0 Ͻ t Ͻ p); 43 sin 3t (t Ͼ p)
3 (e Ϫ 1) e
1
t
t
e Ϫ sin t (0 Ͻ t Ͻ 2p), e Ϫ 2 sin 2t (t Ͼ 2p)
t Ϫ sin t (0 Ͻ t Ͻ 1), cos (t Ϫ 1) ϩ sin (t Ϫ 1) Ϫ sin t (t Ͼ 1)
t ϭ 1 ϩ tෂ, ෂ
y s ϩ 4yෂ ϭ 8(1 ϩ tෂ)2(1 Ϫ u(tෂ Ϫ 4)), cos 2t ϩ 2t 2 Ϫ 1 if t Ͻ 5,
cos 2t ϩ 49 cos (2t Ϫ 10) ϩ 10 sin (2t Ϫ 10) if t Ͼ 5
0.1i r ϩ 25i ϭ 490e؊5t[1 Ϫ u(t Ϫ 1)],
i ϭ 20(e؊5t Ϫ e؊250t) ϩ 20u(t Ϫ 1)[Ϫe؊5t ϩ e؊250tϩ245]
Rq r ϩ q>C ϭ 0, Q ϭ l(q), q(0) ϭ CV0, i ϭ q r (t),
R(sQ Ϫ CV0) ϩ Q>C ϭ 0, q ϭ CV0e؊t>(RC)
100
100
1
1
10I ϩ s I ϭ 2 e؊2s, I ϭ e؊2s a s Ϫ
b, i ϭ 0 if t Ͻ 2 and
s ϩ 10
s
؊10(t؊2)
1Ϫe
if t Ͼ 2
i ϭ (10 sin 10t ϩ 100 sin t)(u(t Ϫ p) Ϫ u(t Ϫ 3p))
(0.5s 2 ϩ 20)I ϭ 78s(1 ϩ e؊ps)>(s 2 ϩ 1),
i ϭ 4 cos t Ϫ 4 cos 240t Ϫ 4u(t Ϫ p)[cos t ϩ cos ( 140 (t Ϫ p))]

9. e

؊3s>2

a

2

3

t

39. i r ϩ 2i ϩ 2

Ύ i(t) dt ϭ 1000(1 Ϫ u(t Ϫ 2)),

I ϭ 1000(1 Ϫ e؊2s)>(s 2 ϩ 2s ϩ 2),

0

i ϭ 1000e

؊t

sin t Ϫ 1000u(t Ϫ 2)e؊tϩ2 sin (t Ϫ 2)

Problem Set 6.4, page 230
3.
5.
7.
9.

y ϭ 8 cos 2t ϩ 12 u(t Ϫ p) sin 2t
sin t (0 Ͻ t Ͻ p); 0 (p Ͻ t Ͻ 2p); Ϫsin t (t Ͼ 2p)
y ϭ e؊t ϩ 4e؊3t sin 12 t ϩ 12 u(t Ϫ 12)e؊3(t؊1>2) sin (12 t Ϫ 14)
y ϭ 0.1[et ϩ e؊2t(Ϫcos t ϩ 7 sin t)] ϩ 0.1u(t Ϫ 10)3Ϫe؊t ϩ
e؊2tϩ30(cos (t Ϫ 10) Ϫ 7 sin (t Ϫ 10))4

bapp02.qxd

11/4/10

7:48 AM

Page A15

App. 2 Answers to Odd-Numbered Problems

A15

11. y ϭ Ϫe؊3t ϩ e؊2t ϩ 16 u(t Ϫ 1)(1 Ϫ 3e؊2(t؊1) ϩ 2e؊3(t؊1)) ϩ
u(t Ϫ 2)(e؊2(t؊2) Ϫ e؊3(t؊2))
15. ke؊ps>(s Ϫ se؊ps) (s Ͼ 0)

Problem Set 6.5, page 237
1. t
5. 12 t sin vt
9. y Ϫ 1 * y ϭ 1, y ϭ et

3. (et Ϫ e؊t)>2 ϭ sinh t
7. et Ϫ t Ϫ 1
11. y ϭ cos t

t

13. y(t) ϩ 2

Ύe

t؊t

y(t) dt ϭ tet,

y ϭ sinh t

0

17. e4t Ϫ e؊1.5t
21. (vt Ϫ sin vt)>v2
25. 1.5t sin 6t

19. t sin pt
23. 4.5(cosh 3t Ϫ 1)

Problem Set 6.6, page 241
3.
7.

1
2

5.

(s ϩ 3)2
2s 3 ϩ 24s

9.

(s 2 Ϫ 4)3
4s 2 Ϫ p2

s 2 Ϫ v2
(s 2 ϩ v2)2
p(3s 2 Ϫ p2)

(s 2 ϩ p2)3
1
1
r
15. F(s) ϭ Ϫ a 2
b,
2 s Ϫ9

f (t) ϭ 16 t sinh 3t
(s 2 ϩ 14p2)2
17. ln s Ϫ ln (s Ϫ 1); (Ϫ1 ϩ et)>t
19. 3ln (s 2 ϩ 1) Ϫ 2 ln (s Ϫ 1)4 r ϭ 2s>(s 2 ϩ 1) Ϫ 2>(s Ϫ 1); 2(Ϫcos t ϩ et)>t
11.

Problem Set 6.7, page 246
3. y1 ϭ Ϫe؊5t ϩ 4e2t, y2 ϭ e؊5t ϩ 3e2t
5. y1 ϭ Ϫcos t ϩ sin t ϩ 1 ϩ u(t Ϫ 1)[Ϫ1 ϩ cos (t Ϫ 1) Ϫ sin (t Ϫ 1)]
y2 ϭ cos t ϩ sin t Ϫ 1 ϩ u(t Ϫ 1)[1 Ϫ cos (t Ϫ 1) Ϫ sin (t Ϫ 1)]
7. y1 ϭ Ϫe؊2t ϩ 4et ϩ 13 u(t Ϫ 1)(Ϫe3؊2t ϩ et),
y2 ϭ Ϫe؊2t ϩ et ϩ 13 u(t Ϫ 1)(Ϫe3؊2t ϩ et)
9. y1 ϭ (3 ϩ 4t)e3t, y2 ϭ (1 Ϫ 4t)e3t
11. y1 ϭ et ϩ e2t, y2 ϭ e2t
13. y1 ϭ Ϫ4et ϩ sin 10t ϩ 4 cos t, y2 ϭ 4et Ϫ sin 10t ϩ 4 cos t
15. y1 ϭ et, y2 ϭ e؊t, y3 ϭ et Ϫ e؊t
19. 4i 1 ϩ 8(i 1 Ϫ i 2) ϩ 2i 1r ϭ 390 cos t, 8i 2 ϩ 8(i 2 Ϫ i 1) ϩ 4i 2r ϭ 0,
i 1 ϭ Ϫ26e؊2t Ϫ 16e؊8t ϩ 42 cos t ϩ 15 sin t,
i 2 ϭ Ϫ26e؊2t ϩ 8e؊8t ϩ 18 cos t ϩ 12 sin t

Chapter 6 Review Questions and Problems, page 251
11.

5s

Ϫ

3

s Ϫ4
s Ϫ1
؊3sϩ3>2
15. e
>(s Ϫ 12)
2

2

13. 12(1 Ϫ cos pt),

p2>(2s 3 ϩ 2p2s)

17. Sec. 6.6; 2s 2>(s 2 ϩ 1)2

bapp02.qxd

11/4/10

A16

7:48 AM

Page A16

App. 2 Answers to Odd-Numbered Problems

12>(s 2(s ϩ 3))
21. tu(t Ϫ 1)
sin (vt ϩ u)
25. 3t 2 ϩ t 3
e؊2t(3 cos t Ϫ 2 sin t)
29. y ϭ e؊2t(13 cos t ϩ 11 sin t) ϩ 10t Ϫ 8
؊t
e ϩ u(t Ϫ p)[1.2 cos t Ϫ 3.6 sin t ϩ 2e؊tϩ p Ϫ 0.8e2t؊2p]
0 (0 Ϲ t Ϲ 2), 1 Ϫ 2e؊(t؊2) ϩ e؊2(t؊2) (t Ͼ 2)
y1 ϭ 4et Ϫ e؊2t, y2 ϭ et Ϫ e؊2t
y1 ϭ cos t Ϫ u(t Ϫ p) sin t ϩ 2u(t Ϫ 2p) sin2 12 t,
y2 ϭ Ϫsin t Ϫ 2u(t Ϫ p) cos2 12 t ϩ u(t Ϫ 2p) sin t
39. y1 ϭ (1> 110) sin 110t, y2 ϭ Ϫ(1> 110) sin 110t
41. 1 Ϫ e؊t (0 Ͻ t Ͻ 4), (e4 Ϫ 1)e؊t (t Ͼ 4)
3
3
8
43. i(t) ϭ e؊4t(26
cos 3t Ϫ 10
39 sin 3t) Ϫ 26 cos 10t ϩ 65 sin 10t
45. 5i 1r ϩ 20(i 1 Ϫ i 2) ϭ 60, 30i 2r ϩ 20(i 2r Ϫ i 1r ) ϩ 20i 2 ϭ 0,
i 1 ϭ Ϫ8e؊2t ϩ 5e؊0.8t ϩ 3, i 2 ϭ Ϫ4e؊2t ϩ 4e؊0.8t
19.
23.
27.
31.
33.
35.
37.

Problem Set 7.1, page 261
3. 3 ϫ 3, 3 ϫ 4, 3 ϫ 6, 2 ϫ 2, 2 ϫ 3, 3 ϫ 2
1
5. B ϭ 15 A, 10
A
7. No, no, yes, no, no
0

6

9. D18

15

3

0

0

0

12

15T , D 2.5
Ϫ1

Ϫ9

Ϫ10

70
13. DϪ28

1.5

2T , D20.5

5.4

32T , same,

28

1

2

26

11. D34

2.5

DϪ4.2
Ϫ0.6

Ϫ1

13
17T , undefined

16.5
2

Ϫ10

0.6
2.4T , same
0.6

56T , same,

ϪD, undefined

0
Ϫ4.5

5.5
15. D 33.0T , same, undefined, undefined
Ϫ11.0

17. DϪ27.0T
9.0

Problem Set 7.2, page 270
5. 10, n(n ϩ 1)>2
I,

2

8.5

28

14

7. 0,

0

c

1

0

0

0

d, c

1

1

0

0

d

bapp02.qxd

11/4/10

7:48 AM

Page A17

App. 2 Answers to Odd-Numbered Problems

10

Ϫ14

11. DϪ5

7

Ϫ5

Ϫ1

1

2

13. D2

13

0

Ϫ6

A17

Ϫ6
Ϫ12T , same,
Ϫ4
Ϫ9

0

Ϫ6T , D 3
4

10

Ϫ5

DϪ14

7

Ϫ2

Ϫ4

Ϫ5
Ϫ1T , undefined,

4

0

DϪ4T , [7

Ϫ1

Ϫ15
Ϫ33T , same
Ϫ4

c

Ϫ9

3

4

Ϫ5

Ϫ1

0

d

8
15. Undefined,

3], same

Ϫ3
Ϫ30

Ϫ18

17. D 45
5

22

9T , undefined,
Ϫ7

4T , undefined
Ϫ12

10.5
19. Undefined,

D

7

D 0 T , DϪ3T , same
Ϫ3

1

25. (d) AB ϭ (AB)T ϭ BTAT ϭ BA; etc.
(e) Answer. If AB ϭ ϪBA.
29. p ϭ [85 62 30]T, v ϭ [44,920 30,940]T

Problem Set 7.3, page 280
1.
5.
9.
11.
13.
19.
21.
23.

x ϭ Ϫ2, y ϭ 0.5
3. x ϭ 1, y ϭ 3, z ϭ Ϫ5
7. x ϭ Ϫ3t, y ϭ t arb., z ϭ 2t
x ϭ 6, y ϭ Ϫ7
x ϭ 3t Ϫ 1, y ϭ Ϫt ϩ 4, z ϭ t arb.
w ϭ 1, x ϭ t 1 arb., y ϭ 2t 2 Ϫ t 1, z ϭ t 2 arb.
w ϭ 4, x ϭ 0, y ϭ 2, z ϭ 6 17. I1 ϭ 2, I2 ϭ 6, I3 ϭ 8
I1 ϭ (R1 ϩ R2)E 0>(R1R2) A, I2 ϭ E 0>R1 A, I3 ϭ E 0>R2 A
x 2 ϭ 1600 Ϫ x 1, x 3 ϭ 600 ϩ x 1, x 4 ϭ 1000 Ϫ x 1. No
C: 3x 1 Ϫ x 3 ϭ 0, H: 8x 1 Ϫ 2x 4 ϭ 0, O: 2x 2 Ϫ 2x 3 Ϫ x 4 ϭ 0, thus
C3H 8 ϩ 5O2 : 3CO2 ϩ 4H 2O

Problem Set 7.4, page 287
1. 1; [2 Ϫ1 3]; [2 Ϫ1]T
3. 3; {[3 5 0], [0 3 5], [0 0 1]}
5. 3; {[2 Ϫ1 4], [0 1 Ϫ46], [0 0 1]}; {[2 0 1], [0 3 23],
[0 0 1]}

bapp02.qxd

11/4/10

A18

7:48 AM

Page A18

App. 2 Answers to Odd-Numbered Problems

7.
9.
11.
19.
23.
27.
29.
33.
35.

2; [8 0 4 0], [0 2 0 4]; [8 0 4], [0 2 0]
3; [9 0 1 0], [0 9 8 9], [0 0 1 0]
(c) 1
17. No
Yes
21. No
Yes
25. Yes
2, [Ϫ2 0 1], [0 2 1]
No
31. No
1, solution of the given system c[1 10
3], basis [1 10
3]
3
3
4
1, [4 2 3 1]

Problem Set 7.7, page 300
7.
11.
15.
19.
23.

cos (a ϩ b)
40
Ϫ64
2
x ϭ 0, y ϭ 4, z ϭ Ϫ1

9.
13.
17.
21.
25.

1
289
2
x ϭ 3.5, y ϭ Ϫ1.0
w ϭ 3, x ϭ 0, y ϭ 2, z ϭ Ϫ2

Problem Set 7.8, page 308

1.

c

1.20

4.64

0.50

3.60

d

0.9

Ϫ3.4

2

0.2

Ϫ0.2T

Ϫ30

Ϫ0.5

3. D

54

1

0

0

5. DϪ2

1

0T

3

Ϫ4

2

7. A؊1 ϭ A

1

0

0

1
2

9. D18

0

0T

0

1
4

11. (A2)؊1 ϭ (A؊1)2 ϭ

0

c

3.760

22.272

2.400

15.280

d

15. AA؊1 ϭ I, (AA؊1)؊1 ϭ (A؊1)؊1A؊1 ϭ I. Multiply by A from the right.

Problem Set 7.9, page 318
1. [1 0]T, [0 1]T; [1
3. 1, [1 11 Ϫ7]T

0]T,

7. Dimension 2, basis xe؊x, e؊x

[0

Ϫ1]T; [1
5. No
9. 3; basis

11. x 1 ϭ 5y1 Ϫ y2, x 2 ϭ 3y1 Ϫ y2
13. x 1 ϭ 2y1 Ϫ 3y2, x 2 ϭ Ϫ10y1 ϩ 16y2 ϩ y3,

1]T,

c

[Ϫ1 1]T

1

0

0

Ϫ1

d, c

0

1

0

0

d, c

x 3 ϭ Ϫ7y1 ϩ 11y2 ϩ y3

0

0

1

0

d

bapp02.qxd

11/4/10

7:48 AM

Page A19

App. 2 Answers to Odd-Numbered Problems

15.
19.
23.
25.

226
1
a ϭ [3
a ϭ [5

1
3

Ϫ4]T,
2]T,

A19

17. 25
21. k ϭ Ϫ20
b ϭ [Ϫ4
8 Ϫ1]T, ʈ a ϩ b ʈ ϭ 2107 Ϲ 5.099 ϩ 9
b ϭ [3
2 Ϫ1]T, 90 ϩ 14 ϭ 2(38 ϩ 14)

Chapter 7 Review Questions and Problems, page 318
Ϫ1

6

1

11. DϪ18

8

Ϫ7T ,

Ϫ13

Ϫ2

Ϫ7

1

18

DϪ6

Ϫ8

Ϫ1

7

13
2T
7

13. [21 Ϫ8 Ϫ31]T, [21 Ϫ8 31]
15. 197, 0
17. Ϫ5, det A2 ϭ (det A)2 ϭ 25, 0
Ϫ2

Ϫ12

19. DϪ12

16

Ϫ12

Ϫ9

23.
27.
31.
35.

Ϫ12
Ϫ9T

21. x ϭ 4, y ϭ Ϫ2, z ϭ 8

Ϫ14

x ϭ 6, y ϭ 2t ϩ 2, z ϭ t arb.
x ϭ 10, y ϭ Ϫ2
Ranks 2, 2, 1
I1 ϭ 4 A, I2 ϭ 5 A, I3 ϭ 1 A

25. x ϭ 0.4, y ϭ Ϫ1.3, z ϭ 1.7
29. Ranks 2, 2, ϱ
33. I1 ϭ 16.5 A, I2 ϭ 11 A, I3 ϭ 5.5 A

Problem Set 8.1, page 329
1.
5.
7.
9.
11.
13.
15.

17.

19.

23.

3, [1 0]T; Ϫ0.6, [0 1]T
3. Ϫ4, [2 9]T; 3, [1 1]T
Ϫ3i, [1 Ϫi]; 3i, [1 i], i ϭ 1Ϫ1
l2 ϭ 0, [1 0]T
0.8 ϩ 0.6i, [1 Ϫi]T; 0.8 Ϫ 0.6i, [1 i]T
Ϫ(l3 Ϫ 18l2 ϩ 99l Ϫ 162)>(l Ϫ 3) ϭ Ϫ(l2 Ϫ 15l ϩ 54); 3, [2 Ϫ2 1]T;
6, [1 2 2]T; 9, [2 1 Ϫ2]T
Ϫ(l Ϫ 9)3; 9, [2 Ϫ2 1]T, defect 2
(l ϩ 1)2(l2 ϩ 2l Ϫ 15); Ϫ1, [1 0 0 0]T, [0 1 0 0]T;
Ϫ5, [Ϫ3 Ϫ3 1 1]T, 3, [3 Ϫ3 1 Ϫ1]T
0 Ϫ1
c
d . Eigenvalues i, Ϫi. Corresponding eigenvectors are complex,
1
0
indicating that no direction is preserved under a rotation.
0 0
0
1
c
d ; 1, c d ; 0, c d . A point onto the x 2-axis goes onto itself,
0 1
1
0
a point on the x 1-axis onto the origin.
Use that real entries imply real coefficients of the characteristic polynomial.

bapp02.qxd

11/4/10

A20

7:48 AM

Page A20

App. 2 Answers to Odd-Numbered Problems

Problem Set 8.2, page 333
1.
3.
5.
7.
9.
11.
13.
15.
17.
19.

1.5, [1 Ϫ1]T, Ϫ45°; 4.5, [1 1]T, 45°
1, [Ϫ1> 16 1]T, 112.2°; 8, [1 1> 16]T, 22.2°
0.5, [1 Ϫ1]T; 1.5, [1 1]T; directions Ϫ45° and 45°
[5 8]T
[11 12 16]T
1.8
c [10 18 25]T
x ϭ (I Ϫ A)Ϫ1y ϭ [0.6747 0.7128 0.7543]T
Axj ϭ ljxj (xj 0), (A Ϫ kI)xj ϭ ljxj Ϫ kxj ϭ (lj Ϫ k)xj.
From Axj ϭ ljxj (xj 0) and Prob. 18 follows k pApxj ϭ k plpj xj and
k qAqxj ϭ k qlqj xj (p м 0, q м 0, integer). Adding on both sides, we see that
k pAp ϩ k qAq has the eigenvalue k plpj ϩ k qlqj . From this the statement follows.

Problem Set 8.3, page 338
1.
3.
5.
7.
9.
15.
19.

0.8 Ϯ 0.6i, [1 Ϯi]T; orthogonal
2 Ϯ 0.8i, [1 Ϯi]. Not skew–symmetric!
1, [0 2 1]T; 6, [1 0 0]T, [0 1 Ϫ2]T; symmetric
0, Ϯ25i, skew–symmetric
1, [0 1 0]T; i, [1 0 i]T; Ϫi, [1 0 Ϫi]T, orthogonal
No
17. A؊1 ϭ (ϪAT)؊1 ϭ Ϫ(A؊1)T
T
No since det A ϭ det (A ) ϭ det (ϪA) ϭ (Ϫ1)3det (A) ϭ Ϫdet (A) ϭ 0.

Problem Set 8.4, page 345
1.

3.

c

Ϫ25

12

Ϫ50

25

c

3.008

d,

Ϫ5,

Ϫ0.544

5.456

6.992

d,

Ϫ9

c d;
3
5

c

4,

c d;
2

5,

xϭ

5

Ϫ17
31

d;

c

Ϫ2
11

d;

Ϫ2
4

xϭ
Ϫ1

2
1

c d, c d
25

10

25

5

3

5. D0

Ϫ5

15T , 0, D3T ; 4, D0T ; 10, D 1T ; x ϭ D0T , D1T ,

0

Ϫ5

15

9.

11.

c
c

2
5

Ϫ25

d
1
5

Ϫ2

1

3

Ϫ1

1

c

1

Ϫ2

2

1

dAc

1

1

3

2

A

1

d, c d

4

1
5

0

6,

c

0

1

d

ϭ

c

5

0

0

0

d

ϭ

c

2

0

0

Ϫ5

d
d

3

1

0

0

1
DϪ1T
1

bapp02.qxd

11/4/10

7:48 AM

Page A21

App. 2 Answers to Odd-Numbered Problems

A21

1

0

0

1

0

0

4

0

13. DϪ2

1

0T A D2

1

0T ϭ D0

Ϫ2

1

Ϫ2

1

3

2

1

0

1
3

1
3

1
3

1

Ϫ2

0

10

0

0

15. DϪ13

1
6

1
6T

A D1

1

Ϫ1T ϭ D 0

1

0T

0

Ϫ12

1

1

0

5

17. C ϭ

c

19. C ϭ

c

21. C ϭ

c

23. C ϭ

c

1
2

7

3

3

7

d,

11

11

3
Ϫ6

Ϫ6

1

0

d,

14y 21 Ϫ 8y 22 ϭ 0, x ϭ

d,

7y 21 Ϫ 5y 22 ϭ 70, x ϭ

Ϫ11

42

42

24

d,

0T

0

4y 21 ϩ 10y 22 ϭ 200, x ϭ

3

1

1

0

1

c

1

1

22 Ϫ1

1

1

c

1

1

22 1

Ϫ1

1

d y;

c

Ϫ1

1

1

1

52y 21 Ϫ 39y 22 ϭ 156, x ϭ

1

1
22

c

d y,

ellipse

pair of straight lines

d y,

hyperbola

2

3

213 3

Ϫ2

d y,

hyperbola

Problem Set 8.5, page 351
Hermitian, 5, [Ϫi 1]T, 7, [i 1]T
Unitary, (1 Ϫ i13)>2, [Ϫ1 1]T; (1 ϩ i13)>2, [1 1]T
Skew-Hermitian, unitary, Ϫi, [0 Ϫ1 1]T, i, [1 0 0]T, [0 1 1]T
Eigenvalues Ϫ1, 1; eigenvectors [1 Ϫ1]T, [1 1]T; [1 Ϫi]T, [1 i]T;
[0 1]T, [1 0]T, resp.
9. Hermitian, 16
11. Skew-Hermitian, Ϫ6i
T
T T T
13. (ABC) ϭ C B A ϭ C ؊1(ϪB)A
T
T
15. A ϭ H ϩ S, H ϭ 12 (A ϩ A ), S ϭ 12 (A Ϫ A ) (H Hermitian, S skew-Hermitian)
T
T
19. AA Ϫ A A ϭ (H ϩ S)(H Ϫ S) Ϫ (H Ϫ S)(H ϩ S) ϭ 2(ϪHS ϩ SH) ϭ 0
if and only if HS ϭ SH.
1.
3.
5.
7.

˛

Chapter 8 Review Questions and Problems, page 352
11. 3, [1
13. 3, [1
15. 0, [2
17. Ϫ1, 1;

1]T; 2, [1 Ϫ1]T
5]T; 7, [1 1]T
Ϫ2 1]T; 9i, [Ϫ1 ϩ 3i
Aϭ

5
1
c
16 Ϫ3

Ϫ3
5

dc

1 ϩ 3i

23

2

39

1

d

ϭ

4]T; Ϫ9i, [Ϫ1 Ϫ 3i
1 Ϫ1
c
8 63

1
1

d

1 Ϫ 3i

4]T

bapp02.qxd

11/4/10

A22

7:48 AM

Page A22

App. 2 Answers to Odd-Numbered Problems

19.

1 2
c
3 Ϫ1

1
1
21. D1
3
0

Ϫ1
2
1
Ϫ1
1

23. C ϭ

c

25. C ϭ

c

dAc

2

1

1

2

Ϫ1

d

ϭ

c

1

2

0T A D 1

Ϫ1

Ϫ1

1

1

4

12

12

Ϫ14

d,

3.7

1.6

1.6

1.3

d,

Ϫ0.9
0

0
0.6

1

d

4

0

1T ϭ D0

Ϫ20

2

0

10y 21 Ϫ 20y 22 ϭ 20, x ϭ

0
0T

0
1 2
c
15 1

4.5y 21 ϩ 0.5y 22 ϭ 4.5, x ϭ

22
1
؊2

1 2
c
15 1

d y,
1

Ϫ2

hyperbola

d y,

ellipse

Problem Set 9.1, page 360
1.
3.
5.
7.
11.
15.
21.
25.
29.
33.
35.
37.

5, 1, 0; 126; [5> 126, 1> 126, 0]
8.5, Ϫ4.0, 1.7; 191.14, [0.890, Ϫ0.419, 0.178]
2, 1, Ϫ2; u ϭ [ 23, 13, Ϫ23 ], position vector of Q
Q: (4, 0, 12), ƒ v ƒ ϭ 116.25
9. Q : (0, 0, Ϫ8), ƒ v ƒ ϭ 8
[6, 4, 0], [32, 1, 0], [Ϫ3, Ϫ2, 0]
13. [1, 5, 8]
7[9, Ϫ7, 8] ϭ [63, Ϫ49, 56]
17. [12, 8, 0]
[4, 9, Ϫ3], 1106
23. [0, 0, 5], 5
[6, 2, Ϫ14] ϭ 2u, 1236
27. p ϭ [0, 0, Ϫ5]
v ϭ [v1, v2, 3], v1, v2 arbitrary
31. k ϭ 10
ƒ p ϩ q ϩ u ƒ Ϲ 18. Nothing
vB Ϫ vA ϭ [Ϫ19, 0] Ϫ [22> 12, 22> 12] ϭ [Ϫ19 Ϫ 22> 12, Ϫ22> 12]
u ϩ v ϩ p ϭ [Ϫk, 0] ϩ [l, l] ϩ [0, Ϫ1000] ϭ 0, Ϫk ϩ l ϩ 0 ϭ 0,
0 ϩ l Ϫ 1000 ϭ 0, l ϭ 1000, k ϭ 1000

Problem Set 9.2, page 367
44, 44, 0
3. 135, 1320, 186
[2,
9,
9]
ϭ
1166
ϭ
12.88
Ͻ
180
ϩ
186
ϭ
18.22
ƒ
ƒ
ƒ Ϫ24 ƒ ϭ 24, ƒ a ƒ ƒ c ƒ ϭ 135186 ϭ 13010 ϭ 54.86; cf. (6)
300; cf. (5a) and (5b)
13. Use (1) and ƒ cos g ƒ Ϲ 1.
ƒ a ϩ b ƒ 2 ϩ ƒ a Ϫ b ƒ 2 ϭ a • a ϩ 2a • b ϩ b • b ϩ (a • a Ϫ 2a • b ϩ b • b)
ϭ 2ƒaƒ2 ϩ 2ƒbƒ2
17. [2, 5, 0] • [2, 2, 2] ϭ 14
19. [0, 4, 3] • [Ϫ3, Ϫ2, 1] ϭ Ϫ5 is negative! Why?
21. Yes, because W ϭ (p ϩ q) • d ϭ p • d ϩ q • d.
23. arccos 0.5976 ϭ 53.3°
27. b Ϫ a is the angle between the unit vectors a and b. Use (2).
29. g ϭ arccos (12>(6 113)) ϭ 0.9828 ϭ 56.3° and 123.7°
31. a1 ϭ Ϫ 28
33. Ϯ[ 35, Ϫ 45 ]
3
2
2
35. (a ϩ b) • (a Ϫ b) ϭ ƒ a ƒ Ϫ ƒ b ƒ ϭ 0, ƒ a ƒ ϭ ƒ b ƒ . A square.
37. 0. Why?
39. If ƒ a ƒ ϭ ƒ b ƒ or if a and b are orthogonal.
1.
5.
7.
9.
15.

bapp02.qxd

11/4/10

7:48 AM

Page A23

App. 2 Answers to Odd-Numbered Problems

A23

Problem Set 9.3, page 374
5.
7.
9.
11.
15.
19.
21.
23.
25.
27.
31.

Ϫm instead of m, tendency to rotate in the opposite sense.
ƒ v ƒ ϭ ƒ [0, 20, 0] ؋ [8, 6, 0] ƒ ϭ ƒ [0, 0, Ϫ160] ƒ ϭ 160
Zero volume in Fig. 191, which can happen in several ways.
[0, 0, 7], [0, 0, Ϫ7], Ϫ4
13. [6, 2, 7], [Ϫ6, Ϫ2, Ϫ7]
0
17. [Ϫ32, Ϫ58, 34], [Ϫ42, Ϫ63, 19]
1, Ϫ1
[Ϫ48, Ϫ72, Ϫ168], 121248 ϭ 189.0, 189.0
0, 0, 13
m ϭ [Ϫ2, Ϫ2, 0] ؋ [2, 3, 0] ϭ [0, 0, Ϫ10], m ϭ 10 clockwise
[6, 2, 0] ؋ [1, 2, 0] ϭ [0, 0, 10]
29. 12 ƒ [Ϫ12, 2, 6] ƒ ϭ 146
33. 474>6 ϭ 79
3x ϩ 2y Ϫ z ϭ 5

Problem Set 9.4, page 380
1.
3.
5.
7.
11.

Hyperbolas
Parallel straight lines (planes in space) y ϭ 34 x ϩ c
Circles, centers on the y-axis
Ellipses
9. Parallel planes
Elliptic cylinders
13. Paraboloids

Problem Set 9.5, page 390
Circle, center (3, 0), radius 2
3. Cubic parabola x ϭ 0, z ϭ y 3
Ellipse
7. Helix
A “Lissajous curve”
11. r ϭ [3 ϩ 113 cos t, 2 ϩ 113 sin t, 1]
r ϭ [2 ϩ t, 1 ϩ 2t, 3]
15. r ϭ [t, 4t Ϫ 1, 5t]
r ϭ [12 cos t, sin t, sin t]
19. r ϭ [cosh t, (13>2) sinh t, Ϫ2]
Use sin (Ϫa) ϭ Ϫsin a.
u ϭ [Ϫsin t, 0, cos t]. At P, r r ϭ [Ϫ8, 0, 6]. q(w) ϭ [6 Ϫ 8w, i, 8 ϩ 6w].
q(w) ϭ [2 ϩ w, 12 Ϫ 14w, 0]
29. 2r r • r r ϭ cosh t, l ϭ sinh l ϭ 1.175
2r r • r r ϭ a, l ϭ ap>2
33. Start from r(t) ϭ [t, f (t)].
v ϭ r r ϭ [1, 2t, 0], ƒ v ƒ ϭ 21 ϩ 4t 2, a ϭ [0, 2, 0]
v(0) ϭ (v ϩ 1) Ri, a(0) ϭ Ϫv2Rj
v ϭ [Ϫsin t Ϫ 2 sin 2t, cos t Ϫ 2 cos 2t], ƒ v ƒ 2 ϭ 5 Ϫ 4 cos 3t,
6 sin 3t
a ϭ [Ϫcos t Ϫ 4 cos 2t, Ϫsin t ϩ 4 sin 2t], and atan ϭ
v.
5 Ϫ 4 cos 3t
41. v ϭ [Ϫsin t, 2 cos 2t, Ϫ2 sin 2t], ƒ v ƒ 2 ϭ 4 ϩ sin2 t,
1.
5.
9.
13.
17.
21.
25.
27.
31.
35.
37.
39.

a ϭ [Ϫcos t, Ϫ4 sin 2t, Ϫ4 cos 2t], and atan ϭ

1
2

sin 2t

v.
4 ϩ sin2 t
43. 1 year ϭ 365 # 86,400 sec, R ϭ 30 # 365 # 86,400>2p ϭ 151 # 106 [km],
ƒ a ƒ ϭ v2R ϭ ƒ v ƒ 2>R ϭ 5.98 # 10؊6 [km>sec2]
45. R ϭ 3960 ϩ 80 mi ϭ 2.133 # 107 ft, g ϭ ƒ a ƒ ϭ v2R ϭ ƒ v ƒ 2>R, ƒ v ƒ ϭ 1gR ϭ
26.61 # 108 ϭ 25,700 [ft>sec] ϭ 17,500 [mph]
49. r(t) ϭ [t, y(t), 0], r r ϭ [1, y r , 0] r • r r ϭ 1 ϩ y r 2, etc.

bapp02.qxd

11/4/10

A24

7:49 AM

Page A24

App. 2 Answers to Odd-Numbered Problems

51.

dr
dr ds
ϭ >> ,
ds
dt dt

d 2r
d 2r > ds 2
a b ϩ Á,
2 ϭ
ds
dt 2 > dt

d 3r
d 3r > ds 3
a b ϩ Á
3 ϭ
ds
dt 3 > dt

53. 3>(1 ϩ 9t 2 ϩ 9t 4)

Problem Set 9.7, page 402
1.
5.
9.
11.
13.
15.
19.
23.
31.
35.
39.
41.
45.

[2y Ϫ 1, 2x ϩ 2]
3. [Ϫy>x 2, 1>x]
3
3
[4x , 4y ]
7. Use the chain rule.
Apply the quotient rule to each component and collect terms.
[ y, x], [5, Ϫ4]
[2x>(x 2 ϩ y 2), 2y>(x 2 ϩ y 2)], [0.16, 0.12]
[8x, 18y, 2z], [40, Ϫ18, Ϫ22]
17. For P on the x- and y-axes.
[Ϫ1.25, 0]
21. [0, Ϫe]
Points with y ϭ 0, Ϯp, Ϯ2p, Á .
25. Ϫ᭞T(P) ϭ [0, 4, Ϫ1]
ٌf ϭ [32x, Ϫ2y], ٌf (P) ϭ [160, Ϫ2]
33. [12x, 4y, 2z], [60, 20, 10]
[Ϫ2x, Ϫ2y, 1], [Ϫ6, Ϫ8, 1]
37. [2, 1] • [1, Ϫ1]> 15 ϭ 1> 15
[1, 1, 1] • [Ϫ3>125, 0, Ϫ4>125]> 13 ϭ Ϫ7>(125 13)
28>3
43. f ϭ xyz
f ϭ ͐ v1 dx ϩ ͐ v2 dy ϩ ͐ v3 dz

Problem Set 9.8, page 405
1. 2x ϩ 8y ϩ 18z; 7
3. 0, after simplification; solenoidal
5. 9x 2y 2z 2; 1296
7. Ϫ2ex (cos y)z
9. (b) ( fv1)x ϩ ( fv2)y ϩ ( fv3)z ϭ f [(v1)x ϩ (v2)y ϩ (v3)z] ϩ fxv1 ϩ fyv2 ϩ fzv3, etc.
11. [v1, v2, v3] ϭ r r ϭ [x r , y r , z r ] ϭ [ y, 0, 0], z r ϭ 0, z ϭ c3, y r ϭ 0, y ϭ c2, and
x r ϭ y ϭ c2, x ϭ c2t ϩ c1. Hence as t increases from 0 to 1, this “shear flow”
transforms the cube into a parallelepiped of volume l.
13. div (w ؋ r) ϭ 0 because v1, v2, v3 do not depend on x, y, z, respectively.
15. Ϫ2 cos 2x ϩ 2 cos 2y
17. 0
19. 2>(x 2 ϩ y 2 ϩ z 2)2

Problem Set 9.9, page 408
3. Use the definitions and direct calculation.
5. [x (z 2 Ϫ y 2), y (x 2 Ϫ z 2), z (y 2 Ϫ x 2)]
7. e؊x[cos y, sin y, 0]
9. curl v ϭ [Ϫ6z, 0, 0] incompressible, v ϭ r r ϭ [x r , y r , z r ] ϭ [0, 3z 2, 0], x ϭ c1,
z ϭ c3, y r ϭ 3z 2 ϭ 3c23, y ϭ 3c23t ϩ c2
11. curl v ϭ [0, 0, Ϫ3], incompressible, x r ϭ y, y r ϭ Ϫ2x, 2xx r ϩ yy r ϭ 0,
x 2 ϩ 12y 2 ϭ c, z ϭ c3
13. curl v ϭ 0, irrotational, div v ϭ 1, compressible, r ϭ [c1et, c2et, c3e؊t]. Sketch it.
15. [Ϫ1, Ϫ1, Ϫ1], same (why?)
17. Ϫyz Ϫ zx Ϫ xy, 0 (why?), Ϫy Ϫ z Ϫ x
19. [Ϫ2z Ϫ y, Ϫ2x Ϫ z, Ϫ2y Ϫ x], same (why?)

bapp02.qxd

11/4/10

7:49 AM

Page A25

App. 2 Answers to Odd-Numbered Problems

A25

Chapter 9 Review Questions and Problems, page 409
11.
13.
15.
17.
19.
21.
23.
25.
29.
33.
35.
37.

Ϫ10, 1080, 1080, 65
[Ϫ10, Ϫ30, 0], [10, 30, 0], 0, 40
[Ϫ1260, Ϫ1830, Ϫ300], [Ϫ210, 120, Ϫ540], undefined
Ϫ125, 125, Ϫ125
[70, Ϫ40, Ϫ50], 0, 2352 ϩ 202 ϩ 252 ϭ 12250
[Ϫ2, Ϫ6, Ϫ13]
g1 ϭ arccos (Ϫ10> 165 # 40) ϭ 1.7682 ϭ Ϫ101.3°, g2 ϭ 23.7°
[5, 2, 0] • [4 Ϫ 1, 3 Ϫ 1, 0] ϭ 19
27. v • w> ƒ w ƒ ϭ 22> 18 ϭ 7.78
[0, 0, Ϫ14], tendency of clockwise rotation 31. 4
1, Ϫ2y
0, same (why?), 2( y 2 ϩ x 2 Ϫ xz)
[0, Ϫ2, 0]
39. 9> 1225 ϭ 35

Problem Set 10.1, page 418
3.
5.
7.
11.
17.

4
r ϭ [2 cos t, 2 sin t], 0 Ϲ t Ϲ p>2; 85
“Exponential helix,” (e6p Ϫ 1)>3
9. 23.5, 0
؊t
؊t2
؊2
؊4
2e ϩ 2te , Ϫ2e Ϫ e ϩ 3
15. 18p, 43 (4p)3, 18p
[4 cos t, ϩ sin t, sin t, 4 cos t], [2, 2, 0] 19. 144t 4, 1843.2

Problem Set 10.2, page 425
3.
7.
9.
13.
17.

sin 12 x cos 2y, 1 Ϫ 1> 12 ϭ 0.293
5. exy sin z, e Ϫ 0
cosh 1 Ϫ 2 ϭ Ϫ0.457
ex cosh y ϩ ez sinh y, e Ϫ (cosh 1 ϩ sinh 1) ϭ 0
2
ea cos 2b
15. Dependent, x 2 Ϫ4y 2, etc.
Dependent, 4 0, etc.
19. sin (a 2 ϩ 2b 2 ϩ c2)

Problem Set 10.3, page 432
3. 8y 3>3,
7.
11.
13.
17.
19.

54

1

5.

Ύ [x Ϫ x

3

Ϫ (x 2 Ϫ x 5)] dx ϭ

0

1
2

cosh 2x Ϫ cosh x,
sinh 4 Ϫ sinh 2
9. 36 ϩ 27y 2, 144
2
z ϭ 1 Ϫ r , dx dy ϭ r dr du, Answer: p>2
x ϭ 2b>3, y ϭ h>3
15. x ϭ 0, y ϭ 4r>3p
Ix ϭ bh3>12, Iy ϭ b 3h>4
Ix ϭ (a ϩ b)h3>24, Iy ϭ h(a 4 Ϫ b 4)>(48(a Ϫ b))

Problem Set 10.4, page 438
1.
5.
7.
13.

(Ϫ1 Ϫ 1) ؒ p>4 ϭ Ϫp>2
3. 9(e2 Ϫ 1) Ϫ 83 (e3 Ϫ 1)
2
2 2
2x Ϫ 2y, 2x(1 Ϫ x ) Ϫ (2 Ϫ x ) ϩ 1, x ϭ Ϫ1 Á 1, Ϫ 56
15
0. Why?
9. 16
5
ٌ2w ϭ cosh x, y ϭ x>2 Á 2, 12 cosh 4 Ϫ 12

1
12

bapp02.qxd

11/4/10

A26

7:49 AM

Page A26

App. 2 Answers to Odd-Numbered Problems

15. ٌ2w ϭ 6xy, 3x(10 Ϫ x 2)2 Ϫ 3x, 486
19. ƒ grad w ƒ 2 ϭ e2x, 52 (e4 Ϫ 1)

17. ٌ2w ϭ 6x Ϫ 6y, Ϫ 38.4

Problem Set 10.5, page 442
1.
3.
5.
7.
11.
13.
15.
17.
19.

Straight lines, k
z ϭ c2x 2 ϩ y 2, circles, straight lines, [Ϫcu cos v, Ϫcu sin v, u]
z ϭ x 2 ϩ y 2, circles, parabolas, [Ϫ2u 2 cos v, Ϫ2u 2 sin v, u]
x 2>a 2 ϩ y 2>b 2 ϩ z 2>c2 ϭ 1, [bc cos2 v cos u, ac cos2 v sin u, ab sin v cos v],
ellipses
~
~, 1]
~, Ϫ2v
[u~, v~, u~2, ϩ v~2], N ϭ [Ϫ2u
Set x ϭ u and y ϭ v.
[2 ϩ 5 cos u, Ϫ1 ϩ 5 sin u, v], [5 cos u, 5 sin u, 0]
[a cos v cos u, Ϫ2.8 ϩ a cos v sin u, 3.2 ϩ a sin v], a ϭ 1.5;
[a 2 cos2 v cos u, a 2 cos2 v sin u, a 2 cos v sin v]
[cosh u, sinh u, v], [cosh u, Ϫsinh u, 0]

Problem Set 10.6, page 450
F(r) • N ϭ [Ϫu 2, v2, 0] • [Ϫ3, 2, 1] ϭ 3u 2 ϩ 2v2, 29.5
F(r) • N ϭ cos3 v cos u sin u from (3), Sec. 10.5. Answer: 13
F(r) • N ϭ Ϫu 3, Ϫ128p
F • N ϭ [0, sin u, cos v] • [1, Ϫ2u, 0], 4 ϩ (Ϫ2 ϩ p2>16 Ϫ p>2)12 ϭ Ϫ0.1775
r ϭ [2 cos u, 2 sin u, v], 0 Ϲ u Ϲ p>4, 0 Ϲ v Ϲ 5. Integrate 2 sinh v sin u to
get 2(1 Ϫ 1> 12)(cosh 5 Ϫ 1) ϭ 42.885.
13. 7p3> 16 ϭ 88.6
15. G(r) ϭ (1 ϩ 9u 4)3>2, ƒ N ƒ ϭ (1 ϩ 9u 4)1>2. Answer: 54.4
1.
3.
5.
7.
9.

21. Ixϭy ϭ

ΎΎ[

1
2 (x

Ϫ y)2 ϩ z 2] s dA

S

2p

23. [u cos v, u sin v, u],

h

Ύ Ύu
0

2

0

25. [cos u cos v, cos u sin v, sin u],
IK ϭ IB ϩ 12 # 4p ϭ 20.9.

⅐ u12 du dv ϭ

p
12

h4

dA ϭ (cos u) du dv, B the z-axis, IB ϭ 8p>3,

Problem Set 10.7, page 457
1.
3.
5.
7.
9.
13.
17.
21.
25.

224
Ϫe؊1؊z ϩ e؊y؊z, Ϫ2e؊1؊z ϩ e؊z, 2e؊3 Ϫ e؊2 Ϫ 2e؊1 ϩ 1
1
1
3
2 (sin 2x) (1 Ϫ cos 2x),
8,
4
[r cos u cos v, cos u sin v, r sin u], dV ϭ r 2 cos u dr du dv, s ϭ v, 2p2a 3>3
div F ϭ 2x ϩ 2z, 48
11. 12(e Ϫ 1>e) ϭ 24 sinh 1
5
div F ϭ Ϫsin z, 0
15. 1> p ϩ 24
ϭ 0.5266
4
2
h p>2
19. 8abc(b ϩ c2)>3
(a 4>4) ؒ 2p ؒ h ϭ ha 4p>2
23. h5p>10
Do Prob. 20 as the last one.

bapp02.qxd

11/4/10

7:49 AM

Page A27

App. 2 Answers to Odd-Numbered Problems

A27

Problem Set 10.8, page 462
1. x ϭ 0, y ϭ 0, z ϭ 0, no contributions. x ϭ a: 0f>0n ϭ 0f>0x ϭ Ϫ2x ϭ Ϫ2a, etc.
Integrals x ϭ a: (Ϫ2a)bc, y ϭ b: (Ϫ2b)ac, z ϭ c: (4c) ab. Sum 0
3. The volume integral of 8y 2 ϩ [0, 8y] ؒ [2x, 0] ϭ 8y 2 is 8y 3>3 ϭ 83. The surface
integral of f 0g>0n ϭ f ؒ 2x ϭ 2f ϭ 8y 2 over x ϭ 1 is 8y 3>3 ϭ 83. Others 0.
5. The volume integral of 6y 2 ؒ 4 Ϫ 2x 2 ؒ 12 is 0; 8(x ϭ 1), Ϫ8(y ϭ 1), others 0.
7. F ϭ [x, 0, 0], div F ϭ 1, use (2*), Sec. 10.7, etc.
9. z ϭ 0 and z ϭ 2a 2 Ϫ x 2 Ϫ y 2 ϭ 2a 2 Ϫ r 2, dx dy ϭ r dr du,
a
Ϫ2p ؒ 12 (a 2 Ϫ r 2)3>2 ؒ 23 ƒ 0 ϭ 23 pa 3
11. r ϭ a, ␾ ϭ 0, cos ␾ ϭ 1, v ϭ 13 a ؒ (4pa 2)

Problem Set 10.9, page 468
1.
3.
5.
7.
9.
11.
15.
17.
19.

S: z ϭ y (0 Ϲ x Ϲ 1, 0 Ϲ y Ϲ 4), [0, 2z, Ϫ2z] • [0, Ϫ1, 1], Ϯ20
[2e؊z cos y, Ϫe؊z, 0] • [0, Ϫy, 1] ϭ ye؊z, Ϯ(2 – 2> 1e)
[0, 2z, 32 ] • [0, 0, 1] ϭ 32 , Ϯ32 a 2
[Ϫez, Ϫex, Ϫey] • [Ϫ2x, 0, 1], Ϯ(e4 Ϫ 2e ϩ 1)
The sides contribute a, 3a 2>2, Ϫa, 0.
Ϫ2p; curl F ϭ 0
13. 5k, 80p
1
[0, Ϫ1, 2x Ϫ 2y] • [0, 0, 1], 3
r ϭ [cos u, sin u, v], [Ϫ3v2, 0, 0] • [cos u, sin u, 0], Ϫ1
r ϭ [u cos v, u sin v, u], 0 Ϲ u Ϲ 1, 0 Ϲ v Ϲ p>2,
[Ϫez, 1, 0] • [Ϫu cos v, Ϫu sin v, u]. Answer: 1>2

Chapter 10 Review Questions and Problems, page 469
11. r ϭ [4 Ϫ 10t, 2 ϩ 8t], F(r) • dr ϭ [2(4 Ϫ 10t)2, Ϫ4(2t ϩ 8t)2] • [Ϫ10, 8] dt;
Ϫ4528>3. Or using exactness.
13. Not exact, curl F ϭ (5 cos x)k, Ϯ10
15. 0 since curl F ϭ 0
17. By Stokes, Ϯ18p
19. F ϭ grad (y 2 ϩ xz), 2p
21. M ϭ 8, x ϭ 85, y ϭ 16
5
x ϭ 87 ϭ 1.14, y ϭ 118
23. M ϭ 63
20 ,
49 ϭ 2.41
5
4
25. M ϭ 4k>15, x ϭ 16, y ϭ 7
27. 288(a ϩ b ϩ c) p
29. div F ϭ 20 ϩ 6z 2. Answer: 21
31. 24 sinh 1 ϭ 28.205
72p
33. Direct integration, 224
35.
3

Problem Set 11.1, page 482
1. 2p, 2p, p, p, 1, 1, 12, 12

1
1
1
1
cos 3x ϩ
cos 5x ϩ Á ) ϩ 2 (sin x ϩ sin 3x ϩ sin 5x ϩ Á )
9
25
3
5
15. 43 p2 ϩ 4 (cos x ϩ 14 cos 2x ϩ 19 cos 3x ϩ Á ) Ϫ 4p (sin x ϩ 12 sin 2x ϩ
1
Á)
3 sin 3x ϩ
p 4
1
1
cos 5x ϩ Á b
17. ϩ acos x ϩ cos 3x ϩ
2
p
9
25
13.

4

5. There is no smallest p Ͼ 0.

p

(cos x ϩ

bapp02.qxd

11/4/10

A28

7:49 AM

Page A28

App. 2 Answers to Odd-Numbered Problems

2
1
1
1
Ϫ acos x ϩ cos 3x ϩ
cos 5x ϩ Á b ϩ sin x Ϫ sin 2x ϩ
4
p
9
25
2
1
sin 3x Ϫ ϩ Á
3
21. 2 (sin x ϩ 12 sin 2x ϩ 13 sin 3x ϩ 14 sin 4x ϩ 15 sin 5x ϩ Á )
19.

p

Problem Set 11.2, page 490
1. Neither, even, odd, odd, neither
3. Even
5. Even
4
px 1
3px
1
5px
asin
9. Odd, L ϭ 2,
ϩ sin
ϩ sin
ϩÁb
p
2
3
2
5
2
1
4
1
1
11. Even, L ϭ 1,
Ϫ 2 acos px Ϫ cos 2px ϩ cos 3px Ϫ ϩ Á b
p
3
4
9
13. Rectifier, L ϭ

1
,
2

1
1
1
1
Ϫ 2 acos 2px ϩ cos 6px ϩ
cos 10px ϩ Á b ϩ
p
8
9
25

1
1
1
1
a sin 2px Ϫ sin 4px ϩ sin 6px Ϫ sin 8px ϩ Ϫ Á b
4
6
8
1
4
1
sin 5x Ϫ ϩ Á b
15. Odd, L ϭ p,
asin x Ϫ sin 3x ϩ
p
9
25
4
1
1
1
17. Even, L ϭ 1,
ϩ 2 acos px ϩ cos 3px ϩ
cos 5px ϩ Á b
2
9
25
p
1

p 2

ϩ 12 cos 2x ϩ 18 cos 4x
4
px 1
3px
1
5px
23. L ϭ 4, (a) 1, (b) asin
ϩ sin
ϩ sin
ϩÁb
p
4
3
4
5
4
p 4
1
1
25. L ϭ p, (a) ϩ acos x ϩ cos 3x ϩ
cos 5x ϩ Á b ,
2
p
9
25
(b) 2 (sin x ϩ 12 sin 2x ϩ 13 sin 3x ϩ 14 sin 4x ϩ Á )
3p
2
1
1
1
27. L ϭ p, (a)
ϩ acos x Ϫ cos 2x ϩ cos 3x ϩ
cos 5x Ϫ
8
p
2
9
25
1
1
1
1
1
cos 6x ϩ
cos 7x ϩ
cos 9x Ϫ
cos 10x ϩ
cos 11x ϩ Á b
18
49
81
50
121
2
1
1
2
1
(b) a1 ϩ b sin x ϩ sin 2x ϩ a Ϫ
b sin 3x ϩ sin 4x ϩ
p
2
3
9p
4
1
2
1
a ϩ
b sin 5x ϩ sin 6x ϩ Á
5
25p
6
29. Rectifier, L ϭ p,
2
4
1
1
1
(a) Ϫ a # cos x ϩ # cos 3x ϩ # cos 5x ϩ Á b , (b) sin x
p p 1 3
3 5
5 7
19.

3
8

Problem Set 11.3, page 494
3. The output becomes a pure cosine series.
5. For An this is similar to Fig. 54 in Sec. 2.8, whereas for the phase shift Bn
the sense is the same for all n.

bapp02.qxd

11/4/10

7:49 AM

Page A29

App. 2 Answers to Odd-Numbered Problems

A29

7. y ϭ C1 cos vt ϩ C2 sin vt ϩ a (v) sin t, a (v) ϭ 1>(v2 Ϫ 1) ϭ Ϫ1.33,
Ϫ5.26, 4.76, 0.8, 0.01. Note the change of sign.
4
1
1
11. y ϭ C1 cos vt ϩ C2 sin vt ϩ a 2
sin t ϩ 2
sin 3t ϩ
p v Ϫ9
v Ϫ 49
1
sin 5t ϩ Á b
2
v Ϫ 121
N

13. y ϭ a (An cos nt ϩ Bn sin nt),

An ϭ [(1 Ϫ n 2)an Ϫ nbnc]>Dn,

nϭ1

Bn ϭ [(1 Ϫ n 2)bn ϩ ncan]>Dn,
15. bn ϭ (Ϫ1)nϩ1 # 12 >n 3 (n odd),

Dn ϭ (1 Ϫ n 2)2 ϩ n 2c2
ؕ

y ϭ a (An cos nt ϩ Bn sin nt),
nϭ1

An ϭ (Ϫ1)n # 12nc>n 3Dn, Bn ϭ (Ϫ1)nϩ1 # 12(1 Ϫ n 2)>(n 3Dn) with Dn as in
Prob. 13.
17. I ϭ 50 ϩ A1 cos t ϩ B1 sin t ϩ A3 cos 3t ϩ B3 sin 3t ϩ Á , An ϭ (10 Ϫ n 2) an>Dn,
Bn ϭ 10nan>Dn, an ϭ Ϫ400>(n 2p), Dn ϭ (n 2 Ϫ 10)2 ϩ 100n 2
ؕ

19. I (t) ϭ a (An cos nt ϩ Bn sin nt),

An ϭ (Ϫ1)nϩ1

2400 (10 Ϫ n 2)

nϭ1

Bn ϭ (Ϫ1)nϩ1

24,000
nDn

,

n 2Dn

,

Dn ϭ (10 Ϫ n 2)2 ϩ 100n 2

Section 11.4, page 498
4
1
1
Ϫ acos x ϩ cos 3x ϩ
cos 5x ϩ Á b , E* ϭ 0.0748,
2
p
9
25
0.0748, 0.0119, 0.0119, 0.0037
4
1
1
5. F ϭ asin x ϩ sin 3x ϩ sin 5x ϩ Á b , E* ϭ 1.1902, 1.1902, 0.6243, 0.6243,
p
3
5
0.4206 (0.1272 when N ϭ 20)
1
7. F ϭ 2 [(p2 Ϫ 6) sin x Ϫ 18 (4p2 Ϫ 6) sin 2x ϩ 27
(9p2 Ϫ 6) sin 3x Ϫ ϩ Á ];
E* ϭ 674.8, 454.7, 336.4, 265.6, 219.0. Why is E* so large?
3. F ϭ

p

Section 11.5, page 503
3.
7.
9.
11.
13.

Set x ϭ ct ϩ k.
5. x ϭ cos u, dx ϭ Ϫsin u du, etc.
lm ϭ (mp>10)2, m ϭ 1, 2, Á ; ym ϭ sin (mpx>10)
l ϭ [(2m ϩ 1)p>(2L)]2, m ϭ 0, 1, Á , ym ϭ sin ((2m ϩ 1) px>(2L))
lm ϭ m 2, m ϭ 1, 2, Á , ym ϭ x sin (m ln ƒ x ƒ )
p ϭ e8x, q ϭ 0, r ϭ e8x, lm ϭ m 2, ym ϭ e؊4x sin mx, m ϭ 1, 2, Á

Section 11.6, page 509
1. 8 (P1(x) Ϫ P3(x) ϩ P5(x))
8
3. 45 P0(x) Ϫ 47 P2(x) Ϫ 35
P4(x)
9. Ϫ0.4775P1(x) Ϫ 0.6908P3(x) ϩ 1.844P5(x) Ϫ 0.8236P7(x) ϩ 0.1658P9(x) ϩ Á ,
m 0 ϭ 9. Rounding seems to have considerable influence in Probs. 8–13.

bapp02.qxd

11/4/10

A30

7:49 AM

Page A30

App. 2 Answers to Odd-Numbered Problems

11. 0.7854P0(x) Ϫ 0.3540P2(x) ϩ 0.0830P4(x) Ϫ Á , m 0 ϭ 4
13. 0.1212P0(x) Ϫ 0.7955P2(x) ϩ 0.9600P4(x) Ϫ 0.3360P6(x) ϩ Á , m 0 ϭ 8
15. (c) am ϭ (2>J 21(a0,m)) (J1(a0,m)>a0,m) ϭ 2>(a0,mJ1(a0,m))

Section 11.7, page 517
1. f (x) ϭ pe؊x(x Ͼ 0) gives A ϭ

Ύ

ؕ

e؊v cos wv dv ϭ

0

1
1ϩw

2

,Bϭ

w
1 ϩ w2

(see Example 3), etc.
ؕ
2
p
1 Ϫ cos pw
3. Use (11); B ϭ
sin wv dv ϭ
p
2
w

Ύ

0

5. B (w) ϭ
2
7. p

Ύ

ؕ

2

p

Ύ

1

0

1
sin w Ϫ w cos w
pv sin wv dv ϭ
2
w2

sin w cos xw
dw
w

0

9. A (w) ϭ

p Ύ
2

ؕ

0

11.

2

p

Ύ

ؕ

0

cos wv
dv ϭ e؊w (w Ͼ 0)
1 ϩ v2

cos pw ϩ 1
cos xw dw
1 Ϫ w2

15. For n ϭ 1, 2, 11, 12, 31, 32, 49, 50 the value of Si (np) Ϫ p>2 equals 0.28, Ϫ0.15,
0.029, Ϫ0.026, 0.0103, Ϫ0.0099, 0.0065, Ϫ0.0064 (rounded).
2

17. p
19.

Ύ

0
ؕ

p Ύ
2

ؕ

1 Ϫ cos w
sin xw dw
w
w Ϫ e (w cos w Ϫ sin w)

0

1 ϩ w2

sin xw dw

Section 11.8, page 522
1.
3.

fˆc (w) ϭ 1(2> p) (2 sin w Ϫ sin 2w)>w
fˆc (w) ϭ 1(2> p) (cos 2w ϩ 2w sin 2w Ϫ 1)>w 2

2
2 (w Ϫ 2) sin w ϩ 2w cos w
Bp
w3
7. Yes. No
9. 12> p w>(a 2 ϩ w 2)
2
3
11. 12> p ((2 Ϫ w ) cos w ϩ 2w sin w Ϫ 2)>w
2 #
1
2 #
1
1
2
2
w
13. fs(e؊x) ϭ aϪfc(e؊x) ϩ
1b ϭ a
ϩ
bϭ
w
w B p w2 ϩ 1 B p
Bp
B p w2 ϩ 1

5. fˆc (w) ϭ

Problem Set 11.9, page 533
3.
5.
7.
11.

i (e؊ibw Ϫ e؊iaw)>(w12p) if a Ͻ b; 0 otherwise
[e(1؊iw)a Ϫ e؊(1؊iw)a]>( 12p(1 Ϫ iw))
(e؊iaw(1 ϩ iaw) Ϫ 1)>(12pw 2)
9. 12> p(cos w ϩ w sin w Ϫ 1)>w 2
2
i12> p (cos w Ϫ 1)>w
13. e؊w >2 by formula 9

bapp02.qxd

11/4/10

7:49 AM

Page A31

App. 2 Answers to Odd-Numbered Problems

A31

17. No, the assumptions in Theorem 3 are not satisfied.
19. [ f1 ϩ f2 ϩ f3 ϩ f4, f1 Ϫ if2 Ϫ f3 ϩ if4, f1 Ϫ f2 ϩ f3 Ϫ f4,
1
1 f1
f1 ϩ f2
21. c
dc d ϭ c
d
1 Ϫ1 f2
f1 Ϫ f2

f1 ϩ if2 Ϫ f3 Ϫ if4]

Chapter 11 Review Questions and Problems, page 537
1
3px
1
5px
sin
ϩ sin
ϩÁb
3
2
5
2
1
2
1
1
13. Ϫ 2 acos px ϩ cos 3px ϩ
cos 5px ϩ Á b ϩ
4
9
25
p
4

11. 1 ϩ

19.
21.

23.
25.
27.

asin

px
2

ϩ

asin px Ϫ

1
1
sin 2px ϩ sin 3px Ϫ ϩ Á b
2
3
17. Cf. Sec. 11.1.
cosh x, sinh x (Ϫ5 Ͻ x Ͻ 5), respectively
1
4
1
2
1
Ϫ 2 acos px ϩ cos 3px ϩ Á b ,
asin px Ϫ sin 2px ϩ Ϫ Á b
2
9
p
2
p
p2
cos t
1 # cos 2t
1 # cos 3t
y ϭ C1 cos vt ϩ C2 sin vt ϩ 2 Ϫ 12 a 2
Ϫ
ϩ
2
4 v Ϫ4
9 v2 Ϫ 9
v
v Ϫ1
1 # cos 4t
Ϫ
ϩϪÁb
16 v2 Ϫ 16
0.82, 0.50, 0.36, 0.28, 0.23
0.0076, 0.0076, 0.0012, 0.0012, 0.0004
1

p

15.

p

Ύ

1

p

ؕ

0

(cos w ϩ w sin w Ϫ 1) cos wx ϩ (sin w Ϫ w cos w) sin wx
dw
w2

29. 12> p (cos aw Ϫ cos w ϩ aw sin aw Ϫ w sin w)>w 2

Problem Set 12.1, page 542
1.
3.
7.
15.
19.
21.
23.
25.

L(c1u 1 ϩ c2u 2) ϭ c1L(u 1) ϩ c2L(u 2) ϭ c1 # 0 ϩ c2 # 0 ϭ 0
5. c ϭ a>b
cϭ2
Any c and v
9. c ϭ p>25
17. u ϭ a( y) cos 4px ϩ b( y) sin 4px
u ϭ 110 Ϫ (110>ln 100) ln (x 2 ϩ y 2)
3
u ϭ c(x) e؊y >3
u ϭ e؊3y(a(x) cos 2y ϩ b(x) sin 2y) ϩ 0.1e3y
u ϭ c1( y)x ϩ c2( y)>x 2 (Euler–Cauchy)
u(x, y) ϭ axy ϩ bx ϩ cy ϩ k; a, b, c, k arbitrary constants

Problem Set 12.3, page 551
5. k cos 3pt sin 3px
8k
1
1
7. 3 acos pt sin px ϩ
cos 3pt sin 3px ϩ
cos 5pt sin 5px ϩ Á b
27
125
p
9.

0.8
2

p

acos pt sin px Ϫ

1
1
cos 3pt sin 3px ϩ
cos 5pt sin 5px Ϫ ϩ Á b
9
25

bapp02.qxd

11/4/10

A32

7:49 AM

Page A32

App. 2 Answers to Odd-Numbered Problems

11.

2

p2
ϩ

13.

a(2 Ϫ 12) cos pt sin px Ϫ

1
(2 ϩ 12) cos 3pt sin 3px
9

1
(2 ϩ 12) cos 5pt sin 5px Ϫ ϩ Á b
25

4

p3

a(4 Ϫ p) cos pt sin px ϩ cos 2pt sin 2px ϩ

4 ϩ 3p
cos 3pt sin 3px
27

4 Ϫ 5p
cos 5pt sin 5px ϩ Á b . No terms with n ϭ 4, 8, 12, Á .
125
8L2
p 2
px
1
3p 2
3px
17. u ϭ 3 acos c c a b t d sin
ϩ 3 cos c c a b t d sin
ϩÁb
p
3
L
L
L
L
ϩ

19. (a) u(0, t) ϭ 0, (b) u(L, t) ϭ 0, (c) u x(0, t) ϭ 0, (d) u x(L, t) ϭ 0. C ϭ ϪA, D ϭ ϪB
from (a), (c). Insert this. The coefficient determinant resulting from (b), (d) must be
zero to have a nontrivial solution. This gives (22).

Problem Set 12.4, page 556
3. c2 ϭ 300>[0.9>(2 # 9.80)] ϭ 80.832 [m2>sec2]
9. Elliptic, u ϭ f1( y ϩ 2ix) ϩ f2( y Ϫ 2ix)
11. Parabolic, u ϭ x f1(x Ϫ y) ϩ f2(x Ϫ y)
13. Hyperbolic, u ϭ f1( y Ϫ 4x) ϩ f2( y Ϫ x)

1
xy r 2 ϩ yy r ϭ 0, y ϭ v, xy ϭ w, u w ϭ z, u ϭ y f1(xy) ϩ f2( y)
17. Elliptic, u ϭ f1( y Ϫ (2 Ϫ i)x) ϩ f2( y Ϫ (2 ϩ i)x). Real or imaginary parts of any
function u of this form are solutions. Why?
15. Hyperbolic,

Problem Set 12.6, page 566
3. u 1 ϭ sin x e؊t, u 2 ϭ sin 2x e؊4t, u 3 ϭ sin 3x e؊9t differ in rapidity of decay.
2
5. u ϭ sin 0.1px e؊1.752p t>100
2
2
800
1
7. u ϭ 3 asin 0.1px e؊0.01752p t ϩ 3 sin 0.3px e؊0.01752(3p) t ϩ Á b
p
3
9. u ϭ u I ϩ u II, where u II ϭ u Ϫ u I satisfies the boundary conditions of the text,
ؕ

npx ؊(cnp>L)2t
2
so that u II ϭ a Bn sin
e
, Bn ϭ
L
L
nϭ1

L

Ύ [ f (x) Ϫ u (x)] sin npL x dx.
I

0

11. F ϭ A cos px ϩ B sin px, F r (0) ϭ Bp ϭ 0, B ϭ 0, F r (L) ϭ ϪAp sin pL ϭ 0,
p ϭ np>L, etc.
13. u ϭ 1
1
4
1
1
15. ϩ 2 acos x e؊t ϩ cos 3x e؊9t ϩ
cos 5x e؊25t ϩ Á b
p
2
9
25
17. Ϫ

2
Kp ؕ
nBn e؊lnt
a
L nϭ1

19. u ϭ 1000 (sin 12 px sinh 12 py)>sinh p
21. u ϭ

ؕ

1
(2n Ϫ 1)px
(2n Ϫ 1)py
sin
sinh
a
p nϭ1 (2n Ϫ 1) sinh (2n Ϫ 1)p
24
24

100

bapp02.qxd

11/4/10

7:49 AM

Page A33

App. 2 Answers to Odd-Numbered Problems
ؕ

23. u ϭ A0 x ϩ a An
nϭ1

A0 ϭ

Ύ

1
2

24

A33

sinh (npx>24)
npy
cos
,
sinh np
24

24

An ϭ

f ( y) dy,

0

Ύ
12
1

24

npy

f ( y) cos

dy

24

0

a

ؕ
npx
np(b Ϫ y)
2
sinh
, An ϭ
25. a An sin
a sinh (npb>a)
a
a
nϭ1

Ύ f (x) sin npa x dx
0

Problem Set 12.7, page 574
3. A ϭ

pΎ
2

ؕ

0

5. A ϭ
7. A ϭ

cos pv
2 # p ؊p
dv ϭ
e ,uϭ
2
1ϩv
p 2

Ύ

ؕ

e؊p؊c

2

p2t

cos px dp

0

1

Ύ v cos pv dv ϭ p2 # cos p ϩ pp sin p Ϫ 1, etc.

2

2

p
2

p

0

Ύ

ؕ

0

2 # p
sin v
cos pv dv ϭ
ϭ 1 if 0 Ͻ p Ͻ 1 and 0 if p Ͼ 1,
v
p 2

1

uϭ

Ύ cos px e

؊c2p2t

dp

0

9. Set w ϭ Ϫv in (21) to get erf (Ϫx) ϭ Ϫerf x.
13. In (12) the argument x ϩ 2cz 1t is 0 (the point where f jumps) when z ϭ Ϫx>(2c 1t).
This gives the lower limit of integration.
15. Set w ϭ s> 12 in (21).

Problem Set 12.9, page 584
(a), (b) It is multiplied by 12. (c) Half
Bmn ϭ (Ϫ1)nϩ18>(mnp2) if m odd, 0 if m even
Bmn ϭ (Ϫ1)mϩn4ab>(mnp2)
u ϭ 0.1 cos 120t sin 2x sin 4y
ؕ
ؕ
6.4
1
cos (t 2m 2 ϩ n 2) sin mx sin ny
13. 2 a a
p mϭ1 nϭ1 m 3n 3
1.
5.
7.
11.

m,n odd

17. cp 1260 (corresponding eigenfunctions F4,16 and F16,14), etc.
4py
36
4
6px
sin
ϩ 2 b sin
19. cos a pt
2
a
b
a
b
B

Problem Set 12.10, page 591
5. 110 ϩ

440

7. 55p Ϫ

440

p
p

(r cos u Ϫ

1 3
1
r cos 3u ϩ r 5 cos 5u Ϫ ϩ Á )
3
5

(r cos u ϩ

1 3
1 5
r cos 3u ϩ
r cos 5u ϩ Á )
9
25

bapp02.qxd

11/4/10

A34

7:49 AM

Page A34

App. 2 Answers to Odd-Numbered Problems

11. Solve the problem in the disk r Ͻ a subject to u 0 (given) on the upper semicircle
and Ϫu 0 on the lower semicircle.
1
1
4u r
u ϭ 0 a sin u ϩ 3 r 3 sin 3u ϩ 5 r 5 sin 5u ϩ Á b
p a
3a
5a
13. Increase by a factor 12
15. T ϭ 6.826rR2f 21
17. No
25. a11>(2p) ϭ 0.6098; See Table A1 in App. 5.

Problem Set 12.11, page 598
5. A4 ϭ A6 ϭ A8 ϭ A10 ϭ 0, A5 ϭ 605>16, A7 ϭ Ϫ4125>128, A9 ϭ 7315>256
9. ٌ2u ϭ u s ϩ 2u r >r ϭ 0, u s >u r ϭ Ϫ2>r, ln ƒ u r ƒ ϭ Ϫ2 ln ƒ r ƒ ϩ c1,
ur ϭ ~
c >r 2, u ϭ c>r ϩ k
13. u ϭ 320>r ϩ 60 is smaller than the potential in Prob. 12 for 2 Ͻ r Ͻ 4.
17. u ϭ 1
19. cos 2␾ ϭ 2 cos2 ␾ Ϫ 1, 2w 2 Ϫ 1 ϭ 43 P2(w) Ϫ 13, u ϭ 43 r 2P2(cos ␾) Ϫ 13
25. Set 1>r ϭ r. Then u(r, u, ␾) ϭ rv(r, u, ␾), u r ϭ (v ϩ rvr)(Ϫ1>r2),
u rr ϭ (2vr ϩ rvrr)(1>r4) ϩ (v ϩ rvr)(2>r3), u rr ϩ (2>r)u r ϭ r 5(vrr ϩ (2>r)vr).
Substitute this and u ␾␾ ϭ rv␾␾ etc. into (7) [written in terms of r] and divide by r 5.

Problem Set 12.12, page 602
c(s)

x

, W(0, s) ϭ 0, c(s) ϭ 0, w(x, t) ϭ x(t Ϫ 1 ϩ e؊t)
x
s (s ϩ 1)
#
7. w ϭ f (x)g(t), xf rg ϩ fg ϭ xt, take f (x) ϭ x to get g ϭ ce؊t ϩ t Ϫ 1 and c ϭ 1 from
w(x, 0) ϭ x(c Ϫ 1) ϭ 0.
11. Set x 2>(4c2t) ϭ z 2. Use z as a new variable of integration. Use erf (ϱ) ϭ 1.
5. W ϭ

s

ϩ

2

Chapter 12 Review Questions and Problems, page 603
17.
21.
25.
27.
29.
39.

19. Hyperbolic, f1(x) ϩ f2( y ϩ x)
u ϭ c1(x)e؊3y ϩ c2(x)e2y Ϫ 3
Hyperbolic, f1( y ϩ 2x) ϩ f2( y Ϫ 2x)
23. 34 cos 2t sin x Ϫ 14 cos 6t sin 3x
sin 0.01px e؊0.001143t
3
؊0.001143t
Ϫ 14 sin 0.03px e؊0.01029t
4 sin 0.01px e
100 cos 2x e؊4t
u ϭ (u 1 Ϫ u 0)(ln r)>ln (r1>r0) ϩ (u 0 ln r1 Ϫ u 1 ln r0)>ln (r1>r0)

Problem Set 13.1, page 612
1.
5.
11.
15.
19.

1>i ϭ i>i 2 ϭ Ϫi, 1>i 3 ϭ i>i 4 ϭ i
x Ϫ iy ϭ Ϫ(x ϩ iy), x ϭ 0
Ϫ8 Ϫ 6i
3Ϫi
(x 2 Ϫ y 2)>(x 2 ϩ y 2), 2xy>(x 2 ϩ y 2)

3.
9.
13.
17.

4.8 Ϫ 1.4i
Ϫ117, 4
Ϫ120 Ϫ 40i
Ϫ4x 2y 2

Problem Set 13.2, page 618
1. 12 (cos 14 p ϩ i sin 14 p)
3. 2(cos 12 p ϩ i sin 12 p), 2(cos 12 p Ϫ i sin 12 p)

bapp02.qxd

11/4/10

7:49 AM

Page A35

App. 2 Answers to Odd-Numbered Problems

5.
9.
13.
17.
23.
25.
27.
29.
33.

1
2 (cos

p ϩ i sin p)

A35

7.
11.
15.
21.

21 ϩ 14 p2 (cos arctan 12 p ϩ i sin arctan 12 p)
Ϯarctan ( 43 ) ϭ Ϯ0.9273
Ϫ3i
6
1
1
2 2 (cos 12
kp ϩ i sin 12
p), k ϭ 1, 9, 17

3p>4
Ϫ1024. Answer: p
2 ϩ 2i
6, Ϫ3 Ϯ 313i
cos (18 p ϩ 12 kp) ϩ i sin (18 p ϩ 12 kp), k ϭ 0, 1, 2, 3
cos 15 p Ϯ i sin 15 p, cos 35 p Ϯ i sin 35 p, Ϫ1
31. Ϯ(1 Ϫ i), Ϯ(2 ϩ 2i)
i, Ϫ1 Ϫ i
ƒ z 1 ϩ z 2 ƒ 2 ϭ (z 1 ϩ z 2)( z 1 ϩ z 2 ) ϭ (z 1 ϩ z 2)( z 1 ϩ z 2). Multiply out and use
Re z 1z 2 Ϲ ƒ z 1z 2 ƒ (Prob. 34).
z 1z 1 ϩ z 1z 2 ϩ z 2z 1 ϩ z 2z 2 ϭ ƒ z 1 ƒ 2 ϩ 2 Re z 1z 2 ϩ ƒ z 2 ƒ 2 Ϲ ƒ z 1 ƒ 2
ϩ 2 ƒ z 1 ƒ ƒ z 2 ƒ ϩ ƒ z 2 ƒ 2 ϭ ( ƒ z 1 ƒ ϩ ƒ z 2 ƒ )2. Hence ƒ z 1 ϩ z 1 ƒ 2 Ϲ ( ƒ z 1 ƒ ϩ ƒ z 2 ƒ )2. Taking
square roots gives (6).
35. [(x 1 ϩ x 2)2 ϩ ( y1 ϩ y2)2] ϩ [(x 1 Ϫ x 2)2 ϩ ( y1 Ϫ y2)2] ϭ 2(x 21 ϩ y 21 ϩ x 22 ϩ y 22)

Problem Set 13.3, page 624
1. Closed disk, center Ϫ1 ϩ 5i, radius 32
3. Annulus (circular ring), center 4 Ϫ 2i, radii p and 3p
5. Domain between the bisecting straight lines of the first quadrant and the fourth
quadrant.
7. Half-plane extending from the vertical straight line x ϭ Ϫ1 to the right.
11. u(x, y) ϭ (1 Ϫ x)>((1 Ϫ x)2 ϩ y 2), u(1, Ϫ1) ϭ 0,
v(x, y) ϭ y((1 Ϫ x)2 ϩ y 2), v(1, Ϫ1) ϭ Ϫ1
15. Yes, since Im ( ƒ z ƒ 2>z) ϭ Im ( ƒ z ƒ 2 z>(zz)) ϭ Im z ϭ Ϫr sin u : 0.
17. Yes, because Re z ϭ r cos u : 0 and 1 Ϫ ƒ z ƒ : 1 as r : 0.
19. f r (z) ϭ 8(z Ϫ 4i)7. Now z Ϫ 4i ϭ 3, hence f r (3 ϩ 4i) ϭ 8 # 37 ϭ 17,496.
21. n(1 Ϫ z) ؊n؊1i, ni
23. 3iz 2>(z ϩ i)4, Ϫ3i>16

Problem Set 13.4, page 629
1. rx ϭ x>r ϭ cos u, ry ϭ sin u, ux ϭ Ϫ(sin u)>r, uy ϭ (cos u)>r
(a) 0 ϭ u x Ϫ vy ϭ u r cos u ϩ u u(Ϫsin u)>r Ϫ vr sin u Ϫ vu (cos u)>r
(b) 0 ϭ u y ϩ vx ϭ u r sin u ϩ u u(cos u)>r ϩ vr cos u ϩ vu(Ϫsin u)>r
Multiply (a) by cos u, (b) by sin u, and add. Etc.
3. Yes
5. No, f (z) ϭ (z 2)
7. Yes, when z 0. Use (7).
9. Yes, when z 0, Ϫ2pi, 2pi
11. Yes
13. f (z) ϭ Ϫ12 i(z 2 ϩ c), c real
15. f (z) ϭ 1>z ϩ c (c real)
17. f (z) ϭ z 2 ϩ z ϩ c (c real)
19. No
21. a ϭ p, v ϭ epx sin py
1
2
2
23. a ϭ 0, v ϭ 2 b( y Ϫ x ) ϩ c 27. f ϭ u ϩ iv implies if ϭ Ϫv ϩ iu.
29. Use (4), (5), and (1).

Problem Set 13.5, page 632
3. e2pie؊2p ϭ e؊2p ϭ 0.001867
7. e 12i ϭ 4.113i
11. 6.3epi

5. e2(Ϫ1) ϭ Ϫ7.389
9. 5ei arctan (3>4) ϭ 5e0.644i
13. 12epi>4

bapp02.qxd

11/4/10

A36

7:49 AM

Page A36

App. 2 Answers to Odd-Numbered Problems

15. exp (x 2 Ϫ y 2) cos 2xy, exp (x 2 Ϫ y 2) sin 2xy
17. Re (exp (z 3)) ϭ exp (x 3 Ϫ 3xy 2) cos (3x 2y Ϫ y 3)
19. z ϭ 2npi, n ϭ 0, 1, Á

Problem Set 13.6, page 636
1.
9.
15.
17.

Use (11), then (5) for eiy, and simplify. 7. cosh 1 ϭ 1.543, i sinh 1 ϭ 1.175i
Both Ϫ0.642 Ϫ 1.069i. Why?
11. i sinh p ϭ 11.55i, both
Insert the definitions on the left, multiply out, and simplify.
z ϭ Ϯ(2n ϩ 1)i>2
19. z ϭ Ϯnpi

Problem Set 13.7, page 640
5. ln 11 ϩ pi
9. i arctan (0.8>0.6) ϭ 0.927i
13. Ϯ2npi, n ϭ 0, 1, Á
15. ln ƒ ei ƒ ϩ i arctan
17.
19.
21.
23.
25.
27.

7. 12 ln 32 Ϫ pi>4 ϭ 1.733 Ϫ 0.785i
11. ln e ϩ pi>2 ϭ 1 ϩ pi>2

sin 1
Ϯ 2npi ϭ 0 ϩ i ϩ 2npi,
cos 1

n ϭ 0, 1, Á

ln (i 2) ϭ ln (Ϫ1) ϭ (1 Ϯ 2n)pi, 2 ln i ϭ (1 Ϯ 4n)pi, n ϭ 0, 1, Á
e4؊3i ϭ e4 (cos 3 Ϫ i sin 3) ϭ Ϫ54.05 Ϫ 7.70i
e0.6e0.4i ϭ e0.6 (cos 0.4 ϩ i sin 0.4) ϭ 1.678 ϩ 0.710i
e(1؊i) Ln (1ϩi) ϭ eln12 ϩ pi>4؊i ln12 ϩ p>4 ϭ 2.8079 ϩ 1.3179i
e(3؊i)(ln 3ϩpi) ϭ 27ep(cos (3p Ϫ ln 3) ϩ i sin (3p Ϫ ln 3)) ϭ Ϫ284.2 ϩ 556.4i
e(2؊i) Ln (؊1) ϭ e(2؊i)pi ϭ ep ϭ 23.14

Chapter 13 Review Questions and Problems, page 641
1.
11.
15.
19.
23.
27.
31.
33.
35.

2 Ϫ 3i
3. 27.46e0.9929i, 7.616e1.976i
Ϫ5 ϩ 12i
13. 0.16 Ϫ 0.12i
i
17. 412e؊3pi>4
؊pi>2
15e
21. Ϯ3, Ϯ3i
(Ϯ1 Ϯ i)> 12
25. f (z) ϭ Ϫiz 2>2
2
؊2z
f (z) ϭ e
29. f (z) ϭ e؊z >2
cos 3 cosh 1 ϩ i sin 3 sinh 1 ϭ Ϫ1.528 ϩ 0.166i
i tanh 1 ϭ 0.7616i
cosh p cos p ϩ i sinh p sin p ϭ Ϫ11.592

Problem Set 14.1, page 651
1.
3.
5.
7.
9.
11.
13.

Straight segment from (2, 1) to (5, 2.5).
Parabola y ϭ x 2 from (1, 2) to (2, 8).
Circle through (0, 0), center (3, Ϫ1), radius 110, oriented clockwise.
Semicircle, center 2, radius 4.
Cubic parabola y ϭ x 3 (Ϫ2 Ϲ x Ϲ 2)
z(t) ϭ t ϩ (2 ϩ t)i (Ϫ1 Ϲ t Ϲ 1)
z(t) ϭ 2 Ϫ i ϩ 2eit (0 Ϲ t Ϲ p)

bapp02.qxd

11/4/10

7:49 AM

Page A37

App. 2 Answers to Odd-Numbered Problems

A37

z(t) ϭ 2 cosh t ϩ i sinh t (Ϫϱ Ͻ t Ͻ ϱ)
Circle z(t) ϭ Ϫa Ϫ ib ϩ re؊it (0 Ϲ t Ϲ 2p)
z(t) ϭ t ϩ (1 Ϫ 14 t 2)i (Ϫ2 Ϲ t Ϲ 2)
z(t) ϭ (1 ϩ i)t (1 Ϲ t Ϲ 3), Re z ϭ t, z r (t) ϭ 1 ϩ i. Answer: 4 ϩ 4i
e2pi Ϫ epi ϭ 1 Ϫ (Ϫ1) ϭ 2
1
1 ؊1
2 i
Ϫ e1) ϭ Ϫsinh 1
2 exp z ƒ 1 ϭ 2 (e
1
1
tan 4 pi Ϫ tan 4 ϭ i tanh 14 Ϫ 1
Im z 2 ϭ 2xy ϭ 0 on the axes. z ϭ 1 ϩ (Ϫ1 ϩ i)t (0 Ϲ t Ϲ 1),
#
(Im z 2) z ϭ 2(1 Ϫ t)y(Ϫ1 ϩ i) integrated: (Ϫ1 ϩ i)>3.
35. ƒ Re z ƒ ϭ ƒ x ƒ Ϲ 3 ϭ M on C, L ϭ 18
15.
17.
19.
21.
23.
25.
27.
29.

Problem Set 14.2, page 659
1.
7.
9.
13.
17.
21.
25.
29.

Use (12), Sec. 14.1, with m ϭ 2.
3. Yes
5. 5
(a) Yes. (b) No, we would have to move the contour across Ϯ2i.
0, yes
11. pi, no
0, yes
15. Ϫp, no
0, no
19. 0, yes
2pi
23. 1>z ϩ 1>(z Ϫ 1), hence 2pi ϩ 2pi ϭ 4pi.
0 (Why?)
27. 0 (Why?)
0

Problem Set 14.3, page 663
1. 2piz 2>(z Ϫ 1) ƒ zϭ؊1 ϭ Ϫpi
5. 2pi(cos 3z)>6 ƒ zϭ0 ϭ pi>3
1
p
`
11. 2pi ؒ
ϭ
z ϩ 2i zϭ2i
2

3. 0
7. 2pi(i>2)3>2 ϭ p>8
13. 2pi(z ϩ 2) ƒ zϭ2 ϭ 8pi

15. 2pi cosh (Ϫp2 Ϫ pi) ϭ Ϫ2pi cosh p2 ϭ Ϫ60,739i since cosh pi ϭ cos p ϭ Ϫ1
and sinh pi ϭ i sin p ϭ 0.
Ln (z ϩ 1)
Ln (1 ϩ i)
17. 2pi
`
ϭ 2pi
ϭ p(ln 12 ϩ ip>4) ϭ 1.089 ϩ 2.467i
zϩi
2i
zϭi
19. 2pie2i>(2i) ϭ pe2i

Problem Set 14.4, page 667
1. (2pi>3!)(Ϫcos 0) ϭ Ϫpi>3
5.

3. (2pi>(n Ϫ 1)!)e0

2pi
pi
(cosh 2z) t ϭ
ؒ 8 sinh 1 ϭ 9.845i
3!
3

7. (2pi>(2n)!) (cos z)(2n) ƒ zϭ0 ϭ (2pi>(2n)!)(Ϫ1)n cos 0 ϭ (Ϫ1)n2pi>(2n)!
9. Ϫ2pi(tan pz) r `
11.

ϭ
zϭ0

Ϫ2pi ؒ p
2

cos pz

`

ϭ Ϫ2p2i
zϭ0

2pi
((1 ϩ z)sin z) r `
ϭ 12 pi(sin z ϩ (1 ϩ z) cos z) ƒ zϭ1>2
4
zϭ1>2
ϭ 12 pi(sin 12 ϩ 32 cos 12 )
ϭ 2.821i

bapp02.qxd

11/4/10

A38

7:49 AM

Page A38

App. 2 Answers to Odd-Numbered Problems

1
13. 2pi # z `
ϭ pi
15. 0. Why?
zϭ2
17. 0 by Cauchy’s integral theorem for a doubly connected domain; see (6) in Sec. 14.2.
19. (2pi>2!)4؊3(e3z) s ƒ zϭ pi>4 ϭ Ϫ9p(1 ϩ i)>(64 12)

Chapter 14 Review Questions and Problems, page 668
21.
23.
25.
27.
29.

1
2

cosh (Ϫ14 p2) Ϫ 12 ϭ 2.469
2pi(ez)(4) ƒ zϭ0 ϭ iez>12 ƒ zϭ0 ϭ pi>12 by Cauchy’s integral formula.
Ϫ2pi(tan pz) r |zϭ1 ϭ Ϫ2p2i>cos2 pz|zϭ1 ϭ Ϫ2p2i
0 since z 2 ϩ z Ϫ 2 ϭ 2(x 2 Ϫ y 2) and y ϭ x
Ϫ4pi

Problem Set 15.1, page 679
1.
3.
5.
7.
9.
17.
21.
25.
29.

z n ϭ (2i>2)n; bounded, divergent, Ϯ1, Ϯi
z n ϭ Ϫ12 pi>(1 ϩ 2>(ni)) by algebra; convergent to Ϫpi>2
Bounded, divergent, Ϯ1 ϩ 10i
Unbounded, hence divergent
Convergent to 0, hence bounded
Divergent; use 1>ln n Ͼ 1>n.
19. Convergent; use S1>n 2.
Convergent
23. Convergent
Divergent
By absolute convergence and Cauchy’s convergence principle, for given P Ͼ 0 we
have for every n Ͼ N(P) and p ϭ 1, 2, Á
ƒ z nϩ1 ƒ ϩ Á ϩ ƒ z nϩp ƒ Ͻ P,
hence ƒ z nϩ1 ϩ Á ϩ z nϩp ƒ Ͻ P by (6*), Sec. 13.2, hence convergence by Cauchy’s
principle.

Problem Set 15.2, page 684
1.
3.
5.
7.
13.

No! Nonnegative integer powers of z (or z Ϫ z 0) only!
At the center, in a disk, in the whole plane
Sanz 2n ϭ San(z 2)n, ƒ z 2 ƒ Ͻ R ϭ lim ƒ an>anϩ1 ƒ ; hence ƒ z ƒ Ͻ 1R.
p>2, ϱ
9. i,13
11. 0,226
5
1
Ϫi, 2
15. 2i, 1
17. 1> 12

Problem Set 15.3, page 689
n

3. f ϭ 2n. Apply l’Hôpital’s rule to ln f ϭ (ln n)>n.
5. 2
7. 13
9. 1> 12
7
11. 23
13. 1
15. 34

Problem Set 15.4, page 697
3. 2z 2 Ϫ

(2z 2)3
3!

4
4 10
ϩ Á ϭ 2z 2 Ϫ z 6 ϩ
z Ϫ ϩ Á, Rϭ ϱ
3
15

bapp02.qxd

11/4/10

7:49 AM

Page A39

App. 2 Answers to Odd-Numbered Problems

5.

A39

4
1 12
1 16
Ϫ 14 z 4 ϩ 18 z 8 Ϫ 16
z ϩ 32
z Ϫ ϩ Á, Rϭ 2
2

1
2

1
1
1
1
1
ϩ cos z ϭ 1 Ϫ
z2 ϩ
z4 Ϫ
z6 ϩ Ϫ Á , R ϭ ϱ
2
2
2 ؒ 2!
2 ؒ 4!
2 ؒ 6!
z
1
1
1
1 5
9. a1 Ϫ t 2 ϩ t 4 Ϫ ϩ Á b dt ϭ z Ϫ z 3 ϩ
z ϪϩÁ, Rϭ ϱ
2
8
6
40
7.

Ύ

0

11. z 3>(1!3) Ϫ z 7>(3!7) ϩ z 11>(5!11) Ϫ ϩ Á , R ϭ ϱ
13. (2> 1p)(z Ϫ z 3>3 ϩ z 5>(2!5) Ϫ z 7>(3!7) ϩ Á ), R ϭ ϱ
17. Team Project. (a) (Ln (1 ϩ z)) r ϭ 1 Ϫ z ϩ z 2 Ϫ ϩ Á ϭ 1>(1 ϩ z).
(c) Use that the terms of (sin iy)>(iy) are all positive, so that the sum cannot be zero.
19. 12 ϩ 12 i ϩ 12 i(z Ϫ i) ϩ (Ϫ14 ϩ 14 i)(z Ϫ i)2 Ϫ 14 (z Ϫ i)3 ϩ Á , R ϭ 12
2

4

6

1
1
1
1
1
1
az Ϫ p b ϩ az Ϫ p b Ϫ az Ϫ p b ϩ Ϫ Á , R ϭ ϱ
2!
2
4!
2
6!
2
1
2
3
4
5
2
3
23. Ϫ4 Ϫ 8 i(z Ϫ i) ϩ 16 (z Ϫ i) ϩ 32 i(z Ϫ i) Ϫ 64 (z Ϫ i)4 ϩ Á , R ϭ 2
21. 1 Ϫ

3

5

1
1
1
23
25
25. 2 az Ϫ ib ϩ
az Ϫ ib ϩ
az Ϫ ib ϩ Á , R ϭ ϱ
2
3!
2
5!
2

Problem Set 15.5, page 704
3.
5.
7.
9.
11.
13.
15.
17.

ƒ z ϩ i ƒ Ϲ 13 Ϫ d, d Ͼ 0
ƒ z ϩ 12 i ƒ Ϲ 14 Ϫ d, d Ͼ 0
Nowhere
ƒ z Ϫ 2i ƒ Ϲ 2 Ϫ d, d Ͼ 0
ƒ z n ƒ Ϲ 1 and S1>n 2 converges. Use Theorem 5.
ƒ sinn ƒ z ƒ ƒ Ϲ 1 for all z, and S1>n 2 converges. Use Theorem 5.
R ϭ 4 by Theorem 2 in Sec. 15.2; use Theorem 1.
R ϭ 1> 1p Ͼ 0.56; use Theorem 1.

Chapter 15 Review Questions and Problems, page 706
11. 1
15. 21
19. ϱ,

13. 3
17. ϱ,
cosh 1z

ؕ
z 4n
21. a
,
(2n ϩ 1)!
nϭ0

23.

Rϭϱ

n
1
1
1 ؕ (Ϫ1)
ϩ cos 2z ϭ 1 ϩ a
(2z)2n,
2
2
2 nϭ1 (2n)!
ؕ

25. a

(Ϫ1)nϩ1

nϭ1

n!

e2z

z 2n؊2,

Rϭϱ

Rϭϱ

27. cos [(z Ϫ 12 p) ϩ 12 p] ϭ Ϫ(z Ϫ 12 p) ϩ 16(z Ϫ 12 p)3 Ϫ ϩ Á ϭ Ϫsin (z Ϫ 12 p)
29. ln 3 ϩ

1
1
1
(z Ϫ 3) Ϫ
(z Ϫ 3)2 ϩ
(z Ϫ 3)3 Ϫ ϩ Á , R ϭ 3
3
2ؒ9
3 ؒ 27

bapp02_B.qxd

11/4/10

A40

7:43 AM

Page A40

App. 2 Answers to Odd-Numbered Problems

Problem Set 16.1, page 714
1
1
Ϫ 720
z2 ϩ Ϫ Á , 0 Ͻ ƒ z ƒ Ͻ ϱ
1. z ؊4 Ϫ 12 z ؊2 ϩ 24
1 5
z ϩ Á , 0 Ͻ ƒzƒ Ͻ ϱ
3. z ؊3 ϩ z ؊1 ϩ 12 z ϩ 16 z 3 ϩ 24
؊2
؊1
2
Á
, 0 Ͻ ƒzƒ Ͻ 1
5. z ϩ z ϩ 1 ϩ z ϩ z ϩ
1 ؊1
1
z ϩ 720
z3 ϩ Á , 0 Ͻ ƒ z ƒ Ͻ ϱ
7. z 3 ϩ 12 z ϩ 24
9. exp [1 ϩ (z Ϫ 1)] (z Ϫ 1)؊2 ϭ e ؒ [(z Ϫ 1)؊2 ϩ (z Ϫ 1)؊1 ϩ 12 ϩ 16 (z Ϫ 1) ϩ Á ],
0 Ͻ ƒz Ϫ 1ƒ Ͻ ϱ
[pi ϩ (z Ϫ pi)]2
(pi)2
2pi
1
ϭ
ϩ
ϩ
11.
4
4
3
(z Ϫ pi)
(z Ϫ pi)
(z Ϫ pi)
(z Ϫ pi)2
؊3
ؕ
Ϫ3
zϪi
b (z Ϫ i)؊2 ϭ a a b i ؊3؊n(z Ϫ i)n؊2 ϭ i(z Ϫ i)؊2
13. i ؊3 a1 ϩ
i
n
nϭ0
؊1
Á
Ϫ3(z Ϫ i) Ϫ 6i ϩ 10(z Ϫ i) ϩ
, 0 Ͻ ƒz Ϫ iƒ Ͻ 1
1
(z Ϫ p)2 ϩ Ϫ Á ,
15. (Ϫcos (z Ϫ p))(z Ϫ p)؊2 ϭ Ϫ(z Ϫ p)؊2 ϩ 12 Ϫ 24
0 Ͻ ƒz Ϫ pƒ Ͻ ϱ
ؕ

19. a z 2n,

ƒ z ƒ Ͻ 1,

nϭ0

ؕ

Ϫ a

1
2nϩ2

nϭ0 z
1
2 ) ϭ Ϫ(z

,

ƒzƒ Ͼ 1

1
ϩ 12 p)؊1 ϩ 12 (z ϩ 12 p) Ϫ 24
(z ϩ 12 p)3 ϩ Ϫ Á ,
21. Ϫ(z ϩ 12 p)؊1 cos (z ϩ p
1
ƒz ϩ 2pƒ Ͼ 0
23. z 8 ϩ z 12 ϩ z 16 ϩ Á , ƒ z ƒ Ͻ 1, Ϫz 4 Ϫ 1 Ϫ z ؊4 Ϫ z ؊8 Ϫ Á , ƒ z ƒ Ͼ 1
i
1
ϩ
ϩ i ϩ (z Ϫ i)
25.
zϪi
(z Ϫ i)2

Section 16.2, page 719
0 Ϯ 2p, Ϯ4p, Á , fourth order
3. Ϫ81i, fourth order
Ϯ1, Ϯ2, Á , second order
7. Ϯ(2 ϩ 2i), Ϯi, simple
1
Á
, simple
2 sin 4z, z ϭ 0, Ϯp>4, Ϯp>2,
f (z) ϭ (z Ϫ z 0)ng(z), g(z 0) 0, hence f 2(z) ϭ (z Ϫ z 0)2ng 2(z).
Second-order poles at i and Ϫ2i
Simple pole at ϱ, essential singularity at 1 ϩ i
Fourth-order poles at Ϯnpi, n ϭ 0, 1, Á , essential singularity at ϱ
ez(1 Ϫ ez) ϭ 0, ez ϭ 1, z ϭ Ϯ2npi simple zeros. Answer: simple poles at Ϯ2npi,
essential singularity at ϱ
21. 1, ϱ essential singularities, Ϯ2npi, n ϭ 0, 1, Á , simple poles
1.
5.
9.
11.
13.
15.
17.
19.

Section 16.3, page 725
4
15

at 0
5. Ϯ4i at ϯi
1> p at 0, Ϯ1, Á
9. Ϫ1 at Ϯ2npi
1
z
(e ) s >2! ƒ zϭ pi ϭ Ϫ2 at z ϭ pi
Simple pole at 14 inside C, residue Ϫ1>(2p). Answer: Ϫi
Simple poles at p>2, residue ep>2>(Ϫsin p>2), and at Ϫp>2, residue
e؊p>2>sin p>2 ϭ e؊p>2. Answer: Ϫ4pi sinh p>2
19. 2pi (sinh 12 i)>2 ϭ Ϫp sin 12
21. z ؊5 cos pz ϭ Á ϩ p4>(4!z) Ϫ ϩ Á . Answer: 2p5i>24
3.
7.
11.
15.
17.

bapp02_B.qxd

11/4/10

7:43 AM

Page A41

App. 2 Answers to Odd-Numbered Problems

A41

23. Residues 12 at z ϭ 12 , 2 at z ϭ 13 . Answer: 5pi
25. Simple poles inside C at 2i, Ϫ2i, 3i, Ϫ3i, residues (2i cosh 2i)>(4z 3 ϩ 26z) ƒ zϭ2i ϭ
1
1
1
1
4
10 , 10 , 10 , 10 , respectively. Answer: 2pi ؒ 10

Problem Set 16.4, page 733
1.
5.
9.
13.
17.
19.

2p> 2k 2 Ϫ 1
3. p> 12
5p>12
7. 2ap> 2a 2 Ϫ 1
0. Why? (Make a sketch.)
11. p>2
0. Why?
15. p>3
0. Why?
Simple poles at Ϯ1, i (and Ϫi); 2pi ؒ 14 i ϩ pi(Ϫ14 ϩ 14 ) ϭ Ϫ12 p

21. Simple poles at 1 and Ϯ2pi, residues i and Ϫi. Answer:

p
5

(cos 1 Ϫ e؊2)

23. Ϫp>2
25. 0
27. Let q(z) ϭ (z Ϫ a1)(z Ϫ a2) Á (z Ϫ ak). Use (4) in Sec. 16.3 to form the sum of
the residues 1>q r(a1) ϩ Á ϩ 1>q r(ak) and show that this sum is 0; here k Ͼ 1.

Chapter 16 Review Questions and Problems, page 733
11.
15.
19.
23.

6pi
2pi(25z 2) r ƒ zϭ5 ϭ 500pi
p>6
0. Why?

13. 2pi(Ϫ10 Ϫ 10)
17. 0 (n even), (Ϫ1)(n؊1)>22pi>(n Ϫ 1)! (n odd)
21. p>60
25. Res eiz>(z 2 ϩ 1) ϭ 1>(2ie). Answer: p>e.
zϭi

Problem Set 17.1, page 741
5.
7.
9.
11.
15.
19.
21.
23.
25.
29.
31.
33.
35.

Only in size
x ϭ c, w ϭ Ϫy ϩ ic; y ϭ k, w ϭ Ϫk ϩ ix
Parallel displacement; each point is moved 2 to the right and 1 up.
ƒ w ƒ Ϲ 14 , Ϫp>4 Ͻ Arg w Ͻ p>4 13. Ϫ5 Ϲ Re z Ϲ Ϫ2
uм1
17. Annulus 12 Ϲ ƒ w ƒ Ϲ 4
0 Ͻ u Ͻ ln 4, p>4 Ͻ v Ϲ 3p>4
z 3 ϩ az 2 ϩ bz ϩ c, z ϭ Ϫ 13 (a Ϯ 2a 2 Ϫ 3b)
z ϭ (Ϫ1 Ϯ 13)>2
sinh z ϭ 0 at z ϭ 0, Ϯpi, Ϯ2pi, Á
M ϭ ƒ z ƒ ϭ 1 on the unit circle, J ϭ ƒ z ƒ 2
ƒ w r ƒ ϭ 1> ƒ z ƒ 2 ϭ 1 on the unit circle, J ϭ 1> ƒ z ƒ 4
M ϭ ex ϭ 1 for x ϭ 0, the y-axis, J ϭ e2x
M ϭ 1> ƒ z ƒ ϭ 1 on the unit circle, J ϭ 1> ƒ z ƒ 2

Problem Set 17.2, page 745
wϩi
2w
11. z ϭ 0, 1>(a ϩ ib)
7. z ϭ

4w ϩ i
Ϫ3iw ϩ 1
13. z ϭ 0, Ϯ12 , Ϯ ϭ Ϯi>2
9. z ϭ

bapp02_B.qxd

11/4/10

A42

7:44 AM

Page A42

App. 2 Answers to Odd-Numbered Problems

15. z ϭ i, 2i

17. w ϭ

az
cz ϩ a

19. w ϭ

az ϩ b
Ϫbz ϩ a

Problem Set 17.3, page 750
3.
9.
13.
17.

Apply the inverse g of f on both sides of z1 ϭ f (z1) to get g(z1) ϭ g( f (z1)) ϭ z1.
w ϭ iz, a rotation. Sketch to see.
11. w ϭ (z ϩ i)>(z Ϫ i)
w ϭ 1>z, almost by inspection
15. w ϭ 1>z Ϫ 1
w ϭ (2z Ϫ i)>(Ϫiz Ϫ 2)
19. w ϭ (z 4 Ϫ i)(Ϫiz 4 ϩ 1)

Problem Set 17.4, page 754
1.
5.
9.
11.
13.
15.
17.
19.
21.
23.
25.

Circle ƒ w ƒ ϭ ec
3. Annulus 1> 1e Ϲ ƒ w ƒ Ϲ 1e
w-plane without w ϭ 0
7. 1 Ͻ ƒ w ƒ Ͻ e, v Ͼ 0
Ϯ(2n ϩ 1)p>2, n ϭ 0, 1, Á
u 2>cosh2 2 ϩ v2>sinh2 2 Ͻ 1, u Ͼ 0, v Ͼ 0
Elliptic annulus bounded by u 2>cosh2 1 ϩ v2>sinh2 1 ϭ 1 and
u 2>cosh2 3 ϩ v2>sinh2 3 ϭ 1
cosh z ϭ cos iz ϭ sin (iz ϩ 12 p)
2
0 Ͻ Im t Ͻ p is the image of R under t ϭ z 2>2. Answer: et ϭ ez >2.
Hyperbolas u 2>cos2 c Ϫ v2>sin2 c ϭ cosh2 c Ϫ sinh2 c ϭ 1 when c 0, p, and
u ϭ Ϯcosh y (thus ƒ u ƒ м 1), v ϭ 0 when c ϭ 0, p.
Interior of u 2>cosh2 2 ϩ v2>sinh2 2 ϭ 1 in the fourth quadrant, or map
p>2 Ͻ x Ͻ p, 0 Ͻ y Ͻ 2 by w ϭ sin z (why?).
vϽ0
The images of the five points in the figure can be obtained directly from the
function w.

Problem Set 17.5, page 756
1.
3.
5.
7.
9.

w moves once around the circle ƒ w ƒ ϭ 12 .
Four sheets, branch point at z ϭ Ϫ1
Ϫi>4, three sheets
z 0, n sheets
1z (z Ϫ i)(z ϩ i), 0, Ϯi, two sheets

Chapter 17 Review Questions and Problems, page 756
11. 1 Ͻ ƒ w ƒ Ͻ 4, ƒ arg w ƒ Ͻ p>4
15. u ϭ 1 Ϫ 14 v2, same (why?)
19. 13 Ͻ ƒ w ƒ Ͻ 12 , v Ͻ 0
10z ϩ 5i
23. w ϭ
z ϩ 2i
27. w ϭ 1>z
31. z ϭ 2 Ϯ 16
35. w ϭ e4z
39. w ϭ z 2>(2c)

13. Horizontal strip Ϫ8 Ͻ v Ͻ 8
17. ƒ w ƒ Ͼ 1
21. w ϭ 1 ϩ iv, v Ͻ 0
25. Rotation w ϭ iz
29. z ϭ 0
33. z ϭ 0, Ϯi, Ϯ3i
37. w ϭ iz 2 ϩ 1

bapp02_B.qxd

11/4/10

7:44 AM

Page A43

App. 2 Answers to Odd-Numbered Problems

A43

Problem Set 18.1, page 762
1. 2.5 mm ϭ 0.25 cm; £ ϭ Re 110 (1 ϩ (Ln z)>ln 4)
20
Ln zb
3. £ ϭ Re a30 Ϫ
ln 10
5. £ (x) ϭ Re (375 ϩ 25z)
7. £ (r) ϭ Re (32 Ϫ z)
13. Use Fig. 391 in Sec. 17.4 with the z- and w-planes interchanged and
cos z ϭ sin (z ϩ 12 p).
15. £ ϭ 220 (x 3 Ϫ 3xy 2) ϭ Re (220z 3)

Problem Set 18.2, page 766
3. w ϭ iz 2 maps R onto the strip Ϫ2 Ϲ u Ϲ 0; and £* ϭ U2 ϩ (U1 Ϫ U2) (1 ϩ 12 u) ϭ
U2 ϩ (U1 Ϫ U2) (1 Ϫ xy).
(x Ϫ 2) (2x Ϫ 1) ϩ 2y 2

ϭ c, (b) x 2 Ϫ y 2 ϭ c, xy ϭ c, ex cos y ϭ c
(x Ϫ 2)2 ϩ y 2
7. See Fig. 392 in Sec. 17.4. £ ϭ Re (sin2 z), sin2 x (y ϭ 0), sin2 x cosh2 1 Ϫ cos2 x
sinh2 1 (y ϭ 1), Ϫsinh2 y (x ϭ 0, p).
9. £ (x, y) ϭ cos2 x cosh2 y Ϫ sin2 x sinh2 y; cosh2 y (x ϭ 0), Ϫsinh y (x ϭ p
2 ),
cos2 x (y ϭ 0), cos2 x cosh2 1 Ϫ sin2 x sinh2 1 (y ϭ 1)
13. Corresponding rays in the w-plane make equal angles, and the mapping is conformal.
15. Apply w ϭ z 2.
17. z ϭ (2Z Ϫ i)>(ϪiZ Ϫ 2) by (3) in Sec. 17.3.
5
5i
19. £ ϭ p Arg (z Ϫ 2), F ϭ Ϫ p Ln (z Ϫ 2)
5. (a)

Problem Set 18.3, page 769
1. (80>d) y ϩ 20. Rotate through p>2.
y
80
80i
5. p arctan x ϭ Re aϪ p Ln zb
y
2
2i
7. T1 ϩ p (T2 Ϫ T1) arctan x ϭ Re aT1 Ϫ p (T2 Ϫ T1) Ln zb
y
y
T1
iT1
zϪa
9. p aarctan x Ϫ b Ϫ arctan x Ϫ a b ϭ Re a p Ln z Ϫ b b
zϩ1
100i
100
11. p (Arg (z Ϫ 1) Ϫ Arg (z ϩ 1)) ϭ Re a p Ln z Ϫ 1 b
100
13. p [Arg (z 2 Ϫ 1) Ϫ Arg (z 2 ϩ 1)] from w ϭ z 2 and Prob. 11.
320i
15. Ϫ20 ϩ (320> p) Arg z ϭ Re aϪ20 Ϫ p Ln zb
17. Re F(z) ϭ 100 ϩ (200> p) Re (arcsin z)

Problem Set 18.4, page 776
1. V(z) continuously differentiable.
3. ƒ F r(iy) ƒ ϭ 1 ϩ 1>y 2, ƒ y ƒ м 1, is maximum at y ϭ Ϯ1, namely, 2.

bapp02_B.qxd

11/4/10

A44

7:44 AM

Page A44

App. 2 Answers to Odd-Numbered Problems

5. Calculate or note that ٌ2 ϭ div grad and curl grad is the zero vector; see Sec. 9.8 and
Problem Set 9.7.
7. Horizontal parallel flow to the right.
9. F(z) ϭ z 4
11. Uniform parallel flow upward, V ϭ F r ϭ iK, V1 ϭ 0, V2 ϭ K
13. F(z) ϭ z 3
15. F(z) ϭ z>r0 ϩ r0>z
17. Use that w ϭ arccos z gives z ϭ cos w and interchanging the roles of the z- and
w-planes.
19. y>(x 2 ϩ y 2) ϭ c or x 2 ϩ (y Ϫ k)2 ϭ k 2

Problem Set 18.5, page 781
5. £ ϭ 32 r 3 sin 3u
7. £ ϭ 12 a ϩ 12 ar 8 cos 8u
9. £ ϭ 3 Ϫ 4r 2 cos 2u ϩ r 4 cos 4u
2
1
1
11. £ ϭ ar sin u Ϫ r 2 sin 2u ϩ r 3 sin 3u Ϫ ϩ Á b
p
2
3
2
1
2 3
1
13. £ ϭ r sin u ϩ r 2 sin 2u Ϫ
r sin 3u Ϫ r 4 sin 4u ϩ ϩ Ϫ Á
p
2
9p
4
1
2
1
1
15. £ ϭ ϩ ar cos u Ϫ r 3 cos 3u ϩ r 5 cos 5u Ϫ ϩ Á b
2
p
3
5
1
4
1
1
17. £ ϭ Ϫ 2 ar cos u Ϫ r 2 cos 2u ϩ r 3 cos 3u Ϫ ϩ Á b
p
3
4
9

Problem Set 18.6, page 784
1. Use (2). F(z 0 ϩ eia) ϭ (72 ϩ eia)3, etc. F( 52) ϭ 343
8
3. Use (2). F(z 0 ϩ eia) ϭ (2 ϩ 3eia)2, etc. F(4) ϭ 100
5. No, because ƒ z ƒ is not analytic.
1

ΎΎ

1
7. £ (2, Ϫ2) ϭ Ϫ3 ϭ p

0

p

ΎΎ

1

2p

ϭ
1
9. £ (1, 1) ϭ 3 ϭ p

ΎΎ
0

0

(1 ϩ r cos a) (Ϫ3 ϩ r sin a)r dr da

0

1

1

2p

2p

0

1
3
(Ϫ3r ϩ Á ) dr da ϭ aϪ b ؒ 2p
p
2

(3 ϩ r cos a ϩ r sin a ϩ r 2 cos a sin a)r dr da

0

1

3
ϭ p ؒ ؒ 2p
2
13. ƒ F(z) ƒ ϭ [cos2 x ϩ sinh2 y]1>2, z ϭ Ϯi, Max ϭ [1 ϩ sinh2 1]1>2 ϭ 1.543
15. ƒ F(z) ƒ 2 ϭ sinh2 2x cos2 2y ϩ cosh2 2x sin2 2y ϭ sinh2 2x ϩ 1 ؒ sin2 2y, z ϭ 1,
Max ϭ sinh 2 ϭ 3.627
17. ƒ F(z) ƒ 2 ϭ 4(2 Ϫ 2 cos 2u), z ϭ p>2, 3p>2, Max ϭ 4
19. No. Make up a counterexample.

bapp02_B.qxd

11/4/10

7:44 AM

Page A45

App. 2 Answers to Odd-Numbered Problems

A45

Chapter 18 Review Questions and Problems, page 785
11. £ ϭ 10(1 Ϫ x ϩ y), F ϭ 10 Ϫ 10(1 ϩ i)z
13. £ ϭ Re (220 Ϫ 95.54 Ln z) ϭ 220 Ϫ
17.
19.
21.
23.
25.

220
ln r ϭ 220 Ϫ 95.54 ln r.
ln 10

2(1 Ϫ (2> p) Arg z)
30(1 Ϫ (2> p) Arg (z Ϫ 1))
£ ϭ x ϩ y ϭ const, V ϭ F r(z) ϭ 1 Ϫ i, parallel flow
T(x, y) ϭ x (2y ϩ 1) ϭ const
F r(z) ϭ z ϩ 1 ϭ x ϩ 1 Ϫ iy

Problem Set 19.1, page 796
1. 0.84175 # 102, Ϫ0.52868 # 103, 0.92414 # 10؊3, Ϫ0.36201 # 106
3.
5.
7.
9.
11.
13.

15.
19.
21.
23.
25.
27.
29.

6.3698, 6.794, 8.15, impossible
Add first, then round.
29.9667, 0.0335; 29.9667, 0.0333704 (6S-exact)
29.97, 0.035; 29.97, 0.03337;
30, 0.0; 30, 0.033
ƒ P ƒ ϭ ƒ x ϩ y Ϫ (xෂ ϩ ෂ
y ) ƒ ϭ ƒ (x Ϫ ෂ
x) ϩ (y Ϫ ෂ
y ) ƒ ϭ ƒ Px ϩ Py ƒ
Ϲ ƒ Px ƒ ϩ ƒ Py ƒ ϭ bx ϩ by
aෂ1 ϩ P1
aෂ1 ϩ P1
P2
P22
aෂ
P
P
a1
aෂ
ÁbϷ 1 ϩ 1 Ϫ 2 # 1,
ϭෂ
ϭ
a1
Ϫ
ϩ
2 Ϫ ϩ
ෂ
ෂ
ෂ
ෂ
ෂ
ෂ
a2
a 2 ϩ P2
a2
a2
a2
a2
a2 ෂ
a2
a2
a1
aෂ1
a1
P1
P2
hence ` a a Ϫ ෂ b^ ` a ` Ϸ ` a Ϫ a ` Ϲ ƒ Pr1 ƒ ϩ ƒ Pr2 ƒ Ϲ br1 ϩ br2
a2
2
2
1
2
(a) 1.38629 Ϫ 1.38604 ϭ 0.00025, (b) ln 1.00025 ϭ 0.000249969 is 6S-exact.
In the present case, (b) is slightly more accurate than (a) (which may produce
nonsensical results; cf. Prob. 20).
c4 # 24 ϩ Á ϩ c0 # 20 ϭ (1 0 1 1 1.)2, NOT (1 1 1 0 1.)2
The algorithm in Prob. 22 repeats 0011 infinitely often.
n ϭ 26. The beginning is 0.09375 (n ϭ 1).
I14 ϭ 0.1812 (0.1705 4S-exact), I13 ϭ 0.1812 (0.1820), I12 ϭ 0.1951 (0.1951),
I11 ϭ 0.2102 (0.2103), etc.
Ϫ0.126 # 10؊2, Ϫ0.402 # 10؊3; Ϫ0.266 # 10؊6, Ϫ0.847 # 10؊7

Problem Set 19.2, page 807
3.
5.
7.
9.
11.
13.
15.
17.

g ϭ 0.5 cos x, x ϭ 0.450184 (ϭ x 10, exact to 6S)
Convergence to 4.7 for all these starting values.
x ϭ x>(ex sin x); 0.5, 0.63256, Á converges to 0.58853 (5S-exact) in 14 steps.
x ϭ x 4 Ϫ 0.12; x 0 ϭ 0, x 3 ϭ Ϫ0.119794 (6S-exact)
g ϭ 4>x ϩ x 3>16 Ϫ x 5>576; x 0 ϭ 2, x n ϭ 2.39165 (n м 6), 2.405 4S-exact
This follows from the intermediate value theorem of calculus.
x 3 ϭ 0.450184
Convergence to x ϭ 4.7, 4.7, 0.8, Ϫ0.5, respectively. Reason seen easily from the
graph of f.

bapp02_B.qxd

11/4/10

A46

7:44 AM

Page A46

App. 2 Answers to Odd-Numbered Problems

19.
21.
23.
25.

0.5, 0.375, 0.377968, 0.377964; (b) 1> 17
1.834243 (ϭ x 4), 0.656620 (ϭ x 4), Ϫ2.49086 (ϭ x 4)
x 0 ϭ 4.5, x 4 ϭ 4.73004 (6S-exact)
(a) ALGORITHM BISECT ( f, a 0, b0, P, N ) Bisection Method
This algorithm computes the solution c of f (x) ϭ 0 ( f continuous) within the
tolerance P, given an initial interval [a0, b0] such that f (a0) f (b0) Ͻ 0.
INPUT: Continuous function f, initial interval [a0, b0], tolerance P, maximum
number of iterations N.
OUTPUT: A solution c (within the tolerance P), or a message of failure.
For n ϭ 0, 1, Á , N Ϫ 1 do:
c ϭ 12 (an ϩ bn)
If f (c) ϭ 0 then OUTPUT c Stop. [Procedure completed]
Else if f (an) f (bn) Ͻ 0 then set anϩ1 ϭ an and bnϩ1 ϭ c.
Else set anϩ1 ϭ c, and bnϩ1 ϭ bn.
If ƒ anϩ1 Ϫ bnϩ1 ƒ Ͻ P ƒ c ƒ then OUTPUT c. Stop. [Procedure completed]
End
OUTPUT [aN, bN] and a message “Failure”. Stop.
[Unsuccessful completion; N iterations did not give an interval of length not
exceeding the tolerance.]
End BISECT

Note that [aN, bN] gives (aN ϩ bN)>2 as an approximation of the zero and (bN Ϫ aN)>2
as a corresponding error bound.
(b) 0.739085; (c) 1.30980, 0.429494
27. x 2 ϭ 1.5, x 3 ϭ 1.76471, Á , x 7 ϭ 1.83424 (6S-exact)
29. 0.904557 (6S-exact)

Problem Set 19.3, page 819
1. L 0(x) ϭ Ϫ2x ϩ 19, L 1(x) ϭ 2x Ϫ 18, p1(9.3) ϭ L 0(9.3) # f0 ϩ L 1(9.3) # f1
ϭ 0.1086 # 9.3 ϩ 1.230 ϭ 2.2297
(x Ϫ 1.02)(x Ϫ 1.04)
(x Ϫ 1)(x Ϫ 1.04)
# 1.0000 ϩ
# 0.9888
3. p2(x) ϭ
(Ϫ0.02)(Ϫ0.04)
0.02 (Ϫ0.02)
(x Ϫ 1)(x Ϫ 1.02)
# 0.9784 ϭ x 2 Ϫ 2.580x ϩ 2.580; 0.9943, 0.9835
ϩ
0.04 # 0.02
5. 0.8033 (error Ϫ0.0245), 0.4872 (error Ϫ0.0148); quadratic: 0.7839 (Ϫ0.0051),
0.4678 (0.0046)
7. p2(x) ϭ 1.1640x Ϫ 0.3357x 2; Ϫ0.5089 (error 0.1262), 0.4053 (Ϫ0.0226),
0.9053 (0.0186), 0.9911 (Ϫ0.0672)
9. p2(x) ϭ Ϫ0.44304x 2 ϩ 1.30896x Ϫ 0.023220, p2(0.75) ϭ 0.70929
(5S-exact 0.71116)
11. L 0 ϭ Ϫ16 (x Ϫ 1)(x Ϫ 2)(x Ϫ 3), L 1 ϭ 12 x(x Ϫ 2)(x Ϫ 3), L 2 ϭ Ϫ12 x(x Ϫ 1)(x Ϫ 3),
L 3 ϭ 16 x(x Ϫ 1)(x Ϫ 2); p3(x) ϭ 1 ϩ 0.039740x Ϫ 0.335187x 2 ϩ 0.060645x 3;
p2(0.5) ϭ 0.943654, p3(1.5) ϭ 0.510116, p3(2.5) ϭ Ϫ0.047991
13. 2x 2 Ϫ 4x ϩ 2
15. p3(x) ϭ 2.1972 ϩ (x Ϫ 9) # 0.1082 ϩ (x Ϫ 9)(x Ϫ 9.5) # 0.005235
17. r ϭ Ϫ1.5, p2(0.3) ϭ 0.6039 ϩ (Ϫ1.5) # 0.1755 ϩ 12 (Ϫ1.5)(Ϫ0.5) # (Ϫ0.0302)
ϭ 0.3293

bapp02_B.qxd

11/4/10

7:44 AM

Page A47

App. 2 Answers to Odd-Numbered Problems

A47

Problem Set 19.4, page 826
9. [Ϫ1.39 (x Ϫ 5)2 ϩ 0.58 (x Ϫ 5)3] s ϭ 0.004 at x ϭ 5.8 (due to roundoff;
should be 0).
11. 1 Ϫ 54 x 2 ϩ 14 x 4
13. 1 Ϫ x 2, Ϫ2(x Ϫ 1) Ϫ (x Ϫ 1)2 ϩ 2(x Ϫ 1)3, Ϫ1 ϩ 2(x Ϫ 2) ϩ 5(x Ϫ 2)2
Ϫ 6(x Ϫ 2)3
15. 4 ϩ x 2 Ϫ x 3, Ϫ8(x Ϫ 2) Ϫ 5(x Ϫ 2)2 ϩ 5(x Ϫ 2)3,
4 ϩ 32(x Ϫ 4) ϩ 25(x Ϫ 4)2 Ϫ 11(x Ϫ 4)3
17. Use the fact that the third derivative of a cubic polynomial is constant, so that g t
is piecewise constant, hence constant throughout under the present assumption.
Now integrate three times.
19. Curvature f s>(1 ϩ f r 2)3>2 Ϸ f s if ƒ f r ƒ is small.

Problem Set 19.5, page 839
1.
3.
5.
7.
9.
11.
13.
15.
17.
19.
21.
23.
25.
27.
29.

0.747131, which is larger than 0.746824. Why?
0.5, 0.375, 0.34375, 0.335 (exact)
P0.5 Ϸ 0.03452 (P0.5 ϭ 0.03307), P0.25 Ϸ 0.00829 (P0.25 ϭ 0.00820)
0.693254 (6S-exact 0.693147)
0.073930 (6S-exact 0.073928)
0.785392 (6S-exact 0.785398)
(0.785398126 Ϫ 0.785392156)>15 ϭ 0.39792 # 10؊6
(a) M 2 ϭ 2, ƒ KM 2 ƒ ϭ 2>(12n 2) ϭ 10 ؊5>2, n ϭ 183. (b) f iv ϭ 24>x 5, M 4 ϭ 24,
ƒ CM 4 ƒ ϭ 24>(180 ؒ (2m)4) ϭ 10 ؊5>2, 2m ϭ 12.8, hence 14.
0.94614588, 0.94608693 (8S-exact 0.94608307)
0.9460831 (7S-exact)
0.9774586 (7S-exact 0.9774377)
Set x ϭ 12 (t ϩ 1), 0.2642411177 (10S-exact), 1 Ϫ 2>e
x ϭ 12 (t ϩ 1), dx ϭ 12 dt, 0.746824127 (9S-exact 0.746824133)
0.08, 0.32, 0.176, 0.256 (exact)
5(0.1040 Ϫ 12 ؒ 0.1760 ϩ 13 ؒ 0.1344 Ϫ 14 ؒ 0.0384) ϭ 0.256

Chapter 19 Review Questions and Problems, page 841
4.375, 4.50, 6.0, impossible
44.885 Ϲ s Ϲ 44.995
a.
The same as that of ෂ
x ϭ 20 Ϯ 1398 ϭ 20.00 Ϯ 19.95, x 1 ϭ 39.95, x 2 ϭ 0.05, x 2 ϭ 2>39.95
ϭ 0.05006 (error less than 1 unit of the last digit)
25. x ϭ x 4 Ϫ 0.1, Ϫ0.1, Ϫ0.999, Ϫ0.99900399
27. 0.824
29. Ϫx ϩ x 3, 2(x Ϫ 1) ϩ 3(x Ϫ 1)2 Ϫ (x Ϫ 1)3
31. 0.26, M 2 ϭ 6, M*2 ϭ 0, Ϫ0.02 Ϲ P Ϲ 0, 0.01
33. 0.90443, 0.90452 (5S-exact 0.90452)
35. (a) (0.43 Ϫ 2 ؒ 0.23 ϩ 0)>0.04 ϭ 1.2, (b) (0.33 Ϫ 2 ؒ 0.23 ϩ 0.13)>0.01 ϭ 1.2 (exact)
17.
19.
21.
23.

bapp02_B.qxd

11/4/10

A48

7:44 AM

Page A48

App. 2 Answers to Odd-Numbered Problems

Problem Set 20.1, page 851
1. x 1 ϭ 7.3, x 2 ϭ Ϫ3.2

5. x 1 ϭ 2, x 2 ϭ 1

3. No solution

Ϫ3

6

Ϫ9

Ϫ46.725

7. D 0

9

Ϫ13

Ϫ51.223 T

0
0 Ϫ2.88889 Ϫ7.38689
x 1 ϭ 3.908, x 2 ϭ Ϫ1.998, x 3 ϭ 2.557
13

Ϫ8

0

178.54

9. D 0

6

13

137.86T

0
0 Ϫ16 Ϫ253.12
x 1 ϭ 6.78, x 2 ϭ Ϫ11.3, x 3 ϭ 15.82
3.4

Ϫ6.12

11. D0

Ϫ2.72

0

0
0T

4.32

0
0
0
0
x 1 ϭ t 1 arbitrary, x 2 ϭ (3.4>6.12)t 1,
5

0

13. D0

Ϫ4

6

Ϫ0.329193

Ϫ3.6

Ϫ2.143144T

x3 ϭ 0

0
0
2.3 Ϫ0.4
x 1 ϭ 0.142856, x 2 ϭ 0.692307, x 3 ϭ Ϫ0.173912
Ϫ1

Ϫ3.1

2.5

0

Ϫ8.7

0

2.2

1.5

Ϫ3.3

Ϫ9.3

0

0

15. E

Ϫ1.493182

Ϫ0.825

0
0
0
6.13826
12.2765
x 1 ϭ 4.2, x 2 ϭ 0, x 3 ϭ Ϫ1.8, x 4 ϭ 2.0

Problem Set 20.2, page 857

c

dc

4

5

0

Ϫ1

d,

x 1 ϭ Ϫ4

1

0

3

1

1

0

0

5

4

1

x 1 ϭ 0.4

3. D2

1

0T D0

1

2T ,

x 2 ϭ 0.8

2

5

1

0

0

3

x 3 ϭ 1.6

1

0

0

3

9

6

1
x 1 ϭ Ϫ15

5. D6

1

0T D0

Ϫ6

3T ,

4
x 2 ϭ 15

3

9

1

1.

0

x2 ϭ

0

6

Ϫ3

x 3 ϭ 25

U

1.03773

bapp02_B.qxd

11/4/10

7:44 AM

Page A49

App. 2 Answers to Odd-Numbered Problems

A49

3

0

0

3

2

4

x 1 ϭ 0.6

7. D 2

3

0T D0

3

1T ,

x 2 ϭ 1.2

4

1

3

0

3

x 3 ϭ 0.4

0.1

0

0.1

0

9. D 0

0

0T D0

0.4

0.3

0

0.2

0.1

0.4

0

1

0

0

0

Ϫ1
11. E
3

2

0

0

Ϫ1

3

0

2

0

Ϫ1

4

0

0.3

x1 ϭ

0.2T ,

x 2 ϭ Ϫ11

0.1

x3 ϭ

2

4

1

Ϫ1

3

2

x1 ϭ

0

2

Ϫ1

0

x 2 ϭ Ϫ3

0

0

3

Ϫ1

0

0

0

4

UE

U,

x3 ϭ

2

4

x 4 ϭ Ϫ1

13. No, since x T (ϪA)x ϭ Ϫx T Ax Ͻ 0; yes; yes; no
15.

c

Ϫ3.5

1.25
Ϫ1.0

3.0
584

17.

19.

d

104

Ϫ66

20

Ϫ12T

1
D 104
36
Ϫ66

Ϫ12

9

21

Ϫ6

Ϫ14

6

Ϫ6
1
E
16 Ϫ14

36

Ϫ12

Ϫ4

Ϫ12

20

Ϫ4

6

Ϫ4

Ϫ4

4

U

Problem Set 20.3, page 863
5. Exact 0.5, 0.5, 0.5
7. x 1 ϭ 2, x 2 ϭ Ϫ4, x 3 ϭ 8
9. Exact 2, 1, 4
11. (a) x (3)T ϭ [0.49983 0.50001 0.500017],
(b) x (3)T ϭ [0.50333 0.49985 0.49968]
13. 8, Ϫ16, 43, 86 steps; spectral radius 0.09, 0.35, 0.72, 0.85, approximately
15. [1.99934 1.00043 3.99684]T (Jacobi, Step 5); [2.00004 0.998059 4.00072]T
(Gauss–Seidel)
19. 1306 ϭ 17.49, 12, 12

Problem Set 20.4, page 871
1. 18, 1110 ϭ 10.49, 8, [0.125 Ϫ0.375 1 0 Ϫ0.75 0]
3. 5.9, 113.81 ϭ 3.716, 3, 13 [0.2 0.6 Ϫ2.1 3.0]
5. 5, 15, 1, [1 1 1 1 1]
7. ab ϩ bc ϩ ca ϭ 0

bapp02_B.qxd

11/4/10

A50

7:44 AM

Page A50

App. 2 Answers to Odd-Numbered Problems

9.
13.
15.
17.
19.
21.
23.

␬ ϭ 5 # 12 ϭ 2.5
11. ␬ ϭ (5 ϩ 15)(1 ϩ 1> 15) ϭ 6 ϩ 215
␬ ϭ 19 # 13 ϭ 247; ill-conditioned
␬ ϭ 20 # 20 ϭ 400; ill-conditioned
167 Ϲ 21 # 15 ϭ 315
[Ϫ2 4]T, [Ϫ144.0 184.0]T, ␬ ϭ 25,921, extremely ill-conditioned
Small residual [0.145 0.120], but large deviation of ~
x.
27, 748, 28,375, 943,656, 29,070,279

Problem Set 20.5, page 875
1.
5.
11.
13.

1.846 Ϫ 1.038x
3. 1.48 ϩ 0.09x
s ϭ 90t Ϫ 675, vav ϭ 90 km>hr
9. Ϫ11.36 ϩ 5.45x Ϫ 0.589x 2
1.89 Ϫ 0.739x ϩ 0.207x 2
2.552 ϩ 16.23x, Ϫ4.114 ϩ 13.73x ϩ 2.500x 2, 2.730 ϩ 1.466x
Ϫ 1.778x 2 ϩ 2.852x 3

Problem Set 20.7, page 884
1.
3.
5.
7.
9.

5, 0, 7; radii 6, 4, 6. Spectrum {Ϫ1, 4, 9}
Centers 0; radii 0.5, 0.7, 0.4. Skew-symmetric, hence l ϭ i␮, Ϫ0.7 Ϲ ␮ Ϲ 0.7.
2, 3, 8; radii 1 ϩ 12, 1, 12; actually (4S) 1.163, 3.511, 8.326
t 11 ϭ 100, t 22 ϭ t 33 ϭ 1
They lie in the intervals with endpoints ajj Ϯ (n Ϫ 1) # 10؊5. Why?

11. r (A) Ϲ Row sum norm ʈ A ʈϱ ϭ max a ƒ ajk ƒ ϭ max( ƒ ajj ƒ ϩ Gerschgorin radius)
j
j
k

13.
15.
17.
19.

1122 ϭ 11.05
10.52 ϭ 0.7211
T
T
Show that AA ϭ A A.
0 lies in no Gerschgorin disk, by (3) with Ͼ; hence det A ϭ l1 Á ln

0.

Problem Set 20.8, page 887
q ϭ 10, 10.9908, 10.9999; ƒ P ƒ Ϲ 3, 0.3028, 0.0275
q Ϯ d ϭ 4 Ϯ 1.633, 4.786 Ϯ 0.619, 4.917 Ϯ 0.398
Same answer as in Prob. 3, possibly except for small roundoff errors.
q ϭ 5.5, 5.5738, 5.6018; ƒ P ƒ Ϲ 0.5, 0.3115, 0.1899; eigenvalues (4S) 1.697,
3.382, 5.303, 5.618
9. y ϭ Ax ϭ lx, y Tx ϭ lx Tx, y Ty ϭ l2x Tx,
P2 Ϲ y Ty>x Tx Ϫ (y Tx>x Tx)2 ϭ l2 Ϫ l2 ϭ 0
11. q ϭ 1, Á , Ϫ2.8993 approximates Ϫ3 (0 of the given matrix),
ƒ P ƒ Ϲ 1.633, Á , 0.7024 (Step 8)
1.
3.
5.
7.

Problem Set 20.9, page 896
0.98

Ϫ0.4418

0

1. DϪ0.4418

0.8702

0.3718T

0

0.3718

0.4898

bapp02_B.qxd

11/4/10

7:44 AM

Page A51

App. 2 Answers to Odd-Numbered Problems

7

Ϫ3.6056

3. DϪ3.6056

13.462

0

A51

0
3.6923T

3.6923

3.5385

Ϫ67.59

3
Ϫ67.59
5. E
0

0

143.5
45.35

0
0
7. Eigenvalues 16, 6, 2
11.2903

Ϫ5.0173

DϪ5.0173

10.6144

0

0

45.35

0

23.34

3.126

3.126

Ϫ33.87

14.9028 Ϫ3.1265

0

0.7499

U

15.8299 Ϫ1.2932

0

0

0.7499T , DϪ3.1265

7.0883

0.1966T , DϪ1.2932

6.1692

0.0625T

2.0952

0.1966

2.0089

0.0625

2.0010

0

0

9. Eigenvalues (4S) 141.4, 68.64, Ϫ30.04
141.1
D

4.926

4.926

0

141.3

0.8691T , D

68.97

0.8691 Ϫ30.03

0

2.400

2.400
68.72

0

0
0.3797T ,

141.4
D

0.3797 Ϫ30.04

1.166
0

1.166
68.66

0
0.1661T

0.1661 Ϫ30.04

Chapter 20 Review Questions and Problems, page 896
15. [3.9 4.3 1.8]T
17. [Ϫ2 0 5]T
0.28193

Ϫ0.15904

19. DϪ0.15904

0.12048

Ϫ0.00482

Ϫ0.00241

5.750
21. D3.600T ,

Ϫ0.00482
Ϫ0.00241T
0.01205

6.400

6.390

D3.559T ,

D3.600T

0.838
1.000
0.997
Exact: [6.4 3.6 1.0]T
1.700
23. D1.180T ,

1.986

2.000

D0.999T ,

D1.000T

4.043
4.002
4.000
Exact: [2 1 4]T
25. 42, 1674 ϭ 25.96, 21
27. 30
29. 5
31. 115 # 0.4458 ϭ 51.27
5
33. 5 # 21
35. 1.514 ϩ 1.129x Ϫ 0.214x 2
63 ϭ 3
37. Centers 15, 35, 90; radii 30, 35, 25, respectively. Eigenvalues (3S) 2.63, 40.8, 96.6
39. Centers 0, Ϫ1, Ϫ4; radii 9, 6, 7, respectively; eigenvalues 0, 4.446, Ϫ9.446

bapp02_B.qxd

11/4/10

A52

7:44 AM

Page A52

App. 2 Answers to Odd-Numbered Problems

Problem Set 21.1, page 910
y ϭ 5e؊0.2x, 0.00458, 0.00830 (errors of y5, y10)
y ϭ x Ϫ tanh x (set y Ϫ x ϭ u), 0.00929, 0.01885 (errors of y5, y10)
y ϭ ex, 0.0013, 0.0042 (errors of y5, y10)
y ϭ 1>(1 Ϫ x 2>2), 0.00029, 0.01187 (errors of y5, y10)
Errors 0.03547 and 0.28715 of y5 and y10 much larger
y ϭ 1>(1 Ϫ x 2>2); error Ϫ10؊8, Ϫ4 # 10؊8, Á , Ϫ6 # 10؊7, ϩ9 # 10؊6;
P ϭ 0.0002>15 ϭ 1.3 # 10؊5 (use RK with h ϭ 0.2)
13. y ϭ tan x; error 0.83 # 10؊7, 0.16 # 10؊6, Á , Ϫ0.56 # 10؊6, ϩ0.13 # 10؊5
15. y ϭ 3 cos x Ϫ 2 cos2 x; error # 107: 0.18, 0.74, 1.73, 3.28, 5.59, 9.04, 14.3, 22.8,
36.8, 61.4
17. y r ϭ 1>(2 Ϫ x 4); error # 109: 0.2, 3.1, 10.7, 23.2, 28.5, Ϫ32.3, Ϫ376, Ϫ1656,
Ϫ3489, ϩ80444
19. Errors for Euler–Cauchy 0.02002, 0.06286, 0.05074; for improved Euler–Cauchy
Ϫ0.000455, 0.012086, 0.009601; for Runge–Kutta. 0.0000011, 0.000016, 0.000536
1.
3.
5.
7.
9.
11.

Problem Set 21.2, page 915
1. y ϭ ex, y*5 ϭ 1.648717, y5 ϭ 1.648722, P5 ϭ Ϫ3.8 # 10؊8,
* ϭ 2.718276, y10 ϭ 2.718284, P10 ϭ Ϫ1.8 # 10؊6
y10
3. y ϭ tan x, y4, Á , y10 (error # 105) 0.422798 (Ϫ0.49), 0.546315 (Ϫ1.2),
0.684161 (Ϫ2.4), 0.842332 (Ϫ4.4), 1.029714 (Ϫ7.5), 1.260288 (Ϫ13),
1.557626 (Ϫ22)
5. RK error smaller in absolute value, error # 105 ϭ 0.4, 0.3, 0.2, 5.6
(for x ϭ 0.4, 0.6, 0.8, 1.0)
7. y ϭ 1>(4 ϩ e؊3x), y4, Á , y10 (error # 105) 0.232490 (0.34), 0.236787 (0.44),
0.240075 (0.42), 0.242570 (0.35), 0.244453 (0.25), 0.245867 (0.16), 0.246926 (0.09)
9. y ϭ exp (x 3) Ϫ 1, y4, Á , y10 (error # 107) 0.008032 (Ϫ4), 0.015749 (Ϫ10),
0.027370 (Ϫ17), 0.043810 (Ϫ26), 0.066096 (Ϫ39), 0.095411 (Ϫ54),
0.133156 (Ϫ74)
13. y ϭ exp (x 2). Errors # 105 from x ϭ 0.3 to 0.7: Ϫ5, Ϫ11, Ϫ19, Ϫ31, Ϫ41
15. (a) 0, 0.02, 0.0884, 0.215848, y4 ϭ 0.417818, y5 ϭ 0.708887 (poor)
(b) By 30–50%

Problem Set 21.3, page 922
1. y1 ϭ Ϫe؊2x ϩ 4ex, y2 ϭ Ϫe؊2x ϩ ex; errors of y1 (of y2) from 0.002 to 0.5
(from Ϫ0.01 to 0.1), monotone
3. y1r ϭ y2, y2r ϭ Ϫ14 y1, y ϭ y1 ϭ 1, 0.99, 0.97, 0.94, 0.9005, error
Ϫ0.005, Ϫ0.01, Ϫ0.015, Ϫ0.02, Ϫ0.0229; exact y ϭ cos 12 x
5. y1r ϭ y2, y2r ϭ y1 ϩ x, y1 (0) ϭ 1, y2 (0) ϭ Ϫ2, y ϭ y1 ϭ e؊x Ϫ x, y ϭ 0.8
(error 0.005), 0.61 (0.01), 0.429 (0.012), 0.2561 (0.0142), 0.0905 (0.0160)
7. By about a factor 105. Pn (y1) # 106 ϭ Ϫ0.082, Á , Ϫ0.27,
Pn(y2) # 106 ϭ 0.08, Á , 0.27
9. Errors of y1 (of y2) from 0.3 # 10؊5 to 1.3 # 10؊5 (from 0.3 # 10؊5 to 0.6 # 10؊5)
11. (y1, y2) ϭ (0, 1), (0.20, 0.98), (0.39, 0.92), Á , (Ϫ0.23, Ϫ0.97), (Ϫ0.42, Ϫ0.91),
(Ϫ0.59), (Ϫ0.81); continuation will give an “ellipse.”

bapp02_B.qxd

11/4/10

7:44 AM

Page A53

App. 2 Answers to Odd-Numbered Problems

A53

Problem Set 21.4, page 930
3. Ϫ3u 11 ϩ u 12 ϭ Ϫ200, u 11 Ϫ 3u 12 ϭ Ϫ100
5. 105, 155, 105, 115; Step 5: 104.94, 154.97, 104.97, 114.98
7. 0, 0, 0, 0. All equipotential lines meet at the corners (why?).
Step 5: 0.29298, 0.14649, 0.14649, 0.073245
9. 0.108253, 0.108253, 0.324760, 0.324760; Step 10: 0.108538, 0.108396,
0.324902, 0.324831
11. (a) u 11 ϭ Ϫu 12 ϭ Ϫ66. (b) Reduce to 4 equations by symmetry.
u 11 ϭ u 31 ϭ Ϫu 15 ϭ Ϫu 35 ϭ Ϫ92.92, u 21 ϭ Ϫu 25 ϭ Ϫ87.45,
u 12 ϭ u 32 ϭ Ϫu 14 ϭ Ϫu 34 ϭ Ϫ64.22, u 22 ϭ Ϫu 24 ϭ Ϫ53.98,
u 13 ϭ u 23 ϭ u 33 ϭ 0
13. u 12 ϭ u 32 ϭ 31.25, u 21 ϭ u 23 ϭ 18.75, u jk ϭ 25 at the others
15. u 21 ϭ u 23 ϭ 0.25, u 12 ϭ u 32 ϭ Ϫ0.25, u jk ϭ 0 otherwise
17. 13, u 11 ϭ u 21 ϭ 0.0849, u 12 ϭ u 22 ϭ 0.3170. (0.1083, 0.3248 are 4S-values
of the solution of the linear system of the problem.)

Problem Set 21.5, page 935
5. u 11 ϭ 0.766, u 21 ϭ 1.109, u 12 ϭ 1.957, u 22 ϭ 3.293
7. A, as in Example 1, right sides Ϫ220, Ϫ220, Ϫ220, Ϫ220.
Solution u 11 ϭ u 21 ϭ 125.7, u 21 ϭ u 22 ϭ 157.1
13. Ϫ4u 11 ϩ u 21 ϩ u 12 ϭ Ϫ3, u 11 Ϫ 4u 21 ϩ u 22 ϭ Ϫ12, u 11 Ϫ 4u 12 ϩ u 22 ϭ 0,
2u 21 ϩ 2u 12 Ϫ 12u 22 ϭ Ϫ14, u 11 ϭ u 22 ϭ 2, u 21 ϭ 4, u 12 ϭ 1.
4
4
Here Ϫ14
3 ϭ Ϫ3 (1 ϩ 2.5) with 3 from the stencil.
T
15. b ϭ [Ϫ200, Ϫ100, Ϫ100, 0] ; u 11 ϭ 73.68, u 21 ϭ u 12 ϭ 47.37, u 22 ϭ 15.79 (4S)

Problem Set 21.6, page 941
5.
7.
9.
11.

0, 0.6625, 1.25, 1.7125, 2, 2.1, 2, 1.7125, 1.25, 0.6625, 0
Substantially less accurate, 0.15, 0.25 (t ϭ 0.04), 0.100, 0.163 (t ϭ 0.08)
Step 5 gives 0, 0.06279, 0.09336, 0.08364, 0.04707, 0.
Step 2: 0 (exact 0), 0.0453 (0.0422), 0.0672 (0.0658), 0.0671 (0.0628), 0.0394
(0.0373), 0 (0)
13. 0.3301, 0.5706, 0.4522, 0.2380 (t ϭ 0.04), 0.06538, 0.10603, 0.10565, 0.6543
(t ϭ 0.20)
15. 0.1018, 0.1673, 0.1673, 0.1018 (t ϭ 0.04), 0.0219, 0.0355, Á (t ϭ 0.20)

Problem Set 21.7, page 944
1. u (x, 1) ϭ 0, Ϫ0.05, Ϫ0.10, Ϫ0.15, Ϫ0.20, 0
3. For x ϭ 0.2, 0.4 we obtain 0.24, 0.40 (t ϭ 0.2), 0.08, 0.16 (t ϭ 0.4),
Ϫ0.08, Ϫ0.16 (t ϭ 0.6), etc.
5. 0, 0.354, 0.766, 1.271, 1.679, 1.834, Á (t ϭ 0.1); 0, 0.575, 0.935, 1.135, 1.296,
1.357, Á (t ϭ 0.2)
7. 0.190, 0.308, 0.308, 0.190, (3S-exact: 0.178, 0.288, 0.288, 0.178)

bapp02_B.qxd

11/4/10

A54

7:44 AM

Page A54

App. 2 Answers to Odd-Numbered Problems

Chapter 21 Review Questions and Problems, page 945
17. y ϭ ex, 0.038, 0.125 (errors of y5 and y10)
19. y ϭ tan x; 0 (0), 0.10050 (Ϫ0.00017), 0.20304 (Ϫ0.00033), 0.30981 (Ϫ0.00048),
0.42341 (Ϫ0.00062), 0.54702 (Ϫ0.00072), 0.68490 (Ϫ0.00076),
0.84295 (Ϫ0.00066), 1.0299 (Ϫ0.0002), 1.2593 (0.0009), 1.5538 (0.0036)
21. 0.1003346 (0.8 # 10؊7) 0.2027099 (1.6 # 10؊7), 0.3093360 (2.1 # 10؊7),
0.4227930 (2.3 # 10؊7), 0.5463023 (1.8 # 10؊7)
23. y ϭ sin x, y0.8 ϭ 0.717366, y1.0 ϭ 0.841496 (errors Ϫ1.0 # 10؊5,
Ϫ2.5 # 10؊5)
25. y1r ϭ y2, y2r ϭ x 2y1, y ϭ y1 ϭ 1, 1, 1, 1.0001, 1.0006, 1.002
27. y1r ϭ y2, y2r ϭ 2ex Ϫ y1, y ϭ ex Ϫ cos x, y ϭ y1 ϭ 0, 0.241, 0.571, Á ;
errors between 10؊6 and 10؊5
29. 3.93, 15.71, 58.93
31. 0, 0.04, 0.08, 0.12, 0.15, 0.16, 0.15, 0.12, 0.08, 0.04, 0 (t ϭ 0.3. 3 time steps)
33. u (P11) ϭ u (P31) ϭ 270, u (P21) ϭ u (P13) ϭ u (P23) ϭ u (P33) ϭ 30,
u (P12) ϭ u (P32) ϭ 90, u (P22) ϭ 60
35. 0.043330, 0.077321, 0.089952, 0.058488 (t ϭ 0.04), 0.010956, 0.017720, 0.017747,
0.010964 (t ϭ 0.20)

Problem Set 22.1, page 953
3. f (x) ϭ 2(x 1 Ϫ 1)2 ϩ (x 2 ϩ 2)2 Ϫ 6; Step 3: (1.037, Ϫ1.926), value Ϫ5.992
9. Step 5: (0.11247, Ϫ0.00012), value 0.000016

Problem Set 22.2, page 957
7.
9.
11.
13.
15.
17.

No
x 3, x 4 is the unused time on M 1, M 2, respectively.
f (2.5, 2.5) ϭ 100
26
1
f (Ϫ11
3 , 3 ) ϭ 198 3
f (9, 6) ϭ 360
0.5x 1 ϩ 0.75x 2 Ϲ 45 (copper), 0.5x 1 ϩ 0.25x 2 Ϲ 30, f ϭ 120x 1 ϩ 100x 2,
fmax ϭ f (45, 30) ϭ 8400
19. f ϭ x1 ϩ x 2, 2x1 ϩ 3x 2 Ϲ 1200, 4x1 ϩ 2x 2 Ϲ 1600, fmax ϭ f (300, 200) ϭ 500
21. x1>3 ϩ x 2>2 Ϲ 100, x1>3 ϩ x 2>6 Ϲ 80, f ϭ 150x1 ϩ 100x 2, fmax ϭ f (210, 60) ϭ
37,500

Problem Set 22.3, page 961
3.
5.
7.
9.
11.

f (120>11, 60>11) ϭ 480>11
Eliminate in Column 3, so that 20 goes. fmin ϭ f (0, 12 ) ϭ Ϫ10.
1500
2200
fmax ϭ f (60
21 , 0, 105 , 0) ϭ 7
fmax ϭ 6 on the segment from (3, 0, 0) to (0, 0, 2)
We minimize! The augmented matrix is
1

1.8

2.1

0

0

0

T0 ϭ D0

15

30

1

0

150T .

0

600

500

0

1

3900

bapp02_B.qxd

11/4/10

7:44 AM

Page A55

App. 2 Answers to Odd-Numbered Problems

A55

The pivot is 600. The calculation gives
1

0

6
10

0

3
Ϫ1000

T1 ϭ D0

0

35
2

1

1
Ϫ40

0

600

500

0

1

The next pivot is

35
2 .

0

0

T2 ϭ D0

0

35
2

600

1.8
Row 1 Ϫ 600
Row 3

105
2 T

15
Row 2 Ϫ 600
Row 3

3900

Row 3

The calculation gives

1

0

Ϫ117
10

0

6
Ϫ175

1

3
Ϫ1400

Ϫ27
2

1
Ϫ40

Ϫ200
7

12
7

Row 1 Ϫ 1.2
35 Row 2

105
2 T

Row 2
Row 3 Ϫ 1000
35 Row 2

2400

Hence Ϫf has the maximum value Ϫ13.5, so that f has the minimum value 13.5, at
the point
(x 1, x 2) ϭ a

2400 105>2
,
b ϭ (4, 3).
600 35>2

13. fmax ϭ f (5, 4, 6) ϭ 478

Problem Set 22.4, page 968
1.
3.
5.
7.
9.

f (6, 3) ϭ 84
f (20, 20) ϭ 40
f (10, 5) ϭ 5500
f (1, 1, 0) ϭ 13
f (4, 0, 12) ϭ 9

Chapter 22 Review Questions and Problems, page 968
9.
11.
17.
19.

Step 5: [0.353 Ϫ0.028]T. Slower. Why?
Of course! Step 5: [Ϫ1.003 1.897]T
f (2, 4) ϭ 100
f (3, 6) ϭ Ϫ54

Problem Set 23.1, page 974
0

1

0

9. D0

0

1T

1

0

0

1

1

1

0

0

0

0

1

0

0

0

0

0

0

0

11. E

U

0

0

1

1

13. D0

0

1T

1

1

0

15.

1

2

3

4

11/4/10

A56

7:44 AM

Page A56

App. 2 Answers to Odd-Numbered Problems

17. If G is complete.
Edge
e1

e2

e3

e4

Ϫ1

Ϫ1

1

Ϫ1

1

0

0

0

0

1

Ϫ1

0

0

0

0

1

1
19.

Vertex

bapp02_B.qxd

2
3

E

4

U

Problem Set 23.2, page 979
1. 5
3. 4
5. The idea is to go backward. There is a vk؊1 adjacent to vk and labeled k Ϫ 1, etc.
Now the only vertex labeled 0 is s. Hence l(v0) ϭ 0 implies v0 ϭ s, so that
v0 Ϫ v1 Ϫ Á Ϫ vk؊1 Ϫ vk is a path s : vk that has length k.
15. Delete the edge (2, 4).
17. No

Problem Set 23.3, page 983
1.
5.
7.
9.

(1, 2), (2, 4), (4, 3); L 2 ϭ 12, L 3 ϭ 36, L 4 ϭ 28
(1, 2), (2, 4), (3, 4), (3, 5); L 2 ϭ 2, L 3 ϭ 4, L 4 ϭ 3, L 5 ϭ 6
(1, 2), (2, 4), (3, 4); L 2 ϭ 10, L 3 ϭ 15, L 4 ϭ 13
(1, 5), (2, 3), (2, 6), (3, 4), (3, 5); L 2 ϭ 9, L 3 ϭ 7, L 4 ϭ 8, L 5 ϭ 4, L 6 ϭ 14

Problem Set 23.4, page 987
1.

2ì
1

ê

4Ϫ3Ϫ5

3. 5 Ϫ 3 Ϫ 6

ì
ê

L ϭ 10

1
2Ϫ4

L ϭ 17

2
5. 1

ì
ê

4ì
ê

3

L ϭ 12

5

9. Yes
2
L ϭ 38
5Ϫ6
13. New York–Washington–Chicago–Dalles–Denver–Los Angeles
15. G is connected. If G were not a tree, it would have a cycle, but this cycle would
provide two paths between any pair of its vertices, contradicting the uniqueness.

11. 1 Ϫ 3 Ϫ 4

ì
ê

bapp02_B.qxd

11/4/10

7:44 AM

Page A57

App. 2 Answers to Odd-Numbered Problems

A57

19. If we add an edge (u, v) to T, then since T is connected, there is a path u : v in T
which, together with (u, v), forms a cycle.

Problem Set 23.5, page 990
1.
3.
7.
9.
11.

If G is a tree.
A shortest spanning tree of the largest connected graph that contains vertex 1.
(1, 4), (1, 3), (1, 2), (2, 6), (3, 5); L ϭ 32
(1, 4), (4, 3), (4, 2), (3, 5); L ϭ 20
(1, 4), (4, 3), (4, 5), (1, 2); L ϭ 12

Problem Set 23.6, page 997
{3, 6}, 11 ϩ 3 ϭ 14
{4, 5, 6}, 10 ϩ 5 ϩ 13 ϭ 28
{3, 6, 7}, 8 ϩ 4 ϩ 4 ϭ 16
S ϭ {1, 4}, 8 ϩ 6 ϭ 14
One is interested in flows from s to t, not in the opposite direction.
¢ 12 ϭ 5, ¢ 24 ϭ 8, ¢ 45 ϭ 2; ¢ 12 ϭ 5, ¢ 25 ϭ 3; ¢ 13 ϭ 4, ¢ 35 ϭ 9
P1: 1 Ϫ 2 Ϫ 4 Ϫ 5, ¢f ϭ 2; P2: 1 Ϫ 2 Ϫ 5, ¢f ϭ 3; P3: 1 Ϫ 3 Ϫ 5, ¢f ϭ 4
15. 1 Ϫ 2 Ϫ 5, ¢f ϭ 2; 1 Ϫ 4 Ϫ 2 Ϫ 5, ¢f ϭ 2, etc.
17. f13 ϭ f35 ϭ 8, f14 ϭ f45 ϭ 5, f12 ϭ f24 ϭ f46 ϭ 4, f56 ϭ 13, f ϭ 4 ϩ 13 ϭ 17,
f ϭ 17 is unique.
19. For instance, f12 ϭ 10, f24 ϭ f45 ϭ 7, f13 ϭ f25 ϭ 5, f35 ϭ 3, f32 ϭ 2,
f ϭ 3 ϩ 5 ϩ 7 ϭ 15, f ϭ 15 is unique.
1.
3.
5.
7.
9.
13.

Problem Set 23.7, page 1000
3. (2, 3) and (5, 6)
5. By considering only edges with one labeled end and one unlabeled end
7. 1 Ϫ 2 Ϫ 5, ¢ t ϭ 2; 1 Ϫ 4 Ϫ 2 Ϫ 5, ¢ t ϭ 1; f ϭ 6 ϩ 2 ϩ 1 ϭ 9, where 6 is
the given flow
9. 1 Ϫ 2 Ϫ 4 Ϫ 6, ¢ t ϭ 2; 1 Ϫ 3 Ϫ 5 Ϫ 6, ¢ t ϭ 1; f ϭ 4 ϩ 2 ϩ 1 ϭ 7, where 4
is the given flow
15. S ϭ {1, 2, 4, 5}, T ϭ {3, 6}, cap (S, T) ϭ 14

Problem Set 23.8, page 1005
1.
5.
7.
13.

No
3. No
Yes, S ϭ {1, 4, 5, 8}
Yes, S ϭ {1, 3, 5}
11. 1 Ϫ 2 ؊ 3 Ϫ 7 ؊ 5 Ϫ 4
1 Ϫ 2 ؊ 3 Ϫ 7 ؊ 5 Ϫ 4 is augmenting and gives 1 ؊ 2 Ϫ 3 ؊ 7 Ϫ 5 ؊ 4 and (1, 2),
(3, 7), (5, 4) is of maximum cardinality.
15. 1 Ϫ 4 ؊ 3 Ϫ 6 ؊ 7 Ϫ 8 is augmenting and gives 1 ؊ 4 Ϫ 3 ؊ 6 Ϫ 7 ؊ 8 and
(1, 4), (3, 6), (7, 8) is of maximum cardinality.
19. 3
21. 2
23. 3
25. K 4

bapp02_B.qxd

11/4/10

A58

7:44 AM

Page A58

App. 2 Answers to Odd-Numbered Problems

Chapter 23 Review Questions and Problems, page 1006
0

0

1

1

0

0

1

1

1

1

0

0

1

1

0

0

11. E

13.

1

2

3

4

0

1

0

1

1

0

1

0

0

1

0

1

1

0

1

0

To vertex
From vertex 1
2
3
4

15.

U

1

2

4

3

E

U

17.
Vertex
Incident Edges
1
(1, 2), (1, 4)
2
(2, 1), (2, 4)
3
(3, 4)
4
(4, 1), (4, 2), (4, 3)
19. (1, 2), (1, 4), (2, 3); L 2 ϭ 2, L 3 ϭ 5, L 4 ϭ 5
23. (1, 6), (4, 5), (2, 3), (7, 8)

Problem Set 24.1, page 1015
1.
5.
9.
13.
17.

qL ϭ 19, qM ϭ 20, qU ϭ 20.5
qL ϭ 199, qM ϭ 201, qU ϭ 201
qL ϭ 89.9, qM ϭ 91.0, qU ϭ 91.8
x ϭ 144.67, s ϭ 8.9735, IQR ϭ 16
3.54, 1.29

3.
7.
11.
15.

qL ϭ 138, qM ϭ 144, qU ϭ 154
qL ϭ 1.3, qM ϭ 1.4, qU ϭ 1.45
x ϭ 19.875, s ϭ 0.835, IQR ϭ 1.5
x ϭ 1.355, s ϭ 0.136, IQR ϭ 0.15

Problem Set 24.2, page 1017
1. 23 outcomes: RRR, RRL, RLR, LRR, RLL, LRL, LLR, LLL
3. 62 ϭ 36 outcomes (1, 1), (1, 2), Á , (6, 6), first number (second number) referring
to the first die (second die)
5. Infinitely many outcomes H TH TTH TTTH Á (H ϭ Head, T ϭ Tail)
7. The space of ordered pairs of numbers
9. 10 outcomes: D ND NND Á NNNNNNNNND
11. Yes
17. A ʜB ϭ B implies A ʕ B by the definition of union. Conversely. A ʕ B implies
that A ʜB ϭ B because always B ʕ A ʜB, and if A ʕ B, we must have equality
in the previous relation.

bapp02_B.qxd

11/4/10

7:44 AM

Page A59

App. 2 Answers to Odd-Numbered Problems

A59

Problem Set 24.3, page 1024
1. 1 Ϫ 4>216 ϭ 98.15%, by Theorem 1
90
88
⅐ 89
3. (a) 0.93 ϭ 72.9%, (b) 100
99 ⅐ 98 ϭ 72.65%
8
5. 9
7. Small sample from a large population containing many items in each class we are
interested in (defectives and nondefectives, etc.)
497 496 495 494
9. 498
500 ؒ 499 ؒ 498 ؒ 497 ؒ 496 Ϸ 0.98008
99
100 100
100 100
11. (a) 100
200 ؒ 199 ϭ 24.874%, (b) 200 ؒ 199 ϩ 200 ؒ 199 ϭ 50.25%, (c) same as (a).
(a) ϩ (b) ϩ (c) ϭ 1. Why?
13. 1 Ϫ 0.963 ϭ 11.5%
15. 1 Ϫ 0.8754 ϭ 0.4138 Ͻ 1 Ϫ 0.752 ϭ 0.4375 Ͻ 0.5 (c Ͻ b Ͻ a)
17. A ϭ B ʜ(AʝB c), hence P(A) ϭ P(B) ϩ P(AʝB c) м P (B) by disjointedness of B
and AʝB c

Problem Set 24.4, page 1028
1. In 10! ϭ 3,628,800 ways
2 1
1
3. 26 ؒ 15 ؒ 44 ؒ 33 ؒ 22 ؒ 11 ϭ 46 ؒ 35 ؒ 24 ؒ 13 ؒ 22 ؒ 11 ϭ 4!2!
6! ϭ 6 ؒ 5 ϭ 15
10 5 6
5. A 3 B A 2 B A 2 B ϭ 18,000

7. 210, 70, 112, 28

9. In 6!>6 ϭ 120 ways
11. 9 ؒ 8 ϭ 72
13. (b) 1>(12n)
15. P (No two people have a birthday in common) ϭ 365 ؒ 364 Á 346>36520 ϭ 0.59.
Answer: 41%, which is surprisingly large.

Problem Set 24.5, page 1034
1
k ϭ 55
by (6)
1
k ϭ 4 by (10), P(0 Ϲ X Ϲ 2) ϭ 12
No, because of (6)
1
k ϭ 100
because of (6) and 1 ϩ 8 ϩ 27 ϩ 64 ϭ 100
k ϭ 5; 50%
0.53 ϭ 12.5%
F(x) ϭ 0 if x Ͻ Ϫ1, F(x) ϭ 12 (x ϩ 1)2 if Ϫ1 Ϲ x Ͻ 0
F(x) ϭ 1 Ϫ 12 (x Ϫ 1)2 if 0 Ϲ x Ͻ 1, F(x) ϭ 1 if x Ϲ 1
Answer: 500 cans, P ϭ 0.125, 0
15. X Ͼ b, X м b, X Ͻ c, X Ϲ c, etc.

1.
3.
5.
7.
9.
11.
13.

Problem Set 24.6, page 1038
1.
5.
9.
13.
17.
19.

k ϭ 12 , ␮ ϭ 43 , s2 ϭ 29
3. ␮ ϭ p, s2 ϭ p2>3; cf. Example 2
1
1
2
␮ ϭ 4 , s ϭ 16
7. C ϭ 12 , ␮ ϭ 2, s2 ϭ 4
750,
1,
0.002
11. c ϭ 0.073
1
$643.50
, (X Ϫ 12 )120
15. 12 , 20
X ϭ Product of the 2 numbers. E (X ) ϭ 12.25, 12 cents
(0 ϩ 1 ؒ 3 ϩ 3 ؒ 8 ϩ 1 ؒ 27)>8 ϭ 54>8 ϭ 6 ؒ 75

bapp02_B.qxd

11/4/10

A60

7:44 AM

Page A60

App. 2 Answers to Odd-Numbered Problems

Problem Set 24.7, page 1044
3.
5.
7.
9.
11.
13.
15.

38%
A 5x B 0.55, 0.03125, 0.15625, 1 Ϫ f (0) ϭ 0.96875, 0.96875
0.265
f (x) ϭ 0.5xe؊0.5>x!, f (0) ϩ f (1) ϭ e؊0.5(1.0 ϩ 0.5) ϭ 0.91. Answer: 9%
1314 %
42%, 47.2%, 10.5%, 0.3%
1 Ϫ e؊0.2 ϭ 18%

Problem Set 24.8, page 1050
1.
5.
9.
13.

0.1587, 0.5, 0.6915, 0.6247
15.9%
About 58%
About 683 (Fig. 521a)

3. 45.065, 56.978, 2.022
7. 31.1%, 95.4%
11. t ϭ 1084 hours

Problem Set 24.9, page 1059
1.
5.
7.
11.
15.
17.

1 3 3
8 , 16 , 8

3. 29 , 19 , 12

f2(y) ϭ 1>(b2 Ϫ a2) if a2 Ͻ y Ͻ b2
27.45 mm, 0.38 mm
25.26 cm, 0.0078 cm
13. 50%
The distributions in Prob. 17 and Example 1
No

Chapter 24 Review Questions and Problems, page 1060
Q L ϭ 110, Q M ϭ 112, Q U ϭ 115
x ϭ 111.9, s ϭ 4.0125, s 2 ϭ 16.1
x min Ϲ x j Ϲ x max. Sum over j from 1.
x ϭ 6, s ϭ 3.65
x
50؊x
f (x) ϭ A 50
Ϸ 1.5xe؊1.5>x!
x B 0.03 0.97
21. f (x) ϭ 2؊x, x ϭ 1, 2, Á
23. 1, 12
0.6306,
0.5,
0.4950
25. 0.1587,
11.
13.
21.
17.
19.

Problem Set 25.2, page 1067
n

ෂ 2 is as before.
1. In Example 1, ␮ ϭ 0 so a x j ϭ 0. 0 ln /> 0/ ϭ 0 and s
jϭ1

Á
3. / ϭ e؊n␮␮(x1 ϩ ϩxn )>(x 1! Á x n!), 0 ln /> 0␮ ϭ Ϫn ϩ (x 1 ϩ Á ϩ x n)>␮ ϭ 0,
n␮ˆ ϭ nx, ␮ˆ ϭ x ϭ 15.3
5. l ϭ p k(1 Ϫ p)n؊k, pˆ ϭ k>n, k ϭ number of successes in n trails
7. 7>12
9. l ϭ f ϭ p(1 Ϫ p)x؊1, etc., pˆ ϭ 1>x
11. uˆ ϭ n>S x j ϭ 1>x
13. uˆ ϭ 1
15. Variability larger than perhaps expected

bapp02_B.qxd

11/4/10

7:44 AM

Page A61

App. 2 Answers to Odd-Numbered Problems

A61

Problem Set 25.3, page 1077
3. Shorter by a factor 12
5. 4, 16
2
#
7. c ϭ 1.96, x ϭ 126, s ϭ 126 674>800 ϭ 106.155, k ϭ cs> 1n ϭ 0.714,
CONF0.95{125.3 Ϲ ␮ Ϲ 126.7}, CONF0.95{0.1566 Ϲ p Ϲ 0.1583}
9. CONF0.99{63.72 Ϲ ␮ Ϲ 66.28}
11. n Ϫ 1 ϭ 5, F(c) ϭ 0.995, c ϭ 4.03, x ϭ 9533.33, s 2 ϭ 49,666.67,
k ϭ 366.66 (Table 25.2), CONF0.99{9166.7 Ϲ ␮ Ϲ 9900}
13. CONF0.95{0.023 Ϲ s2 Ϲ 0.085}
15. n Ϫ 1 ϭ 99 degrees of freedom. F (c1) ϭ 0.025, c1 ϭ 74.2, F(c2) ϭ 0.975,
c2 ϭ 129.6. Hence k 1 ϭ 12.41, k 2 ϭ 7.10. CONF0.95 {7.10 Ϲ s2 Ϲ 12.41}.
17. CONF0.95{0.74 Ϲ s2 Ϲ 5.19}
19. Z ϭ X ϩ Y is normal with mean 105 and variance 1.25.
Answer: P (104 Ϲ Z Ϲ 106) ϭ 63%

Problem Set 25.4, page 1086
t ϭ (0.286 Ϫ 0)>(4.31> 17) ϭ 0.18 Ͻ c ϭ 1.94; accept the hypothesis.
c ϭ 6090 Ͼ 6019: do not reject the hypothesis.
s2>n ϭ 1.8, c ϭ 57.8, accept the hypothesis.
␮ Ͻ 58.69 or ␮ Ͼ 61.31
Alternative ␮ 5000, t ϭ (4990 Ϫ 5000)>(20> 150) ϭ Ϫ3.54 Ͻ c ϭ Ϫ2.01
(Table A9, Appendix 5). Reject the hypothesis ␮ ϭ 5000 g.
13. Two-sided. t ϭ (0.55 Ϫ 0)> 10.546>8 ϭ 2.11 Ͻ c ϭ 2.37 (Table A9, Appendix 5),
no difference
15. 19 ؒ 1.02>0.82 ϭ 29.69 Ͻ c ϭ 30.14 (Table A10. Appendix 5), accept the
hypothesis
17. By (12), t 0 ϭ 116(20.2 Ϫ 19.6)> 10.16 ϩ 0.36 Ͼ c ϭ 1.70. Assert that B is better.
3.
5.
7.
9.
11.

Problem Set 25.5, page 1091
1. LCL ϭ 1 Ϫ 2.58 # 0.02>2 ϭ 0.974, UCL ϭ 1.026
3.
5.
9.
11.
13.
15.

27
Choose 4 times the original sample size
2.5810.0004> 12 ϭ 0.036, LCL ϭ 3.464, UCL ϭ 3.536
LCL ϭ np Ϫ 31np(1 Ϫ p), CL ϭ np, UCL ϭ np ϩ 31np(1 Ϫ p)
In about 30% (5%) of the cases
LCL ϭ ␮ Ϫ 31␮ is negative in (b) and we set LCL ϭ 0, CL ϭ ␮ ϭ 3.6,
UCL ϭ ␮ ϩ 31␮ ϭ 9.3.

Problem Set 25.6, page 1095
1. 0.9825, 0.9384, 0.4060
5. e؊25u(1 ϩ 25u), P(A; 1.5) ϭ 94.5, a ϭ 5.5%
9. (1 Ϫ u)n ϩ nu(1 Ϫ u)n؊1

3. 0.8187, 0.6703, 0.1353
7. 19.5%, 14.7%
11. (1 Ϫ 12 )3 ϩ 3 # 12 (1 Ϫ 12 )2 ϭ 12

9
100
b 0.12x 0.88100؊x ϭ 22% (by the normal approximation)
13. a a
x
xϭ0

15. (1 Ϫ u)5, 3u(1 Ϫ u)5؊14 r ϭ 0, u ϭ 16, AOQL ϭ 6.7%

bapp02_B.qxd

11/4/10

A62

7:44 AM

Page A62

App. 2 Answers to Odd-Numbered Problems

Problem Set 25.7, page 1099
␹20 ϭ (40 Ϫ 50)2>50 ϩ (60 Ϫ 50)2>50 ϭ 4 Ͼ c ϭ 3.84; no
␹20 ϭ 16
10 Ͼ 11.07; yes
␹20 ϭ 10.264 Ͻ 11.07; yes
42 even digits, accept.
(355 Ϫ 358.5)2
(123 Ϫ 119.5)2
ϩ
ϭ 0.137 Ͻ c ϭ 3.84 (1 degree of
13. ␹20 ϭ
358.5
119.5
freedom, 95%)
15. Combining the last three nonzero values, we have K Ϫ r Ϫ 1 ϭ 9 (r ϭ 1 since we
2
estimated the mean, 10,094
2608 Ϸ 3.87). ␹0 ϭ 12.8 Ͻ c ϭ 16.92. Accept the hypothesis.
3.
5.
7.
9.

Problem Set 25.8, page 1102
3. (12)8 ϩ 8 # (12)8 ϭ 3.5% is the probability that 7 cases in 8 trials favor A under the
hypothesis that A and B are equally good. Reject.
5. (12)18(1 ϩ 18 ϩ 153 ϩ 816) ϭ 0.0038
7. x ϭ 9.67, s ϭ 11.87. t 0 ϭ 9.67>(11.87> 115) ϭ 3.16 Ͼ c ϭ 1.76 (a ϭ 5%).
Hypothesis rejected.
ෂ ϭ 0. Alternative ␮
ෂ Ͼ 0, x ϭ 1.58,
9. Hypothesis ␮
t ϭ 110 ؒ 1.58>1.23 ϭ 4.06 Ͼ c ϭ 1.83 (a ϭ 5%). Hypothesis rejected.
ෂ0 .
11. Consider yj ϭ x j Ϫ ␮
13. n ϭ 8; 4 transpositions, P(T Ϲ 4) ϭ 0.007. Assert that fertilizing increases yield.
15. P(T Ϲ 2) ϭ 2.8%. Assert that there is an increase.

Problem Set 25.9, page 1111
y ϭ 0.98 ϩ 0.495x
3. y ϭ Ϫ11,457.9 ϩ 43.2x
y ϭ Ϫ10 ϩ 0.55x
7. y ϭ 0.5932 ϩ 0.1138x, R ϭ 1>0.1138
y ϭ 0.32923 ϩ 0.00032x, y(66) ϭ 0.35035
c ϭ 3.18 (Table A9), k 1 ϭ 43.2, q0 ϭ 54,878, K ϭ 1.502,
CONF0.95{41.7 Ϲ ␬1 Ϲ 44.7}.
15. y Ϫ 1.875 ϭ 0.067(x Ϫ 25), 3s 2x ϭ 500, q0 ϭ 0.023, K ϭ 0.021,
CONF0.95{0.046 Ϲ ␬1 Ϲ 0.088}
1.
5.
9.
13.

Chapter 25 Review Questions and Problems, page 1111
␮ˆ ϭ 20.325, sˆ 2 ϭ (78)s 2 ϭ 3.982
17. CONF0.99{27.94 Ϲ ␮ Ϲ 34.81}
c ϭ 14.74 Ͼ 14.5, reject ␮0; £((14.74 Ϫ 14.50)> 10.025) ϭ 0.9353
2.58 ؒ 10.00024> 12 ϭ 0.028, LCL ϭ 2.722, UCL ϭ 2.778
a ϭ 1 Ϫ (1 Ϫ u)6 ϭ 5.85%, when u ϭ 0.01. For u ϭ 15% we obtain
b ϭ (1 Ϫ u)6 ϭ 37.7%. If n increases, so does a, whereas b decreases.
25. y ϭ 3.4 Ϫ 1.85x
15.
19.
21.
23.

bapp03.qxd

11/3/10

8:27 PM

Page A63

APPENDIX

3

Auxiliary Material
A3.1

Formulas for Special Functions
For tables of numeric values, see Appendix 5.
Exponential function e x (Fig. 545)
e ϭ 2.71828 18284 59045 23536 02874 71353
e xey ϭ exϩy,

(1)

e x/ey ϭ ex؊y,

(e x )y ϭ e xy

Natural logarithm (Fig. 546)
ln (xy) ϭ ln x ϩ ln y,

(2)

ln (x/y) ϭ ln x Ϫ ln y,

ln (x a) ϭ a ln x

ln x is the inverse of e x, and eln x ϭ x, e؊ln x ϭ eln (1/x) ϭ 1/x.
Logarithm of base ten log10 x or simply log x
(3)

log x ϭ M ln x,

1
(4) ln x ϭ ᎏ log x,
M

M ϭ log e ϭ 0.43429 44819 03251 82765 11289 18917
1
ᎏ ϭ ln 10 ϭ 2.30258 50929 94045 68401 79914 54684
M

log x is the inverse of 10x, and 10log x ϭ x, 10؊log x ϭ 1/x.
Sine and cosine functions (Figs. 547, 548). In calculus, angles are measured in radians,
so that sin x and cos x have period 2␲.
sin x is odd, sin (Ϫx) ϭ Ϫsin x, and cos x is even, cos (Ϫx) ϭ cos x.
y
y

5
2

0

–2

0

2

x

Fig. 545. Exponential function e x

5

10

x

–2

Fig. 546. Natural logarithm ln x

A63

bapp03.qxd

11/3/10

A64

8:27 PM

Page A64

APP. 3 Auxiliary Material
y

y

1

1
x

x

–1

–1

Fig. 547. sin x

Fig. 548. cos x

1° ϭ 0.01745 32925 19943 radian
1 radian ϭ 57° 17Ј 44.80625 Љ
ϭ 57.29577 95131°
sin2 x ϩ cos2 x ϭ 1

(5)

sin (x ϩ y) ϭ sin x cos y ϩ cos x sin y

w

(6)

sin (x Ϫ y) ϭ sin x cos y Ϫ cos x sin y
cos (x ϩ y) ϭ cos x cos y Ϫ sin x sin y
cos (x Ϫ y) ϭ cos x cos y ϩ sin x sin y

(7)

sin 2x ϭ 2 sin x cos x,

cos 2x ϭ cos2 x Ϫ sin2 x
␲
␲
sin x ϭ cos (x Ϫ ᎏᎏ) ϭ cos (ᎏᎏ Ϫ x)
2
2

w

(8)

(9)
(10)

(11)

␲
␲
cos x ϭ sin (x ϩ ᎏᎏ) ϭ sin (ᎏᎏ Ϫ x)
2
2

sin (␲ Ϫ x) ϭ sin x,

cos (␲ Ϫ x) ϭ Ϫcos x

cos2 x ϭ _12 (1 ϩ cos 2x),
sin2 x ϭ _12 (1 Ϫ cos 2x)
sin x sin y ϭ _12 [Ϫcos (x ϩ y) ϩ cos (x Ϫ y)]

y

cos x cos y ϭ _12 [cos (x ϩ y) ϩ cos (x Ϫ y)]
sin x cos y ϭ _12 [sin (x ϩ y) ϩ sin (x Ϫ y)]
uϩv
uϪv
sin u ϩ sin v ϭ 2 sin ᎏᎏ cos ᎏᎏ
2
2

(12)

uϩv
uϪv
s cos u ϩ cos v ϭ 2 cos ᎏ2ᎏ cos ᎏ2ᎏ
uϩv
uϪv
cos v Ϫ cos u ϭ 2 sin ᎏᎏ sin ᎏᎏ
2
2

(13)

2
A cos x ϩ B sin x ϭ ͙A
ෆ
ϩ B2ෆ cos (x Ϯ ␦),

sin ␦
B
tan ␦ ϭ ᎏ ϭ ϯ ᎏ
cos ␦
A

(14)

2
ෆ
ϩ B2ෆ sin (x Ϯ ␦),
A cos x ϩ B sin x ϭ ͙A

sin ␦
A
tan ␦ ϭ ᎏ ϭ Ϯ ᎏ
cos ␦
B

bapp03.qxd

11/3/10

8:27 PM

Page A65

SEC. A3.1 Formulas for Special Functions

A65

y

y

5

5

x

x

–5

–5

Fig. 549. tan x

Fig. 550. cot x

Tangent, cotangent, secant, cosecant (Figs. 549, 550)
sin x
(15) tan x ϭ ᎏ ,
cos x
(16)

cos x
cot x ϭ ᎏ ,
sin x

tan x ϩ tan y
tan (x ϩ y) ϭ ᎏᎏ ,
1 Ϫ tan x tan y

1
sec x ϭ ᎏ ,
cos x

1
csc x ϭ ᎏ
sin x

tan x Ϫ tan y
tan (x Ϫ y) ϭ ᎏᎏ
1 ϩ tan x tan y

Hyperbolic functions (hyperbolic sine sinh x, etc.; Figs. 551, 552)
(17)

sinh x ϭ _12 (e x Ϫ e؊x ),

(18)

sinh x
tanh x ϭ ᎏ ,
cosh x
cosh x ϩ sinh x ϭ e x,

(19)

cosh x ϭ _12 (e x ϩ e؊x )
cosh x
coth x ϭ ᎏ
sinh x
cosh x Ϫ sinh x ϭ e؊x

cosh2 x Ϫ sinh2 x ϭ 1

(20)

sinh2 x ϭ _12 (cosh 2x Ϫ 1),

(21)

cosh2 x ϭ _12 (cosh 2x ϩ 1)

y
4

y
4

2

2

–2

2

x

–2

2

–2

–2

–4

–4

Fig. 551. sinh x (dashed) and cosh x

x

Fig. 552. tanh x (dashed) and coth x

bapp03.qxd

11/3/10

A66

8:27 PM

Page A66

APP. 3 Auxiliary Material

(22)

(23)

{

sinh (x Ϯ y) ϭ sinh x cosh y Ϯ cosh x sinh y
cosh (x Ϯ y) ϭ cosh x cosh y Ϯ sinh x sinh y
tanh x Ϯ tanh y
tanh (x Ϯ y) ϭ ᎏᎏ
1 Ϯ tanh x tanh y

Gamma function (Fig. 553 and Table A2 in App. 5). The gamma function ⌫(␣) is defined
by the integral
⌫(␣) ϭ

(24)

͵e
ϱ

؊t ␣؊1

t

(␣ Ͼ 0),

dt

0

which is meaningful only if ␣ Ͼ 0 (or, if we consider complex ␣, for those ␣ whose real
part is positive). Integration by parts gives the important functional relation of the gamma
function,
⌫(␣ ϩ 1) ϭ ␣⌫(␣).

(25)

From (24) we readily have ⌫(1) ϭ 1; hence if ␣ is a positive integer, say k, then by
repeated application of (25) we obtain
⌫(k ϩ 1) ϭ k!

(26)

(k ϭ 0, 1, • • •).

This shows that the gamma function can be regarded as a generalization of the elementary
factorial function. [Sometimes the notation (␣ Ϫ 1)! is used for ⌫(␣), even for noninteger
values of ␣, and the gamma function is also known as the factorial function.]
By repeated application of (25) we obtain
⌫(␣ ϩ 2)
⌫(␣ ϩ k ϩ 1)
⌫(␣ ϩ 1)
⌫(␣) ϭ ᎏᎏ ϭ ᎏᎏ ϭ • • • ϭ ᎏᎏᎏᎏ
␣(␣ ϩ 1)
␣(␣ ϩ 1)(␣ ϩ 2) • • • (␣ ϩ k)
␣

Γ(α )

5

–4

–2

2

4

–2

–4

Fig. 553. Gamma function

α

bapp03.qxd

11/3/10

8:27 PM

Page A67

SEC. A3.1 Formulas for Special Functions

A67

and we may use this relation
⌫(␣ ϩ k ϩ 1)
⌫(␣) ϭ ᎏᎏᎏ
␣(␣ ϩ 1) • • • (␣ ϩ k)

(27)

(␣

0, Ϫ1, Ϫ2, • • •),

for defining the gamma function for negative ␣ ( Ϫ1, Ϫ2, • • •), choosing for k the
smallest integer such that ␣ ϩ k ϩ 1 Ͼ 0. Together with (24), this then gives a definition
of ⌫(␣) for all ␣ not equal to zero or a negative integer (Fig. 553).
It can be shown that the gamma function may also be represented as the limit of a
product, namely, by the formula
(28)

n! n␣
⌫(␣) ϭ lim ᎏᎏᎏᎏ
n*ϱ ␣(␣ ϩ 1)(␣ ϩ 2) • • • (␣ ϩ n)

(␣

0, Ϫ1, • • •).

From (27) or (28) we see that, for complex ␣, the gamma function ⌫(␣) is a meromorphic
function with simple poles at ␣ ϭ 0, Ϫ1, Ϫ2, • • • .
An approximation of the gamma function for large positive ␣ is given by the Stirling
formula

␣ ␣
⌫(␣ ϩ 1) Ϸ ͙2ෆ
␲␣ ( ᎏ )
e

(29)

where e is the base of the natural logarithm. We finally mention the special value
⌫(_12) ϭ ͙␲
ෆ.

(30)
Incomplete gamma functions

͵e
x

(31)

P(␣, x) ϭ

؊t ␣؊1

t

Q(␣, x) ϭ

dt,

؊t ␣؊1

t

dt

(␣ Ͼ 0)

x

0

(32)

ϱ

͵e

⌫(␣) ϭ P(␣, x) ϩ Q(␣, x)

Beta function

͵t
1

(33)

B(x, y) ϭ

(1 Ϫ t)y؊1 dt

x؊1

0

Representation in terms of gamma functions:
(34)

⌫(x)⌫(y)
B(x, y) ϭ ᎏ
⌫(x ϩ y)

Error function (Fig. 554 and Table A4 in App. 5)
(35)

(36)

2
erf x ϭ ᎏ
͙␲
ෆ

͵e
x

؊t2

dt

0

2
x3
x5
x7
erf x ϭ ᎏ (x Ϫ ᎏ ϩ ᎏ Ϫ ᎏ ϩ Ϫ • • •)
1!3
2!5
3!7
͙␲
ෆ

(x Ͼ 0, y Ͼ 0)

bapp03.qxd

11/3/10

A68

8:27 PM

Page A68

APP. 3 Auxiliary Material
erf x
1

0.5

–2

–1

1

2

x

–0.5

–1

Fig. 554. Error function

erf (ϱ) ϭ 1, complementary error function
2
erfc x ϭ 1 Ϫ erf x ϭ ᎏ
͙␲
ෆ

(37)

ϱ

͵e

؊t2

dt

x

Fresnel integrals1 (Fig. 555)

͵ cos (t ) dt,
x

C(x) ϭ

(38)

͵ sin (t ) dt
x

S(x) ϭ

2

0

2

0

C(ϱ) ϭ ͙ෆ,
␲/8 S(ϱ) ϭ ͙ෆ,
␲/8 complementary functions

(39)
s(x) ϭ

ϱ

␲
ᎏ Ϫ C(x) ϭ
8

͵

␲
ᎏ Ϫ S(x) ϭ
8

͵

Ί๶

c(x) ϭ

Ί๶

cos (t 2 ) dt

x

ϱ

sin (t 2 ) dt

x

Sine integral (Fig. 556 and Table A4 in App. 5)
sin t
͵ᎏ
dt
t
x

Si(x) ϭ

(40)

0

y
1

C (x)

0.5
S(x)

0

1

2

3

4

x

Fig. 555. Fresnel integrals
1

AUGUSTIN FRESNEL (1788–1827), French physicist and mathematician. For tables see Ref. [GenRef1].

bapp03.qxd

11/3/10

8:27 PM

Page A69

SEC. A3.2 Partial Derivatives

A69
Si(x)
2
1
0

5

10

x

Fig. 556. Sine integral

Si(ϱ) ϭ ␲ /2, complementary function
(41)

␲
si(x) ϭ ᎏ Ϫ Si(x) ϭ
2

ϱ

͵

x

sin t
ᎏ dt
t

Cosine integral (Table A4 in App. 5)
ci(x) ϭ

(42)

ϱ

͵

x

cos t
ᎏ dt
t

(x Ͼ 0)

Exponential integral
Ei(x) ϭ

(43)

ϱ

͵

x

e؊t
ᎏ dt
t

(x Ͼ 0)

Logarithmic integral
li(x) ϭ

(44)

͵

x

0

A3.2

dt
ᎏ
ln t

Partial Derivatives
For differentiation formulas, see inside of front cover.
Let z ϭ ƒ(x, y) be a real function of two independent real variables, x and y. If we keep
y constant, say, y ϭ y1, and think of x as a variable, then ƒ(x, y1) depends on x alone. If
the derivative of ƒ(x, y1) with respect to x for a value x ϭ x1 exists, then the value of this
derivative is called the partial derivative of ƒ(x, y) with respect to x at the point (x1, y1)
and is denoted by
Ѩƒ
ᎏj
Ѩx

or by

(x1,y1)

Ѩz
ᎏj
Ѩx

.

(x1,y1)

Other notations are
ƒx (x1, y1)

and

zx (x1, y1);

these may be used when subscripts are not used for another purpose and there is no danger
of confusion.

bapp03.qxd

11/5/10

12:09 AM

A70

Page A70

APP. 3 Auxiliary Material

We thus have, by the definition of the derivative,
Ѩƒ
ᎏ j
Ѩx

(1)

(x1,y1)

ƒ(x1 ϩ ⌬x, y1) Ϫ ƒ(x1, y1)
ϭ lim ᎏᎏᎏ .
⌬x
⌬x*0

The partial derivative of z ϭ ƒ(x, y) with respect to y is defined similarly; we now keep
x constant, say, equal to x1, and differentiate ƒ(x1, y) with respect to y. Thus
(2)

Ѩƒ
ᎏ j
Ѩy

(x1,y1)

Ѩz
ϭ ᎏj
Ѩy

(x1,y1)

ƒ(x1, y1 ϩ ⌬y) Ϫ ƒ(x1, y1)
ϭ lim ᎏᎏᎏ .
⌬y
⌬y*0

Other notations are ƒy (x1, y1) and zy (x1, y1).
It is clear that the values of those two partial derivatives will in general depend on the
point (x1, y1). Hence the partial derivatives Ѩz/Ѩx and Ѩz/Ѩy at a variable point (x, y) are
functions of x and y. The function Ѩz /Ѩx is obtained as in ordinary calculus by
differentiating z ϭ ƒ(x, y) with respect to x, treating y as a constant, and Ѩz/Ѩy is obtained
by differentiating z with respect to y, treating x as a constant.
EXAMPLE 1

Let z ϭ ƒ(x, y) ϭ x 2y ϩ x sin y. Then
Ѩƒ
ᎏ ϭ 2xy ϩ sin y,
Ѩx

Ѩƒ
ᎏ ϭ x 2 ϩ x cos y.
Ѩy

᭿

The partial derivatives Ѩz /Ѩx and Ѩz/Ѩy of a function z ϭ ƒ(x, y) have a very simple
geometric interpretation. The function z ϭ ƒ(x, y) can be represented by a surface in
space. The equation y ϭ y1 then represents a vertical plane intersecting the surface in a
curve, and the partial derivative Ѩz/Ѩx at a point (x1, y1) is the slope of the tangent (that
is, tan ␣ where ␣ is the angle shown in Fig. 557) to the curve. Similarly, the partial
derivative Ѩz/Ѩy at (x1, y1) is the slope of the tangent to the curve x ϭ x1 on the surface
z ϭ ƒ(x, y) at (x1, y1).

z

y
x
y1

x1

Fig. 557. Geometrical interpretation of first partial derivatives

bapp03.qxd

11/3/10

8:27 PM

Page A71

SEC. A3.2 Partial Derivatives

A71

The partial derivatives Ѩz/Ѩx and Ѩz/Ѩy are called first partial derivatives or partial
derivatives of first order. By differentiating these derivatives once more, we obtain the
four second partial derivatives (or partial derivatives of second order)2
Ѩ2ƒ
Ѩ
Ѩƒ
ᎏ
2 ϭ ᎏ ( ᎏ ) ϭ ƒxx
Ѩx
Ѩx
Ѩx
Ѩ
Ѩ2ƒ
Ѩƒ
ᎏ ϭ ᎏ ( ᎏ ) ϭ ƒyx
Ѩx
Ѩx Ѩy
Ѩy

(3)

Ѩ2ƒ
Ѩƒ
Ѩ
ᎏ ϭ ᎏ ( ᎏ ) ϭ ƒxy
Ѩy Ѩx
Ѩx
Ѩy
Ѩ
Ѩƒ
Ѩ2ƒ
ᎏ ( ᎏ ) ϭ ƒyy.
ᎏ
2 ϭ
Ѩy
Ѩy
Ѩy

It can be shown that if all the derivatives concerned are continuous, then the two mixed
partial derivatives are equal, so that the order of differentiation does not matter (see Ref.
[GenRef4] in App. 1), that is,
Ѩ2z
Ѩ2z
ᎏ ϭ ᎏ .
Ѩx Ѩy
Ѩy Ѩx

(4)
EXAMPLE 2

For the function in Example 1.
ƒxx ϭ 2y,

ƒxy ϭ 2x ϩ cos y ϭ ƒyx,

ƒyy ϭ Ϫx sin y.

᭿

By differentiating the second partial derivatives again with respect to x and y,
respectively, we obtain the third partial derivatives or partial derivatives of the third
order of ƒ, etc.
If we consider a function ƒ(x, y, z) of three independent variables, then we have the
three first partial derivatives ƒx (x, y, z), ƒy (x, y, z), and ƒz (x, y, z). Here ƒx is obtained by
differentiating ƒ with respect to x, treating both y and z as constants. Thus, analogous to
(1), we now have
ƒ(x1 ϩ ⌬x, y1, z1) Ϫ ƒ(x1, y1, z1)
Ѩƒ
ᎏ j
ϭ lim ᎏᎏᎏᎏ ,
⌬x
Ѩx (x1,y1,z1) ⌬ x*0
etc. By differentiating ƒx, ƒy, ƒz again in this fashion we obtain the second partial
derivatives of ƒ, etc.
EXAMPLE 3

Let ƒ(x, y, z) ϭ x 2 ϩ y 2 ϩ z 2 ϩ xy ez. Then
z
ƒx ϭ 2x ϩ y e ,

2

z
ƒy ϭ 2y ϩ x e ,

z
ƒz ϭ 2z ϩ xy e ,

ƒxx ϭ 2,

z
ƒxy ϭ ƒyx ϭ e ,

z
ƒxz ϭ ƒzx ϭ y e ,

ƒyy ϭ 2,

z
ƒyz ϭ ƒzy ϭ x e ,

z
ƒzz ϭ 2 ϩ xy e .

᭿

CAUTION! In the subscript notation, the subscripts are written in the order in which we differentiate,
whereas in the “Ѩ” notation the order is opposite.

bapp03.qxd

11/3/10

8:27 PM

A72

Page A72

APP. 3 Auxiliary Material

A3.3

Sequences and Series
See also Chap. 15.

Monotone Real Sequences
We call a real sequence x1, x2, • • • , xn, • • • a monotone sequence if it is either monotone
increasing, that is,
x 1 Ϲ x2 Ϲ x3 Ϲ • • •
or monotone decreasing, that is,
x 1 м x2 м x3 м • • • .
We call x1, x2, • • • a bounded sequence if there is a positive constant K such that ͉xn ͉ Ͻ K
for all n.
If a real sequence is bounded and monotone, it converges.

THEOREM 1

PROOF

Let x1, x2, • • • be a bounded monotone increasing sequence. Then its terms are smaller
than some number B and, since x1 Ϲ xn for all n, they lie in the interval x1 Ϲ xn Ϲ B,
which will be denoted by I0. We bisect I0; that is, we subdivide it into two parts of equal
length. If the right half (together with its endpoints) contains terms of the sequence, we
denote it by I1. If it does not contain terms of the sequence, then the left half of I0 (together
with its endpoints) is called I1. This is the first step.
In the second step we bisect I1, select one half by the same rule, and call it I2, and so
on (see Fig. 558).
In this way we obtain shorter and shorter intervals I0, I1, I2, • • • with the following
properties. Each Im contains all In for n Ͼ m. No term of the sequence lies to the right
of Im, and, since the sequence is monotone increasing, all xn with n greater than some
number N lie in Im; of course, N will depend on m, in general. The lengths of the Im
approach zero as m approaches infinity. Hence there is precisely one number, call it L,
that lies in all those intervals,3 and we may now easily prove that the sequence is
convergent with the limit L.
In fact, given an ⑀ Ͼ 0, we choose an m such that the length of Im is less than ⑀. Then
L and all the xn with n Ͼ N(m) lie in Im, and, therefore, ͉ xn Ϫ L͉ Ͻ ⑀ for all those n.
This completes the proof for an increasing sequence. For a decreasing sequence the proof
is the same, except for a suitable interchange of “left” and “right” in the construction of
those intervals.
᭿
3

This statement seems to be obvious, but actually it is not; it may be regarded as an axiom of the real number
system in the following form. Let J1, J2, • • • be closed intervals such that each Jm contains all Jn with n Ͼ m,
and the lengths of the Jm approach zero as m approaches infinity. Then there is precisely one real number that
is contained in all those intervals. This is the so-called Cantor–Dedekind axiom, named after the German
mathematicians GEORG CANTOR (1845–1918), the creator of set theory, and RICHARD DEDEKIND
(1831–1916), known for his fundamental work in number theory. For further details see Ref. [GenRef2] in App. 1.
(An interval I is said to be closed if its two endpoints are regarded as points belonging to I. It is said to be open
if the endpoints are not regarded as points of I.)

bapp03.qxd

11/3/10

8:27 PM

Page A73

SEC. A3.3 Sequences and Series

A73
I0
x1

x2

x3

B

I1
I2

Fig. 558. Proof of Theorem 1

Real Series
THEOREM 2

Leibniz Test for Real Series

Let x1, x2, • • • be real and monotone decreasing to zero, that is,
(1)

(a) x1 м x2 м x3 м • • • ,

(b) lim xm ϭ 0.
m*ϱ

Then the series with terms of alternating signs
x1 Ϫ x2 ϩ x3 Ϫ x4 ϩ Ϫ • • •
converges, and for the remainder Rn after the nth term we have the estimate
͉Rn͉ Ϲ xnϩ1.

(2)

PROOF

Let sn be the nth partial sum of the series. Then, because of (1a),
s1 ϭ x1,

s2 ϭ x1 Ϫ x2 Ϲ s1,

s3 ϭ s2 ϩ x3 м s2,

s3 ϭ s1 Ϫ (x2 Ϫ x3) Ϲ s1,

so that s2 Ϲ s3 Ϲ s1. Proceeding in this fashion, we conclude that (Fig. 559)
(3)

s1 м s3 м s5 м • • • м s6 м s4 м s2

which shows that the odd partial sums form a bounded monotone sequence, and so do the
even partial sums. Hence, by Theorem 1, both sequences converge, say,
lim s2nϩ1 ϭ s,

lim s2n ϭ s*.

n*ϱ

n*ϱ

–x2
x3
–x4
s2

s4

s3

s1

Fig. 559. Proof of the Leibniz test

bapp03.qxd

11/3/10

8:27 PM

A74

Page A74

APP. 3 Auxiliary Material

Now, since s2nϩ1 Ϫ s2n ϭ x2nϩ1, we readily see that (lb) implies
s Ϫ s* ϭ lim s2nϩ1 Ϫ lim s2n ϭ lim (s2nϩ1 Ϫ s2n) ϭ lim x2nϩ1 ϭ 0.
n*ϱ

n*ϱ

n*ϱ

n*ϱ

Hence s* ϭ s, and the series converges with the sum s.
We prove the estimate (2) for the remainder. Since sn * s, it follows from (3) that
s2nϩ1 м s м s2n

s2n؊1 м s м s2n.

and also

By subtracting s2n and s2n؊1, respectively, we obtain
s2nϩ1 Ϫ s2n м s Ϫ s2n м 0,

0 м s Ϫ s2n؊1 м s2n Ϫ s2n؊1.

In these inequalities, the first expression is equal to x2nϩ1, the last is equal to Ϫx2n, and
the expressions between the inequality signs are the remainders R2n and R2n؊1. Thus the
inequalities may be written
x2nϩ1 м R2n м 0,

0 м R2n؊1 м Ϫx2n
᭿

and we see that they imply (2). This completes the proof.

A3.4

Grad, Div, Curl, ٌ2
in Curvilinear Coordinates
To simplify formulas, we write Cartesian coordinates x ϭ x1, y ϭ x2, z ϭ x3. We denote
curvilinear coordinates by q1, q2, q3. Through each point P there pass three coordinate
surfaces q1 ϭ const, q2 ϭ const, q3 ϭ const. They intersect along coordinate curves. We
assume the three coordinate curves through P to be orthogonal (perpendicular to each
other). We write coordinate transformations as
(1)

x1 ϭ x1(q1, q2, q3),

x2 ϭ x2(q1, q2, q3),

x3 ϭ x3(q1, q2, q3).

Corresponding transformations of grad, div, curl, and ٌ2 can all be written by using
3

hj2 ϭ

(2)

Ѩxk

2

)
͚ (ᎏ
Ѩq

kϭ1

.

j

Next to Cartesian coordinates, most important are cylindrical coordinates q1 ϭ r, q2 ϭ ␪,
q3 ϭ z (Fig. 560a) defined by
(3)

x1 ϭ q1 cos q2 ϭ r cos ␪,

x2 ϭ q1 sin q2 ϭ r sin ␪,

x3 ϭ q3 ϭ z

and spherical coordinates q1 ϭ r, q2 ϭ ␪, q3 ϭ ␾ (Fig. 560b) defined by4
(4)

x1 ϭ q1 cos q2 sin q3 ϭ r cos ␪ sin ␾,

x2 ϭ q1 sin q2 sin q3 ϭ r sin ␪ sin ␾

x3 ϭ q1 cos q3 ϭ r cos ␾.

4
This is the notation used in calculus and in many other books. It is logical since in it, ␪ plays the same role
as in polar coordinates. CAUTION! Some books interchange the roles of ␪ and ␾.

bapp03.qxd

11/3/10

8:27 PM

Page A75

SEC. A3.4 Grad, Div, Curl, ٌ2 in Curvilinear Coordinates

A75
z

z

r
z
y

y
r

x

x
(a) Cylindrical coordinates

(b) Spherical coordinates

Fig. 560. Special curvilinear coordinates

In addition to the general formulas for any orthogonal coordinates q1, q2, q3, we shall give
additional formulas for these important special cases.
Linear Element ds. In Cartesian coordinates,
ds 2 ϭ dx12 ϩ dx22 ϩ dx32

(Sec. 9.5).

For the q-coordinates,
(5)
(5Ј)

ds 2 ϭ h12 dq12 ϩ h22 dq22 ϩ h32 dq32.
ds 2 ϭ dr 2 ϩ r 2 d␪ 2 ϩ dz 2

(Cylindrical coordinates).

For polar coordinates set dz 2 ϭ 0.
(5 Љ)

ds 2 ϭ dr 2 ϩ r 2 sin2 ␾ d␪ 2 ϩ r 2 d␾ 2

(Spherical coordinates).

Gradient. grad ƒ ϭ ٌƒ ϭ [ƒx1, ƒx2, ƒx3] (partial derivatives; Sec. 9.7). In the
q-system, with u, v, w denoting unit vectors in the positive directions of the q1, q2, q3
coordinate curves, respectively,
(6)

1 Ѩƒ
1 Ѩƒ
1 Ѩƒ
grad ƒ ϭ ٌƒ ϭ ᎏ ᎏ u ϩ ᎏ ᎏ v ϩ ᎏ ᎏ w
h1 Ѩq1
h2 Ѩq2
h3 Ѩq3

(6Ј)

1 Ѩƒ
Ѩƒ
Ѩƒ
grad ƒ ϭ ٌƒ ϭ ᎏ u ϩ ᎏ ᎏ v ϩ ᎏ w
Ѩr
Ѩz
r Ѩ␪

(6 Љ)

1
1 Ѩƒ
Ѩƒ
Ѩƒ
grad ƒ ϭ ٌƒ ϭ ᎏ u ϩ ᎏ ᎏ v ϩ ᎏ ᎏ w
r sin ␾ Ѩ␪
Ѩr
r Ѩ␾

(Cylindrical coordinates)

(Spherical coordinates).

Divergence div F ϭ ٌ•F ϭ (F1)x1 ϩ (F2)x2 ϩ (F3)x3 (F ϭ [F1, F2, F3], Sec. 9.8);
(7)

Ѩ
Ѩ
1
Ѩ
div F ϭ ٌ•F ϭ ᎏ [ ᎏ (h2 h3 F1) ϩ ᎏ (h3 h1F2) ϩ ᎏ (h1h2 F3)]
Ѩq1
Ѩq2
h1h2 h3
Ѩq3

(7Ј)

ѨF3
1 Ѩ
1 ѨF2
div F ϭ ٌ • F ϭ ᎏ ᎏ (rF1) ϩ ᎏ ᎏ ϩ ᎏ
Ѩz
r Ѩr
r Ѩ␪

(Cylindrical coordinates)

bapp03.qxd

11/3/10

A76

8:27 PM

Page A76

APP. 3 Auxiliary Material

(7 Љ)

ѨF2
1
1
1 Ѩ
Ѩ
div F ϭ ٌ • F ϭ ᎏ2 ᎏ (r 2F1) ϩ ᎏ ᎏ ϩ ᎏ ᎏ (sin ␾ F3 )
r sin ␾ Ѩ␪
r sin ␾ Ѩ␾
r Ѩr
(Spherical coordinates).

Laplacian ٌ2ƒ ϭ ٌ•ٌƒ ϭ div (grad ƒ) ϭ ƒx1x1 ϩ ƒx2x2 ϩ ƒx3x3 (Sec. 9.8):
1
h2 h3 Ѩƒ
h3 h1 Ѩƒ
h1h2 Ѩƒ
Ѩ
Ѩ
Ѩ
(8) ٌ2ƒ ϭ ᎏ [ ᎏ ( ᎏ ᎏ ) ϩ ᎏ ( ᎏ ᎏ ) ϩ ᎏ ( ᎏ ᎏ )]
h1h2 h3 Ѩq1
h1 Ѩq1
h2 Ѩq2
h3 Ѩq3
Ѩq3
Ѩq2
(8Ј)

Ѩ2ƒ
Ѩ2ƒ
1 Ѩƒ
1 Ѩ2ƒ
ٌ2ƒ ϭ ᎏ2 ϩ ᎏ ᎏ ϩ ᎏ2 ᎏ2 ϩ ᎏ
r Ѩr
r Ѩ␪
Ѩz2
Ѩr

(8 Љ)

cot ␾ Ѩƒ
Ѩ2ƒ
Ѩ2ƒ
1
2 Ѩƒ
1 Ѩ2ƒ
ᎏ2 ϩ ᎏ2 ᎏ2 ϩ ᎏ
ᎏ
ٌ2ƒ ϭ ᎏ2 ϩ ᎏ ᎏ ϩ ᎏ
2
2
r 2 Ѩ␾
r sin ␾ Ѩ␪
r Ѩr
r Ѩ␾
Ѩr

(Cylindrical coordinates)

(Spherical coordinates).
Curl (Sec. 9.9):

(9)

1
curl F ϭ ٌ ؋ F ϭ ᎏᎏ
h1h2 h3

h1u

h2v

h3w

ᎏ
lᎏ
Ѩq

Ѩ

1

Ѩ
ᎏᎏ
Ѩq2

Ѩ
ᎏᎏl .
Ѩq3

h1F1

h2 F2

h3F3

For cylindrical coordinates we have in (9) (as in the previous formulas)
h1 ϭ hr ϭ 1,

h2 ϭ h␪ ϭ q1 ϭ r,

h3 ϭ h z ϭ 1

and for spherical coordinates we have
h1 ϭ hr ϭ 1,

h2 ϭ h␪ ϭ q1 sin q3 ϭ r sin ␾,

h3 ϭ h␾ ϭ q1 ϭ r.

bapp04a.qxd

11/3/10

8:35 PM

Page A77

APPENDIX

4

Additional Proofs
Section 2.6, page 74
P R O O F O F T H E O R E M 1 Uniqueness1
Assuming that the problem consisting of the ODE
y Љ ϩ p(x)yЈ ϩ q(x)y ϭ 0

(1)
and the two initial conditions
(2)

y(x0) ϭ K0,

yЈ(x0) ϭ K1

has two solutions y1(x) and y2(x) on the interval I in the theorem, we show that their
difference
y(x) ϭ y1(x) Ϫ y2(x)
is identically zero on I; then y1 ϵ y2 on I, which implies uniqueness.
Since (1) is homogeneous and linear, y is a solution of that ODE on I, and since y1 and
y2 satisfy the same initial conditions, y satisfies the conditions
y(x0) ϭ 0,

(11)

yЈ(x0) ϭ 0.

We consider the function
z(x) ϭ y(x)2 ϩ yЈ(x)2
and its derivative
zЈ ϭ 2yyЈ ϩ 2yЈy Љ.
From the ODE we have
y Љ ϭ ϪpyЈ Ϫ qy.
By substituting this in the expression for zЈ we obtain
zЈ ϭ 2yyЈ Ϫ 2pyЈ2 Ϫ 2qyyЈ.

(12)
Now, since y and yЈ are real,

(y Ϯ yЈ)2 ϭ y 2 Ϯ 2yyЈ ϩ yЈ2 м 0.

1

This proof was suggested by my colleague, Prof. A. D. Ziebur. In this proof, we use some formula numbers
that have not yet been used in Sec. 2.6.

A77

bapp04a.qxd

11/3/10

A78

8:35 PM

Page A78

APP. 4 Additional Proofs

From this and the definition of z we obtain the two inequalities
(13)

(a) 2yyЈ Ϲ y 2 ϩ yЈ2 ϭ z,

(b) Ϫ2yyЈ Ϲ y 2 ϩ yЈ2 ϭ z.

From (13b) we have 2yyЈ м Ϫz. Together, ͉2yyЈ͉ Ϲ z. For the last term in (12) we now
obtain
Ϫ2qyyЈ Ϲ ͉Ϫ2qyyЈ͉ ϭ ͉q͉͉2yyЈ͉ Ϲ ͉q͉z.
Using this result as well as Ϫp Ϲ ͉p͉ and applying (13a) to the term 2yyЈ in (12), we find
zЈ Ϲ z ϩ 2͉p͉yЈ2 ϩ ͉q͉z.
Since yЈ2 Ϲ y 2 ϩ yЈ2 ϭ z, from this we obtain
zЈ Ϲ (1 ϩ 2͉p͉ ϩ ͉q͉)z
or, denoting the function in parentheses by h,
zЈ Ϲ hz

(14a)

for all x on I.

Similarly, from (12) and (13) it follows that
ϪzЈ ϭ Ϫ2yyЈ ϩ 2pyЈ2 ϩ 2qyyЈ

(14b)

Ϲ z ϩ 2͉p͉z ϩ ͉q͉z ϭ hz.

The inequalities (14a) and (14b) are equivalent to the inequalities
(15)

zЈ Ϫ hz Ϲ 0,

zЈ ϩ hz м 0.

Integrating factors for the two expressions on the left are
F1 ϭ e؊͐h(x) dx

and

F2 ϭ e͐h(x) dx.

The integrals in the exponents exist because h is continuous. Since F1 and F2 are positive,
we thus have from (15)
F1(zЈ Ϫ hz) ϭ (F1 z)Ј Ϲ 0

and

F2(zЈ ϩ hz) ϭ (F2 z)Ј м 0.

This means that F1 z is nonincreasing and F2 z is nondecreasing on I. Since z(x0) ϭ 0 by
(11), when x Ϲ x0 we thus obtain
F1 z м (F1 z)x0 ϭ 0,

F2 z Ϲ (F2 z)x0 ϭ 0

and similarly, when x м x0,
F1 z Ϲ 0,

F2 z м 0.

Dividing by F1 and F2 and noting that these functions are positive, we altogether have
z Ϲ 0,

zм0

This implies that z ϭ y 2 ϩ yЈ2 ϵ 0 on I. Hence y ϵ 0 or y1 ϵ y2 on I.

for all x on I.
᭿

bapp04a.qxd

11/3/10

8:35 PM

Page A79

APP. 4 Additional Proofs

A79

Section 5.3, page 182
P R O O F O F T H E O R E M 2 Frobenius Method. Basis of Solutions. Three Cases
The formula numbers in this proof are the same as in the text of Sec. 5.3. An additional
formula not appearing in Sec. 5.3 will be called (A) (see below).
The ODE in Theorem 2 is
b(x)
c(x)
y Љ ϩ ᎏ yЈ ϩ ᎏ
y ϭ 0,
x
x2

(1)

where b(x) and c(x) are analytic functions. We can write it
(1Ј)

x 2y Љ ϩ xb(x)yЈ ϩ c(x)y ϭ 0.

The indicial equation of (1) is
r(r Ϫ 1) ϩ b0 r ϩ c0 ϭ 0.

(4)

The roots r1, r2 of this quadratic equation determine the general form of a basis of solutions
of (1), and there are three possible cases as follows.
Case 1. Distinct Roots Not Differing by an Integer. A first solution of (1) is of the form
(5)

y1(x) ϭ xr1 (a0 ϩ a1x ϩ a2x 2 ϩ • • •)

and can be determined as in the power series method. For a proof that in this case, the
ODE (1) has a second independent solution of the form
(6)

y2(x) ϭ x r2 (A0 ϩ A1 x ϩ A2 x 2 ϩ • • •),

see Ref. [A11] listed in App. 1.
Case 2. Double Root. The indicial equation (4) has a double root r if and only if
(b0 Ϫ 1)2 Ϫ 4c0 ϭ 0, and then r ϭ _12 (1 Ϫ b0). A first solution
(7)

y1(x) ϭ x r (a0 ϩ a1 x ϩ a2 x 2 ϩ • • •),

r ϭ _12 (1 Ϫ b0),

can be determined as in Case 1. We show that a second independent solution is of the
form
(8)

y2(x) ϭ y1(x) ln x ϩ x r (A1 x ϩ A2 x 2 ϩ • • •)

(x Ͼ 0).

We use the method of reduction of order (see Sec. 2.1), that is, we determine u(x) such
that y2(x) ϭ u(x)y1(x) is a solution of (1). By inserting this and the derivatives
y2Ј ϭ uЈy1 ϩ uy1Ј,

y 2Љ ϭ uЉy1 ϩ 2uЈy1Ј ϩ uy 1Љ

into the ODE (1Ј) we obtain
x 2(uЉy1 ϩ 2uЈy1Ј ϩ uy 1Љ) ϩ xb(uЈy1 ϩ uy1Ј) ϩ cuy1 ϭ 0.

bapp04a.qxd

11/3/10

A80

8:35 PM

Page A80

APP. 4 Additional Proofs

Since y1 is a solution of (1Ј), the sum of the terms involving u is zero, and this equation
reduces to
x 2y1 u Љ ϩ 2x 2y1Ј uЈ ϩ xby1 uЈ ϭ 0.
By dividing by x2 y1 and inserting the power series for b we obtain
y1Ј
b0
u Љ ϩ (2 ᎏ ϩ ᎏ ϩ • • •) uЈ ϭ 0.
x
y1
Here, and in the following, the dots designate terms that are constant or involve positive
powers of x. Now, from (7), it follows that
y 1Ј
xr ؊1[ra0 ϩ (r ϩ 1)a1x ϩ • • •]
ᎏ ϭ ᎏᎏᎏᎏ
r
y1

x [a0 ϩ a1x ϩ • • •]

r
1 ra0 ϩ (r ϩ 1)a1x ϩ • • •
ϭ ᎏ ( ᎏᎏᎏ
)ϭ ᎏ ϩ•••.
x
x
a0 ϩ a1x ϩ • • •
Hence the previous equation can be written
(A)

2r ϩ b0
uЉ ϩ ( ᎏ
ϩ • • •) uЈ ϭ 0.
x

Since r ϭ (1 Ϫ b0)/2, the term (2r ϩ b0)/x equals 1/x, and by dividing by uЈ we thus
have
uЉ
1
ᎏ ϭϪᎏ ϩ•••.
x
uЈ
By integration we obtain ln uЈ ϭ Ϫln x ϩ • • • , hence uЈ ϭ (1/x)e(• • •). Expanding the
exponential function in powers of x and integrating once more, we see that u is of the form
u ϭ ln x ϩ k1x ϩ k2 x 2 ϩ • • • .
Inserting this into y2 ϭ uy1, we obtain for y2 a representation of the form (8).
Case 3. Roots Differing by an Integer.
positive integer. A first solution
(9)

We write r1 ϭ r and r2 ϭ r Ϫ p where p is a

y1(x) ϭ xr1 (a0 ϩ a1x ϩ a2 x 2 ϩ • • •)

can be determined as in Cases 1 and 2. We show that a second independent solution is
of the form
(10)

y2(x) ϭ ky1(x) ln x ϩ xr2 (A0 ϩ A1x ϩ A2x 2 ϩ • • •)

where we may have k
0 or k ϭ 0. As in Case 2 we set y2 ϭ uy1. The first steps are
literally as in Case 2 and give Eq. (A),
2r ϩ b0
ϩ • • •) uЈ ϭ 0.
uЉ ϩ ( ᎏ
x

bapp04a.qxd

11/3/10

8:35 PM

Page A81

APP. 4 Additional Proofs

A81

Now by elementary algebra, the coefficient b0 Ϫ 1 of r in (4) equals minus the sum of
the roots,
b0 Ϫ 1 ϭ Ϫ(r1 ϩ r2) ϭ Ϫ(r ϩ r Ϫ p) ϭ Ϫ2r ϩ p.
Hence 2r ϩ b0 ϭ p ϩ 1, and division by uЈ gives
uЉ
pϩ1
ᎏ ϭ Ϫ ( ᎏ ϩ • • •) .
x
uЈ
The further steps are as in Case 2. Integrating, we find
ln uЈ ϭ Ϫ(p ϩ 1) ln x ϩ • • • ,

uЈ ϭ x

thus

؊( pϩ1) (• • •)

e

where dots stand for some series of nonnegative integer powers of x. By expanding the
exponential function as before we obtain a series of the form
k1
kp
kp؊1
1
ᎏ
ᎏ
uЈ ϭ ᎏ
ϩ ᎏ ϩ kpϩ1 ϩ kpϩ2 x ϩ • • • .
p ϩ • • • ϩ
2
pϩ1 ϩ
x
x
x
x
We integrate once more. Writing the resulting logarithmic term first, we get
1
kp؊1
u ϭ kp ln x ϩ (Ϫ ᎏ
Ϫ • • • Ϫ ᎏ ϩ kpϩ1x ϩ • • •) .
px p
x
Hence, by (9) we get for y2 ϭ uy1 the formula
y2 ϭ kp y1 ln x ϩ x

r1؊p

1
p؊1
(Ϫ ᎏ Ϫ • • • Ϫ kp؊1x
ϩ • • •) (a0 ϩ a1x ϩ • • •).
p

But this is of the form (10) with k ϭ kp since r1 Ϫ p ϭ r2 and the product of the two
series involves nonnegative integer powers of x only.
᭿

Section 7.7, page 293
THEOREM

Determinants

The definition of a determinant

(7)

D ϭ det A ϭ

a11

a12

•••

a1n

a21

a22

•••

a2n

•

•

•••

•

•

•

•••

•

an1

an2

•••

ann

l

l

as given in Sec. 7.7 is unambiguous, that is, it yields the same value of D no matter
which rows or columns we choose in the development.

bapp04a.qxd

11/5/10

12:09 AM

A82

Page A82

APP. 4 Additional Proofs

PROOF

In this proof we shall use formula numbers not yet used in Sec. 7.7.
We shall prove first that the same value is obtained no matter which row is chosen.
The proof is by induction. The statement is true for a second-order determinant, for
which the developments by the first row a11a22 ϩ a12(Ϫa21) and by the second row
a21(Ϫa12) ϩ a22 a11 give the same value a11a22 Ϫ a12 a21. Assuming the statement to be
true for an (n Ϫ 1)st-order determinant, we prove that it is true for an nth-order determinant.
For this purpose we expand D in terms of each of two arbitrary rows, say, the ith and
the jth, and compare the results. Without loss of generality let us assume i Ͻ j.
First Expansion. We expand D by the ith row. A typical term in this expansion is
aik Cik ϭ aik ⅐ (Ϫ1)iϩkMik.

(19)

The minor Mik of aik in D is an (n Ϫ 1)st-order determinant. By the induction hypothesis
we may expand it by any row. We expand it by the row corresponding to the jth row of
D. This row contains the entries ajl (l k). It is the ( j Ϫ 1)st row of Mik, because Mik
does not contain entries of the ith row of D, and i Ͻ j. We have to distinguish between
two cases as follows.
Case I. If l Ͻ k, then the entry ajl belongs to the lth column of Mik (see Fig. 561). Hence
the term involving ajl in this expansion is
(20)

ajl ⅐ (cofactor of ajl in Mik) ϭ ajl ⅐ (Ϫ1)(j؊1)ϩlMikjl

where Mikjl is the minor of ajl in Mik. Since this minor is obtained from Mik by deleting
the row and column of ajl , it is obtained from D by deleting the ith and jth rows and the
kth and lth columns of D. We insert the expansions of the Mik into that of D. Then it follows
from (19) and (20) that the terms of the resulting representation of D are of the form
aik ajl ⅐ (Ϫ1)bMikjl

(21a)

(l Ͻ k)

where
b ϭ i ϩ k ϩ j ϩ l Ϫ 1.
Case II. If l Ͼ k, the only difference is that then ajl belongs to the (l Ϫ 1)st column of
Mik, because Mik does not contain entries of the kth column of D, and k Ͻ l. This causes
an additional minus sign in (20), and, instead of (21a), we therefore obtain
Ϫaik ajl ⅐ (Ϫ1)bMikjl

(21b)

(l Ͼ k)

where b is the same as before.
lth
col.

kth
col.

aik
ajl

kth
col.

ith row

lth
col.

aik
ajl

jth row

Case I

Case II

Fig. 561. Cases I and II of the two expansions of D

bapp04a.qxd

11/3/10

8:35 PM

Page A83

APP. 4 Additional Proofs

A83

Second Expansion.
expansion is

We now expand D at first by the jth row. A typical term in this
ajl Cjl ϭ ajl ⅐ (Ϫ1) jϩlMjl.

(22)

By the induction hypothesis we may expand the minor Mjl of ajl in D by its ith row, which
corresponds to the ith row of D, since j Ͼ i.
Case I. If k Ͼ l, the entry aik in that row belongs to the (k Ϫ 1)st column of Mjl , because
Mjl does not contain entries of the lth column of D, and l Ͻ k (see Fig. 561). Hence the
term involving aik in this expansion is
aik ⅐ (cofactor of aik in Mjl) ϭ aik ⅐ (Ϫ1)iϩ(k ؊1)Mikjl ,

(23)

where the minor Mikjl of aik in Mjl is obtained by deleting the i th and j th rows and the
kth and lth columns of D [and is, therefore, identical with Mikjl in (20), so that our notation
is consistent]. We insert the expansions of the Mjl into that of D. It follows from (22) and
(23) that this yields a representation whose terms are identical with those given by (21a)
when l Ͻ k.
Case II. If k Ͻ l, then aik belongs to the kth column of Mjl, we obtain an additional minus
sign, and the result agrees with that characterized by (21b).
We have shown that the two expansions of D consist of the same terms, and this proves
our statement concerning rows.
The proof of the statement concerning columns is quite similar; if we expand D in
terms of two arbitrary columns, say, the k th and the l th, we find that the general term
involving ajl aik is exactly the same as before. This proves that not only all column
expansions of D yield the same value, but also that their common value is equal to the
common value of the row expansions of D.
This completes the proof and shows that our definition of an nth-order determinant is
unambiguous.
᭿

Section 9.3, page 368
PROOF OF FORMULA (2)
We prove that in right-handed Cartesian coordinates, the vector product
v ϭ a ؋ b ϭ [a1,

a2,

a3] ؋ [b1,

b2,

b3]

has the components
(2)

v1 ϭ a2 b3 Ϫ a3 b2,

v2 ϭ a3 b1 Ϫ a1b3,

v3 ϭ a1b2 Ϫ a2 b1.

We need only consider the case v 0. Since v is perpendicular to both a and b, Theorem
1 in Sec. 9.2 gives a • v ϭ 0 and b • v ϭ 0; in components [see (2), Sec. 9.2],
a1v1 ϩ a2v2 ϩ a3v3 ϭ 0
(3)

b1v1 ϩ b2v2 ϩ b3v3 ϭ 0.

bapp04a.qxd

11/3/10

A84

8:35 PM

Page A84

APP. 4 Additional Proofs

Multiplying the first equation by b3, the last by a3, and subtracting, we obtain
(a3 b1 Ϫ a1b3)v1 ϭ (a2 b3 Ϫ a3 b2)v2.
Multiplying the first equation by b1, the last by a1, and subtracting, we obtain
(a1b2 Ϫ a2 b1)v2 ϭ (a3 b1 Ϫ a1b3)v3.
We can easily verify that these two equations are satisfied by
(4)

v1 ϭ c(a2 b3 Ϫ a3 b2),

v2 ϭ c(a3 b1 Ϫ a1b3),

v3 ϭ c(a1b2 Ϫ a2 b1)

where c is a constant. The reader may verify, by inserting, that (4) also satisfies (3). Now
each of the equations in (3) represents a plane through the origin in v1v2v3-space. The
vectors a and b are normal vectors of these planes (see Example 6 in Sec. 9.2). Since
v
0, these vectors are not parallel and the two planes do not coincide. Hence their
intersection is a straight line L through the origin. Since (4) is a solution of (3) and, for
varying c, represents a straight line, we conclude that (4) represents L, and every solution
of (3) must be of the form (4). In particular, the components of v must be of this form,
where c is to be determined. From (4) we obtain
͉v͉2 ϭ v12 ϩ v22 ϩ v32 ϭ c 2[(a2 b3 Ϫ a3 b2)2 ϩ (a3 b1 Ϫ a1b3)2 ϩ (a1b2 Ϫ a2 b1)2].
This can be written
͉v͉2 ϭ c 2[(a12 ϩ a22 ϩ a32)(b12 ϩ b22 ϩ b32) Ϫ (a1b1 ϩ a2 b2 ϩ a3 b3)2],
as can be verified by performing the indicated multiplications in both formulas and
comparing. Using (2) in Sec. 9.2, we thus have
͉v͉2 ϭ c 2[(a • a)(b • b) Ϫ (a • b)2].
By comparing this with formula (12) in Prob. 4 of Problem Set 9.3 we conclude that
c ϭ Ϯ1.
We show that c ϭ ϩ1. This can be done as follows.
If we change the lengths and directions of a and b continuously and so that at the end
a ϭ i and b ϭ j (Fig. 188a in Sec. 9.3), then v will change its length and direction
continuously, and at the end, v ϭ i ؋ j ϭ k. Obviously we may effect the change so that
both a and b remain different from the zero vector and are not parallel at any instant.
Then v is never equal to the zero vector, and since the change is continuous and c can
only assume the values ϩ1 or Ϫ1, it follows that at the end c must have the same value
as before. Now at the end a ϭ i, b ϭ j, v ϭ k and, therefore, a1 ϭ 1, b2 ϭ 1, v3 ϭ 1,
and the other components in (4) are zero. Hence from (4) we see that v3 ϭ c ϭ ϩ1. This
proves Theorem 1.
For a left-handed coordinate system, i ؋ j ϭ Ϫk (see Fig. 188b in Sec. 9.3), resulting
in c ϭ Ϫ1. This proves the statement right after formula (2).
᭿

bapp04b.qxd

11/3/10

8:39 PM

Page A85

APP. 4 Additional Proofs

A85

Section 9.9, page 408
PROOF OF THE INVARIANCE OF THE CURL
This proof will follow from two theorems (A and B), which we prove first.
THEOREM A

Transformation Law for Vector Components

For any vector v the components v1, v2, v3 and v*1 , v*2 , v*3 in any two systems of
Cartesian coordinates x1, x2, x3 and x*1 , x*2 , x*3 , respectively, are related by
v*1 ϭ c11v1 ϩ c12v2 ϩ c13v3
(1)

v*2 ϭ c21v1 ϩ c22v2 ϩ c23v3
v*3 ϭ c31v1 ϩ c32v2 ϩ c33v3,

and conversely
v1 ϭ c11v*1 ϩ c21v*2 ϩ c31v*3
(2)

v2 ϭ c12v*1 ϩ c22v*2 ϩ c32v*3
v3 ϭ c13v*1 ϩ c23v*2 ϩ c33v*3

with coefficients

(3)

c11 ϭ i* • i

c12 ϭ i* • j

c13 ϭ i* • k

c21 ϭ j* • i

c22 ϭ j* • j

c23 ϭ j* • k

c31 ϭ k* • i

c32 ϭ k* • j

c33 ϭ k*⅐k

satisfying
3

(4)

͚c

c

kj mj

ϭ ␦km

(k, m ϭ 1, 2, 3),

jϭ1

where the Kronecker delta2 is given by

␦km ϭ {

0

(k

1

(k ϭ m)

m)

and i, j, k and i*, j*, k* denote the unit vectors in the positive x1-, x2-, x3- and
x*1 -, x*2 -, x*3 -directions, respectively.

2

LEOPOLD KRONECKER (1823–1891), German mathematician at Berlin, who made important
contributions to algebra, group theory, and number theory.
We shall keep our discussion completely independent of Chap. 7, but readers familiar with matrices should
recognize that we are dealing with orthogonal transformations and matrices and that our present theorem
follows from Theorem 2 in Sec. 8.3.

bapp04b.qxd

11/3/10

8:39 PM

A86

Page A86

APP. 4 Additional Proofs

PROOF

The representation of v in the two systems are
(a) v ϭ v1i ϩ v2 j ϩ v3k

(5)

v ϭ v*1 i* ϩ v*2 j* ϩ v*3 k*.

(b)

Since i* • i* ϭ 1, i* • j* ϭ 0, i* • k* ϭ 0, we get from (5b) simply i* • v ϭ v*1 and
from this and (5a)
v*1 ϭ i* • v ϭ i* • v1i ϩ i* • v2 j ϩ i* • v3k ϭ v1i* • i ϩ v2i* • j ϩ v3i* • k.
Because of (3), this is the first formula in (1), and the other two formulas are obtained
similarly, by considering j* • v and then k* • v. Formula (2) follows by the same idea,
taking i • v ϭ v1 from (5a) and then from (5b) and (3)
v1 ϭ i • v ϭ v*1 i • i* ϩ v*2 i • j* ϩ v*3 i • k* ϭ c11v*1 ϩ c21v*2 ϩ c31v*3 ,
and similarly for the other two components.
We prove (4). We can write (1) and (2) briefly as
3

(6)

vj ϭ

(a)

3

͚

(b) v*k ϭ

*,
cmj vm

mϭ1

͚c

kj

vj.

jϭ1

Substituting vj into v*k , we get
3

v*k ϭ

͚

3

ckj

jϭ1

3

͚

͚

* ϭ
cmj vm

mϭ1

mϭ1

3

*
vm

( ͚ ckj cmj) ,
jϭ1

where k ϭ 1, 2, 3. Taking k ϭ 1, we have
3

v*1 ϭ v*1

(͚

3

c1 j c1j) ϩ v*2

jϭ1

(͚

3

c1 j c2j) ϩ v*3

jϭ1

( ͚ c1 j c3j) .
jϭ1

For this to hold for every vector v, the first sum must be 1 and the other two sums 0. This
proves (4) with k ϭ 1 for m ϭ 1, 2, 3. Taking k ϭ 2 and then k ϭ 3, we obtain (4) with
k ϭ 2 and 3, for m ϭ 1, 2, 3.
᭿

THEOREM B

Transformation Law for Cartesian Coordinates

The transformation of any Cartesian x1x2 x3-coordinate system into any other
Cartesian x*1 x*2 x*3 -coordinate system is of the form
3

(7)

*ϭ
xm

͚c

x ϩ bm,

mj j

m ϭ 1, 2, 3,

jϭ1

with coefficients (3) and constants b1, b2, b3; conversely,
3

(8)

xk ϭ

͚c

x* ϩ ෂ
bk,

nk n

nϭ1

k ϭ 1, 2, 3.

bapp04b.qxd

11/3/10

8:39 PM

Page A87

APP. 4 Additional Proofs

A87

Theorem B follows from Theorem A by noting that the most general transformation of a
Cartesian coordinate system into another such system may be decomposed into a
transformation of the type just considered and a translation; and under a translation,
corresponding coordinates differ merely by a constant.

PROOF OF THE INVARIANCE OF THE CURL
We write again x1, x2, x3 instead of x, y, z, and similarly x*1 , x*2 , x*3 for other Cartesian
coordinates, assuming that both systems are right-handed. Let a1, a2, a3 denote the
components of curl v in the x1x2 x3-coordinates, as given by (1), Sec. 9.9, with
x ϭ x1,

y ϭ x2,

z ϭ x3.

Similarly, let a*1 , a*2 , a*3 denote the components of curl v in the x*1 x*2 x*3 -coordinate system.
We prove that the length and direction of curl v are independent of the particular choice
of Cartesian coordinates, as asserted. We do this by showing that the components of curl
v satisfy the transformation law (2), which is characteristic of vector components. We
consider a1. We use (6a), and then the chain rule for functions of several variables (Sec.
9.6). This gives
3
Ѩvm
Ѩvm
*
*
Ѩv3
Ѩv2
Ϫ cm2 ᎏ
(cm3 ᎏ
)
a1 ϭ ᎏ
Ϫ ᎏ
ϭ
Ѩx
Ѩx
2
Ѩx2
3
Ѩx3
mϭ1

͚

3

ϭ

3

͚ ͚ (c

m3

mϭ1 jϭ1

Ѩvm
Ѩvm
* Ѩx*j
* Ѩx*j
Ϫ cm2 ᎏ
ᎏ) .
ᎏ
ᎏ
Ѩxj* Ѩx3
Ѩxj* Ѩx2

From this and (7) we obtain
3

a1 ϭ

3

͚͚

mϭ1 jϭ1

*
Ѩvm
(cm3cj2 Ϫ cm2cj3) ᎏ
Ѩx*j

Ѩv*3
Ѩv*2
Ϫ ᎏ) ϩ • • •
ϭ (c33 c22 Ϫ c32 c23) ( ᎏ
*
Ѩx2
Ѩx3*
ϭ (c33 c22 Ϫ c32 c23)a*1 ϩ (c13 c32 Ϫ c12 c33)a*2 ϩ (c23 c12 Ϫ c22 c13)a*3 .
Note what we did. The double sum had 3 ϫ 3 ϭ 9 terms, 3 of which were zero (when
m ϭ j), and the remaining 6 terms we combined in pairs as we needed them in getting
a*1 , a*2 , a*3 .
We now use (3), Lagrange’s identity (see Formula (15) in Team Project 24 in Problem
Set 9.3) and k* ؋ j* ϭ Ϫi* and k ؋ j ϭ Ϫi. Then
c33 c22 Ϫ c32 c23 ϭ (k* • k)(j* • j) Ϫ (k* • j)(j* • k)
ϭ (k* ؋ j*) • (k ؋ j) ϭ i* • i ϭ c11,

etc.

bapp04b.qxd

11/3/10

A88

8:39 PM

Page A88

APP. 4 Additional Proofs

Hence a1 ϭ c11a*1 ϩ c21a*2 ϩ c31a*3 . This is of the form of the first formula in (2) in
Theorem A, and the other two formulas of the form (2) are obtained similarly. This proves
the theorem for right-handed systems. If the x1x2 x3-coordinates are left-handed, then
k ؋ j ϭ ϩi, but then there is a minus sign in front of the determinant in (1), Sec. 9.9. ᭿

Section 10.2, page 420
PROOF OF THEOREM 1, PART (b)

We prove that if

͵ F(r) • dr ϭ ͵ (F

(1)

1

C

dx ϩ F2 dy ϩ F3 dz)

C

with continuous F1, F2, F3 in a domain D is independent of path in D, then F ϭ grad ƒ
in D for some ƒ; in components
(2Ј)

Ѩƒ
F1 ϭ ᎏ
Ѩx ,

Ѩƒ
F2 ϭ ᎏ
Ѩy ,

Ѩƒ
F3 ϭ ᎏ
Ѩz .

We choose any fixed A: (x0, y0, z0) in D and any B: (x, y, z) in D and define ƒ by

͵ (F
B

(3)

ƒ(x, y, z) ϭ ƒ0 ϩ

1

dx* ϩ F2 dy* ϩ F3 dz*)

A

with any constant ƒ0 and any path from A to B in D. Since A is fixed and we have
independence of path, the integral depends only on the coordinates x, y, z, so that (3)
defines a function ƒ(x, y, z) in D. We show that F ϭ grad ƒ with this ƒ, beginning with
the first of the three relations (2Ј). Because of independence of path we may integrate
from A to B1: (x1, y, z) and then parallel to the x-axis along the segment B1B in Fig. 562
with B1 chosen so that the whole segment lies in D. Then

͵

B1

ƒ(x, y, z) ϭ ƒ0 ϩ

͵ (F
B

(F1 dx* ϩ F2 dy* ϩ F3 dz*) ϩ

A

1

dx* ϩ F2 dy* ϩ F3 dz*).

B1

We now take the partial derivative with respect to x on both sides. On the left we get
Ѩƒ/Ѩx. We show that on the right we get F1. The derivative of the first integral is zero
because A: (x0, y0, z0) and B1: (x1, y, z) do not depend on x. We consider the second
integral. Since on the segment B1B, both y and z are constant, the terms F2 dy* and
B1
B
z

A
y
x

Fig. 562. Proof of Theorem 1

bapp04b.qxd

11/3/10

8:39 PM

Page A89

APP. 4 Additional Proofs

A89

F3 dz* do not contribute to the derivative of the integral. The remaining part can be written
as a definite integral,

͵F
B

B1

͵ F (x*, y, z) dx*.
x

1

dx* ϭ

1

x1

Hence its partial derivative with respect to x is F1(x, y, z), and the first of the relations
(2Ј) is proved. The other two formulas in (2Ј) follow by the same argument.
᭿

Section 11.5, page 500
THEOREM

Reality of Eigenvalues

If p, q, r, and pЈ in the Sturm–Liouville equation (1) of Sec. 11.5 are real-valued and
continuous on the interval a Ϲ x Ϲ b and r(x) Ͼ 0 throughout that interval (or
r(x) Ͻ 0 throughout that interval), then all the eigenvalues of the Sturm–Liouville
problem (1), (2), Sec. 11.5, are real.

PROOF

Let ␭ ϭ ␣ ϩ i␤ be an eigenvalue of the problem and let
y(x) ϭ u(x) ϩ iv(x)
be a corresponding eigenfunction; here ␣, ␤, u, and v are real. Substituting this into (1),
Sec. 11.5, we have
(puЈ ϩ ipvЈ)Ј ϩ (q ϩ ␣r ϩ i␤r)(u ϩ iv) ϭ 0.
This complex equation is equivalent to the following pair of equations for the real and
the imaginary parts:
(puЈ)Ј ϩ (q ϩ ␣r)u Ϫ ␤rv ϭ 0
(pvЈ)Ј ϩ (q ϩ ␣r)v ϩ ␤ru ϭ 0.
Multiplying the first equation by v, the second by Ϫu and adding, we get
Ϫ␤(u2 ϩ v2)r ϭ u(pvЈ)Ј Ϫ v( puЈ)Ј
ϭ [(pvЈ)u Ϫ ( puЈ)v]Ј.
The expression in brackets is continuous on a Ϲ x Ϲ b, for reasons similar to those in
the proof of Theorem 1, Sec. 11.5. Integrating over x from a to b, we thus obtain

͵ (u
b

Ϫ␤

a

b

2

ϩ v2)r dx ϭ [p(uvЈ Ϫ uЈv)] .
a

Because of the boundary conditions, the right side is zero; this is as in that proof. Since
y is an eigenfunction, u2 ϩ v2
0. Since y and r are continuous and r Ͼ 0 (or r Ͻ 0)
on the interval a Ϲ x Ϲ b, the integral on the left is not zero. Hence, ␤ ϭ 0, which means
that ␭ ϭ ␣ is real. This completes the proof.
᭿

bapp04b.qxd

11/3/10

A90

8:39 PM

Page A90

APP. 4 Additional Proofs

Section 13.4, page 627
P R O O F O F T H E O R E M 2 Cauchy–Riemann Equations
We prove that Cauchy–Riemann equations
(1)

ux ϭ vy,

uy ϭ Ϫvx

are sufficient for a complex function ƒ(z) ϭ u(x, y) ϩ iv(x, y) to be analytic; precisely, if
the real part u and the imaginary part v of ƒ(z) satisfy (1) in a domain D in the complex
plane and if the partial derivatives in (1) are continuous in D, then ƒ(z) is analytic in D.
In this proof we write ⌬z ϭ ⌬ x ϩ i⌬y and ⌬ƒ ϭ ƒ(z ϩ ⌬z) Ϫ ƒ(z). The idea of proof
is as follows.
(a) We express ⌬ƒ in terms of first partial derivatives of u and v, by applying the mean
value theorem of Sec. 9.6.
(b) We get rid of partial derivatives with respect to y by applying the Cauchy–Riemann
equations.
(c) We let ⌬z approach zero and show that then ⌬ƒ/⌬z, as obtained, approaches a limit,
which is equal to ux ϩ ivx, the right side of (4) in Sec. 13.4, regardless of the way of
approach to zero.
(a) Let P: (x, y) be any fixed point in D. Since D is a domain, it contains a neighborhood
of P. We can choose a point Q: (x ϩ ⌬x, y ϩ ⌬y) in this neighborhood such that the
straight-line segment PQ is in D. Because of our continuity assumptions we may apply
the mean value theorem in Sec. 9.6. This yields
u(x ϩ ⌬x, y ϩ ⌬y) Ϫ u(x, y) ϭ (⌬x) ux(M1) ϩ (⌬y)uy(M1)
v(x ϩ ⌬x, y ϩ ⌬y) Ϫ v(x, y) ϭ (⌬x) vx(M2) ϩ (⌬y)vy(M2)
where M1 and M2 ( M1 in general!) are suitable points on that segment. The first line
is Re ⌬ƒ and the second is Im ⌬ƒ, so that
⌬ƒ ϭ (⌬x) ux(M1) ϩ (⌬y)uy(M1) ϩ i [(⌬x)vx(M2) ϩ (⌬y)vy(M2)].
(b) uy ϭ Ϫvx and vy ϭ ux by the Cauchy–Riemann equations, so that
⌬ƒ ϭ (⌬x)ux(M1) Ϫ (⌬y)vx (M1) ϩ i [(⌬x)vx (M2) ϩ (⌬y)ux (M2)].
Also ⌬z ϭ ⌬x ϩ i⌬y, so that we can write ⌬x ϭ ⌬z Ϫ i⌬y in the first term and
⌬y ϭ (⌬z Ϫ ⌬x)/i ϭ Ϫi(⌬z Ϫ ⌬x) in the second term. This gives
⌬ƒ ϭ (⌬z Ϫ i⌬y)ux (M1) ϩ i (⌬z Ϫ ⌬x)vx (M1) ϩ i [(⌬x)vx (M2) ϩ (⌬y)ux(M2)].
By performing the multiplications and reordering we obtain
⌬ƒ ϭ (⌬z)ux(M1) Ϫ i ⌬y{ux(M1) Ϫ ux(M2)}
ϩ i [(⌬z) vx(M1) Ϫ ⌬x{vx(M1) Ϫ vx(M2)}].

bapp04b.qxd

11/5/10

12:10 AM

Page A91

APP. 4 Additional Proofs

A91

Division by ⌬z now yields
⌬ƒ
i⌬y
i⌬x
(A) ᎏ ϭ ux(M1) ϩ ivx(M1) Ϫ ᎏ {ux(M1) Ϫ ux(M2)} Ϫ ᎏ {vx(M1) Ϫ vx(M2)}.
⌬z
⌬z
⌬z
(c) We finally let ⌬z approach zero and note that ͉⌬y/⌬z͉ Ϲ 1 and ͉⌬x/⌬z͉ Ϲ 1 in (A).
Then Q: (x ϩ ⌬x, y ϩ ⌬y) approaches P: (x, y), so that M1 and M2 must approach P.
Also, since the partial derivatives in (A) are assumed to be continuous, they approach
their value at P. In particular, the differences in the braces {• • •} in (A) approach zero.
Hence the limit of the right side of (A) exists and is independent of the path along which
⌬z * 0. We see that this limit equals the right side of (4) in Sec. 13.4. This means that
ƒ(z) is analytic at every point z in D, and the proof is complete.
᭿

Section 14.2, pages 653–654
G O U R S A T ’ S P R O O F O F C A U C H Y ’ S I N T E G R A L T H E O R E M Goursat proved Cauchy’s
integral theorem without assuming that ƒЈ(z) is continuous, as follows.
We start with the case when C is the boundary of a triangle. We orient C
counterclockwise. By joining the midpoints of the sides we subdivide the triangle into
four congruent triangles (Fig. 563). Let CI, CII, CIII, CIV denote their boundaries. We
claim that (see Fig. 563).

Ͷ ƒ dz ϭ Ͷ

(1)

C

ƒ dz ϩ

CI

Ͷ

ƒ dz ϩ

CII

Ͷ

ƒ dz ϩ

CIII

Ͷ

ƒ dz.

CIV

Indeed, on the right we integrate along each of the three segments of subdivision in both
possible directions (Fig. 563), so that the corresponding integrals cancel out in pairs, and
the sum of the integrals on the right equals the integral on the left. We now pick an integral
on the right that is biggest in absolute value and call its path C1. Then, by the triangle
inequality (Sec. 13.2),

Ͷ

j

Ͷ

ƒ dzj Ϲ j

C

Ͷ

ƒ dzj ϩ j

CI

CII

Ͷ

ƒ dzj ϩ j

Ͷ

ƒ dzj ϩ j

CIII

Ͷ

ƒ dzj Ϲ 4 j

CIV

ƒ dzj .

C1

We now subdivide the triangle bounded by C1 as before and select a triangle of
subdivision with boundary C2 for which

Ͷ

j

C1

ƒ dzj Ϲ 4 j

Ͷ

C2

ƒ dzj .

Then

j

Ͷ ƒ dzj Ϲ 4 jͶ
2

C

Fig. 563. Proof of Cauchy’s integral theorem

C2

ƒ dzj .

bapp04b.qxd

11/3/10

A92

8:39 PM

Page A92

APP. 4 Additional Proofs

Continuing in this fashion, we obtain a sequence of triangles T1, T2, • • • with boundaries
C1, C2, • • • that are similar and such that Tn lies in Tm when n Ͼ m, and
j

(2)

Ͷ ƒ dzj Ϲ 4 jͶ
n

C

n ϭ 1, 2, • • • .

ƒ dzj ,

Cn

Let z0 be the point that belongs to all these triangles. Since ƒ is differentiable at z ϭ z0,
the derivative ƒЈ(z0) exists. Let
ƒ(z) Ϫ ƒ(z0)
h(z) ϭ ᎏᎏ
Ϫ ƒЈ(z0).
z Ϫ z0

(3)

Solving this algebraically for ƒ(z) we have
ƒ(z) ϭ ƒ(z0) ϩ (z Ϫ z0)ƒЈ(z0) ϩ h(z)(z Ϫ z0).
Integrating this over the boundary Cn of the triangle Tn gives

Ͷ

ƒ(z) dz ϭ

Cn

Ͷ

ƒ(z0) dz ϩ

Cn

Ͷ

Cn

(z Ϫ z0)ƒЈ(z0) dz ϩ

Ͷ

h(z)(z Ϫ z0)dz.

Cn

Since ƒ(z0) and ƒЈ(z0) are constants and Cn is a closed path, the first two integrals on the
right are zero, as follows from Cauchy’s proof, which is applicable because the integrands
do have continuous derivatives (0 and const, respectively). We thus have

Ͷ

ƒ(z) dz ϭ

Cn

Ͷ

h(z)(z Ϫ z0) dz.

Cn

Since ƒЈ(z0) is the limit of the difference quotient in (3), for given ⑀ Ͼ 0 we can find a
␦ Ͼ 0 such that
͉h(z)͉ Ͻ ⑀

(4)

when

͉z Ϫ z0͉ Ͻ ␦.

We may now take n so large that the triangle Tn lies in the disk ͉z Ϫ z0͉ Ͻ ␦. Let Ln be
the length of Cn. Then ͉z Ϫ z0͉ Ͻ Ln for all z on Cn and z0 in Tn. From this and (4) we
have ͉h(z)(z Ϫ z0)͉ Ͻ ⑀ Ln. The ML-inequality in Sec. 14.1 now gives
(5)

Ͷ

ƒ(z) dzj ϭ j

j

Cn

Ͷ

h(z)(z Ϫ z0) dzj Ϲ ⑀ Ln ⅐ Ln ϭ ⑀ Ln2.

Cn

Now denote the length of C by L. Then the path C1 has the length L1 ϭ L/2, the path C2
has the length L2 ϭ L1/2 ϭ L/4, etc., and Cn has the length Ln ϭ L/2n. Hence
Ln2 ϭ L2/4n. From (2) and (5) we thus obtain
j

Ͷ ƒ dzj Ϲ 4 jͶ
n

C

Cn

L2
ƒ dzj Ϲ 4n⑀ Ln2 ϭ 4n⑀ ᎏ
ϭ ⑀ L2.
4n

By choosing ⑀ (Ͼ 0) sufficiently small we can make the expression on the right as small
as we please, while the expression on the left is the definite value of an integral.
Consequently, this value must be zero, and the proof is complete.

bapp04b.qxd

11/5/10

12:10 AM

Page A93

APP. 4 Additional Proofs

A93

The proof for the case in which C is the boundary of a polygon follows from the previous
proof by subdividing the polygon into triangles (Fig. 564). The integral corresponding to
each such triangle is zero. The sum of these integrals is equal to the integral over C,
because we integrate along each segment of subdivision in both directions, the
corresponding integrals cancel out in pairs, and we are left with the integral over C.
The case of a general simple closed path C can be reduced to the preceding one by
inscribing in C a closed polygon P of chords, which approximates C “sufficiently
accurately,” and it can be shown that there is a polygon P such that the integral over P
differs from that over C by less than any preassigned positive real number ⑀˜, no matter
how small. The details of this proof are somewhat involved and can be found in Ref. [D6]
listed in App. 1.
᭿

Fig. 564. Proof of Cauchy’s integral theorem for a polygon

Section 15.1, page 674
P R O O F O F T H E O R E M 4 Cauchy’s Convergence Principle for Series
(a) In this proof we need two concepts and a theorem, which we list first.
1. A bounded sequence s1, s2, • • • is a sequence whose terms all lie in a disk of
(sufficiently large, finite) radius K with center at the origin; thus ͉sn͉ Ͻ K for all n.
2. A limit point a of a sequence s1, s2, • • • is a point such that, given an ⑀ Ͼ 0, there
are infinitely many terms satisfying ͉sn Ϫ a͉ Ͻ ⑀. (Note that this does not imply
convergence, since there may still be infinitely many terms that do not lie within that
circle of radius ⑀ and center a.)
1 1
_5
Example: _14, _34, _81, _78, _
16 , 16 , • • • has the limit points 0 and 1 and diverges.

3. A bounded sequence in the complex plane has at least one limit point.
(Bolzano–Weierstrass theorem; proof below. Recall that “sequence” always means infinite
sequence.)
(b) We now turn to the actual proof that z1 ϩ z2 ϩ • • • converges if and only if, for
every ⑀ Ͼ 0, we can find an N such that
(1)

͉znϩ1 ϩ • • • ϩ znϩp͉ Ͻ ⑀

for every n Ͼ N and p ϭ 1, 2, • • • .

Here, by the definition of partial sums,
snϩp Ϫ sn ϭ znϩ1 ϩ • • • ϩ znϩp.
Writing n ϩ p ϭ r, we see from this that (1) is equivalent to
(1*)

͉sr Ϫ sn͉ Ͻ ⑀

for all r Ͼ N and n Ͼ N.

bapp04b.qxd

11/3/10

8:39 PM

A94

Page A94

APP. 4 Additional Proofs

Suppose that s1, s2, • • • converges. Denote its limit by s. Then for a given ⑀ Ͼ 0 we can
find an N such that
⑀
͉sn Ϫ s͉ Ͻ ᎏ
for every n Ͼ N.
2
Hence, if r Ͼ N and n Ͼ N, then by the triangle inequality (Sec. 13.2),

⑀
⑀
͉sr Ϫ sn͉ ϭ ͉(sr Ϫ s) Ϫ (sn Ϫ s)͉ Ϲ ͉sr Ϫ s͉ ϩ ͉sn Ϫ s͉ Ͻ ᎏ ϩ ᎏ ϭ ⑀,
2
2
that is, (1*) holds.
(c) Conversely, assume that s1, s2, • • • satisfies (1*). We first prove that then the
sequence must be bounded. Indeed, choose a fixed ⑀ and a fixed n ϭ n0 Ͼ N in (1*).
Then (1*) implies that all sr with r Ͼ N lie in the disk of radius ⑀ and center sn0 and only
finitely many terms s1, • • • , sN may not lie in this disk. Clearly, we can now find a circle
so large that this disk and these finitely many terms all lie within this new circle. Hence
the sequence is bounded. By the Bolzano–Weierstrass theorem, it has at least one limit
point, call it s.
We now show that the sequence is convergent with the limit s. Let ⑀ Ͼ 0 be given.
Then there is an N* such that ͉sr Ϫ sn͉ Ͻ ⑀ /2 for all r Ͼ N* and n Ͼ N*, by (1*). Also,
by the definition of a limit point, ͉sn Ϫ s͉ Ͻ ⑀ /2 for infinitely many n, so that we can find
and fix an n Ͼ N* such that ͉sn Ϫ s͉ Ͻ ⑀ /2. Together, for every r Ͼ N*,

⑀
⑀
͉sr Ϫ s͉ ϭ ͉(sr Ϫ sn) ϩ (sn Ϫ s)͉ Ϲ ͉sr Ϫ sn͉ ϩ ͉sn Ϫ s͉ Ͻ ᎏ ϩ ᎏ ϭ ⑀;
2
2
that is, the sequence s1, s2, • • • is convergent with the limit s.
THEOREM

᭿

Bolzano–Weierstrass Theorem3

A bounded infinite sequence z1, z2, z3, • • • in the complex plane has at least one
limit point.

PROOF

It is obvious that we need both conditions: a finite sequence cannot have a limit point,
and the sequence 1, 2, 3, • • • , which is infinite but not bounded, has no limit point. To
prove the theorem, consider a bounded infinite sequence z1, z2, • • • and let K be such that
͉zn͉ Ͻ K for all n. If only finitely many values of the zn are different, then, since the
sequence is infinite, some number z must occur infinitely many times in the sequence,
and, by definition, this number is a limit point of the sequence.
We may now turn to the case when the sequence contains infinitely many different
terms. We draw a large square Q0 that contains all zn. We subdivide Q0 into four congruent
squares, which we number 1, 2, 3, 4. Clearly, at least one of these squares (each taken
with its complete boundary) must contain infinitely many terms of the sequence. The
square of this type with the lowest number (1, 2, 3, or 4) will be denoted by Q1. This is

3
BERNARD BOLZANO (1781–1848), Austrian mathematician and professor of religious studies, was a
pioneer in the study of point sets, the foundation of analysis, and mathematical logic.
For Weierstrass, see Sec. 15.5.

bapp04b.qxd

11/3/10

8:39 PM

Page A95

APP. 4 Additional Proofs

A95

the first step. In the next step we subdivide Q1 into four congruent squares and select a
square Q2 by the same rule, and so on. This yields an infinite sequence of squares Q0,
Q1, Q2, • • • , Qn, • • • with the property that the side of Qn approaches zero as n approaches
infinity, and Qm contains all Qn with n Ͼ m. It is not difficult to see that the number
which belongs to all these squares,4 call it z ϭ a, is a limit point of the sequence. In fact,
given an ⑀ Ͼ 0, we can choose an N so large that the side of the square QN is less than
⑀ and, since QN contains infinitely many zn, we have ͉zn Ϫ a͉ Ͻ ⑀ for infinitely many n.
This completes the proof.
᭿

Section 15.3, pages 688–689
PART (b) OF THE PROOF OF THEOREM 5
We have to show that
`

͚

nϭ2

(z ϩ ⌬z)n Ϫ z n
an [ ᎏᎏ Ϫ nzn؊1]
⌬z

`

ϭ

͚

an ⌬z [(z ϩ ⌬z)n؊2 ϩ 2z(z ϩ ⌬z)n؊3 ϩ • • • ϩ (n Ϫ 1)zn؊2],

nϭ2

thus,
(z ϩ ⌬z)n Ϫ z n
ᎏᎏ Ϫ nzn؊1
⌬z
ϭ ⌬z[(z ϩ ⌬z)n؊2 ϩ 2z(z ϩ ⌬z)n؊3 ϩ • • • ϩ (n Ϫ 1)zn؊2].
If we set z ϩ ⌬z ϭ b and z ϭ a, thus ⌬z ϭ b Ϫ a, this becomes simply
(7a)

bn Ϫ an
ᎏ Ϫ nan؊1 ϭ (b Ϫ a)An
bϪa

(n ϭ 2, 3, • • •),

where An is the expression in the brackets on the right,
(7b)

An ϭ bn؊2 ϩ 2abn؊3 ϩ 3a 2bn؊4 ϩ • • • ϩ (n Ϫ 1)an؊2;

thus, A2 ϭ 1, A3 ϭ b ϩ 2a, etc. We prove (7) by induction. When n ϭ 2, then (7) holds,
since then
b2 Ϫ a2
(b ϩ a)(b Ϫ a)
ᎏ Ϫ 2a ϭ ᎏᎏ Ϫ 2a ϭ b Ϫ a ϭ (b Ϫ a)A2.
bϪa
bϪa
Assuming that (7) holds for n ϭ k, we show that it holds for n ϭ k ϩ 1. By adding and
subtracting a term in the numerator and then dividing we first obtain
bk Ϫ ak
bkϩ1 Ϫ akϩ1
bkϩ1 Ϫ bak ϩ bak Ϫ akϩ1
ᎏᎏ ϭ ᎏᎏᎏ ϭ b ᎏ ϩ ak.
bϪa
bϪa
bϪa

4
The fact that such a unique number z ϭ a exists seems to be obvious, but it actually follows from an axiom
of the real number system, the so-called Cantor–Dedekind axiom: see footnote 3 in App. A3.3.

bapp04b.qxd

11/3/10

A96

8:39 PM

Page A96

APP. 4 Additional Proofs

By the induction hypothesis, the right side equals b[(b Ϫ a)Ak ϩ kak ؊1] ϩ ak. Direct
calculation shows that this is equal to
(b Ϫ a){bAk ϩ kak ؊1} ϩ akak ؊1 ϩ ak.
From (7b) with n ϭ k we see that the expression in the braces {• • •} equals
bk ؊1 ϩ 2abk ؊2 ϩ • • • ϩ (k Ϫ 1)bak ؊2 ϩ kak ؊1 ϭ Akϩ1.
Hence our result is
bkϩ1 Ϫ akϩ1
ᎏᎏ ϭ (b Ϫ a)Akϩ1 ϩ (k ϩ 1)ak.
bϪa
Taking the last term to the left, we obtain (7) with n ϭ k ϩ 1. This proves (7) for any
integer n м 2 and completes the proof.
᭿

Section 18.2, page 763
A N O T H E R P R O O F O F T H E O R E M 1 without the use of a harmonic conjugate
We show that if w ϭ u ϩ iv ϭ ƒ(z) is analytic and maps a domain D conformally onto
a domain D* and ⌽*(u, v) is harmonic in D*, then
(1)

⌽(x, y) ϭ ⌽*(u(x, y), v(x, y))

is harmonic in D, that is, ٌ2⌽ ϭ 0 in D. We make no use of a harmonic conjugate of
⌽*, but use straightforward differentiation. By the chain rule,
⌽x ϭ ⌽u* ux ϩ ⌽*v vx.
We apply the chain rule again, underscoring the terms that will drop out when we form
ٌ2⌽:
⌽xx ϭ ⌽*
ϩ (⌽*
uuxx
uu ux ϩ ⌽*
uv vx)ux
———
—
————
ϩ ⌽*
ϩ (⌽*
uvxx
vu ux ϩ ⌽*
vv vx)vx.
———
—
————
⌽yy is the same with each x replaced by y. We form the sum ٌ2⌽. In it, ⌽*vu ϭ ⌽*
uv is
multiplied by
ux vx ϩ uy vy
which is 0 by the Cauchy–Riemann equations. Also ٌ2u ϭ 0 and ٌ2v ϭ 0. There remains
ٌ2⌽ ϭ ⌽*uu(ux2 ϩ uy2) ϩ ⌽*vv(vx2 ϩ vy2).
By the Cauchy–Riemann equations this becomes
2
2
ٌ2⌽ ϭ (⌽*
uu ϩ ⌽*
vv)(ux ϩ vx )

and is 0 since ⌽* is harmonic.

᭿

bapp05.qxd

11/3/10

9:01 PM

Page A97

APPENDIX

5

Tables
For Tables of Laplace Transforms see Secs. 6.8 and 6.9.
For Tables of Fourier Transforms see Sec. 11.10.
If you have a Computer Algebra System (CAS), you may not need the present tables,
but you may still find them convenient from time to time.
Table A1

Bessel Functions
For more extensive tables see Ref. [GenRef1] in App. 1.
x

J0(x)

J1(x)

x

J0(x)

J1(x)

x

J0(x)

J1(x)

0.0
0.1
0.2
0.3
0.4

1.0000
0.9975
0.9900
0.9776
0.9604

0.0000
0.0499
0.0995
0.1483
0.1960

3.0
3.1
3.2
3.3
3.4

Ϫ0.2601
Ϫ0.2921
Ϫ0.3202
Ϫ0.3443
Ϫ0.3643

0.3391
0.3009
0.2613
0.2207
0.1792

6.0
6.1
6.2
6.3
6.4

0.1506
0.1773
0.2017
0.2238
0.2433

Ϫ0.2767
Ϫ0.2559
Ϫ0.2329
Ϫ0.2081
Ϫ0.1816

0.5
0.6
0.7
0.8
0.9

0.9385
0.9120
0.8812
0.8463
0.8075

0.2423
0.2867
0.3290
0.3688
0.4059

3.5
3.6
3.7
3.8
3.9

Ϫ0.3801
Ϫ0.3918
Ϫ0.3992
Ϫ0.4026
Ϫ0.4018

0.1374
0.0955
0.0538
0.0128
Ϫ0.0272

6.5
6.6
6.7
6.8
6.9

0.2601
0.2740
0.2851
0.2931
0.2981

Ϫ0.1538
Ϫ0.1250
Ϫ0.0953
Ϫ0.0652
Ϫ0.0349

1.0
1.1
1.2
1.3
1.4

0.7652
0.7196
0.6711
0.6201
0.5669

0.4401
0.4709
0.4983
0.5220
0.5419

4.0
4.1
4.2
4.3
4.4

Ϫ0.3971
Ϫ0.3887
Ϫ0.3766
Ϫ0.3610
Ϫ0.3423

Ϫ0.0660
Ϫ0.1033
Ϫ0.1386
Ϫ0.1719
Ϫ0.2028

7.0
7.1
7.2
7.3
7.4

0.3001
0.2991
0.2951
0.2882
0.2786

Ϫ0.0047
0.0252
0.0543
0.0826
0.1096

1.5
1.6
1.7
1.8
1.9

0.5118
0.4554
0.3980
0.3400
0.2818

0.5579
0.5699
0.5778
0.5815
0.5812

4.5
4.6
4.7
4.8
4.9

Ϫ0.3205
Ϫ0.2961
Ϫ0.2693
Ϫ0.2404
Ϫ0.2097

Ϫ0.2311
Ϫ0.2566
Ϫ0.2791
Ϫ0.2985
Ϫ0.3147

7.5
7.6
7.7
7.8
7.9

0.2663
0.2516
0.2346
0.2154
0.1944

0.1352
0.1592
0.1813
0.2014
0.2192

2.0
2.1
2.2
2.3
2.4

0.2239
0.1666
0.1104
0.0555
0.0025

0.5767
0.5683
0.5560
0.5399
0.5202

5.0
5.1
5.2
5.3
5.4

Ϫ0.1776
Ϫ0.1443
Ϫ0.1103
Ϫ0.0758
Ϫ0.0412

Ϫ0.3276
Ϫ0.3371
Ϫ0.3432
Ϫ0.3460
Ϫ0.3453

8.0
8.1
8.2
8.3
8.4

0.1717
0.1475
0.1222
0.0960
0.0692

0.2346
0.2476
0.2580
0.2657
0.2708

2.5
2.6
2.7
2.8
2.9

Ϫ0.0484
Ϫ0.0968
Ϫ0.1424
Ϫ0.1850
Ϫ0.2243

0.4971
0.4708
0.4416
0.4097
0.3754

5.5
5.6
5.7
5.8
5.9

Ϫ0.0068
0.0270
0.0599
0.0917
0.1220

Ϫ0.3414
Ϫ0.3343
Ϫ0.3241
Ϫ0.3110
Ϫ0.2951

8.5
8.6
8.7
8.8
8.9

0.0419
0.0146
Ϫ0.0125
Ϫ0.0392
Ϫ0.0653

0.2731
0.2728
0.2697
0.2641
0.2559

J0(x) ϭ 0 for x ϭ 2.40483, 5.52008, 8.65373, 11.7915, 14.9309, 18.0711, 21.2116, 24.3525, 27.4935, 30.6346
J1(x) ϭ 0 for x ϭ 3.83171, 7.01559, 10.1735, 13.3237, 16.4706, 19.6159, 22.7601, 25.9037, 29.0468, 32.1897

A97

bapp05.qxd

11/3/10

A98

9:01 PM

Page A98

APP. 5 Tables
(continued)

Table A1
x

Y0(x)

Y1(x)

x

Y0(x)

Y1(x)

x

Y0(x)

0.0
0.5
1.0
1.5
2.0

(Ϫϱ)
Ϫ0.445
0.088
0.382
0.510

(Ϫϱ)
Ϫ1.471
Ϫ0.781
Ϫ0.412
Ϫ0.107

2.5
3.0
3.5
4.0
4.5

0.498
0.377
0.189
Ϫ0.017
Ϫ0.195

0.146
0.325
0.410
0.398
0.301

5.0
5.5
6.0
6.5
7.0

Ϫ0.309
Ϫ0.339
Ϫ0.288
Ϫ0.173
Ϫ0.026

Y1(x)
0.148
Ϫ0.024
Ϫ0.175
Ϫ0.274
Ϫ0.303

Table A2 Gamma Function [see (24) in App. A3.1]
␣

⌫(␣)

␣

⌫(␣)

␣

⌫(␣)

␣

⌫(␣)

␣

⌫(␣)

1.00

1.000 000

1.20

0.918 169

1.40

0.887 264

1.60

0.893 515

1.80

0.931 384

1.02
1.04
1.06
1.08

0.988 844
0.978 438
0.968 744
0.959 725

1.22
1.24
1.26
1.28

0.913 106
0.908 521
0.904 397
0.900 718

1.42
1.44
1.46
1.48

0.886 356
0.885 805
0.885 604
0.885 747

1.62
1.64
1.66
1.68

0.895 924
0.898 642
0.901 668
0.905 001

1.82
1.84
1.86
1.88

0.936 845
0.942 612
0.948 687
0.955 071

1.10

0.951 351

1.30

0.897 471

1.50

0.886 227

1.70

0.908 639

1.90

0.961 766

1.12
1.14
1.16
1.18

0.943 590
0.936 416
0.929 803
0.923 728

1.32
1.34
1.36
1.38

0.894 640
0.892 216
0.890 185
0.888 537

1.52
1.54
1.56
1.58

0.887 039
0.888 178
0.889 639
0.891 420

1.72
1.74
1.76
1.78

0.912 581
0.916 826
0.921 375
0.926 227

1.92
1.94
1.96
1.98

0.968 774
0.976 099
0.983 743
0.991 708

1.20

0.918 169

1.40

0.887 264

1.60

0.893 515

1.80

0.931 384

2.00

1.000 000

Table A3 Factorial Function and Its Logarithm with Base 10
n

n!

log (n!)

n

n!

log (n!)

n

n!

log (n!)

1
2
3
4
5

1
2
6
24
120

0.000 000
0.301 030
0.778 151
1.380 211
2.079 181

6
7
8
9
10

720
5 040
40 320
362 880
3 628 800

2.857 332
3.702 431
4.605 521
5.559 763
6.559 763

11
12
13
14
15

39 916 800
479 001 600
6 227 020 800
87 178 291 200
1 307 674 368 000

7.601 156
8.680 337
9.794 280
10.940 408
12.116 500

Table A4

Error Function, Sine and Cosine Integrals [see (35), (40), (42) in App. A3.1]

x

erf x

Si(x)

ci(x)

x

erf x

Si(x)

ci(x)

0.0

0.0000

0.0000

ϱ

2.0

0.9953

1.6054

Ϫ0.4230

0.2
0.4
0.6
0.8
1.0

0.2227
0.4284
0.6039
0.7421
0.8427

0.1996
0.3965
0.5881
0.7721
0.9461

1.0422
0.3788
0.0223
Ϫ0.1983
Ϫ0.3374

2.2
2.4
2.6
2.8
3.0

0.9981
0.9993
0.9998
0.9999
1.0000

1.6876
1.7525
1.8004
1.8321
1.8487

Ϫ0.3751
Ϫ0.3173
Ϫ0.2533
Ϫ0.1865
Ϫ0.1196

1.2
1.4
1.6
1.8
2.0

0.9103
0.9523
0.9763
0.9891
0.9953

1.1080
1.2562
1.3892
1.5058
1.6054

Ϫ0.4205
Ϫ0.4620
Ϫ0.4717
Ϫ0.4568
Ϫ0.4230

3.2
3.4
3.6
3.8
4.0

1.0000
1.0000
1.0000
1.0000
1.0000

1.8514
1.8419
1.8219
1.7934
1.7582

Ϫ0.0553
0.0045
0.0580
0.1038
0.1410

bapp05.qxd

11/3/10

9:01 PM

Page A99

APP. 5 Tables

A99
Table A5 Binomial Distribution

Probability function ƒ(x) [see (2), Sec. 24.7] and distribution function F(x)
n

x

p ϭ 0.1
ƒ(x)
F(x)

p ϭ 0.2
ƒ(x)
F(x)

p ϭ 0.3
ƒ(x)
F(x)

p ϭ 0.4
ƒ(x)
F(x)

p ϭ 0.5
ƒ(x)
F(x)

1

0
1

0.
9000
1000

0.9000
1.0000

0.
8000
2000

0.8000
1.0000

0.
7000
3000

0.7000
1.0000

0.
6000
4000

0.6000
1.0000

0.
5000
5000

0.5000
1.0000

2

0
1
2

8100
1800
0100

0.8100
0.9900
1.0000

6400
3200
0400

0.6400
0.9600
1.0000

4900
4200
0900

0.4900
0.9100
1.0000

3600
4800
1600

0.3600
0.8400
1.0000

2500
5000
2500

0.2500
0.7500
1.0000

3

0
1
2
3

7290
2430
0270
0010

0.7290
0.9720
0.9990
1.0000

5120
3840
0960
0080

0.5120
0.8960
0.9920
1.0000

3430
4410
1890
0270

0.3430
0.7840
0.9730
1.0000

2160
4320
2880
0640

0.2160
0.6480
0.9360
1.0000

1250
3750
3750
1250

0.1250
0.5000
0.8750
1.0000

4

0
1
2
3
4

6561
2916
0486
0036
0001

0.6561
0.9477
0.9963
0.9999
1.0000

4096
4096
1536
0256
0016

0.4096
0.8192
0.9728
0.9984
1.0000

2401
4116
2646
0756
0081

0.2401
0.6517
0.9163
0.9919
1.0000

1296
3456
3456
1536
0256

0.1296
0.4752
0.8208
0.9744
1.0000

0625
2500
3750
2500
0625

0.0625
0.3125
0.6875
0.9375
1.0000

5

0
1
2
3
4
5

5905
3281
0729
0081
0005
0000

0.5905
0.9185
0.9914
0.9995
1.0000
1.0000

3277
4096
2048
0512
0064
0003

0.3277
0.7373
0.9421
0.9933
0.9997
1.0000

1681
3602
3087
1323
0284
0024

0.1681
0.5282
0.8369
0.9692
0.9976
1.0000

0778
2592
3456
2304
0768
0102

0.0778
0.3370
0.6826
0.9130
0.9898
1.0000

0313
1563
3125
3125
1563
0313

0.0313
0.1875
0.5000
0.8125
0.9688
1.0000

6

0
1
2
3
4
5
6

5314
3543
0984
0146
0012
0001
0000

0.5314
0.8857
0.9841
0.9987
0.9999
1.0000
1.0000

2621
3932
2458
0819
0154
0015
0001

0.2621
0.6554
0.9011
0.9830
0.9984
0.9999
1.0000

1176
3025
3241
1852
0595
0102
0007

0.1176
0.4202
0.7443
0.9295
0.9891
0.9993
1.0000

0467
1866
3110
2765
1382
0369
0041

0.0467
0.2333
0.5443
0.8208
0.9590
0.9959
1.0000

0156
0938
2344
3125
2344
0938
0156

0.0156
0.1094
0.3438
0.6563
0.8906
0.9844
1.0000

7

0
1
2
3
4
5
6
7

4783
3720
1240
0230
0026
0002
0000
0000

0.4783
0.8503
0.9743
0.9973
0.9998
1.0000
1.0000
1.0000

2097
3670
2753
1147
0287
0043
0004
0000

0.2097
0.5767
0.8520
0.9667
0.9953
0.9996
1.0000
1.0000

0824
2471
3177
2269
0972
0250
0036
0002

0.0824
0.3294
0.6471
0.8740
0.9712
0.9962
0.9998
1.0000

0280
1306
2613
2903
1935
0774
0172
0016

0.0280
0.1586
0.4199
0.7102
0.9037
0.9812
0.9984
1.0000

0078
0547
1641
2734
2734
1641
0547
0078

0.0078
0.0625
0.2266
0.5000
0.7734
0.9375
0.9922
1.0000

8

0
1
2
3
4
5
6
7
8

4305
3826
1488
0331
0046
0004
0000
0000
0000

0.4305
0.8131
0.9619
0.9950
0.9996
1.0000
1.0000
1.0000
1.0000

1678
3355
2936
1468
0459
0092
0011
0001
0000

0.1678
0.5033
0.7969
0.9437
0.9896
0.9988
0.9999
1.0000
1.0000

0576
1977
2965
2541
1361
0467
0100
0012
0001

0.0576
0.2553
0.5518
0.8059
0.9420
0.9887
0.9987
0.9999
1.0000

0168
0896
2090
2787
2322
1239
0413
0079
0007

0.0168
0.1064
0.3154
0.5941
0.8263
0.9502
0.9915
0.9993
1.0000

0039
0313
1094
2188
2734
2188
1094
0313
0039

0.0039
0.0352
0.1445
0.3633
0.6367
0.8555
0.9648
0.9961
1.0000

bapp05.qxd

11/3/10

A100

9:01 PM

Page A100

APP. 5 Tables
Table A6 Poisson Distribution

Probability function ƒ(x) [see (5), Sec. 24.7] and distribution function F(x)
x

␮ ϭ 0.1
ƒ(x)
F(x)

␮ ϭ 0.2
ƒ(x)
F(x)

␮ ϭ 0.3
ƒ(x)
F(x)

␮ ϭ 0.4
ƒ(x)
F(x)

␮ ϭ 0.5
ƒ(x)
F(x)

0

0.
9048

0.9048

0.
8187

0.8187

0.
7408

0.7408

0.
6703

0.6703

0.
6065

0.6065

0905
0045
0002
0000

0.9953
0.9998
1.0000
1.0000

1637
0164
0011
0001

0.9825
0.9989
0.9999
1.0000

2222
0333
0033
0003

0.9631
0.9964
0.9997
1.0000

2681
0536
0072
0007
0001

0.9384
0.9921
0.9992
0.9999
1.0000

3033
0758
0126
0016
0002

0.9098
0.9856
0.9982
0.9998
1.0000

1
2
3
4
5

x

␮ ϭ 0.6
ƒ(x)
F(x)

␮ ϭ 0.7
ƒ(x)
F(x)

␮ ϭ 0.8
ƒ(x)
F(x)

␮ ϭ 0.9
ƒ(x)
F(x)

ƒ(x)

0

0.
5488

0.5488

0.
4966

0.4966

0.
4493

0.4493

0.
4066

0.4066

0.
3679

0.3679

1
2
3
4
5

3293
0988
0198
0030
0004

0.8781
0.9769
0.9966
0.9996
1.0000

3476
1217
0284
0050
0007

0.8442
0.9659
0.9942
0.9992
0.9999

3595
1438
0383
0077
0012

0.8088
0.9526
0.9909
0.9986
0.9998

3659
1647
0494
0111
0020

0.7725
0.9371
0.9865
0.9977
0.9997

3679
1839
0613
0153
0031

0.7358
0.9197
0.9810
0.9963
0.9994

0001

1.0000

0002

1.0000

0003

1.0000

0005
0001

0.9999
1.0000

6
7

x

␮ ϭ 1.5
ƒ(x)
F(x)

ƒ(x)

0

0.
2231

0.2231

0.
1353

0.1353

0.
0498

0.0498

0.
0183

0.0183

0.
0067

0.0067

1
2
3
4
5

3347
2510
1255
0471
0141

0.5578
0.8088
0.9344
0.9814
0.9955

2707
2707
1804
0902
0361

0.4060
0.6767
0.8571
0.9473
0.9834

1494
2240
2240
1680
1008

0.1991
0.4232
0.6472
0.8153
0.9161

0733
1465
1954
1954
1563

0.0916
0.2381
0.4335
0.6288
0.7851

0337
0842
1404
1755
1755

0.0404
0.1247
0.2650
0.4405
0.6160

6
7
8
9
10

0035
0008
0001

0.9991
0.9998
1.0000

0120
0034
0009
0002

0.9955
0.9989
0.9998
1.0000

0504
0216
0081
0027
0008

0.9665
0.9881
0.9962
0.9989
0.9997

1042
0595
0298
0132
0053

0.8893
0.9489
0.9786
0.9919
0.9972

1462
1044
0653
0363
0181

0.7622
0.8666
0.9319
0.9682
0.9863

0002
0001

0.9999
1.0000

0019
0006
0002
0001

0.9991
0.9997
0.9999
1.0000

0082
0034
0013
0005
0002

0.9945
0.9980
0.9993
0.9998
0.9999

0000

1.0000

11
12
13
14
15
16

␮ϭ2
F(x)

ƒ(x)

␮ϭ3
F(x)

ƒ(x)

␮ϭ4
F(x)

␮ϭ1
F(x)

ƒ(x)

␮ϭ5
F(x)

bapp05.qxd

11/3/10

9:01 PM

Page A101

APP. 5 Tables

A101
Table A7

Normal Distribution

Values of the distribution function ⌽(z) [see (3), Sec. 24.8]. ⌽(Ϫz) ϭ 1 Ϫ ⌽(z)
z

⌽(z)

z

⌽(z)

z

⌽(z)

z

⌽(z)

z

⌽(z)

z

⌽(z)

0.01
0.02
0.03
0.04
0.05

0.
5040
5080
5120
5160
5199

0.51
0.52
0.53
0.54
0.55

0.
6950
6985
7019
7054
7088

1.01
1.02
1.03
1.04
1.05

0.
8438
8461
8485
8508
8531

1.51
1.52
1.53
1.54
1.55

0.
9345
9357
9370
9382
9394

2.01
2.02
2.03
2.04
2.05

0.
9778
9783
9788
9793
9798

2.51
2.52
2.53
2.54
2.55

0.
9940
9941
9943
9945
9946

0.06
0.07
0.08
0.09
0.10

5239
5279
5319
5359
5398

0.56
0.57
0.58
0.59
0.60

7123
7157
7190
7224
7257

1.06
1.07
1.08
1.09
1.10

8554
8577
8599
8621
8643

1.56
1.57
1.58
1.59
1.60

9406
9418
9429
9441
9452

2.06
2.07
2.08
2.09
2.10

9803
9808
9812
9817
9821

2.56
2.57
2.58
2.59
2.60

9948
9949
9951
9952
9953

0.11
0.12
0.13
0.14
0.15

5438
5478
5517
5557
5596

0.61
0.62
0.63
0.64
0.65

7291
7324
7357
7389
7422

1.11
1.12
1.13
1.14
1.15

8665
8686
8708
8729
8749

1.61
1.62
1.63
1.64
1.65

9463
9474
9484
9495
9505

2.11
2.12
2.13
2.14
2.15

9826
9830
9834
9838
9842

2.61
2.62
2.63
2.64
2.65

9955
9956
9957
9959
9960

0.16
0.17
0.18
0.19
0.20

5636
5675
5714
5753
5793

0.66
0.67
0.68
0.69
0.70

7454
7486
7517
7549
7580

1.16
1.17
1.18
1.19
1.20

8770
8790
8810
8830
8849

1.66
1.67
1.68
1.69
1.70

9515
9525
9535
9545
9554

2.16
2.17
2.18
2.19
2.20

9846
9850
9854
9857
9861

2.66
2.67
2.68
2.69
2.70

9961
9962
9963
9964
9965

0.21
0.22
0.23
0.24
0.25

5832
5871
5910
5948
5987

0.71
0.72
0.73
0.74
0.75

7611
7642
7673
7704
7734

1.21
1.22
1.23
1.24
1.25

8869
8888
8907
8925
8944

1.71
1.72
1.73
1.74
1.75

9564
9573
9582
9591
9599

2.21
2.22
2.23
2.24
2.25

9864
9868
9871
9875
9878

2.71
2.72
2.73
2.74
2.75

9966
9967
9968
9969
9970

0.26
0.27
0.28
0.29
0.30

6026
6064
6103
6141
6179

0.76
0.77
0.78
0.79
0.80

7764
7794
7823
7852
7881

1.26
1.27
1.28
1.29
1.30

8962
8980
8997
9015
9032

1.76
1.77
1.78
1.79
1.80

9608
9616
9625
9633
9641

2.26
2.27
2.28
2.29
2.30

9881
9884
9887
9890
9893

2.76
2.77
2.78
2.79
2.80

9971
9972
9973
9974
9974

0.31
0.32
0.33
0.34
0.35

6217
6255
6293
6331
6368

0.81
0.82
0.83
0.84
0.85

7910
7939
7967
7995
8023

1.31
1.32
1.33
1.34
1.35

9049
9066
9082
9099
9115

1.81
1.82
1.83
1.84
1.85

9649
9656
9664
9671
9678

2.31
2.32
2.33
2.34
2.35

9896
9898
9901
9904
9906

2.81
2.82
2.83
2.84
2.85

9975
9976
9977
9977
9978

0.36
0.37
0.38
0.39
0.40

6406
6443
6480
6517
6554

0.86
0.87
0.88
0.89
0.90

8051
8078
8106
8133
8159

1.36
1.37
1.38
1.39
1.40

9131
9147
9162
9177
9192

1.86
1.87
1.88
1.89
1.90

9686
9693
9699
9706
9713

2.36
2.37
2.38
2.39
2.40

9909
9911
9913
9916
9918

2.86
2.87
2.88
2.89
2.90

9979
9979
9980
9981
9981

0.41
0.42
0.43
0.44
0.45

6591
6628
6664
6700
6736

0.91
0.92
0.93
0.94
0.95

8186
8212
8238
8264
8289

1.41
1.42
1.43
1.44
1.45

9207
9222
9236
9251
9265

1.91
1.92
1.93
1.94
1.95

9719
9726
9732
9738
9744

2.41
2.42
2.43
2.44
2.45

9920
9922
9925
9927
9929

2.91
2.92
2.93
2.94
2.95

9982
9982
9983
9984
9984

0.46
0.47
0.48
0.49
0.50

6772
6808
6844
6879
6915

0.96
0.97
0.98
0.99
1.00

8315
8340
8365
8389
8413

1.46
1.47
1.48
1.49
1.50

9279
9292
9306
9319
9332

1.96
1.97
1.98
1.99
2.00

9750
9756
9761
9767
9772

2.46
2.47
2.48
2.49
2.50

9931
9932
9934
9936
9938

2.96
2.97
2.98
2.99
3.00

9985
9985
9986
9986
9987

bapp05.qxd

11/3/10

A102

9:01 PM

Page A102

APP. 5 Tables
Table A8 Normal Distribution

Values of z for given values of ⌽(z) [see (3), Sec. 24.8] and D(z) ϭ ⌽(z) Ϫ ⌽(Ϫz)
Example: z ϭ 0.279 if ⌽(z) ϭ 61%; z ϭ 0.860 if D(z) ϭ 61%.
%

z(⌽)

z(D)

%

z(⌽)

z(D)

%

z(⌽)

z(D)

1
2
3
4
5

Ϫ2.326
Ϫ2.054
Ϫ1.881
Ϫ1.751
Ϫ1.645

0.013
0.025
0.038
0.050
0.063

41
42
43
44
45

Ϫ0.228
Ϫ0.202
Ϫ0.176
Ϫ0.151
Ϫ0.126

0.539
0.553
0.568
0.583
0.598

81
82
83
84
85

0.878
0.915
0.954
0.994
1.036

1.311
1.341
1.372
1.405
1.440

6
7
8
9
10

Ϫ1.555
Ϫ1.476
Ϫ1.405
Ϫ1.341
Ϫ1.282

0.075
0.088
0.100
0.113
0.126

46
47
48
49
50

Ϫ0.100
Ϫ0.075
Ϫ0.050
Ϫ0.025
0.000

0.613
0.628
0.643
0.659
0.674

86
87
88
89
90

1.080
1.126
1.175
1.227
1.282

1.476
1.514
1.555
1.598
1.645

11
12
13
14
15

Ϫ1.227
Ϫ1.175
Ϫ1.126
Ϫ1.080
Ϫ1.036

0.138
0.151
0.164
0.176
0.189

51
52
53
54
55

0.025
0.050
0.075
0.100
0.126

0.690
0.706
0.722
0.739
0.755

91
92
93
94
95

1.341
1.405
1.476
1.555
1.645

1.695
1.751
1.812
1.881
1.960

16
17
18
19
20

Ϫ0.994
Ϫ0.954
Ϫ0.915
Ϫ0.878
Ϫ0.842

0.202
0.215
0.228
0.240
0.253

56
57
58
59
60

0.151
0.176
0.202
0.228
0.253

0.772
0.789
0.806
0.824
0.842

96
97
97.5
98
99

1.751
1.881
1.960
2.054
2.326

2.054
2.170
2.241
2.326
2.576

21
22
23
24
25

Ϫ0.806
Ϫ0.772
Ϫ0.739
Ϫ0.706
Ϫ0.674

0.266
0.279
0.292
0.305
0.319

61
62
63
64
65

0.279
0.305
0.332
0.358
0.385

0.860
0.878
0.896
0.915
0.935

99.1
99.2
99.3
99.4
99.5

2.366
2.409
2.457
2.512
2.576

2.612
2.652
2.697
2.748
2.807

26
27
28
29
30

Ϫ0.643
Ϫ0.613
Ϫ0.583
Ϫ0.553
Ϫ0.524

0.332
0.345
0.358
0.372
0.385

66
67
68
69
70

0.412
0.440
0.468
0.496
0.524

0.954
0.974
0.994
1.015
1.036

99.6
99.7
99.8
99.9

2.652
2.748
2.878
3.090

2.878
2.968
3.090
3.291

31
32
33
34
35

Ϫ0.496
Ϫ0.468
Ϫ0.440
Ϫ0.412
Ϫ0.385

0.399
0.412
0.426
0.440
0.454

71
72
73
74
75

0.553
0.583
0.613
0.643
0.674

1.058
1.080
1.103
1.126
1.150

99.91
99.92
99.93
99.94
99.95

3.121
3.156
3.195
3.239
3.291

3.320
3.353
3.390
3.432
3.481

36
37
38
39
40

Ϫ0.358
Ϫ0.332
Ϫ0.305
Ϫ0.279
Ϫ0.253

0.468
0.482
0.496
0.510
0.524

76
77
78
79
80

0.706
0.739
0.772
0.806
0.842

1.175
1.200
1.227
1.254
1.282

99.96
99.97
99.98
99.99

3.353
3.432
3.540
3.719

3.540
3.615
3.719
3.891

bapp05.qxd

11/3/10

9:01 PM

Page A103

APP. 5 Tables

A103
Table A9 t-Distribution

Values of z for given values of the distribution function F(z) (see (8) in Sec. 25.3).
Example: For 9 degrees of freedom, z ϭ 1.83 when F(z) ϭ 0.95.
Number of Degrees of Freedom
F(z)
1
0.5
0.6
0.7
0.8
0.9
0.95
0.975
0.99
0.995
0.999

2

3

4

5

6

7

8

9

10

0.00
0.32
0.73
1.38
3.08

0.00
0.29
0.62
1.06
1.89

0.00
0.28
0.58
0.98
1.64

0.00
0.27
0.57
0.94
1.53

0.00
0.27
0.56
0.92
1.48

0.00
0.26
0.55
0.91
1.44

0.00
0.26
0.55
0.90
1.41

0.00
0.26
0.55
0.89
1.40

0.00
0.26
0.54
0.88
1.38

0.00
0.26
0.54
0.88
1.37

6.31
12.7
31.8
63.7
318.3

2.92
4.30
6.96
9.92
22.3

2.35
3.18
4.54
5.84
10.2

2.13
2.78
3.75
4.60
7.17

2.02
2.57
3.36
4.03
5.89

1.94
2.45
3.14
3.71
5.21

1.89
2.36
3.00
3.50
4.79

1.86
2.31
2.90
3.36
4.50

1.83
2.26
2.82
3.25
4.30

1.81
2.23
2.76
3.17
4.14

Number of Degrees of Freedom
F(z)
11

12

13

14

15

16

17

18

19

20

0.5
0.6
0.7
0.8
0.9

0.00
0.26
0.54
0.88
1.36

0.00
0.26
0.54
0.87
1.36

0.00
0.26
0.54
0.87
1.35

0.00
0.26
0.54
0.87
1.35

0.00
0.26
0.54
0.87
1.34

0.00
0.26
0.54
0.86
1.34

0.00
0.26
0.53
0.86
1.33

0.00
0.26
0.53
0.86
1.33

0.00
0.26
0.53
0.86
1.33

0.00
0.26
0.53
0.86
1.33

0.95
0.975
0.99
0.995
0.999

1.80
2.20
2.72
3.11
4.02

1.78
2.18
2.68
3.05
3.93

1.77
2.16
2.65
3.01
3.85

1.76
2.14
2.62
2.98
3.79

1.75
2.13
2.60
2.95
3.73

1.75
2.12
2.58
2.92
3.69

1.74
2.11
2.57
2.90
3.65

1.73
2.10
2.55
2.88
3.61

1.73
2.09
2.54
2.86
3.58

1.72
2.09
2.53
2.85
3.55

Number of Degrees of Freedom
F(z)
22

24

26

28

30

40

50

100

200

`

0.5
0.6
0.7
0.8
0.9

0.00
0.26
0.53
0.86
1.32

0.00
0.26
0.53
0.86
1.32

0.00
0.26
0.53
0.86
1.31

0.00
0.26
0.53
0.85
1.31

0.00
0.26
0.53
0.85
1.31

0.00
0.26
0.53
0.85
1.30

0.00
0.25
0.53
0.85
1.30

0.00
0.25
0.53
0.85
1.29

0.00
0.25
0.53
0.84
1.29

0.00
0.25
0.52
0.84
1.28

0.95
0.975
0.99
0.995
0.999

1.72
2.07
2.51
2.82
3.50

1.71
2.06
2.49
2.80
3.47

1.71
2.06
2.48
2.78
3.43

1.70
2.05
2.47
2.76
3.41

1.70
2.04
2.46
2.75
3.39

1.68
2.02
2.42
2.70
3.31

1.68
2.01
2.40
2.68
3.26

1.66
1.98
2.36
2.63
3.17

1.65
1.97
2.35
2.60
3.13

1.65
1.96
2.33
2.58
3.09

bapp05.qxd

11/3/10

A104

9:01 PM

Page A104

APP. 5 Tables
Table A10

Chi-square Distribution

Values of x for given values of the distribution function F(z) (see Sec. 25.3 before (17)).
Example: For 3 degrees of freedom, z ϭ 11.34 when F(z) ϭ 0.99.
Number of Degrees of Freedom
F(z)
1

2

3

0.005
0.01
0.025
0.05

0.00
0.00
0.00
0.00

0.01
0.02
0.05
0.10

0.07
0.11
0.22
0.35

0.95
0.975
0.99
0.995

3.84
5.02
6.63
7.88

5.99
7.38
9.21
10.60

7.81
9.35
11.34
12.84

4

5

6

7

8

9

10

0.21
0.30
0.48
0.71

0.41
0.55
0.83
1.15

0.68
0.87
1.24
1.64

0.99
1.24
1.69
2.17

1.34
1.65
2.18
2.73

1.73
2.09
2.70
3.33

2.16
2.56
3.25
3.94

9.49
11.14
13.28
14.86

11.07
12.83
15.09
16.75

12.59
14.45
16.81
18.55

14.07
16.01
18.48
20.28

15.51
17.53
20.09
21.95

16.92
19.02
21.67
23.59

18.31
20.48
23.21
25.19

19

20

Number of Degrees of Freedom
F(z)
11

12

13

14

15

16

17

18

0.005
0.01
0.025
0.05

2.60
3.05
3.82
4.57

3.07
3.57
4.40
5.23

3.57
4.11
5.01
5.89

4.07
4.66
5.63
6.57

4.60
5.23
6.26
7.26

5.14
5.81
6.91
7.96

5.70
6.41
7.56
8.67

6.26
7.01
8.23
9.39

6.84
7.63
8.91
10.12

7.43
8.26
9.59
10.85

0.95
0.975
0.99
0.995

19.68
21.92
24.72
26.76

21.03
23.34
26.22
28.30

22.36
24.74
27.69
29.82

23.68
26.12
29.14
31.32

25.00
27.49
30.58
32.80

26.30
28.85
32.00
34.27

27.59
30.19
33.41
35.72

28.87
31.53
34.81
37.16

30.14
32.85
36.19
38.58

31.41
34.17
37.57
40.00

Number of Degrees of Freedom
F(z)
21

22

23

24

25

26

27

28

29

30

0.005
0.01
0.025
0.05

8.0
8.9
10.3
11.6

8.6
9.5
11.0
12.3

9.3
10.2
11.7
13.1

9.9
10.9
12.4
13.8

10.5
11.5
13.1
14.6

11.2
12.2
13.8
15.4

11.8
12.9
14.6
16.2

12.5
13.6
15.3
16.9

13.1
14.3
16.0
17.7

13.8
15.0
16.8
18.5

0.95
0.975
0.99
0.995

32.7
35.5
38.9
41.4

33.9
36.8
40.3
42.8

35.2
38.1
41.6
44.2

36.4
39.4
43.0
45.6

37.7
40.6
44.3
46.9

38.9
41.9
45.6
48.3

40.1
43.2
47.0
49.6

41.3
44.5
48.3
51.0

42.6
45.7
49.6
52.3

43.8
47.0
50.9
53.7

Number of Degrees of Freedom
F(z)

Ͼ 100 (Approximation)

40

50

60

70

80

90

100

0.005
0.01
0.025
0.05

20.7
22.2
24.4
26.5

28.0
29.7
32.4
34.8

35.5
37.5
40.5
43.2

43.3
45.4
48.8
51.7

51.2
53.5
57.2
60.4

59.2
61.8
65.6
69.1

67.3
70.1
74.2
77.9

_1 (h
2
_1 (h
2
_1 (h
2
_1 (h
2

Ϫ
Ϫ
Ϫ
Ϫ

2.58)2
2.33)2
1.96)2
1.64)2

0.95
0.975
0.99
0.995

55.8
59.3
63.7
66.8

67.5
71.4
76.2
79.5

79.1
83.3
88.4
92.0

90.5
95.0
100.4
104.2

101.9
106.6
112.3
116.3

113.1
118.1
124.1
128.3

124.3
129.6
135.8
140.2

_1 (h
2
_1 (h
2
_1 (h
2
_1 (h

ϩ
ϩ
ϩ
ϩ

1.64)2
1.96)2
2.33)2
2.58)2

In the last column, h ϭ ͙2ෆm
ෆෆ
Ϫෆ,
1 where m is the number of degrees of freedom.

2

bapp05.qxd

11/3/10

9:01 PM

Page A105

APP. 5 Tables

A105
Table A11

F-Distribution with (m, n) Degrees of Freedom

Values of z for which the distribution function F(z) [see (13), Sec. 25.4] has the value
Example: For (7, 4) d.f., z ϭ 6.09 if F(z) ϭ 0.95.
mϭ1

mϭ2

mϭ3

mϭ4

mϭ5

mϭ6

mϭ7

mϭ8

mϭ9

1
2
3
4
5

161
18.5
10.1
7.71
6.61

200
19.0
9.55
6.94
5.79

216
19.2
9.28
6.59
5.41

225
19.2
9.12
6.39
5.19

230
19.3
9.01
6.26
5.05

234
19.3
8.94
6.16
4.95

237
19.4
8.89
6.09
4.88

239
19.4
8.85
6.04
4.82

241
19.4
8.81
6.00
4.77

6
7
8
9
10

5.99
5.59
5.32
5.12
4.96

5.14
4.74
4.46
4.26
4.10

4.76
4.35
4.07
3.86
3.71

4.53
4.12
3.84
3.63
3.48

4.39
3.97
3.69
3.48
3.33

4.28
3.87
3.58
3.37
3.22

4.21
3.79
3.50
3.29
3.14

4.15
3.73
3.44
3.23
3.07

4.10
3.68
3.39
3.18
3.02

11
12
13
14
15

4.84
4.75
4.67
4.60
4.54

3.98
3.89
3.81
3.74
3.68

3.59
3.49
3.41
3.34
3.29

3.36
3.26
3.18
3.11
3.06

3.20
3.11
3.03
2.96
2.90

3.09
3.00
2.92
2.85
2.79

3.01
2.91
2.83
2.76
2.71

2.95
2.85
2.77
2.70
2.64

2.90
2.80
2.71
2.65
2.59

16
17
18
19
20

4.49
4.45
4.41
4.38
4.35

3.63
3.59
3.55
3.52
3.49

3.24
3.20
3.16
3.13
3.10

3.01
2.96
2.93
2.90
2.87

2.85
2.81
2.77
2.74
2.71

2.74
2.70
2.66
2.63
2.60

2.66
2.61
2.58
2.54
2.51

2.59
2.55
2.51
2.48
2.45

2.54
2.49
2.46
2.42
2.39

22
24
26
28
30

4.30
4.26
4.23
4.20
4.17

3.44
3.40
3.37
3.34
3.32

3.05
3.01
2.98
2.95
2.92

2.82
2.78
2.74
2.71
2.69

2.66
2.62
2.59
2.56
2.53

2.55
2.51
2.47
2.45
2.42

2.46
2.42
2.39
2.36
2.33

2.40
2.36
2.32
2.29
2.27

2.34
2.30
2.27
2.24
2.21

32
34
36
38
40

4.15
4.13
4.11
4.10
4.08

3.29
3.28
3.26
3.24
3.23

2.90
2.88
2.87
2.85
2.84

2.67
2.65
2.63
2.62
2.61

2.51
2.49
2.48
2.46
2.45

2.40
2.38
2.36
2.35
2.34

2.31
2.29
2.28
2.26
2.25

2.24
2.23
2.21
2.19
2.18

2.19
2.17
2.15
2.14
2.12

50
60
70
80
90

4.03
4.00
3.98
3.96
3.95

3.18
3.15
3.13
3.11
3.10

2.79
2.76
2.74
2.72
2.71

2.56
2.53
2.50
2.49
2.47

2.40
2.37
2.35
2.33
2.32

2.29
2.25
2.23
2.21
2.20

2.20
2.17
2.14
2.13
2.11

2.13
2.10
2.07
2.06
2.04

2.07
2.04
2.02
2.00
1.99

100
150
200
1000
ϱ

3.94
3.90
3.89
3.85
3.84

3.09
3.06
3.04
3.00
3.00

2.70
2.66
2.65
2.61
2.60

2.46
2.43
2.42
2.38
2.37

2.31
2.27
2.26
2.22
2.21

2.19
2.16
2.14
2.11
2.10

2.10
2.07
2.06
2.02
2.01

2.03
2.00
1.98
1.95
1.94

1.97
1.94
1.93
1.89
1.88

n

0.95

bapp05.qxd

11/3/10

A106

9:01 PM

Page A106

APP. 5 Tables
Table A11

F-Distribution with (m, n) Degrees of Freedom (continued)

Values of z for which the distribution function F(z) [see (13), Sec. 25.4] has the value

0.95

m ϭ 10

m ϭ 15

m ϭ 20

m ϭ 30

m ϭ 40

m ϭ 50

m ϭ 100

`

1
2
3
4
5

242
19.4
8.79
5.96
4.74

246
19.4
8.70
5.86
4.62

248
19.4
8.66
5.80
4.56

250
19.5
8.62
5.75
4.50

251
19.5
8.59
5.72
4.46

252
19.5
8.58
5.70
4.44

253
19.5
8.55
5.66
4.41

254
19.5
8.53
5.63
4.37

6
7
8
9
10

4.06
3.64
3.35
3.14
2.98

3.94
3.51
3.22
3.01
2.85

3.87
3.44
3.15
2.94
2.77

3.81
3.38
3.08
2.86
2.70

3.77
3.34
3.04
2.83
2.66

3.75
3.32
3.02
2.80
2.64

3.71
3.27
2.97
2.76
2.59

3.67
3.23
2.93
2.71
2.54

11
12
13
14
15

2.85
2.75
2.67
2.60
2.54

2.72
2.62
2.53
2.46
2.40

2.65
2.54
2.46
2.39
2.33

2.57
2.47
2.38
2.31
2.25

2.53
2.43
2.34
2.27
2.20

2.51
2.40
2.31
2.24
2.18

2.46
2.35
2.26
2.19
2.12

2.40
2.30
2.21
2.13
2.07

16
17
18
19
20

2.49
2.45
2.41
2.38
2.35

2.35
2.31
2.27
2.23
2.20

2.28
2.23
2.19
2.16
2.12

2.19
2.15
2.11
2.07
2.04

2.15
2.10
2.06
2.03
1.99

2.12
2.08
2.04
2.00
1.97

2.07
2.02
1.98
1.94
1.91

2.01
1.96
1.92
1.88
1.84

22
24
26
28
30

2.30
2.25
2.22
2.19
2.16

2.15
2.11
2.07
2.04
2.01

2.07
2.03
1.99
1.96
1.93

1.98
1.94
1.90
1.87
1.84

1.94
1.89
1.85
1.82
1.79

1.91
1.86
1.82
1.79
1.76

1.85
1.80
1.76
1.73
1.70

1.78
1.73
1.69
1.65
1.62

32
34
36
38
40

2.14
2.12
2.11
2.09
2.08

1.99
1.97
1.95
1.94
1.92

1.91
1.89
1.87
1.85
1.84

1.82
1.80
1.78
1.76
1.74

1.77
1.75
1.73
1.71
1.69

1.74
1.71
1.69
1.68
1.66

1.67
1.65
1.62
1.61
1.59

1.59
1.57
1.55
1.53
1.51

50
60
70
80
90

2.03
1.99
1.97
1.95
1.94

1.87
1.84
1.81
1.79
1.78

1.78
1.75
1.72
1.70
1.69

1.69
1.65
1.62
1.60
1.59

1.63
1.59
1.57
1.54
1.53

1.60
1.56
1.53
1.51
1.49

1.52
1.48
1.45
1.43
1.41

1.44
1.39
1.35
1.32
1.30

100
150
200
1000
ϱ

1.93
1.89
1.88
1.84
1.83

1.77
1.73
1.72
1.68
1.67

1.68
1.64
1.62
1.58
1.57

1.57
1.54
1.52
1.47
1.46

1.52
1.48
1.46
1.41
1.39

1.48
1.44
1.41
1.36
1.35

1.39
1.34
1.32
1.26
1.24

1.28
1.22
1.19
1.08
1.00

n

bapp05.qxd

11/3/10

9:01 PM

Page A107

APP. 5 Tables

A107
Table A11

F-Distribution with (m, n) Degrees of Freedom (continued)

Values of z for which the distribution function F(z) [see (13), Sec. 25.4] has the value

0.99

mϭ1

mϭ2

mϭ3

mϭ4

mϭ5

mϭ6

mϭ7

mϭ8

mϭ9

1
2
3
4
5

4052
98.5
34.1
21.2
16.3

4999
99.0
30.8
18.0
13.3

5403
99.2
29.5
16.7
12.1

5625
99.2
28.7
16.0
11.4

5764
99.3
28.2
15.5
11.0

5859
99.3
27.9
15.2
10.7

5928
99.4
27.7
15.0
10.5

5981
99.4
27.5
14.8
10.3

6022
99.4
27.3
14.7
10.2

6
7
8
9
10

13.7
12.2
11.3
10.6
10.0

n

10.9
9.55
8.65
8.02
7.56

9.78
8.45
7.59
6.99
6.55

9.15
7.85
7.01
6.42
5.99

8.75
7.46
6.63
6.06
5.64

8.47
7.19
6.37
5.80
5.39

8.26
6.99
6.18
5.61
5.20

8.10
6.84
6.03
5.47
5.06

7.98
6.72
5.91
5.35
4.94

11
12
13
14
15

9.65
9.33
9.07
8.86
8.68

7.21
6.93
6.70
6.51
6.36

6.22
5.95
5.74
5.56
5.42

5.67
5.41
5.21
5.04
4.89

5.32
5.06
4.86
4.69
4.56

5.07
4.82
4.62
4.46
4.32

4.89
4.64
4.44
4.28
4.14

4.74
4.50
4.30
4.14
4.00

4.63
4.39
4.19
4.03
3.89

16
17
18
19
20

8.53
8.40
8.29
8.18
8.10

6.23
6.11
6.01
5.93
5.85

5.29
5.18
5.09
5.01
4.94

4.77
4.67
4.58
4.50
4.43

4.44
4.34
4.25
4.17
4.10

4.20
4.10
4.01
3.94
3.87

4.03
3.93
3.84
3.77
3.70

3.89
3.79
3.71
3.63
3.56

3.78
3.68
3.60
3.52
3.46

22
24
26
28
30

7.95
7.82
7.72
7.64
7.56

5.72
5.61
5.53
5.45
5.39

4.82
4.72
4.64
4.57
4.51

4.31
4.22
4.14
4.07
4.02

3.99
3.90
3.82
3.75
3.70

3.76
3.67
3.59
3.53
3.47

3.59
3.50
3.42
3.36
3.30

3.45
3.36
3.29
3.23
3.17

3.35
3.26
3.18
3.12
3.07

32
34
36
38
40

7.50
7.44
7.40
7.35
7.31

5.34
5.29
5.25
5.21
5.18

4.46
4.42
4.38
4.34
4.31

3.97
3.93
3.89
3.86
3.83

3.65
3.61
3.57
3.54
3.51

3.43
3.39
3.35
3.32
3.29

3.26
3.22
3.18
3.15
3.12

3.13
3.09
3.05
3.02
2.99

3.02
2.98
2.95
2.92
2.89

50
60
70
80
90

7.17
7.08
7.01
6.96
6.93

5.06
4.98
4.92
4.88
4.85

4.20
4.13
4.07
4.04
4.01

3.72
3.65
3.60
3.56
3.54

3.41
3.34
3.29
3.26
3.23

3.19
3.12
3.07
3.04
3.01

3.02
2.95
2.91
2.87
2.84

2.89
2.82
2.78
2.74
2.72

2.78
2.72
2.67
2.64
2.61

100
150
200
1000
ϱ

6.90
6.81
6.76
6.66
6.63

4.82
4.75
4.71
4.63
4.61

3.98
3.91
3.88
3.80
3.78

3.51
3.45
3.41
3.34
3.32

3.21
3.14
3.11
3.04
3.02

2.99
2.92
2.89
2.82
2.80

2.82
2.76
2.73
2.66
2.64

2.69
2.63
2.60
2.53
2.51

2.59
2.53
2.50
2.43
2.41

bapp05.qxd

11/3/10

A108

9:01 PM

Page A108

APP. 5 Tables
Table A11

F-Distribution with (m, n) Degrees of Freedom (continued)

Values of z for which the distribution function F(z) [see (13), Sec. 25.4] has the value
n
1
2
3
4
5

0.99

m ϭ 10

m ϭ 15

m ϭ 20

m ϭ 30

m ϭ 40

m ϭ 50

m ϭ 100

`

6056
99.4
27.2
14.5
10.1

6157
99.4
26.9
14.2
9.72

6209
99.4
26.7
14.0
9.55

6261
99.5
26.5
13.8
9.38

6287
99.5
26.4
13.7
9.29

6303
99.5
26.4
13.7
9.24

6334
99.5
26.2
13.6
9.13

6366
99.5
26.1
13.5
9.02

6
7
8
9
10

7.87
6.62
5.81
5.26
4.85

7.56
6.31
5.52
4.96
4.56

7.40
6.16
5.36
4.81
4.41

7.23
5.99
5.20
4.65
4.25

7.14
5.91
5.12
4.57
4.17

7.09
5.86
5.07
4.52
4.12

6.99
5.75
4.96
4.42
4.01

6.88
5.65
4.86
4.31
3.91

11
12
13
14
15

4.54
4.30
4.10
3.94
3.80

4.25
4.01
3.82
3.66
3.52

4.10
3.86
3.66
3.51
3.37

3.94
3.70
3.51
3.35
3.21

3.86
3.62
3.43
3.27
3.13

3.81
3.57
3.38
3.22
3.08

3.71
3.47
3.27
3.11
2.98

3.60
3.36
3.17
3.00
2.87

16
17
18
19
20

3.69
3.59
3.51
3.43
3.37

3.41
3.31
3.23
3.15
3.09

3.26
3.16
3.08
3.00
2.94

3.10
3.00
2.92
2.84
2.78

3.02
2.92
2.84
2.76
2.69

2.97
2.87
2.78
2.71
2.64

2.86
2.76
2.68
2.60
2.54

2.75
2.65
2.57
2.49
2.42

22
24
26
28
30

3.26
3.17
3.09
3.03
2.98

2.98
2.89
2.81
2.75
2.70

2.83
2.74
2.66
2.60
2.55

2.67
2.58
2.50
2.44
2.39

2.58
2.49
2.42
2.35
2.30

2.53
2.44
2.36
2.30
2.25

2.42
2.33
2.25
2.19
2.13

2.31
2.21
2.13
2.06
2.01

32
34
36
38
40

2.93
2.89
2.86
2.83
2.80

2.65
2.61
2.58
2.55
2.52

2.50
2.46
2.43
2.40
2.37

2.34
2.30
2.26
2.23
2.20

2.25
2.21
2.18
2.14
2.11

2.20
2.16
2.12
2.09
2.06

2.08
2.04
2.00
1.97
1.94

1.96
1.91
1.87
1.84
1.80

50
60
70
80
90

2.70
2.63
2.59
2.55
2.52

2.42
2.35
2.31
2.27
2.24

2.27
2.20
2.15
2.12
2.09

2.10
2.03
1.98
1.94
1.92

2.01
1.94
1.89
1.85
1.82

1.95
1.88
1.83
1.79
1.76

1.82
1.75
1.70
1.65
1.62

1.68
1.60
1.54
1.49
1.46

100
150
200
1000
ϱ

2.50
2.44
2.41
2.34
2.32

2.22
2.16
2.13
2.06
2.04

2.07
2.00
1.97
1.90
1.88

1.89
1.83
1.79
1.72
1.70

1.80
1.73
1.69
1.61
1.59

1.74
1.66
1.63
1.54
1.52

1.60
1.52
1.48
1.38
1.36

1.43
1.33
1.28
1.11
1.00

bapp05.qxd

11/3/10

9:01 PM

Page A109

APP. 5 Tables

A109
Table A12 Distribution Function F( x) ‫ ؍‬P(T Ϲ x) of the Random Variable T in
Section 25.8
x

n
ϭ3

0
1

0.
167
500

x

n
ϭ4

0
1
2

0.
042
167
375

n
x ϭ20
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94

0.
001
002
002
003
004
005
006
007
008
010
012
014
017
020
023
027
032
037
043
049
056
064
073
082
093
104
117
130
144
159
176
193
211
230
250
271
293
315
339
362
387
411
436
462
487

x

n
ϭ5

0
1
2
3
4

0.
008
042
117
242
408

n
x ϭ19
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85

0.
001
002
002
003
003
004
005
006
008
010
012
014
017
021
025
029
034
040
047
054
062
072
082
093
105
119
133
149
166
184
203
223
245
267
290
314
339
365
391
418
445
473
500

x

n
ϭ6

0
1
2
3
4
5
6
7

0.
001
008
028
068
136
235
360
500

n
x ϭ18
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76

0.
001
002
003
003
004
005
007
009
011
013
016
020
024
029
034
041
048
056
066
076
088
100
115
130
147
165
184
205
227
250
275
300
327
354
383
411
441
470
500

x

n
ϭ7

1
2
3
4
5
6
7
8
9
10

0.
001
005
015
035
068
119
191
281
386
500

x

n
ϭ8

2
3
4
5
6
7
8
9
10
11
12
13

0.
001
003
007
016
031
054
089
138
199
274
360
452

n
x ϭ17
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67

0.
001
002
002
003
004
005
007
009
011
014
017
021
026
032
038
046
054
064
076
088
102
118
135
154
174
196
220
245
271
299
328
358
388
420
452
484

x

n
ϭ9

4
5
6
7
8
9
10
11
12
13
14
15
16
17

0.
001
003
006
012
022
038
060
090
130
179
238
306
381
460

n
x ϭ16
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59

0.
001
002
002
003
004
006
008
010
013
016
021
026
032
039
048
058
070
083
097
114
133
153
175
199
225
253
282
313
345
378
412
447
482

n
x ϭ10
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

0.
001
002
005
008
014
023
036
054
078
108
146
190
242
300
364
431
500

n
x ϭ15
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52

0.
001
002
003
004
006
008
010
014
018
023
029
037
046
057
070
084
101
120
141
164
190
218
248
279
313
349
385
423
461
500

n
x ϭ11
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

0.
001
002
003
005
008
013
020
030
043
060
082
109
141
179
223
271
324
381
440
500

n
x ϭ14
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45

0.
001
002
002
003
005
007
010
013
018
024
031
040
051
063
079
096
117
140
165
194
225
259
295
334
374
415
457
500

n
x ϭ13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38

0.
001
001
002
003
005
007
011
015
021
029
038
050
064
082
102
126
153
184
218
255
295
338
383
429
476

n
x ϭ12
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32

0.
001
002
003
004
007
010
016
022
031
043
058
076
098
125
155
190
230
273
319
369
420
473

ffirs.qxd

11/4/10

10:50 AM

Page iv

bindex.qxd

11/4/10

6:06 PM

Page I1

INDEX

Abel, Niels Henrik, 79n.6
Abel’s formula, 79
Absolute convergence (series):
defined, 674
and uniform convergence, 704
Absolute frequency (probability):
of an event, 1019
cumulative, 1012
of a value, 1012
Absolutely integrable nonperiodic
function, 512–513
Absolute value (complex numbers),
613
Acceleration, 386–389
Acceleration of gravity, 8
Acceleration vector, 386
Acceptable lots, 1094
Acceptable quality level (AQL),
1094
Acceptance:
of a hypothesis, 1078
of products, 1092
Acceptance number, 1092
Acceptance sampling, 1092–1096,
1113
errors in, 1093–1094
rectification, 1094–1095
Adams, John Couch, 912n.2
Adams–Bashforth methods, 911–914,
947
Adams–Moulton methods, 913–914,
947
Adaptive integration, 835–836, 843
Addition:
for arbitrary events, 1021–1022
of complex numbers, 609, 610
of matrices and vectors, 126,
259–261
of means, 1057–1058
for mutually exclusive events,
1021
of power series, 687
termwise, 173, 687
of variances, 1058–1059
vector, 309, 357–359
ADI (alternating direction implicit)
method, 928–930
Adjacency matrix:
of a digraph, 973
of a graph, 972–973

Adjacent vertices, 971, 977
Airy, Sir George Bidell, 556n.2,
918n.4
Airy equation, 556
RK method, 917–919
RKN method, 919–920
Airy function:
RK method, 917–919
RKN method, 919–920
Algebraic equations, 798
Algebraic multiplicity, 326,
878
Algorithms:
complexity of, 978–979
defined, 796
numeric analysis, 796
numeric methods as, 788
numeric stability of, 796, 842
ALGORITHMS:
BISECT, A46
DIJKSTRA, 982
EULER, 903
FORD–FULKERSON, 998
GAUSS, 849
GAUSS–SEIDEL, 860
INTERPOL, 814
KRUSKAL, 985
MATCHING, 1003
MOORE, 977
NEWTON, 802
PRIM, 989
RUNGE–KUTTA, 905
SIMPSON, 832
Aliasing, 531
Alternating direction implicit (ADI)
method, 928–930
Alternating path, 1002
Alternative hypothesis, 1078
Ampère, André Marie, 93n.7
Amplification, 91
Amplitude, 90
Amplitude spectrum, 511
Analytic functions, 172, 201, 641
complex analysis, 623–624
conformal mapping, 737–742
derivatives of, 664–668, 688–689,
A95–A96
integration of:
indefinite, 647
by use of path, 647–650

Analytic functions (Cont.)
Laurent series:
analytics at infinity, 718–719
zeros of, 717–718
maximum modulus theorem,
782–783
mean value property, 781–782
power series representation of,
688–689
real functions vs., 694
Analyticity, 623
Angle of intersection:
conformal mapping, 738
between two curves, 36
Angular speed (rotation), 372
Angular velocity (fluid flow),
775
AOQ (average outgoing quality),
1095
AOQL (average outgoing quality
limit), 1095
Apparent resistance (RLC circuits),
95
Approximation(s):
errors involved in, 794
polynomial, 808
by trigonometric polynomials,
495–498
Approximation theory, 495
A priori estimates, 805
AQL (acceptable quality level), 1094
Arbitrary positive, 191
Arc, of a curve, 383
Archimedes, 391n.4
Arc length (curves), 385–386
Area:
of a region, 428
of region bounded by ellipses,
436
of a surface, 448–450
Argand, Jean Robert, 611n.2
Argand diagram, 611n.2
Argument (complex numbers), 613
Artificial variables, 965–968
Assignment problems (combinatorial
optimization), 1001–1006
Associative law, 264
Asymptotically equal, 189, 1027,
1050
Asymptotically normal, 1076

I1

bindex.qxd

11/4/10

I2

6:06 PM

Page I2

Index

Asymptotically stable critical points,
149
Augmented matrices, 258, 272, 273,
321, 845, 959
Augmenting path, 1002–1003. See
also Flow augmenting paths
Autonomous ODEs, 11, 33
Autonomous systems, 152, 165
Auxiliary equation, 54. See also
Characteristic equation
Average flow, 458
Average outgoing quality (AOQ),
1095
Average outgoing quality limit
(AOQL), 1095
Axioms of probability, 1020
Back substitution (linear systems),
274–276, 846
Backward edges:
cut sets, 994
initial flow, 998
of a path, 992
Backward Euler formula, 909
Backward Euler method (BEM):
first-order ODEs, 909–910
stiff systems, 920–921
Backward Euler scheme, 909
Balance law, 14
Band matrices, 928
Bashforth, Francis, 912n.2
Basic feasible solution:
normal form of linear optimization
problems, 957
simplex method, 959
Basic Rule (method of undetermined
coefficients):
higher-order homogeneous linear
ODEs, 115
second-order nonhomogeneous
linear ODEs, 81, 82
Basic variables, 960
Basis:
eigenvectors, 339–340
of solutions:
higher-order linear ODEs, 106,
113, 123
homogeneous linear systems,
290
homogeneous ODEs, 50–52,
75, 104, 106, 113
second-order homogeneous
linear ODEs, 50–52, 75,
104
systems of ODEs, 139
standard, 314
vector spaces, 286, 311, 314
Beats (oscillation), 89

Bellman, Richard, 981n.3
Bellman equations, 981
Bellman’s principle, 980–981
Bell-shaped curve, 13, 574
BEM, see Backward Euler method
Benoulli, Niklaus, 31n.7
Bernoulli, Daniel, 31n.7
Bernoulli, Jakob, 31n.7
Bernoulli, Johann, 31n.7
Bernoulli distribution, 1040. See also
Binomial distributions
Bernoulli equation, 45
defined, 31
linear ODEs, 31–33
Bernoulli’s law of large numbers,
1051
Bessel, Friedrich Wilhelm, 187n.6
Bessel functions, 167, 187–191, 202
of the first kind, 189–190
with half-integer v, 193–194
of order 1, 189
of order v, 191
orthogonality of, 506
of the second kind:
general solution, 196–200
of order v, 198–200
table, A97–A98
of the third kind, 200
Bessel’s equation, 167, 187–196,
202
Bessel functions, 167, 187–191,
196–200
circular membrane, 587
general solution, 194–200
Bessel’s inequality:
for Fourier coefficients, 497
orthogonal series, 508–509
Beta function, formula for, A67
Bezier curve, 827
BFS algorithms, see Breadth First
search algorithms
Bijective mapping, 737n.1
Binomial coefficients:
Newton’s forward difference
formula, 816
probability theory, 1027–1028
Binomial distributions, 1039–1041,
1061
normal approximation of,
1049–1050
sampling with replacement for,
1042
table, A99
Binomial series, 696
Binomial theorem, 1029
Bipartite graphs, 1001–1006, 1008
BISECT, ALGORITHM, A46
Bisection method, 807–808
Bolzano, Bernard, A94n.3

Bolzano–Weierstrass theorem,
A94–A95
Bonnet, Ossian, 180n.3
Bonnet’s recursion, 180
Borda, J. C., 16n.4
Boundaries:
ODEs, 39
of regions, 426n.2
sets in complex plane, 620
Boundary conditions:
one-dimensional heat equation,
559
PDEs, 541, 605
periodic, 501
two-dimensional wave equation,
577
vibrating string, 545–547
Boundary points, 426n.2
Boundary value problem (BVP), 499
conformal mapping for, 763–767,
A96
first, see Dirichlet problem
mixed, see Mixed boundary value
problem
second, see Neumann problem
third, see Mixed boundary value
problem
two-dimensional heat equation,
564
Bounded domains, 652
Bounded regions, 426n.2
Bounded sequence, A93–A95
Boxplots, 1013
Boyle, Robert, 19n.5
Boyle–Mariotte’s law for idea gases,
19
Bragg, Sir William Henry, 938n.5
Bragg, Sir William Lawrence, 938n.5
Branch, of logarithm, 639
Branch cut, of logarithm, 639
Branch point (Riemann surfaces), 755
Breadth First search (BFS)
algorithms, 977
defined, 977, 998
Moore’s, 977–980
BVP, see Boundary value problem
CAD (computer-aided design), 820
Cancellation laws, 306–307
Canonical form, 344
Cantor, Georg, A72n.3
Cantor–Dedekind axiom, A72n.3,
A95n.4
Capacity:
cut sets, 994
networks, 991
Cardano, Girolamo, 608n.1
Cardioid, 391, 437

bindex.qxd

11/4/10

6:06 PM

Page I3

Index
Cartesian coordinates:
linear element in, A75
transformation law, A86–A87
vector product in, A83–A84
writing, A74
Cartesian coordinate systems:
complex plane, 611
left-handed, 369, 370, A84
right-handed, 368–369, A83–A84
in space, 315, 356
transformation law for vector
components, A85–A86
Cartesius, Renatus, 356n.1
Cauchy, Augustin-Louis, 71n.4,
625n.4, 683n.1
Cauchy determinant, 113
Cauchy–Goursat theorem, see
Cauchy’s integral theorem
Cauchy–Hadamard formula, 683
Cauchy principal value, 727, 730
Cauchy–Riemann equations, 38, 642
complex analysis, 623–629
proof of, A90–A91
Cauchy–Schwarz inequality, 363,
871–782
Cauchy’s convergence principle,
674–675, A93–A94
Cauchy’s inequality, 666
Cauchy’s integral formula, 660–663,
670
Cauchy’s integral theorem, 652–660,
669
existence of indefinite integral,
656–658
Goursat’s proof of, A91–A93
independence of path, 655
for multiply connected domains,
658–659
principle of deformation of path,
656
Cayley, Arthur, 748n.2
c-charts, 1092
Center:
as critical point, 144, 165
of a graph, 991
of power series, 680
Center control line (CL), 1088
Center of gravity, of mass in a
region, 429
Central difference notation, 819
Central limit theorem, 1076
Central vertex, 991
Centrifugal force, 388
Centripetal acceleration, 387–388
Chain rules, 392–394
Characteristics, 555
Characteristics, method of, 555
Characteristic determinant, of a
matrix, 129, 325, 326, 353, 877

I3
Characteristic equation:
matrices, 129, 325, 326, 353, 877
PDEs, 555
second-order homogeneous linear
ODEs, 54
Characteristic matrix, 326
Characteristic polynomial, 325, 353,
877
Characteristic values, 87, 324, 353.
See also Eigenvalues
Characteristic vectors, 324, 877. See
also eigenvectors
Chebyshev, Pafnuti, 504n.6
Chebyshev equation, 504
Chebyshev polynomials, 504
Checkerboard pattern (determinants),
294
Chi-square (␹2) distribution,
1074–1076, A104
Chi-square (␹2) test, 1096–1097,
1113
Choice of numeric method, for matrix
eigenvalue problems, 879
Cholesky, André-Louis, 855n.3
Cholesky’s method, 855–856, 898
Chopping, error caused by, 792
Chromatic number, 1006
Circle, 386
Circle of convergence (power series),
682
Circulation, of flow, 467, 774
CL (center control line), 1088
Clairaut equation, 35
Clamped condition (spline
interpolation), 823
Class intervals, 1012
Class marks, 1012
Closed annulus, 619
Closed circular disk, 619
Closed integration formulas, 833, 838
Closed intervals, A72n.3
Closed Newton–Cotes formulas, 833
Closed paths, 414, 645, 975–976
Closed regions, 426n.2
Closed sets, 620
Closed trails, 975–976
Closed walks, 975–976
CN (Crank–Nicolson) method,
938–941
Coefficients:
binomial:
Newton’s forward difference
formula, 816
probability theory, 1027–1028
constant:
higher-order homogeneous
linear ODEs, 111–116
second-order homogeneous
linear ODEs, 53–60

Coefficients: (Cont.)
second-order nonhomogeneous
linear ODEs, 81
systems of ODEs, 140–151
correlation, 1108–1111, 1113
Fourier, 476, 484, 538, 582–583
of kinetic friction, 19
of linear systems, 272, 845
of ODEs, 47
higher-order homogeneous
linear ODEs, 105
second-order homogeneous
linear ODEs, 53–60, 73
second-order nonhomogeneous
linear ODEs, 81–85
series of ODEs, 168, 174
variable, 167, 240–241
of power series, 680
regression, 1105, 1107–1108
variable:
Frobenius method, 180–187
Laplace transforms ODEs
with, 240–241
of ODEs, 167, 240–241
power series method, 167–175
second-order homogeneous
linear ODEs, 73
Coefficient matrices, 257, 273
Hermitian or skew-Hermitian
forms, 351
linear systems, 845
quadratic form, 343
Cofactor (determinants), 294
Collatz, Lothar, 883n.9
Collatz inclusion theorem, 883–884
Columns:
determinants, 294
matrix, 125, 257, 320
Column “sum” norm, 861
Column vectors, 126
matrices, 257, 284–285, 320
rank in terms of, 284–285
Combinations (probability theory),
1024, 1026–1027
of n things taken k at a time
without repetitions, 1026
of n things taken k at a time with
repetitions, 1026
Combinatorial optimization, 970,
975–1008
assignment problems, 1001–1006
flow problems in networks,
991–997
cut sets, 994–996
flow augmenting paths,
992–993
paths, 992
Ford–Fulkerson algorithm for
maximum flow, 998–1001

bindex.qxd

11/4/10

I4

6:06 PM

Page I4

Index

Combinatorial optimization (Cont.)
shortest path problems, 975–980
Bellman’s principle, 980–981
complexity of algorithms,
978–980
Dijkstra’s algorithm, 981–983
Moore’s BFS algorithm,
977–980
shortest spanning trees:
Greedy algorithm, 984–988
Prim’s algorithm, 988–991
Commutation (matrices), 271
Complements:
of events, 1016
of sets in complex plane, 620
Complementation rule,
1020–1021
Complete bipartite graphs, 1005
Complete graphs, 974
Complete matching, 1002
Completeness (orthogonal series),
508–509
Complete orthonormal set, 508
Complex analysis, 607
analytic functions, 623–624
Cauchy–Riemann equations,
623–629
circles and disks, 619
complex functions, 620–623
exponential, 630–633
general powers, 639–640
hyperbolic, 635
logarithm, 636–639
trigonometric, 633–635
complex integration, 643–670
Cauchy’s integral formula,
660–663, 670
Cauchy’s integral theorem,
652–660, 669
derivatives of analytic
functions, 664–668
Laurent series, 708–719
line integrals, 643–652, 669
power series, 671–707
residue integration, 719–733
complex numbers, 608–619
addition of, 609, 610
conjugate, 612
defined, 608
division of, 610
multiplication of, 609, 610
polar form of, 613–618
subtraction of, 610
complex plane, 611
conformal mapping, 736–757
geometry of analytic functions,
737–742
linear fractional
transformations,
742–750

Complex analysis (Cont.)
Riemann surfaces, 754–756
by trigonometric and
hyperbolic analytic
functions, 750–754
half-planes, 619–620
harmonic functions, 628–629
Laplace’s equation, 628–629
Laurent series, 708–719, 734
analytic or singular at infinity,
718–719
point at infinity, 718
Riemann sphere, 718
singularities, 715–717
zeros of analytic functions, 717
power series, 168, 671–707
convergence behavior of,
680–682
convergence tests, 674–676,
A93–A94
functions given by, 685–690
Maclaurin series, 690
in powers of x, 168
radius of convergence,
682–684
ratio test, 676–678
root test, 678–679
sequences, 671–673
series, 673–674
Taylor series, 690–697
uniform convergence,
698–705
residue integration, 719–733
formulas for residues, 721–722
of real integrals, 725–733
several singularities inside
contour, 723–725
Taylor series, 690–697, 707
Complex conjugate numbers, 612
Complex conjugate roots, 72–73
Complex Fourier integral, 523
Complex functions, 620–623
exponential, 630–633
general powers, 639–640
hyperbolic, 635
logarithm, 636–639
trigonometric, 633–635
Complex heat potential, 767
Complex integration, 643–670
Cauchy’s integral formula,
660–663, 670
Cauchy’s integral theorem,
652–660, 669
existence of indefinite integral,
656–658
independence of path, 655
for multiply connected
domains, 658–659
principle of deformation of
path, 656

Complex integration (Cont.)
derivatives of analytic functions,
664–668
Laurent series, 708–719
analytic or singular at infinity,
718–719
point at infinity, 718
Riemann sphere, 718
singularities, 715–717
zeros of analytic functions,
717–718
line integrals, 643–652, 669
basic properties of, 645
bounds for, 650–651
definition of, 643–645
existence of, 646
indefinite integration and
substitution of limits,
646–647
representation of a path,
647–650
power series, 671–707
convergence behavior of,
680–682
convergence tests, 674–676
functions given by, 685–690
Maclaurin series, 690
radius of convergence of,
682–684
ratio test, 676–678
root test, 678–679
sequences, 671–673
series, 673–674
Taylor series, 690–697
uniform convergence,
698–705
residue integration, 719–733
formulas for residues, 721–722
of real integrals, 725–733
several singularities inside
contour, 723–725
Complexity, of algorithms, 978–979
Complex line integrals, see Line
integrals
Complex matrices and forms,
346–352
Complex numbers, 608–619, 641
addition of, 609, 610
conjugate, 612
defined, 608
division of, 610
multiplication of, 609, 610
polar form of, 613–618
subtraction of, 610
Complex plane, 611
extended, 718, 744–745
sets in, 620
Complex potential, 786
electrostatic fields, 760–761
of fluid flow, 771, 773–774

bindex.qxd

11/4/10

6:06 PM

Page I5

Index
Complex roots:
higher-order homogeneous linear
ODEs:
multiple, 115
simple, 113–114
second-order homogeneous linear
ODEs, 57–59
Complex trigonometric polynomials,
529
Complex variables, 620–621
Complex vector space, 309, 310,
349
Components (vectors), 126, 356, 365
Composition, of linear
transformations, 316–317
Computer-aided design (CAD), 820
Condition:
of incompressibility, 405
spline interpolation, 823
Conditionally convergent series, 675
Conditional probability, 1022–1023,
1061
Condition number, 868–870, 899
Confidence intervals, 1063,
1068–1077, 1113
interval estimates, 1065
for mean of normal distribution:
with known variance,
1069–1071
with unknown variance,
1071–1073
for parameters of distributions
other than normal, 1076
in regression analysis, 1107–1108
for variance of a normal
distribution, 1073–1076
Confidence level, 1068
Conformality, 738
Conformal mapping, 736–757
boundary value problems,
763–767, A96
defined, 738
geometry of analytic functions,
737–742
linear fractional transformations,
742–750
extended complex plane,
744–745
mapping standard domains,
747–750
Riemann surfaces, 754–756
by trigonometric and hyperbolic
analytic functions,
750–754
Connected graphs, 977, 981, 984
Connected set, in complex plane,
620
Conservative physical systems, 422
Conservative vector fields, 400, 408
Consistent linear systems, 277

I5
Constant coefficients:
higher-order homogeneous linear
ODEs, 111–116
distinct real roots, 112–113
multiple real roots, 114–115
simple complex roots, 113–114
second-order homogeneous linear
ODEs, 53–60
complex roots, 57–59
real double root, 55–56
two distinct real roots, 54–55
second-order nonhomogeneous
linear ODEs, 81
systems of ODEs, 140–151
critical points, 142–146,
148–151
graphing solutions in phase
plane, 141–142
Constant of gravity, at the Earth’s
surface, 63
Constant of integration, 18
Constant revenue, lines of, 954
Constrained (linear) optimization,
951, 954–958, 969
normal form of problems, 955–957
simplex method, 958–968
degenerate feasible solution,
962–965
difficulties in starting, 965–968
Constraints, 951
Consumers, 1092
Consumer’s risk, 1094
Consumption matrix, 334
Continuity equation (compressible
fluid flow), 405
Continuous complex functions, 621
Continuous distributions, 1029,
1032–1034
marginal distribution of, 1055
two-dimensional, 1053
Continuous random variables, 1029,
1032–1034, 1061
Continuous vector functions, 378–379
Contour integral, 653
Contour lines, 21, 36
Control charts, 1088
for mean, 1088–1089
for range, 1090–1091
for standard deviation, 1090
for variance, 1089–1090
Controlled variables, in regression
analysis, 1103
Control limits, 1088, 1089
Control variables, 951
Convergence:
absolute:
defined, 674
and uniform convergence, 704
of approximate and exact
solutions, 936

Convergence: (Cont.)
circle of, 682
defined, 861
Gauss–Seidel iteration, 861–862
mean square (orthogonal series),
507–508
in the norm, 507
power series, 680–682
convergence tests, 674–676,
A93–A94
radius of convergence of,
682–684, 706
uniform convergence, 698–705
radius of, 172
defined, 172
power series, 682–684, 706
sequence of vectors, 378
speed of (numeric analysis),
804–805
superlinear, 806
uniform:
and absolute convergence, 704
power series, 698–705
Convergence interval, 171, 683
Convergence tests, 674–676
power series, 674–676, A93–A94
uniform convergence, 698–705
Convergent iteration processes,
800
Convergent sequence of functions,
507–508, 672
Convergent series, 171, 673
Convolution:
defined, 232
Fourier transforms, 527–528
Laplace transforms, 232–237
Convolution theorem, 232–233
Coriolis, Gustave Gaspard, 389n.3
Coriolis acceleration, 388–389
Corrector (improved Euler method),
903
Correlation analysis, 1063,
1108–1111, 1113
defined, 1103
test for correlation coefficient,
1110–1111
Correlation coefficient, 1108–1111,
1113
Cosecant, formula for, A65
Cosine function:
conformal mapping by, 752
formula for, A63–A65
Cosine integral:
formula for, A69
table, A98
Cosine series, 781
Cotangent, formula for, A65
Coulomb, Charles Augustin de,
19n.6, 93n.7, 401n.6
Coulomb’s law, 19, 401

bindex.qxd

11/4/10

I6

6:06 PM

Page I6

Index

Covariance:
in correlation analysis, 1109
defined, 1058
Cramer, Gabriel, 31n.7, 298n.2
Cramer’s rule, 292, 298–300, 321
for three equations, 293
for two equations, 292
Cramer’s Theorem, 298
Crank, John, 938n.5
Crank–Nicolson (CN) method,
938–941
Critical damping, 65, 66
Critical points, 33, 165
asymptotically stable, 149
and conformal mapping, 738, 757
constant-coefficient systems of
ODEs, 142–146
center, 144
criteria for, 148–151
degenerate node, 145–146
improper node, 142
proper node, 143
saddle point, 143
spiral point, 144–145
stability of, 149–151
isolated, 152
nonlinear systems, 152
stable, 140, 149
stable and attractive, 140, 149
unstable, 140, 149
Critical region, 1079
Cross product, 368, 410. See also
Vector product
Crout, Prescott Durand, 853n.2
Crout’s method, 853, 898
Cubic spline, 821
Cumulative absolute frequencies (of
values), 1012
Cumulative distribution functions,
1029
Cumulative relative frequencies (of
values), 1012
Curl, A76
invariance of, A85–A88
of vector fields, 406–409, 412
Curvature, of a curve, 389–390
Curves:
arc of, 383
bell-shaped, 13, 574
Bezier, 827
deflection, 120
elastic, 120
equipotential, 36, 759, 761
one-parameter family of, 36–37
operating characteristic, 1081,
1092, 1095
oriented, 644
orthogonal coordinate, A74
parameter, 442
plane, 383

Curves: (Cont.)
regression, 1103
simple, 383
simple closed, 646
smooth, 414, 644
solution, 4–6
twisted, 383
vector differential calculus,
381–392, 411
arc length of, 385–386
length of, 385
in mechanics, 386–389
tangents to, 384–385
and torsion, 389–390
Curve fitting, 872–876
method of least squares, 872–874
by polynomials of degree m,
874–875
Curvilinear coordinates, 354, 412, A74
Cut sets, 994–996, 1008
Cycle (paths), 976, 984
Cylindrical coordinates, 593–594,
A74–A76
D’Alembert, Jean le Rond, 554n.1
D’Alembert’s solution, 553–556
Damped oscillations, 67
Damping constant, 65
Dantzig, George Bernard, 959
Data processing:
frequency distributions,
1011–1012
and randomness, 1064
Data representation:
frequency distributions,
1011–1015
Empirical Rule, 1014
graphic, 1012
mean, 1013–1014
standard deviation, 1014
variation, 1014
and randomness, 1064
Decisions:
false, risks of making, 1080
statistics for, 1077–1078
Dedekind, Richard, A72n.3
Defect (eigenvalue), 328
Defectives, 1092
Definite integrals, complex, see Line
integrals
Deflection curve, 120
Deformation of path, principle of,
656
Degenerate feasible solution (simplex
method), 962–965
Degenerate node, 145–146
Degrees of freedom (d.f.), number of,
1071, 1074
Degree of incidence, 971

Degree of precision (DP), 833
Deleted neighborhood, 720
Demand vector, 334
De Moivre, Abraham, 616n.3
De Moivre–Laplace limit theorem,
1050
De Moivre’s formula, 616
De Morgan’s laws, 1018
Density, 1061
continuous two-dimensional
distributions, 1053
of a distribution, 1033
Dependent random variables, 1055,
1056
Dependent variables, 393, 1055, 1056
Depth First Search (DFS) algorithms,
977
Derivatives:
of analytic functions, 664–668,
688–689, A95–A96
of complex functions, 622, 641
Laplace transforms of, 211–212
of matrices or vectors, 127
of vector functions, 379–380
Derived series, 687
Descartes, René, 356n.1, 391n.4
Determinants, 293–301, 321
Cauchy, 113
Cramer’s rule, 298–300
defined, A81
general properties of, 295–298
of a matrix, 128
of matrix products, 307–308
of order n, 293
proof of, A81–A83
second-order, 291–292
second-order homogeneous linear
ODEs, 76
third-order, 292–293
Vandermonde, 113
Wronski:
second-order homogeneous
linear ODEs, 75–78
systems of ODEs, 139
Developed, in a power series, 683
D.f. (degrees of freedom), number of,
1071, 1074
DFS (Depth First Search) algorithms,
977
DFTs (discrete Fourier transforms),
528–531
Diagonalization of matrices, 341–342
Diagonally dominant matrices, 881
Diagonal matrices, 268
inverse of, 305–306
scalar, 268
Diameter (graphs), 991
Difference:
complex numbers, 610
scalar multiplication, 260

bindex.qxd

11/4/10

6:06 PM

Page I7

Index
Difference equations (elliptic PDEs),
923–925
Difference quotients, 923
Difference table, 814
Differentiable complex functions,
622–623
Differentiable vector functions, 379
Differential (total differential),
20, 45
Differential equations:
applications of, 3
defined, 2
Differential form, 422
exact, 21, 470
first fundamental form, of S, 451
floating-point, of numbers,
791–792
path independence and exactness
of, 422, 470
Differential geometry, 381
Differential operators:
second-order, 60
for second-order homogeneous
linear ODEs, 60–62
Differentiation:
of Laplace transforms, 238–240
matrices or vectors, 127
numeric, 838–839
of power series, 687–688, 703
termwise, 173, 687–688, 703
Diffusion equation, 459–460, 558.
See also Heat equation
Digraphs (directed graphs), 971–972,
1007
computer representation of,
972–974
defined, 972
incidence matrix of, 975
subgraphs, 972
Dijkstra, Edsger Wybe, 981n.4
Dijkstra’s algorithm, 981–983,
1008
DIJKSTRA, ALGORITHM, 982
Dimension of vector spaces, 286,
311, 359
Diocles, 391n.4
Dirac, Paul, 226n.2
Dirac delta function, 226–228, 237
Directed graphs, see Digraphs
(directed graphs)
Directed path, 1000
Directional derivatives (scalar
functions), 396–397, 411
Direction field (slope field), 9–10, 44
Direct methods (linear system
solutions), 858, 898. See also
iteration
Dirichlet, Peter Gustav LeJeune,
462n.8
Dirichlet boundary condition, 564

I7
Dirichlet problem, 605, 923
ADI method, 929
heat equation, 564–566
Laplace equation, 593–596,
925–928, 934–935
Poisson equation, 925–928
two-dimensional heat equation,
564–565
uniqueness theorem for, 462, 784
Dirichlet’s discontinuous factor, 514
Discharge (flow modeling), 776
Discrete distributions, 1029–1032
marginal distributions of,
1053–1054
two-dimensional, 1052–1053
Discrete Fourier transforms (DFTs),
528–531
Discrete random variables, 1029,
1030–1032, 1061
defined, 1030
marginal distributions of, 1054
Discrete spectrum, 525
Disjoint events, 1016
Disks:
circular, open and closed, 619
mapping, 748–750
Poisson’s integral formula, 779–780
Dissipative physical systems, 422
Distance:
graphs, 991
vector norms, 866
Distinct real roots:
higher-order homogeneous linear
ODEs, 112–113
second-order homogeneous linear
ODEs, 54–55
Distinct roots (Frobenius method),
182
Distributions, 226n.2. See also
Frequency distributions;
Probability distributions
Distribution-free tests, 1100
Distribution function, 1029–1032
cumulative, 1029
normal distributions, 1046–1047
of random variables, 1056, A109
sample, 1096
two-dimensional probability
distributions, 1051–1052
Distributive laws, 264
Distributivity, 363
Divergence, A75
fluid flow, 775
of vector fields, 402–406
of vector functions, 411, 453
Divergence theorem of Gauss, 405,
470
applications, 458–463
vector integral calculus, 453–457
Divergent sequence, 672

Divergent series, 171, 673
Division, of complex numbers, 610,
615–616
Domain(s), 393
bounded, 652
doubly connected, 658, 659
of f, 620
holes of, 653
mapping, 737, 747–750
multiply connected:
Cauchy’s integral formula,
662–663
Cauchy’s integral theorem,
658–659
p-fold connected, 652–653
sets in complex plane, 620
simply connected, 423, 646, 652,
653
triply connected, 653, 658, 659
Dominant eigenvalue, 883
Doolittle, Myrick H., 853n.1
Doolittle’s method, 853–855, 898
Dot product, 312, 410. See also Inner
product
Double Fourier series:
defined, 582
rectangular membrane, 577–585
Double integrals (vector integral
calculus), 426–432, 470
applications of, 428–429
change of variables in, 429–431
evaluation of, by two successive
integrations, 427–428
Double precision, floating-point
standard for, 792
Double root (Frobenius method), 183
Double subscript notation, 125
Doubly connected domains, 658, 659
DP (degree of precision), 833
Driving force, see Input (driving
force)
Duffing equation, 160
Duhamel, Jean-Marie Constant,
603n.4
Duhamel’s formula, 603
Eccentricity, of vertices, 991
Edges:
backward:
cut sets, 994
initial flow, 998
of a path, 992
forward:
cut sets, 994
initial flow, 998
of a path, 992
graphs, 971, 1007
incident, 971
Edge chromatic number, 1006

bindex.qxd

11/4/10

I8

6:06 PM

Page I8

Index

Edge condition, 991
Edge incidence list (graphs), 973
Efficient algorithms, 979
Eigenbases, 339–341
Eigenfunctions, 605
circular membrane, 588
one-dimensional heat equation,
560
Sturm–Liouville Problems,
499–500
two-dimensional heat equation,
565
two-dimensional wave equation,
578, 580
vibrating string, 547
Eigenfunction expansion, 504
Eigenspaces, 326, 878
Eigenvalues, 129–130, 166, 353, 605,
877, 899. See also Matrix
eigenvalue problems
circular membrane, 588
complex matrices, 347–351
and critical points, 149
defined, 324
determining, 323–329
dominant, 883
finding, 324–328
one-dimensional heat equation,
560
Sturm–Liouville Problems,
499–500, A89
two-dimensional wave equation,
580
vibrating string, 547
Eigenvalues of A, 322
Eigenvalue problem, 140
Eigenvectors, 129–130, 166, 353,
877, 899
basis of, 339–340
convergent sequence of, 886
defined, 324
determining, 323–329
finding, 324–328
Eigenvectors of A, 322
EISPACK, 789
Elastic curve, 120
Electric circuits:
analogy of electrical and
mechanical quantities,
97–98
second-order nonhomogeneous
linear ODEs, 93–99
Electrostatic fields (potential theory),
759–763
complex potential, 760–761
superposition, 761–762
Electrostatic potential, 759
Electrostatics (Laplace’s equation),
593
Elementary matrix, 281

Elementary row operations (linear
systems), 277
Ellipses, area of region bounded by,
436
Elliptic PDEs:
defined, 923
numeric analysis, 922–936
ADI method, 928–930
difference equations, 923–925
Dirichlet problem, 925–928
irregular boundary, 933–935
mixed boundary value
problems, 931–933
Neumann problem, 931
Empirical Rule, 1014
Energies, 157
Entire function, 630, 642, 707, 718
Entries:
determinants, 294
matrix, 125, 257
Equal complex numbers, 609
Equality:
of matrices, 126, 259
of vectors, 355
Equally likely events, 1018
Equal spacing (interpolation):
Newton’s backward difference
formula, 818–819
Newton’s forward difference
formula, 815–818
Equilibrium harvest, 36
Equilibrium solutions (equilibrium
points), 33–34
Equipotential curves, 36, 759, 761
Equipotential lines, 38
electrostatic fields, 759, 761
fluid flow, 771
Equipotential surfaces, 759
Equivalent vector norms, 871
Error(s):
in acceptance sampling,
1093–1094
of approximations, 495
in numeric analysis, 842
basic error principle, 796
error propagation, 795
errors of numeric results,
794–795
roundoff, 792
in statistical tests, 1080–1081
and step size control, 906–907
trapezoidal rule, 830
vector norms, 866
Error bounds, 795
Error estimate, 908
Error function, 828, A67–A68, A98
Essential singularity, 715–716
Estimation of parameters, 1063
EULER, ALGORITHM, 903
Euler, Leonhard, 31n.7, 71n.4

Euler–Cauchy equations, 71–74,
104
higher-order nonhomogeneous
linear ODEs, 119–120
Laplace’s equation, 595
third-order, IVP for, 108
Euler–Cauchy method, 901
Euler constant, 198
Euler formulas, 58
complex Fourier integral, 523
derivation of, 479–480
exponential function, 631
Fourier coefficients given by, 476,
484
generalized, 582
Taylor series, 694
trigonometric function, 634
Euler graph, 980
Euler’s method:
defined, 10
error of, 901–902, 906, 908
first-order ODEs, 10–11, 901–902
backward method, 909–910
improved method, 902–904
higher order ODEs, 916–917
Euler trail, 980
Even functions, 486–488
Even periodic extension, 488–490
Events (probability theory),
1016–1017, 1060
addition rule for, 1021–1022
arbitrary, 1021–1022
complements of, 1016
defined, 1015
disjoint, 1016
equally likely, 1018
independent, 1022–1023
intersection, 1016, 1017
mutually exclusive, 1016, 1021
simple, 1015
union, 1016–1017
Exact differential equation, 21
Exact differential form, 422, 470
Exact ODEs, 20–27, 45
defined, 21
integrating factors, 23–26
Existence, problem of, 39
Existence theorems:
cubic splines, 822
first-order ODEs, 39–42
homogeneous linear ODEs:
higher-order, 108
second-order, 74
of the inverse, 301–302
Laplace transforms, 209–210
linear systems, 138
power series solutions, 172
systems of ODEs, 137
Expectation, 1035, 1037–1038,
1057

bindex.qxd

11/4/10

6:06 PM

Page I9

Index
Experiments:
defined, 1015, 1060
in probability theory, 1015–1016
random, 1011, 1015–1016, 1060
Experimental error, 794
Explicit formulas, 913
Explicit method:
heat equation, 937, 940–941
wave equation, 943
Explicit solution, 21
Exponential decay, 5, 7
Exponential function, 630–633, 642
formula for, A63
Taylor series, 694
Exponential growth, 5
Exponential integral, formula for, A69
Exposed vertices, 1001, 1003
Extended complex plane:
conformal mapping, 744–745
defined, 718
Extended method (separable ODEs),
17–18
Extended problems, 966
Extrapolation, 808
Extrema (unconstrained
optimization), 951
Factorial function, 1027, A66, A98.
See also Gamma functions
Failing to reject a hypothesis, 1081
Fair die, 1018, 1019
False decisions, risks of making,
1080
False position, method of, 807–808
Family of curves, one-parameter,
36–37
Family of solutions, 5
Faraday, Michael, 93n.7
Fast Fourier transforms (FFTs),
531–532
F-distribution, 1086, A105–A108
Feasibility region, 954
Feasible solutions, 954–955
basic, 957, 959
degenerate, 962–965
normal form of linear optimization
problems, 957
Fehlberg, E., 907
Fehlberg’s fifth-order RK method,
907–908
Fehlberg’s fourth-order RK method,
907–908
FFTs (fast Fourier transforms),
531–532
Fibonacci (Leonardo of Pisa), 690n.2
Fibonacci numbers, 690
Fibonacci’s rabbit problem, 690
Finite complex plane, 718. See also
Complex plane

I9
Finite jumps, 209
First boundary value problem, see
Dirichlet problem
First fundamental form, of S, 451
First-order method, Euler method as,
902
First-order ODEs, 2–45, 44
defined, 4
direction fields, 9–10
Euler’s method, 10–11
exact, 20–27, 45
defined, 21
integrating factors, 23–26
explicit form, 4
geometric meanings of, 9–12
implicit form, 4
initial value problem, 38–43
linear, 27–36
Bernoulli equation, 31–33
homogeneous, 28
nonhomogeneous, 28–29
population dynamics, 33–34
modeling, 2–8
numeric analysis, 901–915
Adams–Bashforth methods,
911–914
Adams–Moulton methods,
913–914
backward Euler method,
909–910
Euler’s method, 901–902
improved Euler’s method,
902–904
multistep methods, 911–915
Runge–Kutta–Fehlberg
method, 906–908
Runge–Kutta methods,
904–906
orthogonal trajectories, 36–38
separable, 12–20, 44
extended method, 17–18
modeling, 13–17
systems of, 165
transformation of systems to,
157–159
First (first order) partial derivatives,
A71
First shifting theorem (s-shifting),
208–209
First transmission line equation, 599
Fisher, Sir Ronald Aylmer, 1086
Fixed points:
defined, 799
of a mapping, 745
Fixed-point iteration (numeric
analysis), 798–801, 842
Fixed-point systems, numbers in, 791
Floating, 793
Floating-point form of numbers,
791–792

Flow augmenting paths, 992–993,
998, 1008
Flow problems in networks
(combinatorial optimization),
991–997
cut sets, 994–996
flow augmenting paths, 992–993
paths, 992
Fluid flow:
Laplace’s equation, 593
potential theory, 771–777
Fluid state, 404
Flux (motion of a fluid), 404
Flux integral, 444, 450
Forced motions, 68, 86
Forced oscillations:
Fourier analysis, 492–495
second-order nonhomogeneous
linear ODEs, 85–92
damped, 89–90
resonance, 88–91
undamped, 87–89
Forcing function, 86
Ford, Lester Randolph, Jr., 998n.7
FORD–FULKERSON,
ALGORITHM, 998
Ford–Fulkerson algorithm for
maximum flow, 998–1001,
1008
Forest (graph), 987
Form(s):
canonical, 344
complex, 351
differential, 422
exact, 21, 470
path independence and
exactness of, 422
Hesse’s normal, 366
Lagrange’s, 812
normal (linear optimization
problems), 955–957, 959,
969
Pfaffian, 422
polar, of complex numbers,
613–618, 631
quadratic, 343–344, 346
reduced echelon, 279
row echelon, 279–280
skew-Hermitian and Hermitian,
351
standard:
first-order ODEs, 27
higher-order homogeneous
linear ODEs, 105
higher-order linear ODEs, 123
power series method, 172
second-order linear ODEs, 46,
103
triangular (Gauss elimination),
846

bindex.qxd

11/4/10

I10

6:06 PM

Page I10

Index

Forward edge:
cut sets, 994
initial flow, 998
of a path, 992
Four-color theorem, 1006
Fourier, Jean-Baptiste Joseph, 473n.1
Fourier analysis, 473–539
approximation by trigonometric
polynomials, 495–498
forced oscillations, 492–495
Fourier integral, 510–517
applications, 513–515
complex form of, 522–523
sine and cosine, 515–516
Fourier series, 474–483
convergence and sum of,
480–481
derivation of Euler formulas,
479–480
even and odd functions,
486–488
half-range expansions, 488–490
from period 2p to 2L,
483–486
Fourier transforms, 522–536
complex form of Fourier
integral, 522–523
convolution, 527–528
cosine, 518–522, 534
discrete, 528–531
fast, 531–532
and its inverse, 523–524
linearity, 526–527
sine, 518–522, 535
spectrum representation, 525
orthogonal series (generalized
Fourier series), 504–510
completeness, 508–509
mean square convergence,
507–508
Sturm–Liouville Problems,
498–504
eigenvalues, eigenfunctions,
499–500
orthogonal functions, 500–503
Fourier–Bessel series, 506–507, 589
Fourier coefficients, 476, 484, 538,
582–583
Fourier constants, 504–505
Fourier cosine integral, 515–516
Fourier cosine series, 484, 486, 538
Fourier cosine transforms, 518–522,
534
Fourier cosine transform method, 518
Fourier integrals, 510–517, 539
applications, 513–515
complex form of, 522–523
heat equation, 568–571
residue integration, 729–730
sine and cosine, 515–516

Fourier–Legendre series, 505–506,
596–598
Fourier matrix, 530
Fourier series, 473–483, 538
convergence and sum or, 480–481
derivation of Euler formulas,
479–480
double, 577–585
even and odd functions, 486–488
half-range expansions, 488–490
heat equation, 558–563
from period 2p to 2L, 483–486
Fourier sine integral, 515–516
Fourier sine series, 477, 486, 538
one-dimensional heat equation,
561
vibrating string, 548
Fourier sine transforms, 518–522,
535
Fourier transforms, 522–536, 539
complex form of Fourier integral,
522–523
convolution, 527–528
cosine, 518–522, 534, 539
defined, 522, 523
discrete, 528–531
fast, 531–532
heat equation, 571–574
and its inverse, 523–524
linearity of, 526–527
sine, 518–522, 535, 539
spectrum representation, 525
Fourier transform method, 524
Four-point formulas, 841
Fraction defective chars, 1091–1092
Francis, J. G. F., 892
Fredholm, Erik Ivar, 198n.7, 263n.3
Free condition (spline interpolation),
823
Free oscillations of mass–spring
system (second-order ODEs),
62–70
critical damping, 65, 66
damped system, 64–65
overdamping, 65–66
undamped system, 63–64
underdamping, 65, 67
Frenet, Jean-Frédéric, 392
Frenet formulas, 392
Frequency (in statistics):
absolute, 1012, 1019
cumulative absolute, 1012
cumulative relative, 1012
relative class, 1012
Frequency (of vibrating string), 547
Frequency distributions, mean and
variance of:
expectation, 1037–1038
moments, 1038
transformation of, 1036–1037

Fresnel, Augustin, 697n.4, A68n.1
Fresnel integrals, 697, A68
Frobenius, Georg, 180n.4
Frobenius method, 167, 180–187,
201
indicial equation, 181–183
proof of, A77–A81
typical applications, 183–185
Frobenius norm, 861
Fulkerson, Delbert Ray, 998n.7
Function, of complex variable,
620–621
Function spaces, 313
Fundamental matrix, 139
Fundamental period, 475
Fundamental region (exponential
function), 632
Fundamental system, 50, 104. See
also Basis, of solutions
Fundamental Theorem:
higher-order homogeneous linear
ODEs, 106
for linear systems, 288
PDEs, 541–542
second-order homogeneous linear
ODEs, 48
Galilei, Galileo, 16n.4
Gamma functions, 190–191, 208
formula for, A66–A67
incomplete, A67
table, A98
GAMS (Guide to Available
Mathematical Software), 789
GAUSS, ALGORITHM, 849
Gauss, Carl Friedrich, 186n.5,
608n.1, 1103
Gauss distribution, 1045. See also
Normal distributions
Gauss “Double Ring,” 451
Gauss elimination, 320, 849
linear systems, 274–280,
844–852, 898
back substitution, 274–276,
846
elementary row operations,
277
if infinitely many solutions
exist, 278
if no solution exists, 278–279
operation count, 850–851
row echelon form, 279–280
operation count, 850–851
Gauss integration formulas, 807,
836–838, 843
Gauss–Jordan elimination, 302–304,
856–857
GAUSS–SEIDEL, ALGORITHM,
860

bindex.qxd

11/4/10

6:06 PM

Page I11

Index
Gauss–Seidel iteration, 858–863,
898
Gauss’s hypergeometric ODE, 186,
202
Geiger, H., 1044, 1100
Generalized Euler formula, 582
Generalized Fourier series, see
Orthogonal series
Generalized solution (vibrating
string), 550
Generalized triangle inequality, 615
General powers, 639–640, 642
General solution:
Bessel’s equation, 194–200
first-order ODEs, 6, 44
higher-order linear ODEs, 106,
110–111, 123
nonhomogeneous linear systems,
160
second-order linear ODEs:
homogeneous, 49–51, 77–78,
104
nonhomogeneous, 80–81
systems of ODEs, 131–132, 139
Generating functions, 179, 241
Geometric interpretation:
partial derivatives, A70
scalar triple product, 373, 374
Geometric multiplicity, 326, 878
Geometric series, 168, 675
Taylor series, 694
uniformly convergent, 698
Gerschgorin, Semyon Aranovich,
879n.6
Gerschgorin’s theorem, 879–881, 899
Gibbs phenomenon, 515
Global error, 902
Golden Rule, 15, 24
Gompertz model, 19
Goodness of fit, 1096–1100
Gosset, William Sealy, 1086n.4
Goursat, Édouard, 654n.1
Goursat’s proof, 654
Gradient, A75
fluid flow, 771
of a scalar field, 395–402
directional derivatives,
396–397
maximum increase, 398
as surface normal vector,
398–399
vector fields that are, 400–401
of a scalar function, 396, 411
unconstrained optimization, 952
Gradient method, 952. See also
Method of steepest descent
Graphs, 970–971, 1007
bipartite, 1001–1006, 1008
center of, 991
complete, 974

I11
Graphs (Cont.)
complete bipartite, 1005
computer representation of,
972–974
connected, 977, 981, 984
diameter of, 991
digraphs (directed graphs),
971–974, 1007
computer representation of,
972–974
defined, 972
incidence matrix of, 975
subgraphs, 972
Euler, 980
forest, 987
incidence matrix of, 975
planar, 1005
radius of, 991
sparse, 974
subgraphs, 972
trees, 984
vertices, 971, 977, 1007
adjacent, 971, 977
central, 991
coloring, 1005–1006
double labeling of, 986
eccentricity of, 991
exposed, 1001, 1003
four-color theorem, 1006
scanning, 998
weighted, 976
Graphic data representation, 1012
Gravitation (Laplace’s equation),
593
Gravity, acceleration of, 8
Gravity constant, at the Earth’s
surface, 63
Greedy algorithm, 984–988
Green, George, 433n.4
Green’s first formula, 461, 470
Green’s second formula, 461, 470
Green’s theorem:
first and second forms of,
461
in the plane, 433–438, 470
Gregory, James, 816n.2
Gregory–Newton’s (Newton’s)
backward difference
interpolation formula,
818–819
Gregory–Newton’s (Newton’s)
forward difference
interpolation formula,
815–818
Growth restriction, 209
Guidepoints, 827
Guide to Available Mathematical
Software (GAMS), 789
Guldin, Habakuk, 452n.7
Guldin’s theorem, 452n.7

Hadamard, Jacques, 683n.1
Half-planes:
complex analysis, 619–620
mapping, 747–749
Half-range expansions (Fourier
series), 488–490, 538
Hamilton, William Rowan, 976n.1
Hamiltonian cycle, 976
Hankel, Hermann, 200n.8
Hankel functions, 200
Harmonic conjugate function
(Laplace’s equation), 629
Harmonic functions, 460, 462, 758
complex analysis, 628–629
under conformal mapping, 763
defined, 758
Laplace’s equation, 593, 628–629
maximum modulus theorem,
783–784
potential theory, 781–784, 786
Harmonic oscillation, 63–64
Heat equation, 459–460, 557–558
Dirichlet problem, 564–566
Laplace’s equation, 564
numeric analysis, 936–941, 948
Crank–Nicolson method,
938–941
explicit method, 937, 940–941
one-dimensional, 559
solution:
by Fourier integrals, 568–571
by Fourier series, 558–563
by Fourier transforms,
571–574
steady two-dimensional heat
problems, 546–566
two-dimensional, 564–566
unifying power of methods, 566
Heat flow:
Laplace’s equation, 593
potential theory, 767–770
Heat flow lines, 767
Heaviside, Oliver, 204n.1
Heaviside calculus, 204n.1
Heaviside expansions, 228
Heaviside function, 217–219
Helix, 386
Henry, Joseph, 93n.7
Hermite, Charles, 510n.8
Hermite interpolation, 826
Hermitian form, 351
Hermitian matrices, 347, 348, 350, 353
Hertz, Heinrich, 63n.3
Hesse, Ludwig Otto, 366n.2
Hesse’s normal form, 366
Heun, Karl, 905n.1
Heun’s method, 903. See also
Improved Euler’s method
Higher functions, 167. See also
Special functions

bindex.qxd

11/4/10

I12

6:06 PM

Page I12

Index

Higher-order linear ODEs, 105–123
homogeneous, 105–116, 123
nonhomogeneous, 116–123
systems of, see Systems
of ODEs
Higher order ODEs (numeric
analysis), 915–922
Euler method, 916–917
Runge–Kutta methods, 917–919
Runge–Kutta–Nyström methods,
919–921
Higher transcendental functions, 920
High-frequency line equations, 600
Hilbert, David, 198n.7, 312n.4
Hilbert spaces, 363
Histograms, 1012
Holes, of domains, 653
Homogeneous first-order linear
ODEs, 28
Homogeneous higher-order linear
ODEs, 105–111
Homogeneous linear systems, 138,
165, 272, 290–291, 845
constant-coefficient systems,
140–151
matrices and vectors, 124–130, 321
trivial solution, 290
Homogeneous PDEs, 541
Homogeneous second-order linear
ODEs, 46–48
basis, 50–52
with constant coefficients, 53–60
complex roots, 57–59
real double root, 55–56
two distinct real-roots, 54–55
differential operators, 60–62
Euler–Cauchy equations, 71–74
existence and uniqueness of
solutions, 74–79
general solution, 49–51, 77–78
initial value problem, 49–50
modeling free oscillations of
mass–spring system, 62–70
particular solution, 49–51
reduction of order, 51–52
Wronskian, 75–78
Hooke, Robert, 62
Hooke’s law, 62
Householder, Alston Scott, 888n.11
Householder’s tridiagonalization
method, 888–892
Hyperbolic analytic functions
(conformal mapping), 750–754
Hyperbolic cosine, 635, 752
Hyperbolic functions, 635, 642
formula for, A65–A66
inverse, 640
Taylor series, 695
Hyperbolic PDEs:
defined, 923
numeric analysis, 942–945

Hyperbolic sine, 635, 752
Hypergeometric distributions,
1042–1044, 1061
Hypergeometric equations, 167,
185–187
Hypergeometric functions, 167, 186
Hypergeometric series, 186
Hypothesis, 1077
Hypothesis testing (in statistics),
1063, 1077–1087
comparison of means, 1084–1085
comparison of variances, 1086
errors in tests, 1080–1081
for mean of normal distribution
with known variance,
1081–1083
for mean of normal distribution
with unknown variance,
1083–1084
one- and two-sided alternatives,
1079–1080
Idempotent matrices, 270
Identity mapping, 745
Identity matrices, 268
Identity operator (second-order
homogeneous linear ODEs), 60
Ill-conditioned equations, 805
Ill-conditioned problems, 864
Ill-conditioned systems, 864, 865,
899
Ill-conditioning (linear systems),
864–872
condition number of a matrix,
868–870
matrix norms, 866–868
vector norms, 866
Image:
conformal mapping, 737
linear transformations, 313
Imaginary axis (complex plane), 611
Imaginary part (complex numbers),
609
Imaginary unit, 609
Impedance (RLC circuits), 95
Implicit formulas, 913
Implicit method:
backward Euler scheme as, 909
for hyperbolic PDEs, 943
Implicit solution, 21
Improper integrals:
defined, 205
residue integration, 726–732
Improper node, 142
Improved Euler’s method:
error of, 904, 906, 908
first-order ODEs, 902–904
Impulse, of a force, 225
short impulses, 225–226
unit impulse function, 226

Incidence matrices (graphs and
digraphs), 975
Incident edges, 971
Inclusion theorems:
defined, 882
matrix eigenvalue problems,
879–884
Incomplete gamma functions,
formula for, A67
Inconsistent linear systems, 277
Indefinite (quadratic form), 346
Indefinite integrals:
defined, 643
existence of, 656–658
Indefinite integration (complex line
integral), 646–647
Independence:
of path, 669
of path in domain (integrals), 470,
655
of random variables, 1055–1056
Independent events, 1022–1023,
1061
Independent sample values, 1064
Independent variables:
in calculus, 393
in regression analysis, 1103
Indicial equation, 181–183, 188, 202
Indirect methods (solving linear
systems), 858, 898
Inference, statistical, 1059, 1063
Infinite dimensional vector space,
311
Infinite populations, 1044
Infinite sequences:
bounded, A93–A95
monotone real, A72–A73
power series, 671–673
Infinite series, 673–674
Infinity:
analytic of singular at, 718–719
point at, 718
Initial conditions:
first-order ODEs, 6, 7, 44
heat equation, 559, 568, 569
higher-order linear ODEs:
homogeneous, 107
nonhomogeneous, 117
one-dimensional heat equation,
559
PDEs, 541, 605
second-order homogeneous linear
ODEs, 49–50, 104
systems of ODEs, 137
two-dimensional wave equation,
577
vibrating string, 545
Initial point (vectors), 355
Initial value problem (IVP):
defined, 6
first-order ODEs, 6, 39, 44, 901

bindex.qxd

11/4/10

6:06 PM

Page I13

Index
Initial value problem (IVP): (Cont.)
bell-shaped curve, 13
existence and uniqueness of
solutions for, 38–43
higher-order linear ODEs, 123
homogeneous, 107–108
nonhomogeneous, 117
Laplace transforms, 213–216
for RLC circuit, 99
second-order homogeneous linear
ODEs, 49, 74–75, 104
systems of ODEs, 137
Injective mapping, 737n.1
Inner product (dot product), 312
for complex vectors, 349
invariance of, 336
vector differential calculus,
361–367, 410
applications, 364–366
orthogonality, 361–363
Inner product spaces, 311–313
Input (driving force), 27, 86, 214
Instability, numeric vs. mathematical,
796
Integrals, see Line integrals
Integral equations:
defined, 236
Laplace transforms, 236–237
Integral of a function, Laplace
transforms of, 212–213
Integral transforms, 205, 518
Integrand, 414, 644
Integrating factors, 23–26, 45
defined, 24
finding, 24–26
Integration. See also Complex
integration
constant of, 18
of Laplace transforms, 238–240
numeric, 827–838
adaptive, 835–836
Gauss integration formulas,
836–838
rectangular rule, 828
Simpson’s rule, 831–835
trapezoidal rule, 828–831
termwise, of power series, 687,
688
Intermediate value theorem, 807–808
Intermediate variables, 393
Intermittent harvesting, 36
INTERPOL, ALGORITHM, 814
Interpolation, 529
defined, 808
numeric analysis, 808–820, 842
equal spacing, 815–819
Lagrange, 809–812
Newton’s backward difference
formula, 818–819
Newton’s divided difference,
812–815

I13
Interpolation (Cont.)
Newton’s forward difference
formula, 815–818
spline, 820–827
Interpolation polynomial, 808, 842
Interquartile range, 1013
Intersection, of events, 1016, 1017
Intervals. See also Confidence
intervals
class, 1012
closed, A72n.3
convergence, 171, 683
open, 4, A72n.3
Interval estimates, 1065
Invariance, of curl, A85–A88
Invariant rank, 283
Invariant subspace, 878
Inverse cosine, 640
Inverse cotangent, 640
Inverse Fourier cosine transform, 518
Inverse Fourier sine transform, 519
Inverse Fourier sine transform
method, 519
Inverse Fourier transform, 524
Inverse hyperbolic function, 640
Inverse hyperbolic sine, 640
Inverse mapping, 741, 745
Inverse of a matrix, 128, 301–309,
321
cancellation laws, 306–307
determinants of matrix
products, 307–308
formulas for, 304–306
Gauss–Jordan method,
302–304, 856–857
Inverse sine, 640
Inverse tangent, 640
Inverse transform, 205, 253
Inverse transformation, 315
Inverse trigonometric function, 640
Irreducible, 883
Irregular boundary (elliptic PDEs),
933–935
Irrotational flow, 774
Isocline, 10
Isolated critical point, 152
Isolated essential singularity, 715
Isolated singularity, 715
Isotherms, 36, 38, 402, 767
Iteration (iterative) methods:
numeric analysis, 798–808
fixed-point iteration, 798–801
Newton’s (Newton–Raphson)
method, 801–805
secant method, 805–806
speed of convergence, 804–805
numeric linear algebra, 858–864,
898
Gauss–Seidel iteration, 858–862
Jacobi iteration, 862–863
IVP, see Initial value problem

Jacobi, Carl Gustav Jacob, 430n.3
Jacobians, 430, 741
Jacobi iteration, 862–863
Jordan, Wilhelm, 302n.3
Joukowski airfoil, 739–740
Kantorovich, Leonid Vitaliyevich,
959n.1
KCL (Kirchhoff’s Current Law),
93n.7, 274
Kernel, 205
Kinetic friction, coefficient of, 19
Kirchhoff, Gustav Robert, 93n.7
Kirchhoff’s Current Law (KCL),
93n.7, 274
Kirchhoff’s law, 991
Kirchhoff’s Voltage Law (KVL), 29,
93, 274
Koopmans, Tjalling Charles, 959n.1
Kreyszig, Erwin, 855n.3
Kronecker, Leopold, 500n.5
Kronecker delta, A85
Kronecker symbol, 500
Kruskal, Joseph Bernard, 985n.5
KRUSKAL, ALGORITHM, 985
Kruskal’s Greedy algorithm,
984–988, 1008
kth backward difference, 818
kth central moment, 1038
kth divided difference, 813
kth forward difference, 815–816
kth moment, 1038, 1065
Kublanovskaya, V. N., 892
Kutta, Wilhelm, 905n.1
Kutta’s third-order method, 911
KVL, see Kirchhoff’s Voltage Law
Lagrange, Joseph Louis, 51n.1
Lagrange interpolation, 809–812
Lagrange’s form, 812, 842
Laguerre, Edmond, 504n.7
Laguerre polynomials, 241, 504
Laguerre’s equation, 240–241
LAPACK, 789
Laplace, Pierre Simon Marquis de,
204n.1
Laplace equation, 400, 564, 593–600,
642, 923
boundary value problem in
spherical coordinates,
594–596
complex analysis, 628–629
in cylindrical coordinates,
593–594
Fourier–Legendre series, 596–598
heat equation, 564
numeric analysis, 922–936, 948
ADI method, 928–930
difference equations, 923–925

bindex.qxd

11/4/10

I14

6:06 PM

Page I14

Index

Laplace equation (Cont.)
Dirichlet problem, 925–928,
934–935
Liebmann’s method, 926–928
in spherical coordinates, 594
theory of solutions of, 460, 786.
See also Potential theory
two-dimensional heat equation,
564
two-dimensional problems, 759
uniqueness theorem for, 462
Laplace integrals, 516
Laplace operator, 401. See also
Laplacian
Laplace transforms, 203–253
convolution, 232–237
defined, 204, 205
of derivatives, 211–212
differentiation of, 238–240
Dirac delta function, 226–228
existence, 209–210
first shifting theorem (s-shifting),
208–209
general formulas, 248
initial value problems, 213–216
integral equations, 236–237
of integral of a function, 212–213
integration of, 238–240
linearity of, 206–208
notation, 205
ODEs with variable coefficients,
240–241
partial differential equations,
600–603
partial fractions, 228–230
second shifting theorem
(t-shifting), 219–223
short impulses, 225–226
systems of ODEs, 242–247
table of, 249–251
uniqueness, 210
unit step function (Heaviside
function), 217–219
Laplacian, 400, 463, 605, A76
in cylindrical coordinates,
593–594
heat equation, 557
Laplace’s equation, 593
in polar coordinates, 585–592
in spherical coordinates, 594
of u in polar coordinates, 586
Lattice points, 925–926
Laurent, Pierre Alphonse, 708n.1
Laurent series, 708–719, 734
analytic or singular at infinity,
718–719
point at infinity, 718
Riemann sphere, 718
singularities, 715–717
zeros of analytic functions, 717

Laurent’s theorem, 709
LCL (lower control limit), 1088
Least squares approximation, of a
function, 875–876
Least squares method, 872–876, 899
Least squares principle, 1103
Lebesgue, Henri, 876n.5
Left-handed Cartesian coordinate
system, 369, 370, A84
Left-hand limit (Fourier series), 480
Left-sided tests, 1079, 1082
Legendre, Adrien-Marie, 175n.1,
1103
Legendre function, 175
Legendre polynomials, 167, 177–179,
202
Legendre’s equation, 167, 175– 177,
201, 202
Laplace’s equation, 595–596
special, 169–170
Leibniz, Gottfried Wilhelm, 15n.3
Leibniz test for real series, A73–A74
Length:
curves, 385
vectors, 355, 356, 410
Leonardo of Pisa, 690n.2
Leontief, Wassily, 334n.1
Leontief input–output model, 334
Leslie model, 331
Level surfaces, 380, 398
LFTs, see Linear fractional
transformations
Libby, Willard Frank, 13n.2
Liebmann’s method, 926–928
Likelihood function, 1066
Limit (sequences), 672
Limit cycle, 158–159, 621
Limit l, 378
Limit point, A93
Limit vector, 378
Linear algebra, 255. See also
Numeric linear algebra
determinants, 293–301
Cramer’s rule, 298–300
general properties of, 295–298
of matrix products, 307–308
second-order, 291–292
third-order, 292–293
inverse of a matrix, 301–309
cancellation laws, 306–307
determinants of matrix
products, 307–308
formulas for, 304–306
Gauss–Jordan method,
302–304
linear systems, 272–274
back substitution, 274–276
elementary row operations, 277
Gauss elimination, 274–280
homogeneous, 290–291

Linear algebra (Cont.)
nonhomogeneous, 291
solutions of, 288–291
matrices and vectors, 257–262
addition and scalar
multiplication of,
259–261
diagonal matrices, 268
linear independence and
dependence of vectors,
282–283
matrix multiplication,
263–266, 269–279
notation, 258
rank of, 283–285
symmetric and skew-symmetric
matrices, 267–268
transposition of, 266–267
triangular matrices, 268
matrix eigenvalue problems,
322–353
applications, 329–334
complex matrices and forms,
346–352
determining eigenvalues and
eigenvectors, 323–329
diagonalization of matrices,
341–342
eigenbases, 339–341
orthogonal matrices, 337–338
orthogonal transformations, 336
quadratic forms, 343–344
symmetric and skewsymmetric matrices,
334–336
transformation to principal
axes, 344
vector spaces:
inner product spaces, 311–313
linear transformations,
313–317
real, 309–311
special, 285–287
Linear combination:
homogeneous linear ODEs:
higher-order, 107
second-order, 48
of matrices, 129, 271
of vectors, 129, 282
of vectors in vector space, 311
Linear dependence, of vectors,
282–283
Linear element, 386
Linear equations, systems of, see
Linear systems
Linear fractional transformations
(LFTs), 742–750, 757
extended complex plane, 744–745
mapping standard domains,
747–750

bindex.qxd

11/4/10

6:06 PM

Page I15

Index
Linear independence:
scalar triple product, 373
of vectors, 282–283
Linear inequalities, 954
Linear interpolation, 809–810
Linearity:
Fourier transforms, 526–527
Laplace transforms, 206–208
line integrals, 645
Linearity principle, see Superposition
principle
Linearization, 152–155
Linearized system, 153
Linearly dependent functions:
higher-order homogeneous linear
ODEs, 106, 109
second-order homogeneous linear
ODEs, 50, 75
Linearly dependent sets, 129, 311
Linearly dependent vectors, 282–283,
285
Linearly independent functions:
higher-order homogeneous linear
ODEs, 106, 109, 113
second-order homogeneous linear
ODEs, 50, 75
Linearly independent sets, 128–129,
311
Linearly independent vectors, 282–283
Linearly related variables, 1109
Linear mapping, 314. See also Linear
transformations
Linear ODEs, 45, 46
first order, 27–36
Bernoulli equation, 31–33
homogeneous, 28
nonhomogeneous, 28–29
population dynamics, 33–34
higher-order, 105–123
homogeneous, 105–116
nonhomogeneous, 116–122
higher-order homogeneous, 105
second-order, 46–104
homogeneous, 46–78, 103
nonhomogeneous, 79–102, 103
Linear operations:
Fourier cosine and sine
transforms as, 520
integration as, 645
Linear operators (second-order
homogeneous linear ODEs), 61
Linear optimization, see Constrained
(linear) optimization
Linear PDEs, 541
Linear programming problems, 954–958
normal form of problems, 955–957
simplex method, 958–968
degenerate feasible solution,
962–965
difficulties in starting, 965–968

I15
Linear systems, 138–139, 165,
272–274, 320, 845
back substitution, 274–276
defined, 267, 845
elementary row operations, 277
Gauss elimination, 274–280,
844–852
applications, 277–180
back substitution, 274–276
elementary row operations, 277
operation count, 850–851
row echelon form, 279–280
Gauss–Jordan elimination,
856–857
homogeneous, 138, 165, 272,
290–291
constant-coefficient systems,
140–151
matrices and vectors, 124–130
ill-conditioning, 864–872
condition number of a matrix,
868–870
matrix norms, 866–868
vector norms, 866
iterative methods, 858–864
Gauss–Seidel iteration,
858–882
Jacobi iteration, 862–863
LU-factorization, 852–855
Cholesky’s method, 855–856
of m equations in n unknowns, 272
nonhomogeneous, 138, 160–163,
272, 290, 291
solutions of, 288–291, 898
Linear transformations, 320
motivation of multiplication by,
265–266
vector spaces, 313–317
Line integrals, 643–652, 669
basic properties of, 645
bounds for, 650–651
definition of, 414, 643–645
existence of, 646
indefinite integration and
substitution of limits,
646–647
path dependence of, and
integration around closed
curves, 421–425
representation of a path, 647–650
vector integral calculus, 413–419
definition and evaluation of,
414–416
path dependence of, 418–426
work done by a force, 416–417
Lines of constant revenue, 954
Lines of force, 760–762
LINPACK, 789
Liouville, Joseph, 499n.4
Liouville’s theorem, 666–667

Lipschitz, Rudolf, 42n.9
Lipschitz condition, 42
Ljapunov, Alexander Michailovich,
149n.2
Local error, 830
Local maximum (unconstrained
optimization), 952
Local minimum (unconstrained
optimization), 951
Local truncation error, 902
Logarithm, 636–639
natural, 636–638, 642, A63
Taylor series, 695
Logarithmic decrement, 70
Logarithmic integral, formula for, A69
Logarithm of base ten, formula for,
A63
Logistic equation, 32–33
Longest path, 976
Loss of significant digits (numeric
analysis), 793–794
Lotka, Alfred J., 155n.3
Lotka–Volterra population model,
155–156
Lot tolerance percent defective
(LTPD), 1094
Lower confidence limits, 1068
Lower control limit (LCL), 1088
Lower triangular matrices, 268
LTPD (lot tolerance percent
defective), 1094
LU-factorization (linear systems),
852–855
Machine numbers, 792
Maclaurin, Colin, 690n.2, 712
Maclaurin series, 690, 694–696
Main diagonal:
determinants, 294
matrix, 125, 258
Malthus, Thomas Robert, 5n.1
Malthus’ law, 5, 33
Maple, 789
Maple Computer Guide, 789
Mapping, 313, 736, 737, 757
bijective, 737n.1
conformal, 736–757
boundary value problems,
763–767, A96
defined, 738
geometry of analytic functions,
737–742
linear fractional
transformations,
742–750
Riemann surfaces, 754–756
by trigonometric and
hyperbolic analytic
functions, 750–754

bindex.qxd

11/4/10

I16

6:06 PM

Page I16

Index

Mapping (Cont.)
of disks, 748–750
fixed points of, 745
of half-planes onto half-planes, 748
identity, 745
injective, 737n.1
inverse, 741, 745
linear, 314. See also Linear
transformations
one-to-one, 737n.1
spectral mapping theorem, 878
surjective, 737n.1
Marconi, Guglielmo, 63n.3
Marginal distributions, 1053–1055,
1062
of continuous distributions, 1055
of discrete distributions,
1053–1054
Mariotte, Edme, 19n.5
Markov, Andrei Andrejevitch, 270n.1
Markov process, 270, 331
MATCHING, ALGORITHM, 1003
Matching, 1008
assignment problems, 1001
complete, 1002
maximum cardinality, 1001, 1008
Mathcad, 789
Mathematica, 789
Mathematica Computer Guide, 789
Mathematical models, see Models
Mathematical modeling, see
Modeling
Mathematical statistics, 1009,
1063–1113
acceptance sampling, 1092–1096
errors in, 1093–1094
rectification, 1094–1095
confidence intervals, 1068–1077
for mean of normal distribution
with known variance,
1069–1071
for mean of normal distribution
with unknown variance,
1071–1073
for parameters of distributions
other than normal, 1076
for variance of a normal
distribution, 1073–1076
correlation analysis, 1108–1111
defined, 1103
test for correlation coefficient,
1110–1111
defined, 1063
goodness of fit, 1096–1100
hypothesis testing, 1077–1087
comparison of means,
1084–1085
comparison of variances, 1086
errors in tests, 1080–1081

for mean of normal distribution
with known variance,
1081–1083
for mean of normal distribution
with unknown variance,
1083–1084
one- and two-sided
alternatives, 1079–1080
main purpose of, 1015
nonparametric tests, 1100–1102
point estimation of parameters,
1065–1068
quality control, 1087–1092
for mean, 1088–1089
for range, 1090–1091
for standard deviation, 1090
for variance, 1089–1090
random sampling, 1063–1065
regression analysis, 1103–1108
confidence intervals in,
1107–1108
defined, 1103
Matlab, 789
Matrices, 124–130, 256–262, 320
addition and scalar multiplication
of, 259–261
calculations with, 126–127
condition number of, 868–870
definitions and terms, 125–126,
128, 257
diagonal, 268
diagonalization of, 341–342
eigenvalues, 129–130
equality of, 126, 259
fundamental, 139
inverse of, 128, 301–309, 321
cancellation laws, 306–307
determinants of matrix
products, 307–308
formulas for, 304–306
Gauss–Jordan method,
302–304, 856–857
matrix multiplication, 127,
263–266, 269–279
applications of, 269–279
cancellation laws, 306–307
determinants of matrix
products, 307–308
scalar, 259–261
normal, 352, 882
notation, 258
orthogonal, 337–338
rank of, 283–285
square, 126
symmetric and skew-symmetric,
267–268
transposition of, 266–267
triangular, 268
unitary, 347–350, 353

Matrix eigenvalue problems,
322–353, 876–896
applications, 329–334
choice of numeric method for,
879
complex matrices and forms,
346–352
determining eigenvalues and
eigenvectors, 323–329
diagonalization of matrices,
341–342
eigenbases, 339–341
inclusion theorems, 879–884
orthogonal matrices, 337–338
orthogonal transformations, 336
power method, 885–888
QR-factorization, 892–896
quadratic forms, 343–344
symmetric and skew-symmetric
matrices, 334–336
transformation to principal axes,
344
tridiagonalization, 888–892
Matrix multiplication, 127, 263–266,
269–279
applications of, 269–279
cancellation laws, 306–307
determinants of matrix products,
307–308
scalar, 259–261
Matrix norms, 861, 866–868
Maximum cardinality matching,
1001, 1003–1004, 1008
Maximum flow:
Ford–Fulkerson algorithm,
998–1000
and minimum cut set, 996
Maximum increase:
gradient of a scalar field, 398
unconstrained optimization, 951
Maximum likelihood estimates
(MLEs), 1066–1067
Maximum likelihood method,
1066–1067, 1113
Maximum modulus theorem, 782–784
Maximum principle, 783
Mean(s), 1013–1014, 1061
comparison of, 1084–1085
control chart for, 1088–1089
of normal distributions:
confidence intervals for,
1069–1073
hypothesis testing for,
1081–1084
probability distributions,
1035–1039
addition of, 1057–1058
transformation of, 1036–1037
sample, 1064

bindex.qxd

11/4/10

6:06 PM

Page I17

Index
Mean square convergence (orthogonal
series), 507–508
Mean value (fluid flow), 774n.1
Mean value property:
analytic functions, 781–782
harmonic functions, 782
Mean value theorem, 395
for double integrals, 427
for surface integrals, 448
for triple integrals, 456–457
Median, 1013, 1100–1101
Mendel, Gregor, 1100
Meromorphic function, 719
Mesh incidence matrix, 262
Mesh points (lattice points, nodes),
925–926
Mesh size, 924
Method of characteristics (PDEs), 555
Method of least squares, 872–876,
899
Method of moments, 1065
Method of separating variables,
12–13
circular membrane, 587
partial differential equations,
545–553, 605
Fourier series, 548–551
satisfying boundary conditions,
546–548
two ODEs from wave
equation, 545–546
vibrating string, 545–546
Method of steepest descent, 952–954
Method of undetermined coefficients:
higher-order homogeneous linear
ODEs, 115, 123
nonhomogeneous linear systems
of ODEs, 161
second-order nonhomogeneous
linear ODEs, 81–85, 104
Method of variation of parameters:
higher-order nonhomogeneous
linear ODEs, 118–120, 123
nonhomogeneous linear systems
of ODEs, 162–163
second-order nonhomogeneous
linear ODEs, 99–102, 104
Minimization (normal form of linear
optimization problems), 957
Minimum (unconstrained
optimization), 951
Minimum cut set, 996
Minors, of determinants, 294
Mixed boundary condition (twodimensional heat equation),
564
Mixed boundary value problem, 605,
923. See also Robin problem
elliptic PDEs, 931–933
heat conduction, 768–769

I17
Mixed type PDEs, 555
Mixing problems, 14
MLEs (maximum likelihood
estimates), 1066–1067
ML-inequality, 650–651
Möbius, August Ferdinand, 447n.5
Möbius strip, 447
Möbius transformations, 743. See
also Linear fractional
transformations (LFTs)
Models, 2
Modeling, 1, 2–8, 44
and concept of solution, 4–6
defined, 2
first-order ODEs, 2–8
initial value problem, 6
separable ODEs, 13–17
typical steps of, 6–7
and unifying power of
mathematics, 766
Modification Rule (method of
undetermined coefficients):
higher-order homogeneous linear
ODEs, 115–116
second-order nonhomogeneous
linear ODEs, 81, 83
Modulus (complex numbers), 613
Moments, method of, 1065
Moments of inertia, of a region, 429
Moment vector (vector moment),
371
Monotone real sequences,
A72–A73
Moore, Edward Forrest, 977n.2
MOORE, ALGORITHM, 977
Moore’s BFS algorithm, 977–980,
1008
Morera’s theorem, 667
Moulton, Forest Ray, 913n.3
Multinomial distribution, 1045
Multiple complex roots, 115
Multiple points, curves with, 383
Multiplication:
of complex numbers, 609, 610,
615
in conditional probability,
1022–1023
matrix, 127, 263–266
applications of, 269–279
cancellation laws, 306–307
determinants of matrix
products, 307–308
scalar, 259–261
of means, 1057–1058
of power series, 687
scalar, 126–127, 259–261, 310
termwise, 173, 687
of transforms, 232. See also
Convolution
Multiplicity, algebraic, 326, 878

Multiply connected domains, 652,
653
Cauchy’s integral formula,
662–663
Cauchy’s integral theorem,
658–659
Multistep methods, 911–915, 947
Adams–Bashforth methods,
911–914
Adams–Moulton methods,
913–914
defined, 908
first-order ODEs, 911
Mutually exclusive events, 1016,
1021
m ϫ n matrix, 258
Nabla, 396
NAG (Numerical Algorithms Group,
Inc.), 789
National Institute of Standards and
Technology (NIST), 789
Natural condition (spline
interpolation), 823
Natural frequency, 63
Natural logarithm, 636–638, 642,
A63
Natural spline, 823
n-dimensional vector spaces, 311
Negative (scalar multiplication), 260
Negative definite (quadratic form),
346
Neighborhood, 619, 720
Net flow, through cut set, 994–995
NETLIB, 789
Networks:
defined, 991
flow problems in, 991–997
cut sets, 994–996
flow augmenting paths,
992–993
paths, 992
Neumann, Carl, 198n.7
Neumann, John von, 959n.1
Neumann boundary condition, 564
Neumann problem, 605, 923
elliptic PDEs, 931
Laplace’s equation, 593
two-dimensional heat equation,
564
Neumann’s function, 198
NEWTON, ALGORITHM, 802
Newton, Sir Isaac, 15n.3
Newton–Cotes formulas, 833, 843
Newton’s (Gregory–Newton’s)
backward difference
interpolation formula, 818–819
Newton’s divided difference
interpolation, 812–815, 842

bindex.qxd

11/4/10

I18

6:06 PM

Page I18

Index

Newton’s divided difference
interpolation formula, 814–815
Newton’s (Gregory–Newton’s)
forward difference
interpolation formula,
815–818, 842
Newton’s law of cooling, 15–16
Newton’s law of gravitation, 377
Newton’s (Newton–Raphson)
method, 801–805, 842
Newton’s second law, 11, 63, 245,
544, 576
Neyman, Jerzy, 1068n.1, 1077n.2
Nicolson, Phyllis, 938n.5
Nicomedes, 391n.4
Nilpotent matrices, 270
NIST (National Institute of Standards
and Technology), 789
Nodal incidence matrix, 262
Nodal lines, 580–581, 588
Nodes, 165, 925–926
degenerate, 145–146
improper, 142
interpolation, 808
proper, 143
spline interpolation, 820
trapezoidal rule, 829
vibrating string, 547
Nonbasic variables, 960
Nonconservative physical systems,
422
Nonhomogeneous linear ODEs:
convolution, 235–236
first-order, 28–29
higher-order, 106, 116–122
second-order, 79–102
defined, 47
method of undetermined
coefficients, 81–85
modeling electric circuits,
93–99
modeling forced oscillations,
85–92
particular solution, 80
solution by variation of
parameters, 99–102
Nonhomogeneous linear systems,
138, 160–163, 166, 272, 290,
291, 845
method of undetermined
coefficients, 161
method of variation of parameters,
162–163
Nonhomogeneous PDEs, 541
Nonlinear ODEs, 46
first-order, 27
higher-order homogeneous,
105
second-order, 46
Nonlinear PDEs, 541

Nonlinear systems, qualitative
methods for, 152–160
linearization, 152–155
Lotka–Volterra population model,
155–156
transformation to first-order
equation in phase plane,
157–159
Nonparametric tests (statistics),
1100–1102, 1113
Nonsingular matrices, 128, 301
Norm(s):
matrix, 861, 866–868
orthogonal functions, 500
vector, 312, 355, 410, 866
Normal accelerations, 391
Normal acceleration vector, 387
Normal derivative, 437
defined, 437
mixed problems, 768, 931
Neumann problems, 931
solutions of Laplace’s equation,
460
Normal distributions, 1045–1051,
1062
as approximation of binomial
distribution, 1049–1050
confidence intervals:
for means of, 1069–1073
for variances of, 1073–1076
distribution function, 1046–1047
means of:
confidence intervals for,
1069–1073
hypothesis testing for,
1081–1084
numeric values, 1047–1048
tables, A101–A102
two-dimensional, 1110
working with normal tables,
1048–1049
Normal equations, 873, 1105–1106
Normal form (linear optimization
problems), 955–957, 959, 969
Normalizing, eigenvectors, 326
Normal matrices, 352, 882
Normal mode:
circular membrane, 588
vibrating string, 547–548
Normal plane, 390
Normal random variables, 1045
Normal vectors, 366, 441
Not rejecting a hypothesis, 1081
No trend hypothesis, 1101
nth order linear ODEs, 105, 123
nth-order ODEs, 134–135
nth partial sum, 170
Fourier series, 495
of series, 673
nth roots, 616

nth roots of unity, 617
Null hypothesis, 1078
Nullity, 287, 291
Null space, 287, 291
Numbers:
acceptance, 1092
Bernoulli’s law of large numbers,
1051
chromatic, 1006
complex, 608–619, 641
addition of, 609, 610
conjugate, 612
defined, 608
division of, 610
multiplication of, 609, 610
polar form of, 613–618
subtraction of, 610
condition, 868–870, 899
Fibonacci, 690
floating-point form of, 791–792
machine, 792
random, 1064
Number of degrees of freedom, 1071,
1074
Numerics, see Numeric analysis
Numerical Algorithms Group, Inc.
(NAG), 789
Numerically stable algorithms, 796,
842
Numerical Recipes, 789
Numeric analysis (numerics),
787–843
algorithms, 796
basic error principle, 796
error propagation, 795
errors of numeric results, 794–795
floating-point form of numbers,
791–792
interpolation, 808–820
equal spacing, 815–819
Lagrange, 809–812
Newton’s backward difference
formula, 818–819
Newton’s divided difference,
812–815
Newton’s forward difference
formula, 815–818
spline, 820–827
loss of significant digits, 793–794
numeric differentiation, 838–839
numeric integration, 827–838
adaptive, 835–836
Gauss integration formulas,
836–838
rectangular rule, 828
Simpson’s rule, 831–835
trapezoidal rule, 828–831
for ODEs, 901–922
first-order, 901–915
higher order, 915–922

bindex.qxd

11/4/10

6:06 PM

Page I19

Index
numeric integration (Cont.)
for PDEs, 922–945
elliptic, 922–936
hyperbolic, 942–945
parabolic, 936–942
roundoff, 792–793
software for, 788–789
solution of equations by iteration,
798–808
fixed-point iteration, 798–801
Newton’s (Newton–Raphson)
method, 801–805
secant method, 805–806
speed of convergence, 804–805
spline interpolation, 820–827
Numeric differentiation, 838–839
Numeric integration, 827–838
adaptive, 835–836
Gauss integration formulas,
836–838
rectangular rule, 828
Simpson’s rule, 831–835
trapezoidal rule, 828–831
Numeric linear algebra, 844–899
curve fitting, 872–876
least squares method, 872–876
linear systems, 845
Gauss elimination, 844–852
Gauss–Jordan elimination,
856–857
ill-conditioning norms,
864–872
iterative methods, 858–864
LU-factorization, 852–855
matrix eigenvalue problems,
876–896
inclusion theorems, 879–884
power method, 885–888
QR-factorization, 892–896
tridiagonalization, 888–892
Numeric methods:
choice of, 791, 879
defined, 791
n ϫ n matrix, 125
Nyström, E. J., 919
Objective function, 951, 969
OCs (operating characteristics), 1081
OC curve, see Operating
characteristic curve
Odd functions, 486–488
Odd periodic extension, 488–490
ODEs, see Ordinary differential
equations
Ohm, Georg Simon, 93n.7
Ohm’s law, 29
One-dimensional heat equation, 559
One-dimensional wave equation,
544–545

I19
One-parameter family of curves, 36–37
One-sided alternative (hypothesis
testing), 1079–1080
One-sided tests, 1079
One-step methods, 908, 911, 947
One-to-one mapping, 737n.1
Open annulus, 619
Open circular disk, 619
Open integration formula, 838
Open intervals, 4, A72n.3
Open Leontief input–output model,
334
Open set, in complex plane, 620
Operating characteristic curve (OC
curve), 1081, 1092, 1095
Operating characteristics (OCs), 1081
Operational calculus, 60, 203
Operation count (Gauss elimination),
850
Operators, 60–61, 313
Optimal solutions (normal form of
linear optimization problems),
957
Optimization:
combinatorial, 970, 975–1008
assignment problems,
1001–1006
flow problems in networks,
991–997
Ford–Fulkerson algorithm for
maximum flow,
998–1001
shortest path problems,
975–980
constrained (linear), 951, 954–968
normal form of problems,
955–957
simplex method, 958–968
unconstrained:
basic concepts, 951–952
method of steepest descent,
952–954
Optimization methods, 949
Optimization problems, 949,
954–958
normal form of problems,
955–957
objective, 951
simplex method, 958–968
degenerate feasible solution,
962–965
difficulties in starting, 965–968
Order:
and complexity of algorithms, 978
Gauss elimination, 850
of iteration process, 804
of PDE, 540
singularities, 714
Ordering (Greedy algorithm), 987
Order statistics, 1100

Ordinary differential equations
(ODEs), 44
autonomous, 11, 33
defined, 1, 3–4
first-order, 2–45
direction fields, 9–10
Euler’s method, 10–11
exact, 20–27
geometric meanings of, 9–12
initial value problem, 38–43
linear, 27–36
modeling, 2–8
numeric analysis, 901–915
orthogonal trajectories, 36–38
separable, 12–20
higher-order linear, 105–123
homogeneous, 105–116, 123
nonhomogeneous, 116–123
systems of, see Systems of
ODEs
Laplace transforms, 203–253
convolution, 232–237
defined, 204, 205
of derivatives, 211–212
differentiation of, 238–240
Dirac delta function, 226–228
existence, 209–210
first shifting theorem
(s-shifting), 208–209
general formulas, 248
initial value problems,
213–216
integral equations, 236–237
of integral of a function,
212–213
integration of, 238–240
linearity of, 206–208
notation, 205
ODEs with variable
coefficients, 240–241
partial differential equations,
600–603
partial fractions, 228–230
second shifting theorem
(t-shifting), 219–223
short impulses, 225–226
systems of ODEs, 242–247
table of, 249–251
uniqueness, 210
unit step function (Heaviside
function), 217–219
linear, 46
nonlinear, 46
numeric analysis, 901–922
first-order ODEs, 901–915
higher order ODEs, 915–922
second-order linear, 46–104
homogeneous, 46–79
nonhomogeneous, 79–102
second-order nonlinear, 46

bindex.qxd

11/4/10

I20

6:06 PM

Page I20

Index

Ordinary differential equations (Cont.)
series solutions of ODEs, 167–202
Bessel functions, 187–194,
196–200
Bessel’s equation, 187–200
Frobenius method, 180–187
Legendre polynomials,
177–179
Legendre’s equation, 175– 179
power series method, 167–175
systems of, 124–166
basic theory, 137–139
constant-coefficient, 140–151
conversion of nth-order ODEs
to, 134–135
homogeneous, 138
Laplace transforms, 242–247
linear, 124–130, 138–151,
160–163
matrices and vectors, 124–130
as models of applications,
130–134
nonhomogeneous, 138, 160–163
nonlinear, 152–160
in phase plane, 124, 141–146,
157–159
qualitative methods for
nonlinear systems,
152–160
Orientable surfaces, 446–447
Oriented curve, 644
Oriented surfaces, integrals over,
446–447
Origin (vertex), 980
Orthogonal, to a vector, 362
Orthogonal coordinate curves, A74
Orthogonal expansion, 504
Orthogonal functions:
defined, 500
Sturm–Liouville Problems,
500–503
Orthogonality:
trigonometric system, 479–480, 538
vector differential calculus,
361–363
Orthogonal matrices, 335, 337–338,
353, A85n.2
Orthogonal polynomials, 179
Orthogonal series (generalized
Fourier series), 504–510
completeness, 508–509
mean square convergence,
507–508
Orthogonal trajectories:
defined, 36
first-order ODEs, 36–38
Orthogonal transformations, 336,
A85n.2
Orthogonal vectors, 312, 362, 410
Orthonormal functions, 500, 501, 508

Orthonormal system, 337
Oscillations:
forced, 85–92
free, 62–70
harmonic, 63–64
second-order linear ODEs:
homogeneous, 62–70
nonhomogeneous, 85–92
Osculating plane, 389, 390
Outcomes:
of experiments, 1015, 1060
probability theory, 1015
Outer normal derivative, 460, 931
Outliers, 1013–1015
Output (response to input), 27, 86,
214
Overdamping, 65–66
Overdetermined linear systems, 277
Overflow (floating-point numbers),
792
Overrelaxation factor, 863
Paired comparison, 1084, 1113
Pappus, theorem of, 452
Pappus of Alexandria, 452n.7
Parabolic PDEs:
defined, 923
numeric analysis, 936–942
Parallelogram law, 357
Parallel processing of products (on
computer), 265
Parameters, 175, 381, 1112
estimation of, 1063
point estimation of, 1065–1068
probability distributions,
1035
of a sample, 1065
Parameter curves, 442
Parametric representations, 381,
439–441
Parseval, Marc Antoine, 497n.3
Parseval equality, 509
Parseval’s identity, 497
Parseval’s theorem, 497
Partial derivatives, A69–A71
defined, A69
first (first order), A71
second (second order), A71
third (third order), A71
of vector functions, 380
Partial differential equations (PDEs),
473, 540–605
basic concepts of, 540–543
d’Alembert’s solution, 553–556
defined, 540
double Fourier series solution,
577–585
heat equation, 557–558
Dirichlet problem, 564–566

Partial differential equations (Cont.)
Laplace’s equation, 564
solution by Fourier integrals,
568–571
solution by Fourier series,
558–563
solution by Fourier transforms,
571–574
steady two-dimensional heat
problems, 546–566
unifying power of methods,
566
homogeneous, 541
Laplace’s equation, 593–600
boundary value problem in
spherical coordinates,
594–596
in cylindrical coordinates,
593–594
Fourier–Legendre series,
596–598
in spherical coordinates, 594
Laplace transforms, solution by,
600–603
Laplacian in polar coordinates,
585–592
linear, 541
method of separating variables,
545–553
Fourier series, 548–551
satisfying boundary conditions,
546–548
two ODEs from wave
equation, 545–546
nonhomogeneous, 541
nonlinear, 541
numeric analysis, 922–945
elliptic, 922–936
hyperbolic, 942–945
parabolic, 936–942
ODEs vs., 4
wave equation, 544–545
d’Alembert’s solution,
553–556
solution by separating
variables, 545–553
two-dimensional, 575–584
Partial fractions (Laplace transforms),
228–230
Partial pivoting, 276, 846–848, 898
Partial sums, of series, 477, 478, 495
Particular solution(s):
first-order ODEs, 6, 44
higher-order homogeneous linear
ODEs, 106
nonhomogeneous linear systems,
160
second-order linear ODEs:
homogeneous, 49–51, 104
nonhomogeneous, 80

bindex.qxd

11/4/10

6:06 PM

Page I21

Index
Partitioning, of a path, 645
Pascal, Blaise, 391n.4
Pascal, Étienne, 391n.4
Paths:
alternating, 1002
augmenting, 1002–1003
closed, 414, 645, 975–976
deformation of, 656
directed, 1000
flow augmenting, 992–993, 998,
1008
flow problems in networks, 992
integration by use of, 647–650
longest, 976
partitioning of, 645
principle of deformation of, 656
shortest, 976
shortest path problems, 975–976
simple closed, 652
Path dependence (line integrals),
418–426, 470, 649–650
defined, 418
and integration around closed
curves, 421–425
Path independence, 669
Cauchy’s integral theorem, 655
in a domain D in space, 419
proof of, A88–A89
Stokes’s Theorem applied to,
468
Path of integration, 414, 644
Pauli spin matrices, 351
p-charts, 1091–1092
PDEs, see Partial differential
equations
Pearson, Egon Sharpe, 1077n.2
Pearson, Karl, 1077, 1086n.4
Period, 475
Periodic boundary conditions, 501
Periodic extensions, 488–490
Periodic function, 474–475, 538
Periodic Sturm–Liouville problem,
501
Permutations:
of n things taken k at a time,
1025
of n things taken k at a time with
repetitions, 1025–1026
probability theory, 1024–1026
Perron, Oskar, 882n.8
Perron–Frobenius Theorem, 883
Perron’s theorem, 334, 882–883
Pfaff, Johann Friedrich, 422n.1
Pfaffian form, 422
p-fold connected domains, 652–653
Phase angle, 90
Phase lag, 90
Phase plane, 134, 165
linear systems, 141, 148
nonlinear systems, 152

I21
Phase plane method, 124
linear systems:
critical points, 142–146
graphing solutions, 141–142
nonlinear systems, 152
linearization, 152–155
Lotka–Volterra population
model, 155–156
transformation to first-order
equation in, 157–159
Phase plane representations, 134
Phase portrait, 165
linear systems, 141–142, 148
nonlinear systems, 152
Picard, Emile, 42n.10
Picard’s Iteration Method, 42
Picard’s theorem, 716
Piecewise continuous functions, 209
Piecewise smooth path of integration,
414, 645
Piecewise smooth surfaces, 442, 447
Pivot, 276, 898, 960
Pivot equation, 276, 846, 898, 960
Planar graphs, 1005
Plane:
complex, 611
extended, 718, 744–745
finite, 718
sets in, 620
normal, 390
osculating, 389, 390
phase, 134, 165
linear systems, 141, 148
nonlinear systems, 152
rectifying, 390
tangent, 398, 441–442
vectors in, 309
Plane curves, 383
Planimeters, 436
Poincaré, Henri, 141n.1, 510n.8
Points:
boundary, 426n.2, 620
branch, 755
center, 144, 165
critical, 33, 144, 165
asymptotically stable, 149
and conformal mapping, 738,
757
constant-coefficient systems of
ODEs, 142–151
isolated, 152
nonlinear systems, 152
stable, 140, 149
stable and attractive, 140, 149
unstable, 140, 149
equilibrium, 33–34
fixed, 745, 799
guidepoints, 827
at infinity, 718
initial (vectors), 355

Points: (Cont.)
lattice, 925–926
limit, A93
mesh, 925–926
regular, 181
regular singular, 180n.4
saddle, 143, 165
sample, 1015
singular, 181, 201
analytic functions, 693
regular, 180n.4
spiral, 144–145, 165
stagnation, 773
stationary, 952
terminal (vectors), 355
Point estimation of parameters
(statistics), 1065–1068, 1113
defined, 1065
maximum likelihood method,
1066–1067
Point set, in complex plane, 620
Point source (flow modeling), 776
Point spectrum, 525
Poisson, Siméon Denis, 779n.2
Poisson distributions, 1041–1042,
1061, A100
Poisson equation:
defined, 923
numeric analysis, 922–936
ADI method, 928–930
difference equations, 923–925
Dirichlet problem, 925–928
mixed boundary value
problem, 931–933
Poisson’s integral formula:
derivation of, 778–778
potential theory, 777–781
series for potentials in disks,
779–780
Polar coordinates, 431
Laplacian in, 585–592
notation for, 594
two-dimensional wave equation
in, 586
Polar form, of complex numbers,
613–618, 631
Polar moment of inertia, of a region,
429
Poles (singularities), 714–715
of order m, 735
and zeros, 717
Polynomials, 624
characteristic, 325, 353, 877
Chebyshev, 504
interpolation, 808, 842
Laguerre, 241, 504
Legendre, 167, 177–179, 202
orthogonal, 179
trigonometric:
approximation by, 495–498

bindex.qxd

11/4/10

I22

6:06 PM

Page I22

Index

Polynomials (Cont.)
complex, 529
of the same degree N, 495
Polynomial approximations, 808
Polynomial interpolation, 808, 842
Polynomially bounded, 979
Polynomial matrix, 334, 878–879
Populations:
infinite, 1044
for statistical sampling, 1063
Population dynamics:
defined, 33
logistic equation, 33–34
Position vector, 356
Positive correlation, 1111
Positive definite (quadratic form),
346
Positive sense, on curve, 644
Possible values (random variables),
1030
Postman problem, 980
Potential (potential function), 400
complex, 760–761
Laplace’s equation, 593
Poisson’s integral formula for,
777–781
Potential theory, 179, 420, 460,
758–786
conformal mapping for boundary
value problems, 763–767
defined, 758
electrostatic fields, 759–763
complex potential, 760–761
superposition, 761–762
fluid flow, 771–777
harmonic functions, 781–784
heat problems, 767–770
Laplace’s equation, 593, 628
Poisson’s integral formula, 777–781
Power function, of a test, 1081, 1113
Power method (matrix eigenvalue
problems), 885–888, 899
Power series, 168, 671–707
convergence behavior of, 680–682
convergence tests, 674–676,
A93–A94
functions given by, 685–690
Maclaurin series, 690
in powers of x, 168
radius of convergence, 682–684
ratio test, 676–678
root test, 678–679
sequences, 671–673
series, 673–674
Taylor series, 690–697
uniform convergence, 698–705
and absolute convergence, 704
properties of, 700–701
termwise integration, 701–703
test for, 703–704

Power series method, 167–175, 201
extension of, see Frobenius method
idea and technique of, 168–170
operations on, 173–174
theory of, 170–174
Practical resonance, 90
Predator–prey population model,
155–156
Predictor–corrector method, 913
PRIM, ALGORITHM, 989
Prim, Robert Clay, 988n.6
Prim’s algorithm, 988–991, 1008
Principal axes, transformation to, 344
Principal branch, of logarithm, 639
Principal directions, 330
Principal minors, 346
Principal part, 735
of isolated singularities, 715
of singularities, 708, 709
Principal value (complex numbers),
614, 617, 642
complex logarithm, 637
general powers, 639
Principle of deformation of path, 656
Prior estimates, 805
Probability, 1060
axioms of, 1020
basic theorems of, 1020–1022
conditional, 1022–1023
definitions of, 1018–1020
independent events, 1023
Probability distributions, 1029, 1061
binomial, 1039–1042
continuous, 1032–1034
discrete, 1030–1032
hypergeometric, 1042–1044
mean and variance of, 1035–1039
multinomial, 1045
normal, 1045–1051
Poisson, 1041–1042
of several random variables,
1051–1060
addition of means, 1057–1058
addition of variances,
1058–1059
continuous two-dimensional
distributions, 1053
discrete two-dimensional
distributions, 1052–1053
function of random variables,
1056
independence of random
variables, 1055–1056
marginal distributions,
1053–1055
symmetric, 1036
two-dimensional, 1051
continuous, 1053
discrete, 1052–1053
uniform, 1035–1036

Probability function, 1030–1032,
1052, 1061
Probability theory, 1009, 1015–1062
binomial coefficients, 1027–1028
combinations, 1024, 1026–1027
distributions (probability
distributions), 1029
binomial, 1039–1042
continuous, 1032–1034
discrete, 1030–1032
hypergeometric, 1042–1044
mean and variance of,
1035–1039
normal, 1045–1051
Poisson, 1041–1042
of several random variables,
1051–1060
events, 1016–1017
experiments, 1015–1016
factorial function, 1027
outcomes, 1015
permutations, 1024–1026
probability:
basic theorems of, 1020–1022
conditional, 1022–1023
definition of, 1018–1020
independent events, 1023
random variables, 1029–1030
continuous, 1032–1034
discrete, 1030–1032
Problem of existence, 39
Problem of uniqueness, 39
Producers, 1092
Producer’s risk, 1094
Product:
inner (dot), 312
for complex vectors, 349
invariance of, 336
vector differential calculus,
361–367, 410
of matrix, 260
determinants of, 307–308
inverting, 306
matrix multiplication, 263, 320
parallel processing of (on
computer), 265
scalar multiplication, 260
scalar triple, 373–374, 411
vector (cross):
in Cartesian coordinates,
A83–A84
vector differential calculus,
368–375, 410
Product method, 605. See also
Method of separating variables
Projection (vectors), 365
Proper node, 143
Pseudocode, 796
Pure imaginary complex numbers,
609

bindex.qxd

11/4/10

6:06 PM

Page I23

Index
QR-factorization, 892–896
Quadrant, of a circle, 604
Quadratic forms (matrix eigenvalue
problems), 343–344
Quadratic interpolation,
810–811
Qualitative methods, 124,
141n.1
defined, 152
for nonlinear systems, 152–160
linearization, 152–155
Lotka–Volterra population
model, 155–156
transformation to first-order
equation in phase plane,
157–159
Quality control (statistics),
1087–1092, 1113
for mean, 1088–1089
for range, 1090–1091
for standard deviation, 1090
for variance, 1089–1090
Quantitative methods, 124
Quasilinear equations, 555, 923
Quotient:
complex numbers, 610
difference, 923
Rayleigh, 885, 899
Radius:
of convergence, 172
defined, 172
power series, 682–684, 706
of a graph, 991
Random experiments, 1011,
1015–1016, 1060
Randomly selected samples, 1064
Randomness, 1015, 1064. See also
Random variables
Random numbers, 1064
Random number generators, 1064
Random sampling (statistics),
1063–1065
Random selections, 1064
Random variables, 1011, 1029–1030,
1061
continuous, 1029, 1032–1034,
1055
defined, 1030
dependent, 1055
discrete, 1029–1032, 1054
function of, 1056
independence of, 1055–1056
marginal distribution of, 1054,
1055
normal, 1045
occurrence of, 1063
probability distributions of,
1051–1060

I23
addition of means, 1057–1058
addition of variances, 1058–1059
continuous two-dimensional
distributions, 1053
discrete two-dimensional
distributions, 1052–1053
function of random variables,
1056
independence of random
variables, 1055–1056
marginal distributions,
1053–1055
skewness of, 1039
standardized, 1037
two-dimensional, 1051, 1062
Random variation, 1063
Range, 1013
control chart for, 1090–1091
defined, 1090
of f, 620
Rank:
of A, 279
of a matrix, 279, 283, 321
in terms of column vectors,
284–285
in terms of determinants, 297
of R, 279
Raphson, Joseph, 801n.1
Rational functions, 624, 725–729
Ratio test (power series), 676–678
Rayleigh, Lord (John William Strutt),
160n.5, 885n.10
Rayleigh equation, 160
Rayleigh quotient, 885, 899
Reactance (RLC circuits), 94
Real axis (complex plane), 611
Real different roots, 71
Real double root, 55–56, 72
Real functions, complex analytic
functions vs., 694
Real inner product space, 312
Real integrals, residue integration of,
725–733
Fourier integrals, 729–730
improper integrals, 730–732
of rational functions of cos u
sin u, 725–729
Real part (complex numbers), 609
Real pre-Hilbert space, 312
Real roots:
different, 71
double, 55–56
higher-order homogeneous linear
ODEs:
distinct, 112–113
multiple, 114–115
second-order homogeneous linear
ODEs:
distinct, 54–55
double, 55–56

Real sequence, 671
Real series, A73–A74
Real vector spaces, 309–311, 359,
410
Recording, of sample values,
1011–1012
Rectangular cross-section, 120
Rectangular matrix, 258
Rectangular membrane R, 577–584
Rectangular rule (numeric
integration), 828
Rectifiable (curves), 385
Rectification (acceptance sampling),
1094–1095
Rectifying plane, 390
Recurrence formula, 201
Recurrence relation, 176
Recursion formula, 176
Reduced echelon form, 279
Reduction of order (second-order
homogeneous linear ODEs),
51–52
Regions, 426n.2
bounded, 426n.2
center of gravity of mass in, 429
closed, 426n.2
critical, 1079
feasibility, 954
fundamental (exponential
function), 632
moments of inertia of, 429
polar moment of inertia of, 429
rejection, 1079
sets in complex plane, 620
total mass of, 429
volume of, 428
Regression analysis, 1063,
1103–1108, 1113
confidence intervals in,
1107–1108
defined, 1103
Regression coefficient, 1105,
1107–1108
Regression curve, 1103
Regression line, 1103, 1104, 1106
Regular point, 181
Regular singular point, 180n.4
Regular Sturm–Liouville problem,
501
Rejectable quality level (RQL), 1094
Rejection:
of a hypothesis, 1078
of products, 1092
Rejection region, 1079
Relative class frequency, 1012
Relative error, 794
Relative frequency (probability):
of an event, 1019
class, 1012
cumulative, 1012

bindex.qxd

11/4/10

I24

6:06 PM

Page I24

Index

Relaxation methods, 862
Remainder, 170
of a series, 673
of Taylor series, 691
Remarkable parallelogram, 375
Removable singularities, 717
Repeated factors, 220, 221
Representation, 315
by Fourier series, 476
by power series, 683
spectral, 525
Residual, 805, 862, 899
Residues, 708, 720, 735
at mth-order pole, 722
at simple poles, 721–722
Residue integration, 719–733
formulas for residues, 721–722
of real integrals, 725–733
Fourier integrals, 729–730
improper integrals, 730–732
of rational functions of cos u
sin u, 725–729
several singularities inside
contour, 723–725
Residue theorem, 723–724
Resistance, apparent, 95
Resonance:
practical, 90
undamped forced oscillations,
88–89
Resonance factor, 88
Response to input, see Output
(response to input)
Resultant, of forces, 357
Riccati equation, 35
Riemann, Bernhard, 625n.4
Riemannian geometry, 625n.4
Riemann sphere, 718
Riemann surfaces (conformal
mapping), 754–757
Right-hand derivatives (Fourier
series), 480
Right-handed Cartesian coordinate
system, 368–369, A83–A84
Right-handed triple, 369
Right-hand limit (Fourier series), 480
Right-sided tests, 1079, 1082
Risks of making false decisions, 1080
RKF method, see
Runge–Kutta–Fehlberg method
RK methods, see Runge–Kutta
methods
RKN methods, see
Runge–Kutta–Nyström methods
Robin problem:
Laplace’s equation, 593
two-dimensional heat equation, 564
Rodrigues, Olinde, 179n.2
Rodrigues’s formula, 179, 241
Romberg integration, 840, 843

Roots:
complex:
higher-order homogeneous
linear ODEs, 113–115
second-order homogeneous
linear ODEs, 57–59
complex conjugate, 72–73
differing by an integer, 183
Frobenius method, 183
distinct (Frobenius method), 182
double (Frobenius method), 183
of equations, 798
multiple complex, 115
nth, 616
nth roots of unity, 617
simple complex, 113–114
Root test (power series), 678–679
Rotation (vorticity of flow), 774
Rounding, 792
Rounding unit, 793
Roundoff (numeric analysis), 792–793
Roundoff errors, 792, 794, 902
Roundoff rule, 793
Rows:
determinants, 294
matrix, 125, 257, 320
Row echelon form, 279–280
Row-equivalent matrices, 283–284
Row-equivalent systems, 277
Row operations (linear systems), 276,
277
Row scaling (Gauss elimination), 850
Row “sum” norm, 861
Row vectors, 126, 257, 320
RQL (rejectable quality level), 1094
Runge, Carl, 820n.3
Runge, Karl, 905n.1
RUNGE–KUTTA, ALGORITHM, 905
Runge–Kutta–Fehlberg (RKF)
method, 947
error of, 908
first-order ODEs, 906–908
Runge–Kutta (RK) methods, 915, 947
error of, 908
first-order ODEs, 904–906
higher order ODEs, 917–919
Runge–Kutta–Nyström (RKN)
methods, 919–921, 947
Rutherford, E., 1044, 1100
Rutherford–Geiger experiments,
1044, 1100
Rutishauser, Heinz, 892n.12
Saddle point, 143, 165
Samples:
for experiments, 1015
in mathematical statistics,
1063–1064
selection of, 1063–1064

Sample covariance, 1105
Sampled function, 529
Sample distribution function, 1096
Sample mean, 1064, 1113
Sample points, 1015
Sample regression line, 1104
Sample size, 1015, 1064
Sample space, 1015, 1016, 1060
Sample standard deviation, 1065
Sample variance, 1015, 1113
Sampling:
from a population, 1023
random, 1063–1065
with replacement, 1023
binomial distribution, 1042
hypergeometric distribution,
1043–1044
in statistics, 1063
without replacement, 1018, 1023
binomial distribution,
1042–1043
hypergeometric distribution,
1043–1044
Sampling plan, 1092–1093
Scalar(s), 260, 310, 354
Scalar fields, vector fields that are
gradients of, 400–401
Scalar functions:
defined, 376
vector differential calculus, 376
Scalar matrices, 268
Scalar multiplication, 126–127, 310
of matrices and vectors, 259–261
vectors in 2-space and 3-space,
358–359
Scalar triple product, 373–374, 411
Scale (vectors), 886–887
Scanning labeled vertices, 998
Schrödinger, Erwin, 226n.2
Schur, Issai, 882n.7
Schur’s inequality, 882
Schur’s theorem, 882
Schwartz, Laurent, 226n.2
Secant, formula for, A65
Secant method (numeric analysis),
805–806, 842
Second boundary value problem, see
Neumann problem
Second-order determinants, 291–292
Second-order differential operator, 60
Second-order linear ODEs, 46–104
homogeneous, 46–79
basis, 50–52
with constant coefficients,
53–60
differential operators, 60–62
Euler–Cauchy equations,
71–74
existence and uniqueness of
solutions, 74–79

bindex.qxd

11/4/10

6:06 PM

Page I25

Index
Second-order linear ODEs (Cont.)
general solution, 49–51, 77–78
initial value problem, 49–50
modeling free oscillations of
mass–spring system,
62–70
reduction of order, 51–52
superposition principle, 47–48
Wronskian, 75–78
nonhomogeneous, 79–102
defined, 47
general solution, 80–81
method of undetermined
coefficients, 81–85
modeling electric circuits, 93–99
modeling forced oscillations,
85–92
solution by variation of
parameters, 99–102
Second-order method, improved
Euler method as, 904
Second-order nonlinear ODEs, 46
Second-order PDEs, 540–541
Second (second order) partial
derivatives, A71
Second shifting theorem (t-shifting),
219–223
Second transmission line equation,
599
Seidel, Philipp Ludwig von, 858n.4
Self-starting methods, 911
Sense reversal (complex line
integrals), 645
Separable equations, 12–13
Separable ODEs, 44
first-order, 12–20
extended method, 17–18
modeling, 13–17
reduction of nonseparable ODEs
to, 17–18
Separating variables, method of,
12–13
circular membrane, 587
partial differential equations,
545–553, 605
Fourier series, 548–551
satisfying boundary conditions,
546–548
two ODEs from wave
equation, 545–546
vibrating string, 545–546
Separation constant, 546
Sequences (infinite sequences):
bounded, A93–A95
convergent, 507–508, 672
divergent, 672
limit point of, A93
monotone real, A72–A73
power series, 671–673
real, 671

I25
Series, A73–A74
binomial, 696
conditionally convergent, 675
convergent, 171, 673
cosine, 781
derived, 687
divergent, 171, 673
double Fourier:
defined, 582
rectangular membrane,
577–585
Fourier, 473–483, 538
convergence and sum or,
480–481
derivation of Euler formulas,
479–480
double, 577–585
even and odd functions,
486–488
half-range expansions, 488–490
heat equation, 558–563
from period 2p to 2L,
483–486
Fourier–Bessel, 506–507, 589
Fourier cosine, 484, 486, 538
Fourier–Legendre, 505–506,
596–598
Fourier sine, 477, 486, 538
one-dimensional heat equation,
561
vibrating string, 548
geometric, 168, 675
Taylor series, 694
uniformly convergent, 698
hypergeometric, 186
infinite, 673–674
Laurent, 708–719, 734
analytic or singular at infinity,
718–719
point at infinity, 718
Riemann sphere, 718
singularities, 715–717
zeros of analytic functions, 717
Maclaurin, 690, 694–696
orthogonal, 504–510
completeness, 508–509
mean square convergence,
507–508
power, 168, 671–707
convergence behavior of,
680–682
convergence tests, 674–676,
A93–A94
functions given by, 685–690
Maclaurin series, 690
in powers of x, 168
radius of convergence,
682–684
ratio test, 676–678
root test, 678–679

Series (Cont.)
sequences, 671–673
series, 673–674
Taylor series, 690–697
uniform convergence, 698–705
real, A73–A74
Taylor, 690–697, 707
trigonometric, 476, 484
value (sum) of, 171, 673
Series solutions of ODEs, 167–202
Bessel functions, 187–188
of the first kind, 189–194
of the second kind, 196–200
Bessel’s equation, 187–196
Bessel functions, 187–188,
196–200
general solution, 194–200
Frobenius method, 180–187
indicial equation, 181–183
typical applications, 183–185
Legendre polynomials, 177–179
Legendre’s equation, 175– 179
power series method, 167–175
idea and technique of,
168–170
operations on, 173–174
theory of, 170–174
Sets:
complete orthonormal, 508
in the complex plane, 620
cut, 994–996, 1008
linearly dependent, 129, 311
linearly independent, 128–129,
311
Shewhart, W. A., 1088
Shifted function, 219
Shortest path, 976
Shortest path problems
(combinatorial optimization),
975–980, 1008
Bellman’s principle, 980–981
complexity of algorithms,
978–980
Dijkstra’s algorithm, 981–983
Moore’s BFS algorithm, 977–980
Shortest spanning trees:
combinatorial optimization, 1008
Greedy algorithm, 984–988
Prim’s algorithm, 988–991
defined, 984
Short impulses (Laplace transforms),
225–226
Sifting property, 226
Significance (in statistics), 1078
Significance level, 1078, 1080, 1113
Significance tests, 1078
Significant digits, 791–792
Similarity transformation, 340
Similar matrices, 340–341, 878
Simple closed curves, 646

bindex.qxd

11/4/10

I26

6:06 PM

Page I26

Index

Simple closed path, 652
Simple complex roots, 113–114
Simple curves, 383
Simple events, 1015
Simple general properties of the line
integral, 415–416
Simple poles, 714
Simplex method, 958–968
degenerate feasible solution,
962–965
difficulties in starting, 965–968
Simplex table, 960
Simplex tableau, 960
Simple zero, 717
Simply connected domains, 423, 646,
652, 653
SIMPSON, ALGORITHM, 832
Simpson, Thomas, 832n.4
Simpson’s rule, 832, 843
adaptive integration with, 835–836
numeric integration, 831–835
Simultaneous corrections, 862
Sine function:
conformal mapping by, 750–751
formula for, A63–A65
Sine integral, 514, 697, A68–A69, A98
Single precision, floating-point
standard for, 792
Singularities (singular, having a
singularity), 693, 707, 715
analytic functions, 693
essential, 715–716
inside a contour, 723–725
isolated, 715
isolated essential, 715
Laurent series, 715–719
principal part of, 708
removable, 717
Singular matrices, 301
Singular point, 181, 201
analytic functions, 693
regular, 180n.4
Singular solutions:
first-order ODEs, 8, 35
higher-order homogeneous linear
ODEs, 110
second-order homogeneous linear
ODEs, 50, 78
Singular Sturm–Liouville problem,
501, 503
Sink(s):
motion of a fluid, 404, 458, 775,
776
networks, 991
Size:
of matrices, 258
sample, 1015, 1064
Skew-Hermitian form, 351
Skew-Hermitian matrices, 347, 348,
350, 353

Skewness, of a random variables, 1039
Skew-symmetric matrices, 268, 320,
334–336, 353
Slack variables, 956, 969
Slope field (direction field), 9–10
Smooth curves, 414, 644
Smooth surfaces, 442
Sobolev, Sergei L’Vovich, 226n.2
Software:
for data representation in statistics,
1011
numeric analysis, 788–789
variable step size selection in, 902
Solenoid, 405
Solutions. See also specific methods
defined, 4, 798
first-order ODEs:
concept of, 4–6
equilibrium solutions, 33–34
explicit solutions, 21
family of solutions, 5
general solution, 6, 44
implicit solutions, 21
particular solution, 6, 44
singular solution, 8, 35
solution by calculus, 5
trivial solution, 28, 35
graphing in phase plane, 141–142
higher-order homogeneous linear
ODEs, 106
general solution, 106, 110–111
particular solution, 106
singular solution, 110
linear systems, 273, 745
nonhomogeneous linear systems:
general solution, 160
particular solution, 160
PDEs, 541
second-order homogeneous linear
ODEs:
general solution, 49–51, 77–78
linear dependence and
independence of, 75
particular solution, 49–51
singular solution, 50, 78
second-order linear ODEs, 47
second-order nonhomogeneous
linear ODEs:
general solution, 80–81
particular solution, 80
systems of ODEs, 137, 139
Solution curves, 4–6
Solution space, 290
Solution vector, 273, 745
SOR (successive overrelaxation), 863
SOR formula for Gauss–Seidel, 863
Sorting, of sample values, 1011–1012
Source(s):
motion of a fluid, 404, 458, 775
networks, 991

Source intensity, 458
Source line (flow modeling), 776
Span, of vectors, 286
Spanning trees, 984, 988
Sparse graphs, 974
Sparse matrices, 823, 925
Sparse systems, 858
Special functions, 167, 202
formulas for, A63–A69
theory of, 175
Special vector spaces, 285–287
Specific circulation, of flow, 467
Spectral density, 525
Spectral mapping theorem, 878
Spectral radius, 324, 861
Spectral representation, 525
Spectral shift, 896
Spectrum, 877
of matrix, 324
vibrating string, 547
Speed, 386, 391
angular (rotation), 372
of convergence, 804–805
Spherical coordinates, A74–A76
boundary value problem in,
594–596
defined, 594
Laplacian in, 594
Spiral point, 144–145, 165
Spline, 821, 843
Spline interpolation, 820–827
Spring constant, 62
Square error, 496–497, 539
Square matrices, 126, 257, 258,
301–309, 320
s-shifting, 208–209
Stability:
of critical points, 165
of solutions, 33–34, 124, 936
of systems, 84, 124
Stability chart, 149
Stable algorithms, 796, 842
Stable and attractive critical points,
140, 149
Stable critical points, 140, 149
Stable equilibrium solution, 33–34
Stable systems, 84
Stagnation points, 773
Standard basis, 314, 359, 365
Standard deviation, 1014, 1035, 1090
Standard form:
first-order ODEs, 27
higher-order homogeneous linear
ODEs, 105
higher-order linear ODEs, 123
power series method, 172
second-order linear ODEs, 46,
103
Standardized normal distribution,
1046

bindex.qxd

11/4/10

6:06 PM

Page I27

Index
Standardized random variables, 1037
Standard trick (confidence intervals),
1068
Stationary point (unconstrained
optimization), 952
Statistics, 1015, 1063. See also
Mathematical statistics
Statistical inference, 1059, 1063
Steady flow, 405, 458
Steady heat flow, 767
Steady-state case (heat problems),
591
Steady-state current, 98
Steady-state heat flow, 460
Steady-state solution, 31, 84, 89–91
Steady two-dimensional heat
problems, 546–566, 605
Steepest descent, method of, 952–954
Steiner, Jacob, 451n.6
Stem-and-leaf plots, 1012
Stencil (pattern, molecule, star), 925
Step-by-step methods, 901
Step function, 828, 1031
Step size, 901, 902
Stereographic projection, 718
Stiff ODEs, 909–910
Stiff systems, 920–921
Stirling, James, 1027n.2
Stirling formula, 1027, A67
Stochastic matrices, 270
Stochastic variables, 1029. See also
Random variables
Stokes, Sir George Gabriel, 464n.9,
703n.5
Stokes’s Theorem, 463–470
Stream function, 771
Streamline, 771
Strength (flow modeling), 776
Strictly diagonally dominant matrices,
881
Sturm, Jacques Charles François,
499n.4
Sturm–Liouville equation, 499
Sturm–Liouville expansions, 474
Sturm–Liouville Problems, 498–504
eigenvalues, eigenfunctions,
499–500
orthogonal functions, 500–503
Subgraphs, 972
Submarine cable equations, 599
Submatrices, 288
Subsidiary equation, 203, 253
Subspace, of vector space, 286
Subtraction:
of complex numbers, 610
termwise, of power series, 687
Success corrections, 862
Successive overrelaxation (SOR), 863
Sufficient convergence condition,
861

I27
Sum:
of matrices, 320
partial, of series, 477, 478, 495
of a series, 171, 673
of vectors, 357
Sum Rule (method of undetermined
coefficients):
higher-order homogeneous linear
ODEs, 115
second-order nonhomogeneous
linear ODEs, 81, 83–84
Superlinear convergence, 806
Superposition (electrostatic fields),
761–762
Superposition (linearity) principle:
higher-order homogeneous linear
ODEs, 106
higher-order linear ODEs, 123
homogeneous linear systems,
138
PDEs, 541–542
second-order homogeneous linear
ODEs, 47–48, 104
undamped forced oscillations, 87
Surfaces, for surface integrals,
439–443
orientation of, 446–447
representation of surfaces,
439–441
tangent plane and surface normal,
441–442
Surface integrals, 470
defined, 443
surfaces for, 439–443
orientation of, 446–447
representation of surfaces,
439–441
tangent plane and surface
normal, 441–442
vector integral calculus, 443–452
orientation of surfaces,
446–447
without regard to orientation,
448–450
Surface normal, 398–399, 442
Surface normal vector, 398–399
Surjective mapping, 737n.1
Sustainable yield, 36
Symbol O, 979
Symmetric coefficient matrix, 343
Symmetric distributions, 1036
Symmetric matrices, 267–268, 320,
334–336, 353
Systems of ODEs, 124–166
basic theory of, 137–139
constant-coefficient, 140–151
critical points, 142–146,
148–151
graphing solutions in phase
plane, 141–142

Systems of ODEs (Cont.)
conversion of nth-order ODEs to,
134–135
homogeneous, 138
Laplace transforms, 242–247
linear, 138–139. See also Linear
systems
constant-coefficient systems,
140–151
matrices and vectors, 124–130
nonhomogeneous, 160–163
matrices and vectors, 124–130
calculations with, 125–127
definitions and terms,
125–126, 128–129
eigenvalues and eigenvectors,
129–130
systems of ODEs as vector
equations, 127–128
as models of applications:
electrical network, 132–134
mixing problem involving two
tanks, 130–132
nonhomogeneous, 138, 160–163
method of undetermined
coefficients, 161
method of variation of
parameters, 162–163
nonlinear systems:
qualitative methods for,
152–160
transformation to first-order
equation in phase plane,
157–159
in phase plane, 124
critical points, 142–146
graphing solutions in, 141–142
transformation to first-order
equation in, 157–159
qualitative methods for nonlinear
systems, 152–160
linearization, 152–155
Lotka–Volterra population
model, 155–156
Tangent:
to a curve, 384
formula for, A65
Tangent function, conformal mapping
by, 752–753
Tangential accelerations, 391
Tangential acceleration vector, 387
Tangent plane, 398, 441–442
Tangent vector, 384, 411
Target (networks), 991
Taylor, Brook, 690n.2
Taylor series, 690–697, 707
Taylor’s formula, 691
Taylor’s theorem, 691

bindex.qxd

11/4/10

I28

6:06 PM

Page I28

Index

t-distribution, 1071–1073, 1078,
A103
Telegraph equations, 599
Term(s):
of a sequence, 671
of a series, 673
Terminal point (vectors), 355
Termination criterion, 802–803
Termwise addition, 173, 687
Termwise differentiation, 173,
687–688, 703
Termwise integration, 687, 688,
701–703
Termwise multiplication, 173, 687
Termwise subtraction, 687
Tests, statistical, 1077, 1113
Theory of special functions, 175
Thermal diffusivity, 460
Third boundary value problem, see
Robin problem
Third-order determinants, 292–293
Third (third order) partial derivatives,
A71
3-space, vectors in, 309, 354
components of a vector, 356–357
scalar multiplication, 358–359
vector addition, 357–359
Three-sigma limits, 1047
Time (curves in mechanics), 386
TI-Nspire, 789
Todd, John, 855n.3
Tolerance (adaptive integration), 835
Torricelli, Evangelista, 16n.4
Torricelli’s law, 16–17
Torsion, curvature and, 389–390
Total differential, 20, 45
Total energy, of physical system, 525
Total error, 902
Total mass, of a region, 429
Total orthonormal set, 508
Total pivoting, 846
Trace, 345
Trail (shortest path problems), 975
closed trails, 975–976
Euler trail, 980
Trajectories, 134, 165
linear systems, 141–142, 148
nonlinear systems, 152
Transcendental equations, 798
Transducers, 98
Transfer function, 214
Transformation(s), 313
orthogonal, 336
to principal axes, 344
Transient solution, 84, 89
Transient-state solution, 31
Translation (vectors), 355
Transposition(s):
of matrices or vectors, 128, 320
in samples, 1101

Trapezoidal rule, 828, 843
error bounds and estimate for,
829–831
numeric integration, 828–831
Trees (graphs), 984, 988. See also
Shortest spanning trees
Trials (experiments), 1011, 1015
Triangle inequality, 363, 614–615
Triangular form (Gauss elimination),
846
Triangular matrices, 268
Tricomi, Francesco, 556n.2
Tricomi equation, 555, 556
Tridiagonalization (matrix eigenvalue
problems), 888–892
Tridiagonal matrices, 823, 888, 928
Trigonometric analytic functions
(conformal mapping), 750–754
Trigonometric function, 633–635,
642
inverse, 640
Taylor series, 695
Trigonometric polynomials:
approximation by, 495–498
complex, 529
of the same degree N, 495
Trigonometric series, 476, 484
Trigonometric system, 475, 479–480,
538
Trihedron, 390
Triple integrals, 470
defined, 452
mean value theorem for, 456–457
vector integral calculus, 452–458
Triply connected domains, 653, 658,
659
Trivial solution, 28, 35
homogeneous linear systems,
290
linear systems, 273
Sturm–Liouville problem, 499
Truncating, 794
t-shifting, 219–223
Tuning (vibrating string), 548
Twisted curves, 383
2-space (plane), vectors in, 354
components of a vector, 356–357
scalar multiplication, 358–359
vector addition, 357–359
2 ϫ 2 matrix, 125
Two-dimensional heat equation,
564–566
Two-dimensional normal distribution,
1110
Two-dimensional probability
distributions:
continuous, 1053
discrete, 1052–1053
Two-dimensional problems (potential
theory), 759, 771

Two-dimensional random variables,
1051, 1062
Two-dimensional wave equation,
575–584, 586
Two-sided alternative (hypothesis
testing), 1079–1080
Two-sided tests, 1079, 1082–1083
Type I errors, 1080, 1081
Type II errors, 1080–1081
UCL (upper control limit), 1088
Unacceptable lots, 1094
Unconstrained optimization, 969
basic concepts, 951–952
method of steepest descent,
952–954
Uncorrelated related variables, 1109
Underdamping, 65, 67
Underdetermined linear systems, 277
Underflow (floating-point numbers),
792
Undetermined coefficients, method of:
higher-order homogeneous linear
ODEs, 115
higher-order linear ODEs, 123
nonhomogeneous linear systems
of ODEs, 161
second-order linear ODEs:
homogeneous, 104
nonhomogeneous, 81–85
Uniform convergence:
and absolute convergence, 704
power series, 698–705
properties of uniform
convergence, 700–701
termwise integration, 701–703
test for, 703–704
Uniform distributions, 1035–1036,
1053
Unifying power of mathematics, 97
Union, of events, 1016–1017
Uniqueness:
of Laplace transforms, 210
of Laurent series, 712
of power series representation,
685–686
problem of, 39
Uniqueness theorems:
cubic splines, 822
Dirichlet problem, 462, 784
first-order ODEs, 39–42
higher-order homogeneous linear
ODEs, 108
Laplace’s equation, 462
linear systems, 138
proof of, A77–A79
second-order homogeneous linear
ODEs, 74
systems of ODEs, 137

bindex.qxd

11/4/10

6:06 PM

Page I29

Index
Unitary matrices, 347–350, 353
Unitary systems, 349
Unitary transformation, 349
Unit binormal vector, 389
Unit circle, 617, 619
Unit impulse function, 226. See also
Dirac delta function
Unit matrices, 128, 268
Unit normal vectors, 366, 441
Unit principal normal vector, 389
Unit step function (Heaviside
function), 217–219
Unit tangent vector, 384
Unit vectors, 312, 355
Universal gravitational constant, 63
Unknowns, 257
Unrepeated factors, 220–221
Unstable algorithms, 796
Unstable critical points, 140, 149
Unstable equilibrium solution,
33–34
Unstable systems, 84
Upper bound, for flows, 995
Upper confidence limits, 1068
Upper control limit (UCL), 1088
Upper triangular matrices, 268
Value (sum) of series, 171, 673
Vandermonde, Alexandre Théophile,
113n.1
Vandermonde determinant, 113
Van der Pol, Balthasar, 158n.4
Van der Pol equation, 158–160
Variables:
artificial, 965–968
basic, 960
complex, 620–621
control, 951
controlled, 1103
dependent, 393, 1055, 1056
independent, 393, 1103
intermediate, 393
linearly, 1109
nonbasic, 960
random, 1011, 1029–1030, 1061
continuous, 1029, 1032–1034,
1055
defined, 1030
dependent, 1055
discrete, 1029–1032, 1054
function of, 1056
independence of, 1055–1056
marginal distribution of, 1054,
1055
normal, 1045
occurrence of, 1063
probability distributions of,
1051–1060
skewness of, 1039

I29
Variables: (Cont.)
standardized, 1037
two-dimensional, 1051, 1062
slack, 956, 969
stochastic, 1029
uncorrelated related, 1109
Variable coefficients:
Frobenius method, 180–187
indicial equation, 181–183
typical applications,
183–185
Laplace transforms ODEs with,
240–241
power series method, 167–175
idea and technique of,
168–170
operations on, 173–174
theory of, 170–174
second-order homogeneous linear
ODEs, 73
Variance(s), 1014, 1061
comparison of, 1086
control chart for, 1089–1090
equality of, 1084n.3
of normal distributions,
confidence intervals for,
1073–1076
of probability distributions,
1035–1039
addition of, 1058–1059
transformation of, 1036–1037
sample, 1015
Variation, random, 1063
Variation of parameters, method of:
higher-order linear ODEs, 123
high-order nonhomogeneous linear
ODEs, 118–120
nonhomogeneous linear systems
of ODEs, 162–163
second-order linear ODEs:
homogeneous, 104
nonhomogeneous, 99–102
Vectors, 256, 259
addition and scalar multiplication
of, 259–261
calculations with, 126–127
definitions and terms, 126,
128–129, 257, 259, 309
eigenvalues, 129–130
eigenvectors, 129–130
linear independence and
dependence of, 282–283
multiplying matrices by,
263–265
in the plane, 309, 355
systems of ODEs as vector
equations, 127–128
in 3-space, 309
transposition of, 266–267
Vector addition, 309, 357–359

Vector calculus, 354, 378–380
differential, see Vector differential
calculus
integral, see Vector integral
calculus
Vector differential calculus, 354–412
curves, 381–392
arc length of, 385–386
length of, 385
in mechanics, 386–389
tangents to, 384–385
and torsion, 389–390
gradient of a scalar field, 395–402
directional derivatives,
396–397
maximum increase, 398
as surface normal vector,
398–399
vector fields that are,
400–401
inner product (dot product),
361–367
applications, 364–366
orthogonality, 361–363
scalar functions, 376
and vector calculus, 378–380
vector fields, 377–378
curl of, 406–409
divergence of, 402–406
that are gradients of scalar
fields, 400–401
vector functions, 375–376
partial derivatives of, 380
of several variables, 392–395
vector product (cross product),
368–375
applications, 371–372
scalar triple product, 373–374
vectors in 2-space and 3-space:
components of a vector,
356–357
scalar multiplication, 358–359
vector addition, 357–359
Vector fields:
defined, 376
vector differential calculus,
377–378
curl of, 406–409, 412
divergence of, 402–406
that are gradients of scalar
fields, 400–401
Vector functions:
continuous, 378–379
defined, 375–376
differentiable, 379
divergence theorem of Gauss,
453–457
of several variables, 392–395
chain rules, 392–394
mean value theorem, 395

bindex.qxd

11/4/10

I30

6:06 PM

Page I30

Index

Vector functions: (Cont.)
vector differential calculus,
375–376, 411
partial derivatives of, 380
of several variables, 392–395
Vectors in 2-space and 3-space:
components of a vector,
356–357
scalar multiplication, 358–359
vector addition, 357–359
Vector integral calculus, 413–471
divergence theorem of Gauss,
453–463
double integrals, 426–432
applications of, 428–429
change of variables in,
429–431
evaluation of, by two
successive integrations,
427–428
Green’s theorem in the plane,
433–438
line integrals, 413–419
definition and evaluation of,
414–416
path dependence of, 418–426
work done by a force, 416–417
path dependence of line integrals,
418–426
defined, 418
and integration around closed
curves, 421–425
Stokes’s Theorem, 463–469
surface integrals, 443–452
orientation of surfaces,
446–447
without regard to orientation,
448–450
surfaces for surface integrals,
439–443
representation of surfaces,
439–441

Vector integral calculus (Cont.)
tangent plane and surface
normal, 441–442
triple integrals, 452–458
Vector moment, 371
Vector norms, 866
Vector product (cross product):
in Cartesian coordinates,
A83–A84
vector differential calculus,
368–375, 410
applications, 371–372
scalar triple product, 373–374
Vector spaces, 482
complex, 309–310, 349
inner product spaces, 311–313
linear transformations, 313–317
real, 309–311
special, 285–287
Velocity, 391, 411, 771
Velocity potential, 771
Velocity vector, 386, 771
Venn, John, 1017n.1
Venn diagrams, 1017
Verhulst, Pierre-François, 32n.8
Verhulst equation, 32–33
Vertices (graphs), 971, 977, 1007
adjacent, 971, 977
central, 991
coloring, 1005–1006
double labeling of, 986
eccentricity of, 991
exposed, 1001, 1003
four-color theorem, 1006
scanning, 998
Vertex condition, 991
Vertex incidence list (graphs), 973
Volta, Alessandro, 93n.7
Voltage drop, 29
Volterra, Vito, 155n.3, 198n.7, 236n.3
Volterra integral equations, of the
second kind, 236–237

Volume, of a region, 428
Vortex (fluid flow), 777
Vorticity, 774
Walk (shortest path problems), 975
Wave equation, 544–545, 942
d’Alembert’s solution, 553–556
numeric analysis, 942–944, 948
one-dimensional, 544–545
solution by separating variables,
545–553
two-dimensional, 575–584
Weber’s equation, 510
Weber’s functions, 198n.7
Weierstrass, Karl, 625n.4, 703n.5
Weierstrass approximation theorem,
809
Weierstrass M-test for uniform
convergence, 703–704
Weighted graphs, 976
Weight function, 500
Well-conditioned problems, 864
Well-conditioning (linear systems),
865
Wessel, Caspar, 611n.2
Work done by a force, 416–417
Work integral, 415
Wronski, Josef Maria Höne, 76n.5
Wronskian (Wronski determinant):
second-order homogeneous linear
ODEs, 75–78
systems of ODEs, 139
Zeros, of analytic functions, 717
Zero matrix, 260
Zero surfaces, 598
Zero vector, 129, 260, 357
z-score, 1014

bcredit.qxd

11/4/10

10:15 AM

Page 1

PHOTO

CREDITS

Part A Opener: © Denis Jr. Tangney/iStockphoto
Part B Opener: © Jill Fromer/iStockphoto
Part C Opener: © Science Photo Library/Photo Researchers, Inc
Part D Opener: © Rafa Irusta/iStockphoto
Part E Opener: © Alberto Pomares/iStockphoto
Chapter 19, Figure 437: © Eddie Gerald/Alamy
Part F Opener: © Rainer Plendl/iStockphoto
Part G Opener: © Sean Locke/iStockphoto
Appendix 1 Opener: © Ricardo De Mattos/iStockphoto
Appendix 2 Opener: © joel-t/iStockphoto
Appendix 3 Opener: © Luke Daniek/iStockphoto
Appendix 4 Opener: © Andrey Prokhorov/iStockphoto
Appendix 5 Opener: © Pedro Castellano/iStockphoto

P1

bcredit.qxd

11/4/10

10:15 AM

Page 2

bendpaper.qxd

11/4/10

12:24 PM

Page 2

Some Constants

Polar Coordinates

e ϭ 2.71828 18284 59045 23536
͙eෆ ϭ 1.64872 12707 00128 14685
e 2 ϭ 7.38905 60989 30650 22723

x ϭ r cos ␪

y ϭ r sin ␪

r ϭ ͙ෆ
x2 ϩ y2ෆ

y
tan ␪ ϭ ᎏ
x

dx dy ϭ r dr d␪

␲ ϭ 3.14159 26535 89793 23846
␲ 2 ϭ 9.86960 44010 89358 61883
͙␲
ෆ ϭ 1.77245 38509 05516 02730
log10 ␲
ln ␲
log10 e
ln 10

ϭ
ϭ
ϭ
ϭ

0.49714 98726 94133 85435
1.14472 98858 49400 17414
0.43429 44819 03251 82765
2.30258 50929 94045 68402

sin x ϭ

`

cos x ϭ

␯

Nu

␤

Beta

␰

Xi

␥, ⌫

Gamma

␱

Omicron

␦, ⌬

Delta

␲

Pi

⑀, ␧

Epsilon

␳

Rho

␶

␪ , ␽ , ⍜ Theta
␫

Iota

␬

Kappa

␭, ⌳
␮

␷, ⌼

mϭ0

(Ϫ1)mx 2m
ᎏᎏ
(2m)!

`

͚

mϭ1
`

arctan x ϭ

͚

mϭ0

xm
ᎏ
m

(͉x͉ Ͻ 1)

(Ϫ1)mx 2mϩ1
ᎏᎏ
2m ϩ 1

(͉x͉ Ͻ 1)

Vectors

Alpha

Eta

͚

ln (1 Ϫ x) ϭ Ϫ

xm
ᎏ
m!

(Ϫ1)mx 2mϩ1
ᎏᎏ
(2m ϩ 1)!

͚

mϭ0

␣

␩

`

͚

mϭ0

Greek Alphabet

␴, ⌺

͚

`

␥ ϭ 0.57721 56649 01532 86061
ln ␥ ϭ Ϫ0.54953 93129 81644 82234
(see Sec. 5.6)
1° ϭ 0.01745 32925 19943 29577 rad
1 rad ϭ 57.29577 95130 82320 87680°
ϭ 57°17Ј44.806Љ

Zeta

`

1
ᎏϭ
x m (͉x͉ Ͻ 1)
1 Ϫ x mϭ0
ex ϭ

͙2ෆ ϭ 1.41421 35623 73095 04880
3
͙ෆ2 ϭ 1.25992 10498 94873 16477
͙3ෆ ϭ 1.73205 08075 68877 29353
3
͙ෆ3 ϭ 1.44224 95703 07408 38232
ln 2 ϭ 0.69314 71805 59945 30942
ln 3 ϭ 1.09861 22886 68109 69140

␨

Series

Sigma
Tau
Upsilon

␾ , ␸ , ⌽ Phi
␹

Chi

Lambda

␺, ⌿

Psi

Mu

␻, ⍀

Omega

a • b ϭ a1b1 ϩ a2 b2 ϩ a3 b3
j

k

a ؋ b ϭ l a1

a2

a3l

b1

b2

b3

i

Ѩƒ
Ѩƒ
Ѩƒ
grad ƒ ϭ ٌƒ ϭ ᎏ i ϩ ᎏ j ϩ ᎏ k
Ѩy
Ѩz
Ѩx
Ѩv1
Ѩv3
Ѩv2
div v ϭ ٌ • v ϭ ᎏ ϩ ᎏ ϩ ᎏ
Ѩy
Ѩx
Ѩz
i
Ѩ
curl v ϭ ٌ ؋ v ϭ lᎏᎏ
Ѩx
v1

j
Ѩ
ᎏᎏ
Ѩy
v2

k
Ѩ
ᎏᎏl
Ѩz
v3


